{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a34a57e",
   "metadata": {},
   "source": [
    "# Langsmith\n",
    "**The companion (tool) I've been waiting for**\n",
    "![viusalize your runs with langsmith](https://docs.smith.langchain.com/assets/images/run_details-f579aef109cb8f576e7a4a864e405143.png)\n",
    "\n",
    "debugging is a pain when building complicated LLM application. There is a lot of text to parse from each step, each input and output is different and if there is an a step mid-way, it is hard to change just that value and run it again. Basically when developing I want a tool that\n",
    "- shows me exactly what the input/outputs are from the LLM, tools etc (visualisation)\n",
    "- allows me to edit prompts at each step and see how it affects the output (live debugging)\n",
    "- show the exact sequence of events, the time taken at each step, tokens used\n",
    "- share the my debug traces with other (honestly I didn't know I needed this until I saw langsmith has support for it ðŸ˜‚)\n",
    "\n",
    "and boy does langsmith solve these problems. In-face I've changed my workflow to always have langsmith open in the side, logging everything, ready to be help me visualize what is happening with my apps.\n",
    "\n",
    "Langsmith also has a few more features and you can see the whole list [here](https://docs.smith.langchain.com/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e153513",
   "metadata": {},
   "source": [
    "## Tracing\n",
    "\n",
    "use the langsmith UI to get a trace of the LLM calls in your application. This is a much better way to probe into in internals of you LLM app and understand what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "296878ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"]=\"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=\"<your-api-key>\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=\"<your-project>\"  # if not specified, defaults to \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8d70977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()\n",
    "llm.predict(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab40ef42",
   "metadata": {},
   "source": [
    "and this is the output of the run in langsmith's dashboard\n",
    "\n",
    "![](imgs/smith-getstarted.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58594b2",
   "metadata": {},
   "source": [
    "## Tags\n",
    "\n",
    "you can also tag LLM for later to provide additional information which you can use as a filter also in the dashboard. Tags are mostly used for organizing entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "955be674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Hello, World!', 'text': 'Hello, World!'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, tags=[\"my-llm-tag\"])\n",
    "prompt = PromptTemplate.from_template(\"Say {input}\")\n",
    "chain = LLMChain(llm=llm, prompt=prompt, tags=[\"my-bash-tag\", \"another-tag\"])\n",
    "\n",
    "chain(\"Hello, World!\", tags=[\"shared-tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f77b236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-3.5-turbo'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012de53",
   "metadata": {},
   "source": [
    "![](./imgs/smith-tags.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc3620f",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "\n",
    "They allow you to pass extra information about the requests. One usecase could be when performing A/B testing. You can assign a `variant` key to give a unique name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c994409c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the meaning of life?',\n",
       " 'text': 'The meaning of life is subjective and can vary depending on individual beliefs, perspectives, and values. Some people find meaning in religious or spiritual beliefs, others in personal relationships, achievements, or pursuing their passions. It is ultimately up to each individual to determine their own purpose and meaning in life.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chat_model = ChatOpenAI()\n",
    "chain = LLMChain.from_string(llm=chat_model, template=\"What's the answer to {input}?\")\n",
    "\n",
    "chain(\n",
    "    {\"input\": \"What is the meaning of life?\"}, \n",
    "    metadata={\n",
    "        \"my_key\": \"My Value\",\n",
    "        \"variant\": \"abc123\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc95ce2c",
   "metadata": {},
   "source": [
    "![](./imgs/smith-metadata.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e89f0",
   "metadata": {},
   "source": [
    "## Grouping\n",
    "\n",
    "Certain LLM calls are part of a sequence and makes sense to put those together. You can use the callbacks from langchain here to group together LLM calls so that it makes more sense when viewing them in the dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "abafe2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import (\n",
    "    trace_as_chain_group, \n",
    "    atrace_as_chain_group,\n",
    ")\n",
    "\n",
    "with trace_as_chain_group(\"my_group_name\") as group_manager:\n",
    "    \"\"\"Pass the group_manager as a callback to group all runs\n",
    "    within this context\"\"\"\n",
    "\n",
    "# Or for async code\n",
    "async with atrace_as_chain_group(\"my_group_name\") as async_group_manager:\n",
    "    \"\"\"Async applications are better suited with the async callback manager\"\"\"\n",
    "\n",
    "# Example usage:\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"What is the answer to {question}?\",\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "with trace_as_chain_group(\"my_group\") as group_manager:\n",
    "    chain.run(question=\"What is your name?\", callbacks=group_manager)\n",
    "    chain.run(question=\"What is your quest?\", callbacks=group_manager)\n",
    "    chain.run(question=\"What is your favorite color?\", callbacks=group_manager)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bd2a31",
   "metadata": {},
   "source": [
    "![](./imgs/smith-group.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed048ca",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Probably the most exciting piece for me. Evaluation with langsmith is an extension of what is available via langchain. It offers\n",
    "\n",
    "- manage your test datasets: langsmith has a dataset store where you can upload your datasets. You can also add datasets to it from the traces UI. That means you can continiously add examples from the traces. So while developing or in production, if you see any test cases that need to be covered, you can directly add them.\n",
    "- run your evaluations: evaluations supported by langchain can be run from langsmith. You can also define your own evaluation methods.\n",
    "- analyse the results: the langsmith UI also makes it easy to review the result, see the scores and explore how each exampled performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6cef4c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "example_inputs = [\n",
    "  \"a rap battle between Atticus Finch and Cicero\",\n",
    "  \"a rap battle between Barbie and Oppenheimer\",\n",
    "  \"a Pythonic rap battle between two swallows: one European and one African\",\n",
    "  \"a rap battle between Aubrey Plaza and Stephen Colbert\",\n",
    "]\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"Rap Battle Dataset\"\n",
    "\n",
    "# Storing inputs in a dataset lets us\n",
    "# run chains and LLMs over a shared set of examples.\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name, description=\"Rap battle prompts.\",\n",
    ")\n",
    "for input_prompt in example_inputs:\n",
    "    # Each example must be unique and have inputs defined.\n",
    "    # Outputs are optional\n",
    "    client.create_example(\n",
    "        inputs={\"question\": input_prompt},\n",
    "        outputs=None,\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4259d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "be29ea74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project '2023-07-29-10-57-47-ChatOpenAI' at:\n",
      "https://smith.langchain.com/projects/p/eac0779d-9135-4a42-9838-71574d23a60c?eval=true\n",
      "4 processed\r"
     ]
    }
   ],
   "source": [
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "  evaluators=[\n",
    "    # You can specify an evaluator by name/enum.\n",
    "    # In this case, the default criterion is \"helpfulness\"\n",
    "    \"criteria\",\n",
    "    # Or you can configure the evaluator\n",
    "    RunEvalConfig.Criteria(\"harmfulness\"),\n",
    "    RunEvalConfig.Criteria(\"misogyny\"),\n",
    "    RunEvalConfig.Criteria(\n",
    "      {\"cliche\": \"Are the lyrics cliche? \"\n",
    "      \"Respond Y if they are, N if they're entirely unique.\"}\n",
    "      )\n",
    "  ]\n",
    ")\n",
    "r = run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=llm,\n",
    "    evaluation=eval_config,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3c94f8",
   "metadata": {},
   "source": [
    "![](./imgs/evaluation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04bab5d",
   "metadata": {},
   "source": [
    "## Feedback\n",
    "\n",
    "There are 2 types of feedback in langsmith, Human and Automated. Human feedback is from humans while Automated is from evaulations like above. Both are important for measuring performance.\n",
    "\n",
    "This snippet shows how you can add human feedback into langchain LLM call and view it in langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15f4478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "chain = LLMChain.from_string(ChatOpenAI(), \"Say hi to {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8b4eb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feedback(id=UUID('ecd72d0c-7f78-4fe0-adc3-87018946c3b0'), created_at=datetime.datetime(2023, 7, 25, 12, 53, 34, 508597), modified_at=datetime.datetime(2023, 7, 25, 12, 53, 34, 508603), run_id=UUID('33ce107f-d86f-4b3c-8121-bc14d782d5c0'), key='thumbs_up', score=1.0, value=None, comment=None, correction=None, feedback_source=FeedbackSourceBase(type='api', metadata=None))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = chain(\"Clara\", include_run_info=True)\n",
    "run_id = response[\"__run\"].run_id\n",
    "# ... User copies the generated response\n",
    "client.create_feedback(run_id, \"did_copy\", score=True)\n",
    "# ... User clicks a thumbs up button\n",
    "client.create_feedback(run_id, \"thumbs_up\", score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca071229",
   "metadata": {},
   "source": [
    "![](./imgs/smith-feedback-human.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
