{"docstore/metadata": {"aef2a166-0712-45c2-bb36-09e7045e08c1": {"doc_hash": "253a7d2252555bde75fb277d02461c6dcd1bd705aaec2291e37bb3295948e354"}, "64411d7e-6a32-4c01-b88e-2aed643b2657": {"doc_hash": "21f133375462239d82b1ae5dee3b5d22e988a7dc1b5edd1dd93a08e679bdd595"}, "4fc45785-6dbf-4612-850e-f05194cafe45": {"doc_hash": "96661caa37f563e93e709f83043a30ce6d9dfa788dc0f794b2ac6b401f3f7fd7"}, "de718598-3ff9-4424-9331-9cf36d6af3e0": {"doc_hash": "ae7e8d7a8782163093b5660792c806cb36f67bae6aff6e1d2ee2c9aa477b7819"}, "035e0516-a0e1-4bd8-a828-e7f4bcb9b02f": {"doc_hash": "bbf996c9910dd61858286f06c1c81300f14af24b30137950ee79a3ef9a8c8cb2"}, "d3ea2d0b-aa99-4af7-b94d-e1c6f526133a": {"doc_hash": "506096ee011514fe304af20ae9ff0f660ae786d94b6b67b9df4925f1d2f6fd27"}, "d8662ecb-8c77-4f22-96e8-b223e930ee0c": {"doc_hash": "275fb5af1b5a40b7773e7fc6f481512d14ded751c47c9e14f86cdccde46a340b"}, "fca6b329-8ebf-4504-9598-7a25e9b677bc": {"doc_hash": "d2c03a753066ce5290fef4bf3888490c752ef7c2627fa877269f880ebb547cd7"}, "5e7bc5b4-0b67-4c5a-a893-268c550a3abb": {"doc_hash": "0b4a04a119bee58a19a623664c84678fccde968f35129a08f92b88202da694a5"}, "5e98a887-73f9-4e55-a4ca-96e985805783": {"doc_hash": "1d3562eeb507abc2065617edbbbe501a76bd14ed486b0e2b088a8d5377bdc54a"}, "31165a1b-a78a-4f9e-bc74-e87582a28c61": {"doc_hash": "ea7be0664a49b7b1602758f1bc5cee2610d2e8f105241f858013174894661e5c"}, "cdb55b56-a520-4e68-a7f3-a410dabf27d4": {"doc_hash": "6744922e00dba0eaf3f7b724fd20b59f2b5839a1279a74fbcffe50ba7102b62c"}, "2797e83c-4e17-4161-b0e8-4670879fd46d": {"doc_hash": "a9a90dd247dbf0ec9575d99e9facbe36965f3ab97cd30c1a98b52bde1e986ebb"}, "32caede3-2d9c-4c3f-a2b8-ef9608a8efda": {"doc_hash": "0152c2b3390b663d9c9eaaa253fff1cb38f31df021da3eebe895c192b9e3a7a6"}, "92db31fd-64b7-4603-9417-8743b5e0ce44": {"doc_hash": "1d2d58f6b49d40199104a17b389efc3c29692755ce3e8abc854b8677c7b04cae"}, "30b8bf60-b0a3-4d25-ba1d-0665188f6124": {"doc_hash": "8e4f5877b18cde8f429cc21f6554566e76279d6b00dcd575ef5e78c62993a6f6"}, "6d80b32d-19ee-46cc-9d84-38d1d98ff2e7": {"doc_hash": "b34a74462d9524571681422e5dbcc0610ae141a9b5ac27149e14a56445cf7930"}, "a322bbac-55d8-4024-aafb-3a18236b59e2": {"doc_hash": "51d528a36ddcb23ecc562a39b1dab8a7de660e27a9c4f6e78fd4688649232298"}, "76e6460c-f850-4db3-84a3-c90d0d4d108c": {"doc_hash": "96ed761d0ffd63ff7a08c63941ff15ff5690688075c2f725d75e88fc364e85c1"}, "4bd4c3a1-61f8-4f02-86a6-017c0f023645": {"doc_hash": "29a3c7729207736e7dd6f5750fe217673401f77d3f48bc001e988e53582fd08c"}, "da6b4d72-fd10-4b13-89f5-8c3604e4fefa": {"doc_hash": "af1a275b993bd2f396c73303bfb9b0605183b250c6373d8130eb760099e37de3"}, "d94be76f-1a2c-46e7-979b-389c19fd3b82": {"doc_hash": "19e8dd881f1e52da70848874b44a290e4b58fb4981fe672b538bf5d5e5753bfb"}, "a73e442e-f027-42b5-b16a-3cf06321c6df": {"doc_hash": "bdaf3f06e9bfe17eb1282df628e40450c76fd0bcee8abcfe0822fbd117cec25b"}, "cb90669e-9444-4916-b9a7-101e2ea7d96b": {"doc_hash": "d188fb5b764273f3483ee6a9967a01ed3cd1bc64809a44e38fda68a62ce4a4d7"}, "6dcddd5f-dfe2-47f8-8106-bc3416d45f0f": {"doc_hash": "e26bc83b70f8446d4c284430df89420030e629e58a481281ad552053e58499bb"}, "c402d675-8b25-4e53-9b00-3ddde2cc03c4": {"doc_hash": "3c617bf1b908f302848c0b0efd52ef96870ff6c3ae1b63c325832b47cc5bcf37"}, "d2814921-d22c-45db-8c20-209e09adc6c2": {"doc_hash": "30ba51b8495aad853bfff9664b2e56530470631bfad84cbcd604c2cf82286c0a"}, "c3ead361-c540-4751-afbe-ccfe6f550959": {"doc_hash": "7fb2201138492ea3abf3fc1186ce77984b6b75b42875fd85ca58c857f3c666ef"}, "6c50c7bb-d1a3-470a-92f9-df245329242c": {"doc_hash": "a6e2e8d912d924891dedcd7b6743e1d96d6b58fe26abd90ea2886a69e5ee4adc"}, "c322d402-f3b8-4588-9192-a7a9d0f09457": {"doc_hash": "23530eb8d4dab612355d2eda6f1e5e5ab123a8e5d02904e5c5d8a478ce58f5ba"}, "cd2b9134-d1eb-4afb-b4a2-13c7dfd106e1": {"doc_hash": "6dda72cf52aad94a19a9d1e78eed710336f03e92a1f98179ea518d76e48594df"}, "289a0fc7-c14f-4200-906d-9991f20c2cab": {"doc_hash": "ddf3110e3f4a3b37776ab3e4928ab989d4909a25add9b06c13459b48de554e28"}, "3b815434-5509-48f3-a18a-47d3cf16db98": {"doc_hash": "2b2bcde186dc8d0b5b829f500d6fec0683717b9aec150d1b23d1201f8d530511"}, "336e4992-5ac6-4372-9099-e6c746f70250": {"doc_hash": "678bad8dfe8c377fba0b87b092a07dc6cc971d9b5432ed65546b5d1e911da2d3"}, "d5bb378b-222b-4cef-a97b-09e5b54acfa7": {"doc_hash": "978b485835d5b691e8b5c44eff326f37ebcd41ee1c27a3251d777c71aa7116c3"}, "0bf2ba39-83a9-45d4-9814-13d20291b2b0": {"doc_hash": "3aa5945cfb06eb99cdeb14e0b13c694a1cc36376d1adb1fbce584ba871667164"}, "dfe84564-5c00-4edc-998d-fdb7b356cfe0": {"doc_hash": "eb5b4d0fffcd97205361632a35f367cc9945528ebdf14bb7c06a3ed3fbf053bc"}, "6b639655-4cf4-4fc4-b263-c719dfbefd41": {"doc_hash": "54c72eba04c7d8ac49f77cc37ec76b168b98c80d26d387d30456712687ca13cd"}, "0f40fa86-ec61-4f3a-876b-7076516f73f1": {"doc_hash": "67ada805fa6faf29137c99510c9b4875b63e620ad1a18ef003f63e83e44df8a0"}, "019bbff9-f260-4c69-a1ff-ac34882dab33": {"doc_hash": "001f45543cab8b0419622f0faf22be514590ca090124a0f86a63c7d0745d8386"}, "718b525c-8452-4fb2-a017-735f3fc2725d": {"doc_hash": "a06d9e348a719f45160a8240f9212fcee76c28ca016345c243c07f1c36165494"}, "1002b06f-4a58-47f5-bada-303fcfdb722b": {"doc_hash": "b58610248d2bfea5aecb203068790937f9e724f8929ea642273a530ddf4a5bb8"}, "d578ed6c-fe42-4513-bc2c-a4d2b8cc92c8": {"doc_hash": "27ac438b164699c8e3cd925bedbd5415216e41b2e8efc1e3b80ba21731b28ec4"}, "d0c966e3-49f6-471d-ab0b-8c874f7e822b": {"doc_hash": "9440bda2bc9fd006612c48381d0a859b1c1e920b8fca162bf4e84d59f879d647"}, "ef2ffb70-a742-4e82-8f25-5953600ab075": {"doc_hash": "56b81422a0e14035fcbf1047055a1cd9ede0285a2fe023772e80aae66de07d23"}, "1d1e02e8-6587-4553-acb5-c0f5437307a3": {"doc_hash": "a3ee109f9ddf8cf85db2a56d348975385a06487b860d995f7769f940ca517372"}, "066aa014-8322-42ee-ab37-5b2d37726129": {"doc_hash": "35530197eb27b85cc89a701cebbfcd516bd066f0ff4284cf37d7fd9589fc8286"}, "2735b0a1-d30e-45a0-99be-1e60cd9830f5": {"doc_hash": "bf30e38748c7d811ab7a679755f17de9c18cbc7aa18a56b2332b15ed35e31254"}, "1f45cb23-bfea-4041-bdba-f6e9a790db2a": {"doc_hash": "7f94cac26a1948279422622a7b09d200262219d8c73dddf4cd4764299b5c3ff8"}, "cbe35ac3-764d-4dc3-a82d-79683d795e2a": {"doc_hash": "fcb58eec8053f924ed3b9275fb5aec3dcfee7b1e124c62a226fe6390601c852e"}, "e0fe866c-0595-42c6-932a-a1c0cca353ec": {"doc_hash": "1611d467edc1e2d42f9f806f0ed18150f91850277955a71656c8764deffd6afa"}, "5ddd3602-df76-409c-9975-77e97fb1d52d": {"doc_hash": "400a8045cabc885c9e9e4ab7c98da65bea7c5b3bde6c64d34fe0ee6c22251530"}, "2b2a1ebf-0aa3-4a53-beb3-bf146bd06540": {"doc_hash": "fa5280b9dfbfc703f7704f565d3493017f817e007eb1a19b4351d0962575e51d"}, "0e16a3fb-8fc1-4891-a583-21dd8afd8356": {"doc_hash": "0c8c0ac50b0d478e54c06b2dcf72d4619b4d6e8f65b5c88571acd4064e90953d"}, "87d2e3c6-4452-4b9e-bbfa-65a394905e4e": {"doc_hash": "199251803da515f3a7d840fd84420ad6184390a7136c53d0045f8ddb9e9b0005"}, "05017b20-5666-4cd9-9a86-29ca466e2e33": {"doc_hash": "0437bc2cbc0228c7b0b11be06c8b7a13da2c059d9dd11a0680a2f3632577782d"}, "04167c03-1197-4caa-993f-1db41fed7f7d": {"doc_hash": "4088d62b03013003c53e04268c8cc9f22af3d47c28b1c58eeafa821439f15dbc"}, "558aabdc-ac0a-4131-87fb-e9351c01caca": {"doc_hash": "6515c2a8074f771ccf2ac3a92e22883c54a89c5d05f7c1498d216a648dacfdc9"}, "b0bb577b-65bf-4e62-a71c-0720fe801dcf": {"doc_hash": "62d45f1e5ba414b01574ee15c5ceb8037112d221909bb6fbc2bad96b5b1e895d"}, "0521f782-b619-4268-92e9-cde7d75f6fc2": {"doc_hash": "d99fe071f00aff07bf78d292e9a574a35c22d069f52f6f72b42f9ee0742d974c"}, "a2eaf068-12a3-4a47-b796-c9d13306cab9": {"doc_hash": "234ad0a891840fb38df76207c897d2163b2c96106ab89a8bad52ceb398c8cd93"}, "19004571-ee66-46ad-99d0-eba9866758c3": {"doc_hash": "ae52a553c71c391712f6537f0f4a451d4c72c6bafe70147ca209ce083e945baa"}, "e6b1fc99-8048-4656-bc23-b2a6a2bc326c": {"doc_hash": "02d7886e3937902df8ad5fd498850171c9f6030b7fb128bd86fea6d9407e1b38"}, "4037ae60-e748-4652-b18e-67f34a2960dd": {"doc_hash": "c1cc1d2611e220f8a326f0c3f74985c79d26a63275ec68557920f49b625c1521"}, "4c66e2e6-0588-4327-973d-5776b26e67e6": {"doc_hash": "191046a1e58fbbcd5c5b8821efdd86e31244b819fd240f42495a0ce01e0d368f"}, "2f9d15aa-5c08-4cde-99bb-a233f1b579f0": {"doc_hash": "105d3871d6233132bc681d15c8a04e62bb1f1954102e6beacc6c02ac5e6b6304"}, "724bf392-6e79-4b38-8f3c-0985f0824a0c": {"doc_hash": "185a65d7069d60f6935310c079b7e46ad14b774d1ef7faa6d512e527b9770b90"}, "0f643fb8-b06e-490b-b3e9-909d6292b389": {"doc_hash": "8ff71837be83957d13ccadfb0569d6a14b68f9ac0a14afba5bad0b4c1e602b77"}, "0fb92626-4265-408b-a0c6-ba0be90c2643": {"doc_hash": "5d722b8fa7b717e9e745ff1da6781bb16d1e1d360d2577eb4714273efd20672f"}, "2cff0bdf-dd24-4e4f-b1e2-934ef0398901": {"doc_hash": "71afd0618449464b5487b3dfe3d124ad485b48cb39f512a6a3b33136f66a931a"}, "208aea5f-885c-49b5-beaf-016acff75fa9": {"doc_hash": "67fbd21f2709b33d24fa80c8c6e3fd9fac59f5593b9eebe1a809b1cd492fd7e7"}, "c7a4f182-d990-4c2c-9a2e-97548e03eb8b": {"doc_hash": "2368d516ae13327a81a0ac2c5b40332b3b538d45b6e1a4eaf2f23d7cf6f31825"}, "53b70b3b-1485-4a7c-9de5-4490c71dd120": {"doc_hash": "d1d9360b1367b6bed32a8083c9cd397325b67a0b9413f7112794a590b47df755"}, "606bf2d5-8b74-490e-82c5-4d7995453e0d": {"doc_hash": "4a72894ffe4de17a614f2db53696c647c19623326e684907c8a82097ab073686"}, "6137caa2-d008-463d-bbad-6c37b53d7a39": {"doc_hash": "8c37a75377f42b70e413a05541e376329574a251ccdb02efddeb2fb82cffb9fa"}, "44d116a9-5dc5-4866-962c-377581814389": {"doc_hash": "8d185ee42d301694408a829a8364fda67e409d4b766a88888e5b49223ca193dd"}, "4689cc6b-9f9b-46dd-93c1-559c5c1e7f08": {"doc_hash": "df0036da89582cbe00d67cab95b10176aab00cd9db9f790106db575b4ae335a0"}, "7095e65f-fb04-4bf9-897b-614c9d5524ca": {"doc_hash": "89a4c84b1d37bb61f4e6fa767b8597bb116bfdf7e578a19ca2e5ceeecefbbc92"}, "4946db25-fac9-4260-bd32-94c4b9c5b838": {"doc_hash": "ff0252b6919e759383260c4a34c0a483932b2f03c6dad60b954a19c896291e6c"}, "fc2578e5-f9ca-4125-931f-681cf3b1566a": {"doc_hash": "e2e638b41fdc7f3a1079ac1f03bb1b3d199fbc3bcbf94c7577de43482bb26eb0"}, "e45afb9f-6fed-477f-bcfa-d877f32c9658": {"doc_hash": "421d142c686209b3a39328ece0b86502bcfb72060b0a893160d571a15772ef79"}, "a1229b78-9125-4cc5-b4e7-5dd9d13a6e19": {"doc_hash": "bf69af0d33bd6f6672acc8b028892d3cc6813c669cfa41293b83d73779537537"}, "d7f4886d-fc40-4339-9763-26ef2704bbc6": {"doc_hash": "a14f5e8e43bb9de9818d287519fec06002174ce35edeff8660314aaf95f3ad73"}, "9513c1ed-af72-4ae6-96ef-1ec019cdfb1e": {"doc_hash": "12cef3fbef748e19107b84f6753d53af66d039d9794c164da95df20dc9828a7a"}, "06a4455f-7516-4a87-a65f-5b202f6e1001": {"doc_hash": "5d39b589aacaf4d6e3ad6ca3afdbae31d1f4e585832ffb07d540fbd0545eb274"}, "446ea4bd-d744-4aee-8fe8-8631c6a6007e": {"doc_hash": "7c8339d0efe414104d8aacde8f721718ae4376a425fc49a01f4205553d44454b"}, "ca575dbe-9a0c-427d-9cba-690f1a65a0ee": {"doc_hash": "e53b41c91ee17e23d25bbb07f15fcd2c01b1ce8388105ca386a6bd0c58c921fe"}, "4a1502c8-8b8d-43fe-9459-97798f3c3283": {"doc_hash": "f41ca852fffc7a28f93b9be7782b042d7895ed0ac5c10778907115d129131279"}, "8d1f3851-add1-4e50-91dc-c33dbf3c7a93": {"doc_hash": "2e0eb5899d2dbf5ad4c269c5fba1202709996eab0f80d6854e30a6d045c46ba2"}, "680c7591-3db1-481b-bf89-cffa94d6aaac": {"doc_hash": "253a7d2252555bde75fb277d02461c6dcd1bd705aaec2291e37bb3295948e354", "ref_doc_id": "aef2a166-0712-45c2-bb36-09e7045e08c1"}, "35d70a01-7141-44c2-9619-192647e015f7": {"doc_hash": "21f133375462239d82b1ae5dee3b5d22e988a7dc1b5edd1dd93a08e679bdd595", "ref_doc_id": "64411d7e-6a32-4c01-b88e-2aed643b2657"}, "1234c90e-1b55-4931-a123-c9f0523057d1": {"doc_hash": "9c2122fe226d649a35614fcd4e2cb2bb25d7bc4733cf9bff853099b0e8d86e0d", "ref_doc_id": "4fc45785-6dbf-4612-850e-f05194cafe45"}, "950ac749-b405-4e42-948c-7524f3250b28": {"doc_hash": "fdc99298052ea9f27f2b7973b3f5e380f159f8cb105b5d6c1c50e99c6dd72b0a", "ref_doc_id": "4fc45785-6dbf-4612-850e-f05194cafe45"}, "cf796dc6-21a2-429b-9e20-25d42565c7f0": {"doc_hash": "ae7e8d7a8782163093b5660792c806cb36f67bae6aff6e1d2ee2c9aa477b7819", "ref_doc_id": "de718598-3ff9-4424-9331-9cf36d6af3e0"}, "6249c657-f3fc-4c4a-9b65-e0fc63a91c1e": {"doc_hash": "a433ec66b36290072102942784be25bdf3a7571e2e0670a78e7598ae7f902676", "ref_doc_id": "035e0516-a0e1-4bd8-a828-e7f4bcb9b02f"}, "2b9d3ef7-1eca-48d2-bb74-fd0d81ed5f95": {"doc_hash": "3f54d7e3bd95836a19b854edc2baa7f40c93c0c1b268b06eb4f1ce0c4daa43ad", "ref_doc_id": "035e0516-a0e1-4bd8-a828-e7f4bcb9b02f"}, "84d9d203-41b3-4c5d-bb30-301013d3f233": {"doc_hash": "f2ba9f6bc2fa3c81fab37cdf774d614a005dbbab2789c3a541e089d125f09c11", "ref_doc_id": "d3ea2d0b-aa99-4af7-b94d-e1c6f526133a"}, "f40aa89d-8ce6-4e8f-9ca8-2a861d4a9134": {"doc_hash": "770ce3139d15dcb953b90dda1bfb78dd0b5e90abf996cb260c726da375b871bf", "ref_doc_id": "d3ea2d0b-aa99-4af7-b94d-e1c6f526133a"}, "cbd6b41f-98c7-448b-9fb1-9db0c71ed4c6": {"doc_hash": "6edefc7bcf1f78db2a857a8119fee18b61429280567c7561b07ac5f70ebd0f63", "ref_doc_id": "d8662ecb-8c77-4f22-96e8-b223e930ee0c"}, "4d3d6771-fa9f-4f38-8c0a-68a39c13baaa": {"doc_hash": "cadd06aeb5528e5a94f0ee7a917286af58cc30104e3adc5fdfe704f3da306d6a", "ref_doc_id": "d8662ecb-8c77-4f22-96e8-b223e930ee0c"}, "1a5d552b-e299-4dcc-8350-4b90710e33b3": {"doc_hash": "d90a6e7027bfaee6b62dbec0d68499154c33b87f2736f66fb9f0dc73e6d1eeb6", "ref_doc_id": "fca6b329-8ebf-4504-9598-7a25e9b677bc"}, "959781a9-7849-4b2a-b213-89f25189a72a": {"doc_hash": "6f2a3552bffcd579f298847696223543e2cffebe5fb5f03396471f9604d1de4e", "ref_doc_id": "fca6b329-8ebf-4504-9598-7a25e9b677bc"}, "8162bf79-863e-49f9-8d40-b0329bd68b6a": {"doc_hash": "a90619a32620ca23911f9fb1c5df92cee7c4ab6ec4e122786a201ff7030662d8", "ref_doc_id": "5e7bc5b4-0b67-4c5a-a893-268c550a3abb"}, "f2c48808-43db-4927-a36a-d1a013f1479c": {"doc_hash": "c846b183ed7dd4bc0bc7ebfb1ebd3a5c1355bdb2a2930ea2adf1dee32b97bf54", "ref_doc_id": "5e7bc5b4-0b67-4c5a-a893-268c550a3abb"}, "2ef6d38f-9c00-4c55-993b-fa5bef23c4a2": {"doc_hash": "1d3562eeb507abc2065617edbbbe501a76bd14ed486b0e2b088a8d5377bdc54a", "ref_doc_id": "5e98a887-73f9-4e55-a4ca-96e985805783"}, "3b99d716-ecdf-4e0f-968b-85a3f7535f64": {"doc_hash": "ea7be0664a49b7b1602758f1bc5cee2610d2e8f105241f858013174894661e5c", "ref_doc_id": "31165a1b-a78a-4f9e-bc74-e87582a28c61"}, "7688cb58-efb8-4253-b488-d6a6acb150f4": {"doc_hash": "a160715f7685af5d7b044ff6dc16e1e768796fed3b992139dd752f311e0a346a", "ref_doc_id": "cdb55b56-a520-4e68-a7f3-a410dabf27d4"}, "2dbf0002-0e09-4c42-b9a9-133a614a3409": {"doc_hash": "1049093f22f77219759c56ba8fa5f5befb1b538285bdf33064072590e6b0c7c2", "ref_doc_id": "cdb55b56-a520-4e68-a7f3-a410dabf27d4"}, "4e912775-8bfe-438d-a92a-a411df0aef65": {"doc_hash": "0804163e59fad7a3c96a5db867ff72d408800dd48b6381077d2a87cfdebdc2cf", "ref_doc_id": "2797e83c-4e17-4161-b0e8-4670879fd46d"}, "007664a8-83f5-4530-b6d1-d4fd70c0488e": {"doc_hash": "f2e08066463632cc42207851edfffc2938ac237b218ddf22e5dfc6d64c141597", "ref_doc_id": "2797e83c-4e17-4161-b0e8-4670879fd46d"}, "8dac8b99-7844-4832-8190-64046c95b28f": {"doc_hash": "89f318de3122455af7d0604ebe6aba1e391efd927dd55ef1d3a21f70d36744c7", "ref_doc_id": "32caede3-2d9c-4c3f-a2b8-ef9608a8efda"}, "311df028-ffba-48d2-87eb-7544267a4b83": {"doc_hash": "a68f1c1323ad60b34c83661189265bbc69d596d1c824e7faa7bc9257a71c8b42", "ref_doc_id": "32caede3-2d9c-4c3f-a2b8-ef9608a8efda"}, "e7a7412b-fd26-4785-88ae-5d4f1ce3d9d1": {"doc_hash": "1d2d58f6b49d40199104a17b389efc3c29692755ce3e8abc854b8677c7b04cae", "ref_doc_id": "92db31fd-64b7-4603-9417-8743b5e0ce44"}, "3b072c93-b875-4d91-8399-67e5d39ce8b7": {"doc_hash": "d44acbbe71d290479e6584e7b09a2a80d22e143e4c3384f60f1ddaca4f89ece7", "ref_doc_id": "30b8bf60-b0a3-4d25-ba1d-0665188f6124"}, "bd9f4dab-d6c8-4fca-a754-61b399002eef": {"doc_hash": "d27952f3618d2685237b7a4aa99f0802cfa59de699ca3b15b58a6179857d19dc", "ref_doc_id": "30b8bf60-b0a3-4d25-ba1d-0665188f6124"}, "c39c02d5-3afb-4bdc-b84f-ab5259c1b5da": {"doc_hash": "a746ea52c380b6d4a098b6e9566e04e0ad950c414c28916e56c3a793bcdb0fb8", "ref_doc_id": "6d80b32d-19ee-46cc-9d84-38d1d98ff2e7"}, "bbcd8385-2908-4832-a9b9-16e9bb32d580": {"doc_hash": "e16b6dae19ce7a2e7d41be7490e98c9984c383889d625b16b67ff3a5a9f1c82f", "ref_doc_id": "6d80b32d-19ee-46cc-9d84-38d1d98ff2e7"}, "0a8184b8-1064-421b-a59b-1101caff8be7": {"doc_hash": "4bd9acc4f0db6dcc28ad4ed43904b021286f98b5c9755bec147ef29ec7f8976a", "ref_doc_id": "a322bbac-55d8-4024-aafb-3a18236b59e2"}, "1366e121-f984-43b4-a772-dbcb39632bda": {"doc_hash": "0c822df6108085f4ff49e9e6c15032a45978d07dac0b23df591530667ada135d", "ref_doc_id": "a322bbac-55d8-4024-aafb-3a18236b59e2"}, "30ac4b62-3548-47e5-961c-8f2d4c86f360": {"doc_hash": "c542904cae04b8672cfd7852f6d1aade2ca044c52f09433f8dd676a8fa79b8ed", "ref_doc_id": "76e6460c-f850-4db3-84a3-c90d0d4d108c"}, "108db81e-6951-468c-a52c-8bace41261d4": {"doc_hash": "cb3f53096770359a093324df4286b5689b981f29f602f0be86bf1f8bb5c70566", "ref_doc_id": "76e6460c-f850-4db3-84a3-c90d0d4d108c"}, "5375fc8f-5497-4fb2-a4be-9c58efc07577": {"doc_hash": "4d37b177857a622d1e67507bf4ca4464f01dedaf2d51941d4ff9616a63b0755d", "ref_doc_id": "4bd4c3a1-61f8-4f02-86a6-017c0f023645"}, "e2102027-836c-407c-ba7c-0c5ee0492fb6": {"doc_hash": "c3458af8c76935f37cc2570fde63d6a8669c0132ef0744c695c86dd26c32cd7f", "ref_doc_id": "4bd4c3a1-61f8-4f02-86a6-017c0f023645"}, "b692bd4f-765e-4503-a82c-8a2fb897c0d5": {"doc_hash": "29d3dd72b8bf61c3bb092d05e4d3498ad8c1c228bc07e04edd39b35caffb6c71", "ref_doc_id": "da6b4d72-fd10-4b13-89f5-8c3604e4fefa"}, "197d7bd5-e401-4506-8bd7-ac68b66276bf": {"doc_hash": "1d2f3005f7a3e566382e6ceaf8aba67e5f3f8b60e486b638899a3b9cc19537a3", "ref_doc_id": "da6b4d72-fd10-4b13-89f5-8c3604e4fefa"}, "df5099fc-9b9f-4dcf-9e7a-13837bacc5a1": {"doc_hash": "19e8dd881f1e52da70848874b44a290e4b58fb4981fe672b538bf5d5e5753bfb", "ref_doc_id": "d94be76f-1a2c-46e7-979b-389c19fd3b82"}, "9dd4669d-2859-44dc-8333-8c5efe456dbc": {"doc_hash": "bdaf3f06e9bfe17eb1282df628e40450c76fd0bcee8abcfe0822fbd117cec25b", "ref_doc_id": "a73e442e-f027-42b5-b16a-3cf06321c6df"}, "0d3408db-ac49-43ce-8b3b-8724c62e525c": {"doc_hash": "260202c3f899243c8857f32228708f3517b45cdfba93836814b2efc326ac5096", "ref_doc_id": "cb90669e-9444-4916-b9a7-101e2ea7d96b"}, "13c17320-1a7b-4e6e-bf5f-172679e963f0": {"doc_hash": "05656a424d16f9f9b63a7ddbbc2e87fdd231164b03b2808116cf7f31b5ef9b31", "ref_doc_id": "cb90669e-9444-4916-b9a7-101e2ea7d96b"}, "c037ee30-8133-4839-b08b-7c652fb85248": {"doc_hash": "e26bc83b70f8446d4c284430df89420030e629e58a481281ad552053e58499bb", "ref_doc_id": "6dcddd5f-dfe2-47f8-8106-bc3416d45f0f"}, "76b58edd-ef42-4c84-be01-57af7fda0024": {"doc_hash": "15d296dabfc74e77b41a70189029100b924344a096ef3a475d3dc4302a33b47e", "ref_doc_id": "c402d675-8b25-4e53-9b00-3ddde2cc03c4"}, "c8474b79-dfec-41da-8800-44a185accb65": {"doc_hash": "8ddd806b4ebfb2faf68d3c64d116e668e87c4eeac66a415781d878937b7ef247", "ref_doc_id": "c402d675-8b25-4e53-9b00-3ddde2cc03c4"}, "fd6b9d1b-3ce9-470f-8b1d-0ef4283b90fb": {"doc_hash": "f33e1a38d61c4e776ff24dfc83a473912226ed9b236b63e1233c8660615f8d3a", "ref_doc_id": "d2814921-d22c-45db-8c20-209e09adc6c2"}, "49b06c9a-2624-4921-9962-bb6f5c36b2a5": {"doc_hash": "cf4206d7a7618c54d5096b4ecd43f06b27e07d9afe1a6bdd0bb1f4aa207f14fe", "ref_doc_id": "d2814921-d22c-45db-8c20-209e09adc6c2"}, "d527e945-a7e4-42dd-b463-2d3b427514da": {"doc_hash": "7fb2201138492ea3abf3fc1186ce77984b6b75b42875fd85ca58c857f3c666ef", "ref_doc_id": "c3ead361-c540-4751-afbe-ccfe6f550959"}, "77a13371-9836-4f53-9a3f-f252b9716849": {"doc_hash": "f1287f1a91e961286dc57c890a4301b6f10fa10fc3acaa4ad522fca268c337cc", "ref_doc_id": "6c50c7bb-d1a3-470a-92f9-df245329242c"}, "1de9c534-ed62-4759-9689-25566bf91a84": {"doc_hash": "bbaf218f2fbf951c32899fea4b5d6b7aeb2083a92ea06144cb435b2cd0c50b1f", "ref_doc_id": "6c50c7bb-d1a3-470a-92f9-df245329242c"}, "04cebd34-d844-4bde-9e44-55afa023d9e8": {"doc_hash": "23530eb8d4dab612355d2eda6f1e5e5ab123a8e5d02904e5c5d8a478ce58f5ba", "ref_doc_id": "c322d402-f3b8-4588-9192-a7a9d0f09457"}, "693cfe0b-d036-44a9-b7f2-868501c09665": {"doc_hash": "ed5a3f45ca9cbd6acf1cb37fc433124ba4b99842abc0cf2bd40a0040d31da905", "ref_doc_id": "cd2b9134-d1eb-4afb-b4a2-13c7dfd106e1"}, "c99b79c7-0fb0-4f10-8dc9-330276e166bf": {"doc_hash": "2f22f4a895e251bbcd6d3fc91c1ae721bb5b10668467b1b72e36572e8f68fe54", "ref_doc_id": "cd2b9134-d1eb-4afb-b4a2-13c7dfd106e1"}, "04d7545b-4492-4144-b8fb-820ce2afaa3c": {"doc_hash": "1266cdf83819efad3ffe2fdf56f09ede89aa73661daca9be539eac9ce080478f", "ref_doc_id": "289a0fc7-c14f-4200-906d-9991f20c2cab"}, "78335aaf-5fe7-4da6-8e9f-1d46ccc6bc3b": {"doc_hash": "b2be700e25e530a2807542c21c8988173c6ddce37ab61c76317e93308055006a", "ref_doc_id": "289a0fc7-c14f-4200-906d-9991f20c2cab"}, "f3860c03-49d7-4dd6-b527-a4760b19a7c3": {"doc_hash": "73688bbeead6941c308ac258c613935b02066e670c86a5af77385b19453dcfb4", "ref_doc_id": "3b815434-5509-48f3-a18a-47d3cf16db98"}, "e7943d8d-04a3-4ec7-bc75-66dd8b15fefb": {"doc_hash": "fa0cdf7a68aafe6f62c7fc33930e789e6f03d3f59eb8ded9634a75bfe6575f07", "ref_doc_id": "3b815434-5509-48f3-a18a-47d3cf16db98"}, "2da53847-a6f9-4db8-b7e5-9a3b832ca833": {"doc_hash": "dfb63b3a5321ee22c0ce6e040b407704cec833afaed8be4d3570710bc812b139", "ref_doc_id": "336e4992-5ac6-4372-9099-e6c746f70250"}, "c6195407-1203-4178-8fbc-990540c57f42": {"doc_hash": "5749fc1050c11411746f156a42c5452ac15e9d7e29d12f9f19b6573b062f5163", "ref_doc_id": "336e4992-5ac6-4372-9099-e6c746f70250"}, "c8862aa2-a8b0-4bd3-9c2f-895f1e4ca56a": {"doc_hash": "d4d960d667113a426aff448c8f123047382be0a2dc92574f1d532c7182ec5380", "ref_doc_id": "d5bb378b-222b-4cef-a97b-09e5b54acfa7"}, "d3894d84-648a-42c2-93cb-ebff7d3dbc0f": {"doc_hash": "99da92db405e611f8af200163f3a6670acb2b6f1cdd849f737837a4b005693e4", "ref_doc_id": "d5bb378b-222b-4cef-a97b-09e5b54acfa7"}, "f5ac39b8-8089-44df-86fd-91fb77f031d1": {"doc_hash": "8d3be7ef2168a075d65925e719622325595c26791b122623cf08c2857250b193", "ref_doc_id": "0bf2ba39-83a9-45d4-9814-13d20291b2b0"}, "8a8ec2c2-9bdf-4806-843e-6bb42388ae00": {"doc_hash": "9bb1ee98495bb710c3b0da484c99bfcfd3c4b0684e06e936383989143417d4d0", "ref_doc_id": "0bf2ba39-83a9-45d4-9814-13d20291b2b0"}, "065f5cc5-0336-4413-bf85-42bcadb1c76e": {"doc_hash": "3c794a1f78d92e63a4fb2d2337a29e8fd613cf700e69f5981c81a11ba730d363", "ref_doc_id": "dfe84564-5c00-4edc-998d-fdb7b356cfe0"}, "a03da7be-4d95-4004-abff-356c0117a88d": {"doc_hash": "a9f63ec4124f7a7f183ca2903c19db2561ca0ee2670c5547711b26ebc23605b0", "ref_doc_id": "dfe84564-5c00-4edc-998d-fdb7b356cfe0"}, "e42104b7-2c60-4246-9b39-4afd0d15a53d": {"doc_hash": "5eb573e831855c983b5665044d44493f39dd033fc33500c0dace254c78ae74ee", "ref_doc_id": "dfe84564-5c00-4edc-998d-fdb7b356cfe0"}, "b7a83585-17c0-42cc-a453-d5b8ae67bbd5": {"doc_hash": "7208c5897998717e2a841eb66ff18b58204be2c7d4566a8da39febe1efbff379", "ref_doc_id": "6b639655-4cf4-4fc4-b263-c719dfbefd41"}, "53a631c9-fa33-42ec-a7d7-61e2f240a66f": {"doc_hash": "b73802415ad657cbdbdd997afb62c60a9456e954514abaa047e552fccf89e69c", "ref_doc_id": "6b639655-4cf4-4fc4-b263-c719dfbefd41"}, "18aa9c90-7a35-4ca2-8e8c-af96978dc1bc": {"doc_hash": "7374c3e8c724373bc90ec2b4cea914459937d80648245c862bf097416f5e4c87", "ref_doc_id": "6b639655-4cf4-4fc4-b263-c719dfbefd41"}, "bd11d076-87d2-4b16-b62e-a5c94c86b520": {"doc_hash": "67ada805fa6faf29137c99510c9b4875b63e620ad1a18ef003f63e83e44df8a0", "ref_doc_id": "0f40fa86-ec61-4f3a-876b-7076516f73f1"}, "cd90c7a9-4104-4364-8241-984097fc4137": {"doc_hash": "6f812cb402c92fcbf8a81d24824000c4c64c442668ba05486e2ec2a2b66f4b98", "ref_doc_id": "019bbff9-f260-4c69-a1ff-ac34882dab33"}, "302a8d5c-6348-44d5-935b-2a02f23c7f37": {"doc_hash": "f4e7962e1e5d6c02aa8f0c6b8a58d53e8bf3ad23208262297a09604672fc5190", "ref_doc_id": "019bbff9-f260-4c69-a1ff-ac34882dab33"}, "c6f4b3ee-aed7-4923-9da2-3b35ac9d12b7": {"doc_hash": "b5d37fbbf9b68285e75f95b6bb2d775aa25e2ae79d3800fea75e4eab77cefdef", "ref_doc_id": "718b525c-8452-4fb2-a017-735f3fc2725d"}, "487f951b-7582-44a0-85de-f92bb3b90338": {"doc_hash": "f3df8a4a787b3f287562971e3961cd228896d7ded57babb8b01d6fd4c2a6b482", "ref_doc_id": "718b525c-8452-4fb2-a017-735f3fc2725d"}, "8b5451c1-4abe-47a3-8da5-f75e644dce5d": {"doc_hash": "479ac7a48b0c81a8f934f4f6012f6090f1213912abb125a73e851903301c6e0d", "ref_doc_id": "1002b06f-4a58-47f5-bada-303fcfdb722b"}, "72f07dcb-e55f-4d27-b77b-2aa3f9d6cf0c": {"doc_hash": "d1f4bb94fd7e28e7488f3ff09296e14ea3f922e9050aa6f365207e6cd70a0677", "ref_doc_id": "1002b06f-4a58-47f5-bada-303fcfdb722b"}, "68af7989-bde8-4ce5-a0db-6ff8fc4d94fa": {"doc_hash": "1025c5d57f9eca5302e7568886e87f9875421a3d445ccff73b4485657fd5badf", "ref_doc_id": "d578ed6c-fe42-4513-bc2c-a4d2b8cc92c8"}, "d34ac9c1-9944-42b2-a752-56325798331e": {"doc_hash": "a3870b4d055456019ee4a4ac52c16c8c832188d6f4a8ee1c39d1e54e40163da1", "ref_doc_id": "d578ed6c-fe42-4513-bc2c-a4d2b8cc92c8"}, "e4690e11-1b8d-47e9-a444-b68d330b092a": {"doc_hash": "23b6ea41c7e3ff5efc49acfaeecc36ef5920eb00be626c216ef524bc6dd2873f", "ref_doc_id": "d0c966e3-49f6-471d-ab0b-8c874f7e822b"}, "4c90538c-b368-465a-a8d4-622d4a89eadf": {"doc_hash": "a811e9b87fa4dfb75e67d74277321919851c28f5fc260f9e274930e7af8ed049", "ref_doc_id": "d0c966e3-49f6-471d-ab0b-8c874f7e822b"}, "5a8a4aae-e59a-45c7-a61f-0ff627d70a42": {"doc_hash": "ed36cd27afd8a4ff686c7e29dd878275fdb7e6b03964f10bc14f42f41ac66cff", "ref_doc_id": "ef2ffb70-a742-4e82-8f25-5953600ab075"}, "3fd8f3b7-f1fb-413f-b074-f4f719395c99": {"doc_hash": "b63b192bcca82d396a841904c58e274d52379f7cbc3c78e02910106ef9c9ad1e", "ref_doc_id": "ef2ffb70-a742-4e82-8f25-5953600ab075"}, "a2730dc0-dcd0-4abf-8968-c1328d855cd7": {"doc_hash": "a3ee109f9ddf8cf85db2a56d348975385a06487b860d995f7769f940ca517372", "ref_doc_id": "1d1e02e8-6587-4553-acb5-c0f5437307a3"}, "e9a66b0d-7bd4-48df-8d91-ab0a2eb0297c": {"doc_hash": "8b53093b6d061b2aef13e92e6a2a36f851a54157027cdaa753ea0c1549e3634b", "ref_doc_id": "066aa014-8322-42ee-ab37-5b2d37726129"}, "71297b9c-7a19-42dd-a820-4fad38649288": {"doc_hash": "8c4c5ab8bb27e468fb49c9ac155c85135b681426769202b40d1e7a6b1c4d3262", "ref_doc_id": "066aa014-8322-42ee-ab37-5b2d37726129"}, "e1a95dc3-c4dd-4997-a3c8-a4837df5b320": {"doc_hash": "d373080d944e4db206dc2633aaa851d35cdedccc8dc1da7a4ed0ef98ede0a774", "ref_doc_id": "2735b0a1-d30e-45a0-99be-1e60cd9830f5"}, "55e09196-11fa-412e-91c1-26ad4d6b0dbf": {"doc_hash": "4630917f6d485f0ac0098056cd25de462f11827edd970d2c08f72cfb9be47e38", "ref_doc_id": "2735b0a1-d30e-45a0-99be-1e60cd9830f5"}, "e13c8d83-73dc-424b-9924-4b2105aee24d": {"doc_hash": "1ed0c1d4920153a4c86ee2e09c5cad92be4bc10ab02defa0a4c1c9d370f10f53", "ref_doc_id": "1f45cb23-bfea-4041-bdba-f6e9a790db2a"}, "925effbc-39d1-463b-a41b-378982fe6119": {"doc_hash": "f36dcd6a9b945f84cee18cb35fd6468fa6a7abde9b249aa4e92a316af4303dc2", "ref_doc_id": "1f45cb23-bfea-4041-bdba-f6e9a790db2a"}, "5c4c75a7-d59e-46b7-b13e-aa538ac70c33": {"doc_hash": "321491c9529124e935d25cfb0ed365ccef2601940377095d0f670610e221e5c3", "ref_doc_id": "cbe35ac3-764d-4dc3-a82d-79683d795e2a"}, "797609bd-80f4-432a-9a6a-ef0f3be2b5d4": {"doc_hash": "d7a955b01efedf6b092d57b42d75a5decfbf180c0976d14590c9ca95bb6cc2f1", "ref_doc_id": "cbe35ac3-764d-4dc3-a82d-79683d795e2a"}, "ae8561aa-972f-4c77-a63a-66579ee07ed2": {"doc_hash": "73257d521dad3ec9fdd39b58dabae7c08b3d145b15ef9f00a9d71224e4a4455d", "ref_doc_id": "e0fe866c-0595-42c6-932a-a1c0cca353ec"}, "5463b287-7526-47dd-8d98-de5361182a08": {"doc_hash": "19412164c515dff922f0f4b4748b21c13d0df3d17e4fee32f0abcf7788713778", "ref_doc_id": "e0fe866c-0595-42c6-932a-a1c0cca353ec"}, "9bf05942-d94d-4c6c-8ce8-ba879a98bef7": {"doc_hash": "400a8045cabc885c9e9e4ab7c98da65bea7c5b3bde6c64d34fe0ee6c22251530", "ref_doc_id": "5ddd3602-df76-409c-9975-77e97fb1d52d"}, "97aa10c0-d7a4-490f-af32-2aa692bbbfaf": {"doc_hash": "bf56cd5e8e829c9b934f3b9e4973c132f6fe08672d606f6fc2fd80e5428bfcaf", "ref_doc_id": "2b2a1ebf-0aa3-4a53-beb3-bf146bd06540"}, "11b0eb41-1d7c-4781-9b34-b9de27768f03": {"doc_hash": "0297bccb6d7d8a6f4ec1174e21e773e4ffd4e1e78837402c712597377db78a67", "ref_doc_id": "2b2a1ebf-0aa3-4a53-beb3-bf146bd06540"}, "e2dff16d-89a6-48b8-bd7b-5304655867a2": {"doc_hash": "25370ba48c145184f18650160342261ee60d009bb07ded8be9d12666122395f9", "ref_doc_id": "0e16a3fb-8fc1-4891-a583-21dd8afd8356"}, "a2185e42-ddef-4835-88ae-df806aa54c3e": {"doc_hash": "4f7a4adf60881fc647517be37931e298e765b110e7ba86a5daa8dfc5f2d8d5c1", "ref_doc_id": "0e16a3fb-8fc1-4891-a583-21dd8afd8356"}, "f1b3d76a-df51-4569-b5b4-e3857b11c872": {"doc_hash": "b91615926f8f95eb53cc6c44acc0c3771ac29a01ab767d62c52fa6e2c04eb241", "ref_doc_id": "87d2e3c6-4452-4b9e-bbfa-65a394905e4e"}, "1b2fb2d3-a1ac-4536-97b7-3a5399acc488": {"doc_hash": "984969c18c93cfbebf5e78c6eb39072e3d112046582d98bfd03359e627ff4aad", "ref_doc_id": "87d2e3c6-4452-4b9e-bbfa-65a394905e4e"}, "7a52b1d1-909c-4ead-a4d9-73ebfbd581b9": {"doc_hash": "0437bc2cbc0228c7b0b11be06c8b7a13da2c059d9dd11a0680a2f3632577782d", "ref_doc_id": "05017b20-5666-4cd9-9a86-29ca466e2e33"}, "478eee57-a17d-4ed1-b12b-eec7ee47dcc0": {"doc_hash": "4088d62b03013003c53e04268c8cc9f22af3d47c28b1c58eeafa821439f15dbc", "ref_doc_id": "04167c03-1197-4caa-993f-1db41fed7f7d"}, "25b2116a-4dde-4a3e-8633-7d486d102a01": {"doc_hash": "6515c2a8074f771ccf2ac3a92e22883c54a89c5d05f7c1498d216a648dacfdc9", "ref_doc_id": "558aabdc-ac0a-4131-87fb-e9351c01caca"}, "49597c31-bc55-41dc-b18a-946066c88ae2": {"doc_hash": "e8caa2afcf337fe5df5ac7d35cd7c6a20381cc6a14f4bec98c1f46dbb7591587", "ref_doc_id": "b0bb577b-65bf-4e62-a71c-0720fe801dcf"}, "da73fe1c-4c99-4a8c-ba60-a90e371e014c": {"doc_hash": "b8f29d0b1537b732eddff5582e6540a992699f5b9dd69f08bf43767ff33f5164", "ref_doc_id": "b0bb577b-65bf-4e62-a71c-0720fe801dcf"}, "d14f2c6f-635b-4e07-b44a-bfb346c93977": {"doc_hash": "d3c15ff5d5df9a1f2011fe0cab82ab0000460deb0f134ff25aea72a8751c93a9", "ref_doc_id": "0521f782-b619-4268-92e9-cde7d75f6fc2"}, "fc09df64-ca00-4506-ac63-d172b4445dd8": {"doc_hash": "9a756e08ef4567833d7d703ace58ac02674df54b78dde1376ea2b1ccea4f81a1", "ref_doc_id": "0521f782-b619-4268-92e9-cde7d75f6fc2"}, "1d518517-d758-4ce6-bbe9-30fad3f5aa68": {"doc_hash": "7e9448643680bd7a7a1160b00426749ec72c65716faab015cb7fcd5ce4f7a16b", "ref_doc_id": "a2eaf068-12a3-4a47-b796-c9d13306cab9"}, "79eeb578-880a-42c4-81c6-d5d0ffafbd0d": {"doc_hash": "58ada316950ccecaae4ae71372c79e78e2389f38b5f417b14dc6111c92814d39", "ref_doc_id": "a2eaf068-12a3-4a47-b796-c9d13306cab9"}, "188c676f-50c0-45bc-81a1-48eed91d1866": {"doc_hash": "ae52a553c71c391712f6537f0f4a451d4c72c6bafe70147ca209ce083e945baa", "ref_doc_id": "19004571-ee66-46ad-99d0-eba9866758c3"}, "ee6895fb-9704-4772-873a-3a3be7515948": {"doc_hash": "d0cd36c900e39e063be707104feec9b143ae13f170ec38d7ee2e68ec48663c65", "ref_doc_id": "e6b1fc99-8048-4656-bc23-b2a6a2bc326c"}, "c9a4c489-b6e7-4fc2-9aea-7d84fa71703c": {"doc_hash": "f5de6419100abf8e4a65d178ded54d705069eb9dbb2a1b22e02ef8fe69ddb925", "ref_doc_id": "e6b1fc99-8048-4656-bc23-b2a6a2bc326c"}, "54098528-d356-4665-919b-d5060a1f6503": {"doc_hash": "3160b596c92efb1bd776f6aff79681b8bc6b6019edfef75827cb15c0ad24a645", "ref_doc_id": "4037ae60-e748-4652-b18e-67f34a2960dd"}, "993377cd-0f5d-480b-8168-7b6634716d97": {"doc_hash": "1699eb457471be2bb94b4b342eb763bb00395f91e383f9dc1b9a706d4f830c6d", "ref_doc_id": "4037ae60-e748-4652-b18e-67f34a2960dd"}, "a215aae4-5dba-42df-b308-20cc0e752ff6": {"doc_hash": "191046a1e58fbbcd5c5b8821efdd86e31244b819fd240f42495a0ce01e0d368f", "ref_doc_id": "4c66e2e6-0588-4327-973d-5776b26e67e6"}, "157f391d-1774-4ec1-a4a0-63f259bb5ec6": {"doc_hash": "105d3871d6233132bc681d15c8a04e62bb1f1954102e6beacc6c02ac5e6b6304", "ref_doc_id": "2f9d15aa-5c08-4cde-99bb-a233f1b579f0"}, "32ae6ca5-d98f-4555-80b0-ce5a11ae731e": {"doc_hash": "185a65d7069d60f6935310c079b7e46ad14b774d1ef7faa6d512e527b9770b90", "ref_doc_id": "724bf392-6e79-4b38-8f3c-0985f0824a0c"}, "b33375fc-dbd2-4930-8026-d5c2e5c33c3f": {"doc_hash": "808b24b012007396a66480bdfbc7bfdbb901273aa851fb3fd3f8e3690df348e5", "ref_doc_id": "0f643fb8-b06e-490b-b3e9-909d6292b389"}, "3c154563-728a-4cfe-a93d-33ebee912126": {"doc_hash": "fad87224ae3579265e10043a4fa2856dcc52d4f8b98311a4a097ac3e6fda71d3", "ref_doc_id": "0f643fb8-b06e-490b-b3e9-909d6292b389"}, "714da7de-3625-4641-8b80-17676d0dd35b": {"doc_hash": "5d722b8fa7b717e9e745ff1da6781bb16d1e1d360d2577eb4714273efd20672f", "ref_doc_id": "0fb92626-4265-408b-a0c6-ba0be90c2643"}, "09f86ebd-84e2-465a-a362-8432855eceab": {"doc_hash": "71afd0618449464b5487b3dfe3d124ad485b48cb39f512a6a3b33136f66a931a", "ref_doc_id": "2cff0bdf-dd24-4e4f-b1e2-934ef0398901"}, "5710873f-708e-4d54-a2d8-1b6977cf6a03": {"doc_hash": "c66d3bd23ef9e78d8569311ce0091ccd1f1882d99a4519a67b20ae3d98f623b4", "ref_doc_id": "208aea5f-885c-49b5-beaf-016acff75fa9"}, "705780c9-c843-42c7-ad41-94c5988c706e": {"doc_hash": "379f818d44a2712bd14a34c9c98e78f398b39dbce970c32a4330a1fb71682623", "ref_doc_id": "208aea5f-885c-49b5-beaf-016acff75fa9"}, "085aec09-83e4-4d6b-8e35-06577191b8d9": {"doc_hash": "2368d516ae13327a81a0ac2c5b40332b3b538d45b6e1a4eaf2f23d7cf6f31825", "ref_doc_id": "c7a4f182-d990-4c2c-9a2e-97548e03eb8b"}, "28edaeb2-8077-4ec4-b006-f8784f939fcd": {"doc_hash": "ed51bda83ecdd0d78796fff9c909e0aa636d3e7d20dfa89e6ee8a6f9723e4307", "ref_doc_id": "53b70b3b-1485-4a7c-9de5-4490c71dd120"}, "e11d07b4-7bef-485c-908b-1fcff15fa321": {"doc_hash": "7bc14746040dade3571dc36f659fb3fc8577226c2837851ec75654a0bbfd86df", "ref_doc_id": "53b70b3b-1485-4a7c-9de5-4490c71dd120"}, "3002dda1-f9ad-4b77-a875-5fd3cbfb9272": {"doc_hash": "7926b6fdcdbe4fddbb2717b2f1c340cface666215ab29702b05f21de91653f4c", "ref_doc_id": "606bf2d5-8b74-490e-82c5-4d7995453e0d"}, "366429d4-41bb-4e6e-9177-81ef84a6c571": {"doc_hash": "a63ff2016185f32dc4218b8159669178cc385990f2d2c44eae7c3f475d2d13af", "ref_doc_id": "606bf2d5-8b74-490e-82c5-4d7995453e0d"}, "78e626a2-3ae5-46ec-a91d-65af9b82ac33": {"doc_hash": "8c37a75377f42b70e413a05541e376329574a251ccdb02efddeb2fb82cffb9fa", "ref_doc_id": "6137caa2-d008-463d-bbad-6c37b53d7a39"}, "518edc28-ea32-4802-8965-d3f86b569bbc": {"doc_hash": "6f6885cba4072c59d5766baad0a5f127a22723ae065d61204777d76ba60fabfe", "ref_doc_id": "44d116a9-5dc5-4866-962c-377581814389"}, "540f99c3-e1c8-4e72-9857-86c8f568c94e": {"doc_hash": "a86e9ede0bae62509696e3bc9472e5dc03971e39a3781bfbae813f074a00e744", "ref_doc_id": "44d116a9-5dc5-4866-962c-377581814389"}, "a63100a3-f57c-457d-ba80-fc482dfd5f56": {"doc_hash": "2aeccb7b7a0ee3162ee0574c746a3f945a630eef8dc9735a351210b363fc0f90", "ref_doc_id": "4689cc6b-9f9b-46dd-93c1-559c5c1e7f08"}, "41e50465-360e-4d54-9b27-e9d2b91f4af0": {"doc_hash": "8c54309d8c1a33736d09173ab80ab05dfa1b4a29c35577164f80c4127f516394", "ref_doc_id": "4689cc6b-9f9b-46dd-93c1-559c5c1e7f08"}, "87b1fe1b-0470-4a3d-b9ab-64b7e05e8b5c": {"doc_hash": "8ec58b9432d3ff0e39c24497e6d8c5eef4d534e429e115984aed6596c40cec17", "ref_doc_id": "7095e65f-fb04-4bf9-897b-614c9d5524ca"}, "17aa8243-b803-4f95-90da-a27d33ba8ebe": {"doc_hash": "e7b1cb22bd65d605e0b342e4b19241d963c62ed7ecc8a6a3ab84e430f6cedb41", "ref_doc_id": "7095e65f-fb04-4bf9-897b-614c9d5524ca"}, "77e6e8bc-49a6-484f-a81a-b35b2521e6e2": {"doc_hash": "a4ac54777fb5e935c4f8a8d9a558770ab1e92d2864b8e0d6a8a70427bd470e14", "ref_doc_id": "4946db25-fac9-4260-bd32-94c4b9c5b838"}, "c81cb8d8-b0ce-46c4-8b19-7e451e36370c": {"doc_hash": "b55ce29140e215b62d373b9ced31738be9eaab8ed4cfbc9a792f88109a25bf90", "ref_doc_id": "4946db25-fac9-4260-bd32-94c4b9c5b838"}, "7c296753-c436-4f67-80c9-b316fdbd5e3a": {"doc_hash": "480d2aa577e70124cff93bfa297f8877c0611862fb2c2a0ee617c5e3f9625813", "ref_doc_id": "fc2578e5-f9ca-4125-931f-681cf3b1566a"}, "fc53e336-b57f-4a8d-b408-cd941cdd8ccc": {"doc_hash": "1155fa6c3b593c1cf8dc228e2b7289a73102fd093217bf129787c95a34a92614", "ref_doc_id": "fc2578e5-f9ca-4125-931f-681cf3b1566a"}, "1f4b5d80-2026-4b65-b044-571b7c975c31": {"doc_hash": "7079cef39ac8ee7135db9b3e3d2a0f08814cf27d05dc3b03d56df085f7475b54", "ref_doc_id": "fc2578e5-f9ca-4125-931f-681cf3b1566a"}, "e380dea2-bfc0-4b13-9434-8a0d9b58cae7": {"doc_hash": "0bbb8a8650c7d8f15850fa5fa5b5fb9e6584180ef5aab55e61e8177464969df2", "ref_doc_id": "e45afb9f-6fed-477f-bcfa-d877f32c9658"}, "ffd272f7-7798-4852-b1b6-ae3b1b964370": {"doc_hash": "65a62cdad24355903a7495641160f2f4d07086a429b70c406962ac773227eca4", "ref_doc_id": "e45afb9f-6fed-477f-bcfa-d877f32c9658"}, "15d5497e-73c5-4354-aeee-72b97c7f914e": {"doc_hash": "6719650ff280912f6c4f6849f30419dc7ce0bea971339787a07525279c1ec5db", "ref_doc_id": "a1229b78-9125-4cc5-b4e7-5dd9d13a6e19"}, "2486c76a-fefa-4b3a-90ba-de82faf2a2d1": {"doc_hash": "346916da036986dfec1b823cb7549ffc16f0e014f7889376d961a6db3962efb9", "ref_doc_id": "a1229b78-9125-4cc5-b4e7-5dd9d13a6e19"}, "98337baa-5a3b-47bb-80db-947a190218c3": {"doc_hash": "355c77d6f8b7d7994a98783a8e0fe9348a389004fef160008a521f552decb103", "ref_doc_id": "d7f4886d-fc40-4339-9763-26ef2704bbc6"}, "17e5fbd5-a8aa-4d50-afec-90cce86ac83b": {"doc_hash": "82dfcfbf68f1a627604aa2f5dbe1521e2821db179d44e40ff9a360959f2f3787", "ref_doc_id": "d7f4886d-fc40-4339-9763-26ef2704bbc6"}, "12dbeb4b-9df0-4829-a88b-4154ec7c3e84": {"doc_hash": "d4c597fc62c509907e5d48a2bad7c01b2ff5629425a1d8ddba81a0bad3c727b4", "ref_doc_id": "9513c1ed-af72-4ae6-96ef-1ec019cdfb1e"}, "2308c4ca-156a-4602-bcfd-f5fde7e2ddef": {"doc_hash": "6f1b240de13d59e8752525e3fd4b3397c4fea4e53028e9e0b41691ff23a40ea1", "ref_doc_id": "9513c1ed-af72-4ae6-96ef-1ec019cdfb1e"}, "3385d8f8-559e-4744-8876-94f649b6f829": {"doc_hash": "238f98ad357a76832a64ec2bfc92bbc8fdf8c9d72ca797998f8addb32705beca", "ref_doc_id": "06a4455f-7516-4a87-a65f-5b202f6e1001"}, "d819170f-c131-4bbc-894b-b8b99e3112dd": {"doc_hash": "51117c275cb70f89779513951c5e3af6e3f070befe154dc1451b28b46a24e1b1", "ref_doc_id": "06a4455f-7516-4a87-a65f-5b202f6e1001"}, "1bd21ce6-da1e-4d6b-a4bd-7fe899733266": {"doc_hash": "bb0123394b6cdc264f99e9ad22c3ff660d8fc98301e177752cb4778f1e5eaaa0", "ref_doc_id": "446ea4bd-d744-4aee-8fe8-8631c6a6007e"}, "3c5c9e79-5f94-4ef2-b3ff-46f2540cf49b": {"doc_hash": "cf39adb14f18ffa6f59d16fd624d564ca3a3d5273b709164c01501164a04c7e2", "ref_doc_id": "446ea4bd-d744-4aee-8fe8-8631c6a6007e"}, "eba08c57-1d0b-47b4-a315-9d109dd1502a": {"doc_hash": "6d4f37d541035c838c5b794d359be5558295897ec68ad0539b15f1b386036bb3", "ref_doc_id": "446ea4bd-d744-4aee-8fe8-8631c6a6007e"}, "2f5b7e0e-f0fd-4a65-a628-367d430d61f7": {"doc_hash": "23af748f95d7eb4d64f834e8f467c6cc8e79d5dbf8cef080c0fc17d97287bf62", "ref_doc_id": "446ea4bd-d744-4aee-8fe8-8631c6a6007e"}, "af89c182-3a76-468c-b35c-44d8b14b168d": {"doc_hash": "619af2aaec873f9df6979a793cf4c5f335249ad3062030c533f4f2e9c898a5de", "ref_doc_id": "ca575dbe-9a0c-427d-9cba-690f1a65a0ee"}, "6ea21953-f114-4145-b496-97203eedf2e5": {"doc_hash": "dd03a1f92af3e88db3c8614ddf8737280837acc2a1ecc0a5991a912853e601ab", "ref_doc_id": "ca575dbe-9a0c-427d-9cba-690f1a65a0ee"}, "646fb8a8-b3e2-49c6-8588-00e9634e8329": {"doc_hash": "2678412ba2179fc029541ac0f3b40bddde179850198204ee58f741489aee4969", "ref_doc_id": "4a1502c8-8b8d-43fe-9459-97798f3c3283"}, "914d4f34-98c3-4f33-90f7-c73efadf56eb": {"doc_hash": "348c87535a4874ebbdc2d2eb4b26753712c5467195fa63f0bda85a52303dc13b", "ref_doc_id": "4a1502c8-8b8d-43fe-9459-97798f3c3283"}, "0942c7f2-4bed-4610-8ba9-4a1003e5206f": {"doc_hash": "7e34483c368acce46627cd3863eda38ab5ced5955cbe82aff68fc49f2519d5fe", "ref_doc_id": "8d1f3851-add1-4e50-91dc-c33dbf3c7a93"}, "1e06fbfd-1dd1-4d70-8e08-6ad929bf0b77": {"doc_hash": "b06cb5c94272133a7e6c92b75bb90ebbaca78e2c1a4b8ac092a005389b9af5aa", "ref_doc_id": "8d1f3851-add1-4e50-91dc-c33dbf3c7a93"}}, "docstore/data": {"680c7591-3db1-481b-bf89-cffa94d6aaac": {"__data__": {"id_": "680c7591-3db1-481b-bf89-cffa94d6aaac", "embedding": null, "metadata": {"page_label": "1", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aef2a166-0712-45c2-bb36-09e7045e08c1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "253a7d2252555bde75fb277d02461c6dcd1bd705aaec2291e37bb3295948e354", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35d70a01-7141-44c2-9619-192647e015f7", "node_type": "1", "metadata": {}, "hash": "483efad5e851efaeffbfe4cf5851757b09e42817b47c1959e8e24b1fa329dfa7", "class_name": "RelatedNodeInfo"}}, "text": "Identifying Dogmatism in Social Media: Signals and Models\nEthan Fast and Eric Horvitz\nethaen@stanford.edu, horvitz@microsoft.com\nAbstract\nWe explore linguistic and behavioral features\nof dogmatism in social media and construct\nstatistical models that can identify dogmatic\ncomments. Our model is based on a corpus of\nReddit posts, collected across a diverse set of\nconversational topics and annotated via paid\ncrowdsourcing. We operationalize key aspects\nof dogmatism described by existing psychol-\nogy theories (such as over-con\ufb01dence), \ufb01nd-\ning they have predictive power. We also \ufb01nd\nevidence for new signals of dogmatism, such\nas the tendency of dogmatic posts to refrain\nfrom signaling cognitive processes. When we\nuse our predictive model to analyze millions\nof other Reddit posts, we \ufb01nd evidence that\nsuggests dogmatism is a deeper personality\ntrait, present for dogmatic users across many\ndifferent domains, and that users who engage\non dogmatic comments tend to show increases\nin dogmatic posts themselves.\n1 Introduction\n\u201cI\u2019m supposed to trust the opinion of a MS min-\nion? The people that produced Windows ME, Vista\nand 8? They don\u2019t even understand people, yet they\nthink they can predict the behavior of new, self-\nguiding AI?\u201d \u2013 anonymous\n\u201cI think an AI would make it easier for Patients to\ncon\ufb01de their information because by nature, a robot\ncannot judge them. Win-win? :D\u201d\u2019 \u2013 anonymous\nDogmatism describes the tendency to lay down\nopinions as incontrovertibly true, without respect for\ncon\ufb02icting evidence or the opinions of others (Ox-\nford Dictionary, 2016). Which user is more dog-\nmatic in the examples above? This question is sim-\nple for humans. Phrases like \u201cthey think\u201d and \u201ctheydon\u2019t even understand,\u201d suggest an intractability of\nopinion, while \u201cI think\u201d and \u201cwin-win?\u201d suggest\nthe opposite. Can we train computers to draw sim-\nilar distinctions? Work in psychology has called\nout many aspects of dogmatism that can be modeled\ncomputationally via natural language, such as over-\ncon\ufb01dence and strong emotions (Rokeach, 1954).\nWe present a statistical model of dogmatism that\naddresses two complementary goals. First, we val-\nidate psychological theories by examining the pre-\ndictive power of feature sets that guide the model\u2019s\npredictions. For example, do linguistic signals of\ncertainty help to predict a post is dogmatic, as the-\nory would suggest? Second, we apply our model to\nanswer four questions:\nR1: What kinds of topics (e.g., guns, LGBT) at-\ntract the highest levels of dogmatism?\nR2: How do dogmatic beliefs cluster?\nR3: How does dogmatism in\ufb02uence a conversa-\ntion on social media?\nR4: How do other user behaviors (e.g., frequency\nand breadth of posts) relate to dogmatism?\nWe train a predictive model to classify dogmatic\nposts from Reddit, one of the most popular discus-\nsion communities on the web.1Posts on Reddit cap-\nture discussion and debate across a diverse set of do-\nmains and topics \u2013 users talk about everything from\nclimate change and abortion, to world news and re-\nlationship advice, to the future of arti\ufb01cial intelli-\ngence. As a prerequisite to training our model, we\nhave created a corpus of 5,000 Reddit posts anno-\ntated with levels of dogmatism, which we are releas-\ning to share with other researchers.\n1http://www.reddit.comarXiv:1609.00425v1  [cs.CL]  1 Sep 2016", "start_char_idx": 0, "end_char_idx": 3330, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35d70a01-7141-44c2-9619-192647e015f7": {"__data__": {"id_": "35d70a01-7141-44c2-9619-192647e015f7", "embedding": null, "metadata": {"page_label": "2", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "64411d7e-6a32-4c01-b88e-2aed643b2657", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "21f133375462239d82b1ae5dee3b5d22e988a7dc1b5edd1dd93a08e679bdd595", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "680c7591-3db1-481b-bf89-cffa94d6aaac", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "253a7d2252555bde75fb277d02461c6dcd1bd705aaec2291e37bb3295948e354", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1234c90e-1b55-4931-a123-c9f0523057d1", "node_type": "1", "metadata": {}, "hash": "16859032d31deb633d3fb829b64cddd1252fabfa6c56e5eed974031ecc9f4e91", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1: We crowdsourced dogmatism labels for 5000 com-\nments. The distribution is slightly skewed towards higher lev-\nels of dogmatism. For example, crowdworkers unanimously la-\nbeled 206 comments as highly dogmatic ( 5\u00d73 = 15 ), but only\n47 as minimally dogmatic ( 1\u00d73 = 3 ).\nUsing the model, we operationalize key domain-\nindependent aspects of psychological theories of\ndogmatism drawn from the literature. We \ufb01nd these\nfeatures have predictive power that largely supports\nthe underlying theory. For example, posts that use\nless con\ufb01dent language tend to be less dogmatic.\nWe also discover evidence for new attributes of dog-\nmatism. For example, dogmatic posts tend not to\nverbalize cognition, through terms such as \u201cI think,\u201d\n\u201cpossibly,\u201d or \u201cmight be.\u201d\nOur model is trained on only 5,000 annotated\nposts, but once trained, we use it to analyze millions\nof other Reddit posts to answer our research ques-\ntions. We \ufb01nd a diverse set of topics are colored by\ndogmatic language (e.g., people are dogmatic about\nreligion, but also about LGBT issues). Further, we\n\ufb01nd some evidence for dogmatism as a deeper per-\nsonality trait \u2013 people who are strongly dogmatic\nabout one topic are more likely to express dogmatic\nviews about others as well. Finally, in conversation,\nwe discover that one user\u2019s dogmatism tends to bring\nout dogmatism in their conversational partner, form-\ning a vicious cycle.\n2 Dogmatism data\nPosts on Reddit capture debate and discussion across\na diverse set of topics, making them a natural start-\ning point for untangling domain-independent lin-\nguistic features of dogmatism.\nData collection. Subreddits are sub-communities\non Reddit oriented around speci\ufb01c interests or top-\nics, such as technology orpolitics . Sampling from\nReddit as a whole would bias the model towards themost commonly discussed content. But by sampling\nposts from individual subreddits, we can control the\nkinds of posts we use to train our model. To collect a\ndiverse training dataset, we have randomly sampled\n1000 posts each from the subreddits politics ,busi-\nness,science , and AskReddit , and 1000 additional\nposts from the Reddit frontpage. All posts in our\nsample appeared between January 2007 and March\n2015, and to control for length effects, contain be-\ntween 300 and 400 characters. This results in a total\ntraining dataset of 5000 posts.\nDogmatism annotations. Building a useful com-\nputational model requires labeled training data. We\nlabeled the Reddit dataset using crowdworkers on\nAmazon Mechanical Turk (AMT), creating the \ufb01rst\npublic corpus annotated with levels of dogmatism.\nWe asked crowdworkers to rate levels of dogmatism\non a 5-point Likert scale, as supported by similar\nannotation tasks in prior work (Danescu-Niculescu-\nMizil et al., 2013). Concretely, we gave crowdwork-\ners the following task:\nGiven a comment, imagine you hold a well-\ninformed, different opinion from the com-\nmenter in question. We\u2019d like you to tell us\nhow likely that commenter would be to engage\nyou in a constructive conversation about your\ndisagreement, where you each are able to ex-\nplore the other\u2019s beliefs. The options are:\n(5): It\u2019s unlikely you\u2019ll be able to engage in\nany substantive conversation. When you re-\nspectfully express your disagreement, they are\nlikely to ignore you or insult you or otherwise\nlower the level of discourse.\n(4): They are deeply rooted in their opinion,\nbut you are able to exchange your views with-\nout the conversation degenerating too much.\n(3): It\u2019s not likely you\u2019ll be able to change\ntheir mind, but you\u2019re easily able to talk and\nunderstand each other\u2019s point of view.\n(2): They may have a clear opinion about the\nsubject, but would likely be open to discussing\nalternative viewpoints.\n(1): They are not set in their opinion, and it\u2019s\npossible you might change their mind. If the\ncomment does not convey an opinion of any\nkind, you may also select this option.\nTo ensure quality work, we restricted the task\nto Masters workers and provided examples corre-\nsponding to each point on the scale. Including ex-\namples in a task has been shown to signi\ufb01cantly\nincrease the agreement and quality of crowdwork", "start_char_idx": 0, "end_char_idx": 4148, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1234c90e-1b55-4931-a123-c9f0523057d1": {"__data__": {"id_": "1234c90e-1b55-4931-a123-c9f0523057d1", "embedding": null, "metadata": {"page_label": "3", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fc45785-6dbf-4612-850e-f05194cafe45", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "96661caa37f563e93e709f83043a30ce6d9dfa788dc0f794b2ac6b401f3f7fd7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "35d70a01-7141-44c2-9619-192647e015f7", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "21f133375462239d82b1ae5dee3b5d22e988a7dc1b5edd1dd93a08e679bdd595", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "950ac749-b405-4e42-948c-7524f3250b28", "node_type": "1", "metadata": {}, "hash": "c34c9f2fef9ff99527d4917c136995d9b7344154fe11962967a8cdd05e8b7797", "class_name": "RelatedNodeInfo"}}, "text": "(Doroudi et al., 2016). For instance, here is an ex-\nample of a highly dogmatic (5)comment:\nI won\u2019t be happy until I see the executive\nsuite of BofA, Wells, and all the others, frog-\nmarched into waiting squad cars. It\u2019s AL-\nREADY BEEN ESTABLISHED that...\nAnd a minimally dogmatic (1)comment:\nI agree. I would like to compile a playlist for\nus trance yogi\u2019s, even if you just would like to\nexperiment with it. Is there any preference on\nwhich platform to use?\nEach comment has been annotated by three indepen-\ndent workers on AMT, which is enough to produce\nreliable results in most labeling tasks (Sheng et al.,\n2008). To compute an aggregate measure of dogma-\ntism for each comment, we summed the scores of all\nthree workers. We show the resulting distribution of\nannotations in Figure 1.\nInter-annotator agreement. To evaluate the reli-\nability of annotations we compute Krippendorff\u2019s \u03b1,\na measure of agreement designed for variable levels\nof measurement such as a Likert scale (Hayes and\nKrippendorff, 2007). An \u03b1of0indicates agreement\nindistinguishable from chance, while an \u03b1of 1 indi-\ncates perfect agreement. Across all annotations we\n\ufb01nd\u03b1= 0.44. While workers agree much more\nthan chance, clearly dogmatism is also subjective.\nIn fact, when we examine only the middle two quar-\ntiles of the dogmatism annotations, we \ufb01nd agree-\nment is no better than chance. Alternatively, when\nwe measure agreement only among the top and bot-\ntom quartiles of annotations, we \ufb01nd agreement of\n\u03b1= 0.69. This suggests comments with scores that\nare only slightly dogmatic are unreliable and often\nsubject to human disagreement. For this reason, we\nuse only the top and bottom quartiles of comments\nwhen training our model.\n3 Approaches to Identifying Dogmatism\nWe now consider strategies for identifying dog-\nmatism based on prior work in psychology. We\nstart with the Linguistic Inquiry and Word Count\n(LIWC), a lexicon popular in the social sciences\n(Pennebaker et al., 2001). LIWC provides human\nvalidated lists of words that correspond to high-\nlevel psychological categories such as certainty or\nperception . In other studies, LIWC has uncoveredlinguistic signals relating to politeness (Danescu-\nNiculescu-Mizil et al., 2013), deception (Yoo and\nGretzel, 2009), or authority in texts (Gilbert, 2012).\nHere, we examine how dogmatism relates to 17 of\nLIWC\u2019s categories (Table 1).\nTo compute the relationships between LIWC cat-\negories and dogmatism, we \ufb01rst count the relevant\ncategory terms that appear in each annotated Reddit\ncomment, normalized by its word count. We then\ncalculate odds ratios on the aggregate counts of each\nLIWC category over the top and bottom quartiles of\ndogmatic comments. As we have discussed, using\nthe top and bottom quartiles of comments provides\na more reliable signal of dogmatism. We check for\nsigni\ufb01cant differences in categories between dog-\nmatic and non-dogmatic comments using the Mann-\nWhitney U test and apply Holmes method for cor-\nrection. All odds we report in this section are signif-\nicant after correction.\nDogmatic statements tend to express a high de-\ngree of certainty (Rokeach, 1954). Here we consider\nLIWC categories that express certainty both posi-\ntively ( certainty ) and negatively ( tentativeness ). For\nexample, the word \u201calways\u201d is certain, while \u201cpossi-\nbly\u201d is tentative. Conforming to existing theory, cer-\ntainty is more associated with dogmatic comments\n(1.52 odds), while tentativeness is more associated\nwith the absence of dogmatism (0.88 odds).\nTerms used to verbalize cognition can act as a\nhedge that often characterizes non-dogmatic lan-\nguage. LIWC\u2019s insight category captures this effect\nthrough words such as \u201cthink,\u201d \u201cknow,\u201d or \u201cbelieve.\u201d\nThese words add nuance to a statement (Pennebaker\nand Francis, 1996), signaling it is the product of\nsomeone\u2019s mind (\u201c I think you should give this paper\na good review\u201d) and not meant to be interpreted as\nan objective truth.", "start_char_idx": 0, "end_char_idx": 3935, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "950ac749-b405-4e42-948c-7524f3250b28": {"__data__": {"id_": "950ac749-b405-4e42-948c-7524f3250b28", "embedding": null, "metadata": {"page_label": "3", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fc45785-6dbf-4612-850e-f05194cafe45", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "96661caa37f563e93e709f83043a30ce6d9dfa788dc0f794b2ac6b401f3f7fd7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1234c90e-1b55-4931-a123-c9f0523057d1", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "9c2122fe226d649a35614fcd4e2cb2bb25d7bc4733cf9bff853099b0e8d86e0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cf796dc6-21a2-429b-9e20-25d42565c7f0", "node_type": "1", "metadata": {}, "hash": "e618506207dfb05a681bb87e41574a24157af31a6d5e903fc2edfef2194470b6", "class_name": "RelatedNodeInfo"}}, "text": "For\nexample, the word \u201calways\u201d is certain, while \u201cpossi-\nbly\u201d is tentative. Conforming to existing theory, cer-\ntainty is more associated with dogmatic comments\n(1.52 odds), while tentativeness is more associated\nwith the absence of dogmatism (0.88 odds).\nTerms used to verbalize cognition can act as a\nhedge that often characterizes non-dogmatic lan-\nguage. LIWC\u2019s insight category captures this effect\nthrough words such as \u201cthink,\u201d \u201cknow,\u201d or \u201cbelieve.\u201d\nThese words add nuance to a statement (Pennebaker\nand Francis, 1996), signaling it is the product of\nsomeone\u2019s mind (\u201c I think you should give this paper\na good review\u201d) and not meant to be interpreted as\nan objective truth. Along these lines, we \ufb01nd the use\nof terms in the insight category is associated with\nnon-dogmatic comments (0.83 odds).\nSensory language, with its focus on description\nand detail, often signals a lack of any kind of opin-\nion, dogmatic or otherwise. LIWC\u2019s perception cat-\negory captures this idea through words associated\nwith hearing, feeling, or seeing. For example, these\nwords might occur when recounting a personal ex-\nperience (\u201cI saw his incoming \ufb01st\u201d), which even if\nemotionally charged or negative, is less likely to\nbe dogmatic. We \ufb01nd perception is associated with", "start_char_idx": 3254, "end_char_idx": 4513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cf796dc6-21a2-429b-9e20-25d42565c7f0": {"__data__": {"id_": "cf796dc6-21a2-429b-9e20-25d42565c7f0", "embedding": null, "metadata": {"page_label": "4", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "de718598-3ff9-4424-9331-9cf36d6af3e0", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "ae7e8d7a8782163093b5660792c806cb36f67bae6aff6e1d2ee2c9aa477b7819", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "950ac749-b405-4e42-948c-7524f3250b28", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "fdc99298052ea9f27f2b7973b3f5e380f159f8cb105b5d6c1c50e99c6dd72b0a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6249c657-f3fc-4c4a-9b65-e0fc63a91c1e", "node_type": "1", "metadata": {}, "hash": "5b2a21e61ff157808b749357ab39c58c28352e7f8cfed5daa1360805f1ab2a14", "class_name": "RelatedNodeInfo"}}, "text": "Strategy Odds Example\nCertainty 1.33* Be a hate monger allyou want... Your life will never truly be\nhappy though, and you will never know peace.\nTentativeness 0.88* Most are likely to be more technically advanced and, ifstill using\nradio, might very well be emitting signals we could detect\nInsight 0.83* I think stating the obvious is a necessary function. Information\nlike this is important to consider ...\nPerception 0.77* I saw four crows on that same branch, staring at the deceased.\nThesilence of the crows was deafening .\nRelativity 0.82* I\u2019ve known a number to go into shock during the procedure\nComparison 0.91 This may be more than a coincidence.\nI (pronouns) 0.68* Like Isaid, Iwant to believe the former. I\u2019mglad it worked out.\nYou (pronouns) 2.18* I don\u2019t give a fuck what youdo.You can get drink yourself to\ndeath, youcan get yourself pregnant...\nWe (pronouns) 0.96 Weneed a bigger, better, colder fridge. Wehave worked hard...\nThey (pronouns) 1.63* They want the ability to prosecute who they please.\nPast 0.69* I waswalking past and thought about asking if they needed help.\nPresent 1.11* Can I steal your organs and nutrients if I need them and you don\u2019t\nwant to give them up?\nFuture 1.06 Trump\u2019s thugs willbe pretending to be Bernie supporters and will\nset \ufb01re to Philadelphia.\nInterrogatory 1.12* Gee, where was the NY Times back in the day? Why didn\u2019t we\nhear of the Kennedys, LBJ and FDR?\nNegation 1.35* If you didn\u2019t know the woman well enough to know she didn\u2019t\ntake BC regularly, you certainly don\u2019t know her well enough to\nknow she doesn\u2019t have an std.\nNegative emotion 2.32* A prank?!? You arrogant son of a bitch\nPositive emotion 0.96 They were excellent \ufb01shermen - they built \ufb01neboats.\nTable 1: Linguistic features that capture high level psychological categories and their relationship with dogmatic comments.\nStrategy describes the psychological category. Odds describes the likelihood that a category will appear more often in a dogmatic\ncomment (e.g., dogmatic comments are 2.18 times more likely to mention you-oriented phrases). Example illustrates a comment\nthat matches the category. * indicates signi\ufb01cance ( p < 0.05) after correction with Holmes method.\nnon-dogmatic comments at 0.77 odds.\nDrawing comparisons or qualifying something as\nrelative to something else conveys a nuance that is\nabsent from traditionally dogmatic language. The\nLIWC categories comparison andrelativity capture\nthese effects through comparison words such as\n\u201cthan\u201d or \u201cas\u201d and qualifying words such as \u201cdur-\ning\u201d or \u201cwhen.\u201d For example, the statement \u201cI hate\npoliticians\u201d is more dogmatic than \u201cI hate politicians\nwhen they can\u2019t get anything done.\u2019 Relativity is as-\nsociated with non-dogmatic comments at 0.80 odds,butcomparison does not reach signi\ufb01cance.\nPronouns can be surprisingly revealing indicators\nof language: for example, signaling one\u2019s gender\nor hierarchical status in a conversation (Pennebaker,\n2011). We \ufb01nd \ufb01rst person singular pronouns are\na useful negative signal for dogmatism (0.46 odds),\nwhile second person singular pronouns (2.18 odds)\nand third person plural (1.63 odds) are a useful pos-\nitive signal. Looking across the corpus, we see Iof-\nten used with a hedge (\u201cI think\u201d or \u201cI know\u201d), while\nyouandthey tend to characterize the beliefs of oth-", "start_char_idx": 0, "end_char_idx": 3288, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6249c657-f3fc-4c4a-9b65-e0fc63a91c1e": {"__data__": {"id_": "6249c657-f3fc-4c4a-9b65-e0fc63a91c1e", "embedding": null, "metadata": {"page_label": "5", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "035e0516-a0e1-4bd8-a828-e7f4bcb9b02f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "bbf996c9910dd61858286f06c1c81300f14af24b30137950ee79a3ef9a8c8cb2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cf796dc6-21a2-429b-9e20-25d42565c7f0", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "ae7e8d7a8782163093b5660792c806cb36f67bae6aff6e1d2ee2c9aa477b7819", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2b9d3ef7-1eca-48d2-bb74-fd0d81ed5f95", "node_type": "1", "metadata": {}, "hash": "9fd76cdf9c80f50110aa60b478412e14d8f461575367dc4e4935cab5ef5f78f8", "class_name": "RelatedNodeInfo"}}, "text": "ers, often in a strongly opinionated way (\u201cyou are a\nmoron\u201d or \u201cthey are keeping us down\u201d). Other pro-\nnoun types do not show signi\ufb01cant relationships.\nLike pronouns, verb tense can reveal subtle sig-\nnals in language use, such as the tendency of medi-\ncal inpatients to focus on the past (Wolf et al., 2007).\nOn social media, comments written in the present\ntense are more likely to be oriented towards a user\u2019s\ncurrent interaction (\u201cthis isall so stupid\u201d), creating\nopportunities to signal dogmatism. Alternatively,\ncomments in the past tense are more likely to re-\nfer to outside experiences (\u201cit wasan awful party\u201d),\nspeaking less to a user\u2019s stance towards an ongoing\ndiscussion. We \ufb01nd present tense is a positive sig-\nnal for dogmatism (1.11 odds) and past tense is a\nnegative signal (0.69 odds).\nDogmatic language can be either positively or\nnegatively charged in sentiment: for example, con-\nsider the positive statement \u201c Trump is the SAVIOR\nof this country!!! \u201d or the negative statement \u201c Are\nyou REALLY that stupid?? Education is the only\nway out of this horrible mess. It\u2019s hard to imagine\nhow anyone could be so deluded. \u201d In diverse com-\nmunities, where people hold many different kinds\nof opinions, dogmatic opinions will often tend to\ncome into con\ufb02ict with one another (McCluskey and\nHmielowski, 2012), producing a greater likelihood\nof negative sentiment. Perhaps for this reason, neg-\native emotion (2.09 odds) and swearing (3.80 odds)\nare useful positive signals of dogmatism, while pos-\nitive emotion shows no signi\ufb01cant relationship.\nFinally, we \ufb01nd that interrogative language (1.12\nodds) and negation (1.35 odds) are two additional\npositive signals of dogmatism. While interrogative\nwords like \u201chow\u201d or \u201cwhat\u201d have many benign uses,\nthey disproportionately appear in our data in the\nform of rhetorical or emotionally charged questions,\nsuch as \u201chow can anyone be that dumb?\u201d\nMany of these linguistic signals are correlated\nwith each other, suggesting that dogmatism is the\ncumulative effect of many component relationships.\nFor example, consider the relatively non-dogmatic\nstatement: \u201cI think the reviewers are wrong in this\ninstance.\u201d Removing signals of insight , we have:\n\u201cthe reviewers are wrong in this instance,\u201d which\nis slightly more dogmatic. Then removing relativ-\nity, we have: \u201cthe reviewers are wrong.\u201d And \ufb01-\nnally, adding certainty , we have a dogmatic state-Classi\ufb01er In-domain Cross-domain\nBOW 0.853 0.776\nSENT 0.677 0.646\nLING 0.801 0.728\nBOW + SENT 0.860 0.783\nBOW + LING 0.881 0.791\nTable 2: The AUC scores for dogmatism classi\ufb01ers within and\nacross domains. BOW (bag-of-words) and SENT (sentiment\nsignals) are baselines, and LING uses the linguistic features\nfrom Table 1. We compute in-domain accuracy using 15-fold\ncross-validation on the Reddit dataset, and cross-domain accu-\nracy by training on Reddit and evaluating on comments on arti-\ncles from the New York Times. Chance AUC is 0.5.\nment: \u201cthe reviewers are always wrong.\u201d\n4 Predicting dogmatism\nWe now show how we can use the linguistic feature\nsets we have described to build a classi\ufb01er that pre-\ndicts dogmatism in comments. A predictive model\nfurther validates our feature sets, and also allows us\nto analyze dogmatism in millions of other Reddit\ncomments in a scalable way, with multiple uses in\nongoing, downstream analyses.\nPrediction task. Our goal is (1) to understand\nhow well we can use the strategies in Section 3\nto predict dogmatism, and (2) to test the domain-\nindependence of these strategies. First, we test the\nperformance of our model under cross-validation\nwithin the Reddit comment dataset. We then eval-\nuate the Reddit-based model on a held out corpus\nof New York Times comments annotated using the\ntechnique in Section 2. We did not refer to this sec-\nond dataset during feature construction.\nFor classi\ufb01cation, we consider two classes of\ncomments: dogmatic andnon-dogmatic .", "start_char_idx": 0, "end_char_idx": 3907, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b9d3ef7-1eca-48d2-bb74-fd0d81ed5f95": {"__data__": {"id_": "2b9d3ef7-1eca-48d2-bb74-fd0d81ed5f95", "embedding": null, "metadata": {"page_label": "5", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "035e0516-a0e1-4bd8-a828-e7f4bcb9b02f", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "bbf996c9910dd61858286f06c1c81300f14af24b30137950ee79a3ef9a8c8cb2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6249c657-f3fc-4c4a-9b65-e0fc63a91c1e", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a433ec66b36290072102942784be25bdf3a7571e2e0670a78e7598ae7f902676", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "84d9d203-41b3-4c5d-bb30-301013d3f233", "node_type": "1", "metadata": {}, "hash": "ff6c3cb54e4f91f197435d4990f2f96aa4ec3ff1e6a0b67fff2c0f4d2b5a8ad2", "class_name": "RelatedNodeInfo"}}, "text": "A predictive model\nfurther validates our feature sets, and also allows us\nto analyze dogmatism in millions of other Reddit\ncomments in a scalable way, with multiple uses in\nongoing, downstream analyses.\nPrediction task. Our goal is (1) to understand\nhow well we can use the strategies in Section 3\nto predict dogmatism, and (2) to test the domain-\nindependence of these strategies. First, we test the\nperformance of our model under cross-validation\nwithin the Reddit comment dataset. We then eval-\nuate the Reddit-based model on a held out corpus\nof New York Times comments annotated using the\ntechnique in Section 2. We did not refer to this sec-\nond dataset during feature construction.\nFor classi\ufb01cation, we consider two classes of\ncomments: dogmatic andnon-dogmatic . As in the\nprior analysis, we draw these comments from the top\nand bottom quartiles of the dogmatism distribution.\nThis means the classes are balanced, with 2,500 total\ncomments in the Reddit training data and 500 total\ncomments in the New York Times testing data.\nWe compare the predictions of logistic regression\nmodels based on unigram bag-of-words features\n(BOW), sentiment signals2(SENT), the linguistic\n2For SENT, we use normalized word counts from LIWC\u2019s\npositive and negative emotional categories.", "start_char_idx": 3136, "end_char_idx": 4412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84d9d203-41b3-4c5d-bb30-301013d3f233": {"__data__": {"id_": "84d9d203-41b3-4c5d-bb30-301013d3f233", "embedding": null, "metadata": {"page_label": "6", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3ea2d0b-aa99-4af7-b94d-e1c6f526133a", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "506096ee011514fe304af20ae9ff0f660ae786d94b6b67b9df4925f1d2f6fd27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2b9d3ef7-1eca-48d2-bb74-fd0d81ed5f95", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "3f54d7e3bd95836a19b854edc2baa7f40c93c0c1b268b06eb4f1ce0c4daa43ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f40aa89d-8ce6-4e8f-9ca8-2a861d4a9134", "node_type": "1", "metadata": {}, "hash": "3cc1d3be9836db1208dfb8698a1ddf7f73267bdcb53df7d96edd162f02ec7632", "class_name": "RelatedNodeInfo"}}, "text": "features from our earlier analyses (LING), and com-\nbinations of these features. BOW and SENT provide\nbaselines for the task. We compute BOW features\nusing term frequency-inverse document frequency\n(TF-IDF) and category-based features by normaliz-\ning counts for each category by the number of words\nin each document. The BOW classi\ufb01ers are trained\nwith regularization (L2 penalties of 1.5).\nClassi\ufb01cation results. We present classi\ufb01cation\naccuracy in Table 2. BOW shows an AUC of 0.853\nwithin Reddit and 0.776 on the held out New York\nTimes comments. The linguistic features boost clas-\nsi\ufb01cation results within Reddit (0.881) and on the\nheld out New York Times comments (0.791). While\nlinguistic signals by themselves provide strong pre-\ndictive power (0.801 AUC within domain), senti-\nment signals are much less predictive.\nThese results suggest that linguistic features in-\nspired by prior efforts in psychology are useful\nfor predicting dogmatism in practice and generalize\nacross new domains.\n5 Dogmatism in the Reddit Community\nWe now apply our dogmatism classi\ufb01er to a larger\ndataset of posts, examining how dogmatic language\nshapes the Reddit community. Concretely, we ap-\nply the BOW+LING model trained on the full Red-\ndit dataset to millions of new unannotated posts, la-\nbeling these posts with a probability of dogmatism\naccording to the classi\ufb01er (0=non-dogmatic, 1=dog-\nmatic). We then use these dogmatism annotations to\naddress four research questions.\n5.1 What subreddits have the highest and\nlowest levels of dogmatism? (R1)\nA natural starting point for analyzing dogmatism on\nReddit is to examine how it characterizes the site\u2019s\nsub-communities. For example, we might expect to\nsee that subreddits oriented around topics such as\nabortion or climate change are more dogmatic, and\nsubreddits about cooking are less so.\nTo answer this question, we randomly sample 1.6\nmillion posts from the entire Reddit community be-\ntween 2007 and 2015. We then annotate each of\nthese posts with dogmatism using our classi\ufb01er, and\ncompute the average dogmatism level for each sub-\nreddit in the sample with at least 100 posts.Highest Score Lowest Score\ncringepics 0.553 photography 0.399\nDebateAChristian 0.551 DIY 0.399\nDebateReligion 0.540 homebrewing 0.401\npolitics 0.536 cigars 0.402\nukpolitics 0.533 wicked edge 0.404\natheism 0.529 guitar 0.406\nlgbt 0.527 gamedeals 0.406\nTumblrInAction 0.524 buildapc 0.407\nislam 0.523 techsupport 0.410\nSubredditDrama 0.520 travel 0.410\nTable 3: Subreddits with the highest and lowest dogmatism\nscores. Politics and religion are common themes among the\nmost dogmatic subreddits, while hobbies (e.g., photography,\nhomebrewing, buildapc) show the least dogmatism.\nWe present the results of this analysis in Table 3.\nThe subreddits with the highest levels of dogmatism\ntend to be oriented around politics and religion ( De-\nbateAChristian orukpolitics ), while those with the\nlowest levels tend to focus on hobbies ( photogra-\nphyorhomebrewing ). The subreddit with the high-\nest average dogmatism level, cringepics , is a place\nto make fun of socially awkward messages, often\nfrom would-be romantic partners. Dogmatism here\ntends to take the form of \u201chow could someone be\nthat stupid\u201d and is directed at the subject of the post,\nas opposed to other members of the community.\nSimilarly, SubredditDrama is a community where\npeople come to talk about \ufb01ghts on the internet or\nsocial media. These \ufb01ghts are often then extended\nin discussion, for example: \u201cIf the best you can\ncome up with is that something you did was legal,\nit\u2019s probably time to own up to being an ass. \u201d The\npresence of this subreddit in our analysis provides\na further sanity check that our model is capturing a\nrobust signal of dogmatism.", "start_char_idx": 0, "end_char_idx": 3747, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f40aa89d-8ce6-4e8f-9ca8-2a861d4a9134": {"__data__": {"id_": "f40aa89d-8ce6-4e8f-9ca8-2a861d4a9134", "embedding": null, "metadata": {"page_label": "6", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3ea2d0b-aa99-4af7-b94d-e1c6f526133a", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "506096ee011514fe304af20ae9ff0f660ae786d94b6b67b9df4925f1d2f6fd27", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "84d9d203-41b3-4c5d-bb30-301013d3f233", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "f2ba9f6bc2fa3c81fab37cdf774d614a005dbbab2789c3a541e089d125f09c11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cbd6b41f-98c7-448b-9fb1-9db0c71ed4c6", "node_type": "1", "metadata": {}, "hash": "4cabaa9e7011837209661fd4fa34707b53b64ac15864468604212da371fe9252", "class_name": "RelatedNodeInfo"}}, "text": "The subreddit with the high-\nest average dogmatism level, cringepics , is a place\nto make fun of socially awkward messages, often\nfrom would-be romantic partners. Dogmatism here\ntends to take the form of \u201chow could someone be\nthat stupid\u201d and is directed at the subject of the post,\nas opposed to other members of the community.\nSimilarly, SubredditDrama is a community where\npeople come to talk about \ufb01ghts on the internet or\nsocial media. These \ufb01ghts are often then extended\nin discussion, for example: \u201cIf the best you can\ncome up with is that something you did was legal,\nit\u2019s probably time to own up to being an ass. \u201d The\npresence of this subreddit in our analysis provides\na further sanity check that our model is capturing a\nrobust signal of dogmatism.\n5.2 How do dogmatic beliefs cluster? (R2)\nDogmatism is widely considered to be a domain-\nspeci\ufb01c attitude (for example, oriented towards re-\nligion or politics) as opposed to a deeper personality\ntrait (Rokeach, 1954). Here we use Reddit as a lens\nto examine this idea more closely. Are users who\nare dogmatic about one topic likely to be dogmatic\nabout others? Do clusters of dogmatism exist around\nparticular topics? To \ufb01nd out, we examine the re-", "start_char_idx": 2987, "end_char_idx": 4197, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cbd6b41f-98c7-448b-9fb1-9db0c71ed4c6": {"__data__": {"id_": "cbd6b41f-98c7-448b-9fb1-9db0c71ed4c6", "embedding": null, "metadata": {"page_label": "7", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8662ecb-8c77-4f22-96e8-b223e930ee0c", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "275fb5af1b5a40b7773e7fc6f481512d14ded751c47c9e14f86cdccde46a340b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f40aa89d-8ce6-4e8f-9ca8-2a861d4a9134", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "770ce3139d15dcb953b90dda1bfb78dd0b5e90abf996cb260c726da375b871bf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4d3d6771-fa9f-4f38-8c0a-68a39c13baaa", "node_type": "1", "metadata": {}, "hash": "d695ff8a83b0c32bb6509dd18f6734c074a9c48b21702a02ad4ad95204f6a3f1", "class_name": "RelatedNodeInfo"}}, "text": "Libertarianism business conspiracy science Christianity lgbt\nAnarcho Capitalism Bitcoin Republican Christianity DebateAChristian feminisms\nBitcoin economy conspiritard relationship advice DebateReligion Equality\nronpaul entertainment ronpaul worldpolitics science SubredditDrama\nConservative TrueReddit collapse MensRights videos TwoXChromosomes\nAndroid socialism guns IAmA news MensRights\nukpolitics bestof worldpolitics TwoXChromosomes Libertarianism offbeat\nEquality philosophy occupywallstreet WTF atheism fffffffuuuuuuuuuuuu\nTable 4: Clusters of subreddits that share dogmatic users. For example, users who are dogmatic on the conspiracy subreddit (a\nplace to discuss conspiracy theories) are also likely to be dogmatic on guns oroccupywallstreet .\nlationships between subreddits over which individ-\nual users are dogmatic. For example, if many users\noften post dogmatic comments on both the politics\nandChristianity subreddits, but less often on world-\nnews , that would suggest politics andChristianity\nare linked per a boost in likelihood of individuals\nbeing dogmatic in both.\nWe sample 1000 Reddit users who posted at least\nonce a year between 2007 and 2015 to construct a\ncorpus of 10 million posts that constitute their entire\npost history. We then annotate these posts using the\nclassi\ufb01er and compute the average dogmatism score\nper subreddit per user. For example, one user might\nhave an average dogmatism level of 0.55 for the pol-\nitics subreddit and 0.45 for the economics subred-\ndit. Most users do not post in all subreddits, so we\ntrack only subreddits for which a user had posted at\nleast 10 times. Any subreddits with an average dog-\nmatism score higher than 0.50 we consider to be a\nuser\u2019s dogmatic subreddits. We then count all pairs\nof these dogmatic subreddits. For example, 45 users\nhave politics andtechnology among their dogmatic\nsubreddits, so we consider politics andtechnology\nas linked 45 times. We compute the mutual informa-\ntion (Church and Hanks, 1990) between these links,\nwhich gives us a measure of the subreddits that are\nmost related through dogmatism.\nWe present the results of this analysis in Table 4,\nchoosing clusters that represent a diverse set of top-\nics. For example, Libertarianism is linked through\ndogmatism to other political communities like An-\narcho Capitalism ,ronpaul , orukpolitics , as well as\nother topical subreddits like guns oreconomy . Sim-\nilarly, people who are dogmatic in the business sub-\nreddit also tend to be dogmatic in subreddits for Bit-\ncoin,socialism , and technology . Notably, when we\napply the same mutual information analysis to links\nde\ufb01ned by subreddits posted in by the same user, weFeature Direction\ntotal user posts \u2191\nproportion of posts in most active subreddit \u2191\nnumber of subreddits posted in \u2193\naverage number of posts in active articles \u2193\nTable 5: User behavioral features that are positively and nega-\ntively associated with dogmatism. \u2191means the feature is pos-\nitively predictive with dogmatism, and \u2193means the feature is\nnegatively predictive. For example, the more subreddits a user\nposts in, the less likely they are to be dogmatic. All features are\nstatistically signi\ufb01cant ( p < 0.001).\nsee dramatically different results. For example, the\nsubreddits most linked to science through user posts\nareUpliftingNews ,photoshopbattles , and \ufb01rstworl-\ndanarchist , and millionairemakers .\nFinally, we see less obvious connections between\nsubreddits that suggest some people may be dog-\nmatic by nature. For example, among the users who\nare dogmatic on politics , they are also disproportion-\nately dogmatic on unrelated subreddits such as sci-\nence (p < 0.001),technology (p < 0.001),IAmA\n(p < 0.001), and AskReddit (p < 0.05), with p-\nvalues computed under a binomial test.\n5.3 What user behaviors are predictive of\ndogmatism? (R3)\nWe have shown dogmatism is captured by many lin-\nguistic features, but can we discover other high-level\nuser behaviors that are similarly predictive?", "start_char_idx": 0, "end_char_idx": 3976, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4d3d6771-fa9f-4f38-8c0a-68a39c13baaa": {"__data__": {"id_": "4d3d6771-fa9f-4f38-8c0a-68a39c13baaa", "embedding": null, "metadata": {"page_label": "7", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d8662ecb-8c77-4f22-96e8-b223e930ee0c", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "275fb5af1b5a40b7773e7fc6f481512d14ded751c47c9e14f86cdccde46a340b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cbd6b41f-98c7-448b-9fb1-9db0c71ed4c6", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "6edefc7bcf1f78db2a857a8119fee18b61429280567c7561b07ac5f70ebd0f63", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1a5d552b-e299-4dcc-8350-4b90710e33b3", "node_type": "1", "metadata": {}, "hash": "08e71cb27b0ea730f27a272743015b4e6ddbb8df7569836587467189275fb27e", "class_name": "RelatedNodeInfo"}}, "text": "For example, the\nsubreddits most linked to science through user posts\nareUpliftingNews ,photoshopbattles , and \ufb01rstworl-\ndanarchist , and millionairemakers .\nFinally, we see less obvious connections between\nsubreddits that suggest some people may be dog-\nmatic by nature. For example, among the users who\nare dogmatic on politics , they are also disproportion-\nately dogmatic on unrelated subreddits such as sci-\nence (p < 0.001),technology (p < 0.001),IAmA\n(p < 0.001), and AskReddit (p < 0.05), with p-\nvalues computed under a binomial test.\n5.3 What user behaviors are predictive of\ndogmatism? (R3)\nWe have shown dogmatism is captured by many lin-\nguistic features, but can we discover other high-level\nuser behaviors that are similarly predictive?\nTo \ufb01nd out, we compute metrics of user behavior\nusing the data sample of 1000 users and 10 million\nposts described in Section 5.2. Speci\ufb01cally, we cal-\nculate (1) activity : a user\u2019s total number of posts, (2)\nbreadth : the number of subreddits a user has posted\nin, (3) focus : the proportion of a user\u2019s posts that\nappear in the subreddit where they are most active,\nand (4) engagement : the average number of posts a\nuser contributes to each discussion they engage in.", "start_char_idx": 3225, "end_char_idx": 4448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1a5d552b-e299-4dcc-8350-4b90710e33b3": {"__data__": {"id_": "1a5d552b-e299-4dcc-8350-4b90710e33b3", "embedding": null, "metadata": {"page_label": "8", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fca6b329-8ebf-4504-9598-7a25e9b677bc", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "d2c03a753066ce5290fef4bf3888490c752ef7c2627fa877269f880ebb547cd7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4d3d6771-fa9f-4f38-8c0a-68a39c13baaa", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "cadd06aeb5528e5a94f0ee7a917286af58cc30104e3adc5fdfe704f3da306d6a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "959781a9-7849-4b2a-b213-89f25189a72a", "node_type": "1", "metadata": {}, "hash": "199ab957e0012bba0c01b77c1d4896896870a41439ac6d733bd6fd3bdb87507e", "class_name": "RelatedNodeInfo"}}, "text": "We then \ufb01t these behavioral features to a linear re-\ngression model where we predict each user\u2019s average\ndogmatism level. Positive coef\ufb01cients in this model\nare positively predictive of dogmatism, while nega-\ntive coef\ufb01cients are negatively predictive.\nWe \ufb01nd this model is signi\ufb01cantly predicitive of\ndogmatism ( R2= 0.1,p< 0.001), with all features\nreaching statistical signi\ufb01cance ( p < 0.001).Activ-\nityandfocus are positively associated with dogma-\ntism, while breadth andengagement are negatively\nassociated (Table 5). Together, these results suggest\ndogmatic users tend to post frequently and in spe-\nci\ufb01c communities, but are not as inclined to continue\nto engage with a discussion, once it has begun.\n5.4 How does dogmatism impact a\nconversation? (R4)\nHow does interacting with a dogmatic comment im-\npact a conversation? Are users able to shrug it off?\nOr do otherwise non-dogmatic users become more\ndogmatic themselves?\nTo answer this question, we sample 600,000 con-\nversations triples from Reddit. These conversations\nconsist of two people (A and B) talking, with the\nstructure: A1 \u2192B\u2192A2. This allows us to mea-\nsure the impact of B\u2019s dogmatism on A\u2019s response,\nwhile also controlling for the dogmatism level ini-\ntially set by A. Concretely, we model the impact of\ndogmatism on these conversations through a linear\nregression. This model takes two features, the dog-\nmatism levels of A1 and B, and predicts the dogma-\ntism response of A2. If B\u2019s dogmatism has no effect\non A\u2019s response, the coef\ufb01cient that corresponds to\nB will not be signi\ufb01cant in the model. Alternatively,\nif B\u2019s dogmatism does have some effect, it will be\ncaptured by the model\u2019s coef\ufb01cient.\nWe \ufb01nd the coef\ufb01cient of the B feature in the\nmodel is positively associated with dogmatism ( p<\n0.001). In other words, engagement with a dog-\nmatic comment tends to make a user more dogmatic\nthemselves. This effect holds when we run the same\nmodel on data subsets consisting only of dogmatic\nor non-dogmatic users, and also when we conserva-\ntively remove all words used by B from A\u2019s response\n(i.e., controlling for quoting effects).6 Related Work\nIn contrast to the computational models we have pre-\nsented, dogmatism is usually measured in psychol-\nogy through survey scales, in which study partic-\nipants answer questions designed to reveal under-\nlying personality attributes (Rokeach, 1954). Over\ntime, these surveys have been updated (Shearman\nand Levine, 2006) and improved to meet standards\nof psychometric validity (Crowson, 2009).\nThese surveys are often used to study the rela-\ntionship between dogmatism and other psychologi-\ncal phenomena. For example, dogmatic people tend\nto show an increased tendency for confrontation (El-\nNawawy and Powers, 2010) or moral conviction and\nreligiosity (Swink, 2011), and less likelihood of cog-\nnitive \ufb02exibility (Martin et al., 2011), even among\nstereotypically non-dogmatic groups like atheists\n(Gurney et al., 2013). From a behavioral standpoint,\ndogmatic people solve problems differently, spend-\ning less time framing a problem and expressing more\ncertainty in their solution (Lohman, 2010). Here we\nsimilarly examine how user behaviors on Reddit re-\nlate to a language model of dogmatism.\nErtel sought to capture dogmatism linguistically,\nthough a small lexicon of words that correspond\nwith high-level concepts like certainty and compro-\nmise (1985). McKenny then used this dictionary to\nrelate dogmatism to argument quality in student es-\nsays (2005). Our work expands on this approach,\napplying supervised models based on a broader set\nof linguistic categories to identify dogmatism in text.", "start_char_idx": 0, "end_char_idx": 3629, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "959781a9-7849-4b2a-b213-89f25189a72a": {"__data__": {"id_": "959781a9-7849-4b2a-b213-89f25189a72a", "embedding": null, "metadata": {"page_label": "8", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fca6b329-8ebf-4504-9598-7a25e9b677bc", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "d2c03a753066ce5290fef4bf3888490c752ef7c2627fa877269f880ebb547cd7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1a5d552b-e299-4dcc-8350-4b90710e33b3", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "d90a6e7027bfaee6b62dbec0d68499154c33b87f2736f66fb9f0dc73e6d1eeb6", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8162bf79-863e-49f9-8d40-b0329bd68b6a", "node_type": "1", "metadata": {}, "hash": "7f80339d971615566fc4a7166cff90c3dd6b6cf44c98e9c4a6ec20b68892a61f", "class_name": "RelatedNodeInfo"}}, "text": "From a behavioral standpoint,\ndogmatic people solve problems differently, spend-\ning less time framing a problem and expressing more\ncertainty in their solution (Lohman, 2010). Here we\nsimilarly examine how user behaviors on Reddit re-\nlate to a language model of dogmatism.\nErtel sought to capture dogmatism linguistically,\nthough a small lexicon of words that correspond\nwith high-level concepts like certainty and compro-\nmise (1985). McKenny then used this dictionary to\nrelate dogmatism to argument quality in student es-\nsays (2005). Our work expands on this approach,\napplying supervised models based on a broader set\nof linguistic categories to identify dogmatism in text.\nOther researchers have studied topics similar to\ndogmatism, such as signals of cognitive style in\nright-wing political thought (Van Hiel et al., 2010),\nthe language used by trolls on social media (Cheng\net al., 2015), or what makes for impartial language\non twitter (Zafar et al., 2016). A similar \ufb02avor of\nwork has examined linguistic models that capture\npoliteness (Danescu-Niculescu-Mizil et al., 2013),\ndeception (Ott et al., 2011), and authority (Gilbert,\n2012). We took inspiration from these models when\nconstructing the feature sets in our work.\nFinally, while we examine what makes an opin-\nion dogmatic, other work has pushed further into the\nstructure of arguments, for example classifying their\njusti\ufb01cations (Hasan and Ng, 2014), or what makes\nan argument likely to win (Tan et al., 2016). Our", "start_char_idx": 2949, "end_char_idx": 4436, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8162bf79-863e-49f9-8d40-b0329bd68b6a": {"__data__": {"id_": "8162bf79-863e-49f9-8d40-b0329bd68b6a", "embedding": null, "metadata": {"page_label": "9", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e7bc5b4-0b67-4c5a-a893-268c550a3abb", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "0b4a04a119bee58a19a623664c84678fccde968f35129a08f92b88202da694a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "959781a9-7849-4b2a-b213-89f25189a72a", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "6f2a3552bffcd579f298847696223543e2cffebe5fb5f03396471f9604d1de4e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2c48808-43db-4927-a36a-d1a013f1479c", "node_type": "1", "metadata": {}, "hash": "0d9504d4e4bcf664b69970f54279c03194c004e7da3d5575d8da2287c63c6a9e", "class_name": "RelatedNodeInfo"}}, "text": "model may allow future researchers to probe these\nquestions more deeply.\n7 Conclusion\nWe have constructed the \ufb01rst corpus of social me-\ndia posts annotated with dogmatism scores, allowing\nus to explore linguistic features of dogmatism and\nbuild a predictive model that analyzes new content.\nWe apply this model to Reddit, where we discover\nbehavioral predictors of dogmatism and topical pat-\nterns in the comments of dogmatic users.\nCould we use this computational model to help\nusers shed their dogmatic beliefs? Looking forward,\nour work makes possible new avenues for encourag-\ning pro-social behavior in online communities.\nReferences\n[Cheng et al.2015] Justin Cheng, Cristian Danescu-\nNiculescu-Mizil, and Jure Leskovec. 2015. Antisocial\nbehavior in online discussion communities. arXiv\npreprint arXiv:1504.00680 .\n[Church and Hanks1990] Kenneth Ward Church and\nPatrick Hanks. 1990. Word association norms,\nmutual information, and lexicography. Computational\nlinguistics , 16(1):22\u201329.\n[Crowson2009] H Michael Crowson. 2009. Does the\ndog scale measure dogmatism? another look at con-\nstruct validity. The Journal of social psychology ,\n149(3):365\u2013383.\n[Danescu-Niculescu-Mizil et al.2013] Cristian Danescu-\nNiculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure\nLeskovec, and Christopher Potts. 2013. A computa-\ntional approach to politeness with application to social\nfactors. arXiv preprint arXiv:1306.6078 .\n[Doroudi et al.2016] Shayan Doroudi, Ece Kamar, Emma\nBrunskill, and Eric Horvitz. 2016. Toward a learning\nscience for complex crowdsourcing tasks. In Proceed-\nings of the 2016 CHI Conference on Human Factors in\nComputing Systems , pages 2623\u20132634. ACM.\n[El-Nawawy and Powers2010] Mohammed El-Nawawy\nand Shawn Powers. 2010. Al-jazeera english a con-\nciliatory medium in a con\ufb02ict-driven environment?\nGlobal Media and Communication , 6(1):61\u201384.\n[Ertel1985] S Ertel. 1985. Content analysis: An alter-\nnative approach to open and closed minds. The High\nSchool Journal , 68(4):229\u2013240.\n[Gilbert2012] Eric Gilbert. 2012. Phrases that signal\nworkplace hierarchy. In Proceedings of the ACM 2012\nconference on Computer Supported Cooperative Work ,\npages 1037\u20131046. ACM.[Gurney et al.2013] Daniel J Gurney, Shelley McKeown,\nJamie Churchyard, and Neil Howlett. 2013. Believe it\nor not: Exploring the relationship between dogmatism\nand openness within non-religious samples. Personal-\nity and Individual Differences , 55(8):936\u2013940.\n[Hasan and Ng2014] Kazi Saidul Hasan and Vincent Ng.\n2014. Why are you taking this stance? identifying and\nclassifying reasons in ideological debates. In EMNLP ,\npages 751\u2013762.\n[Hayes and Krippendorff2007] Andrew F Hayes and\nKlaus Krippendorff. 2007. Answering the call\nfor a standard reliability measure for coding data.\nCommunication methods and measures , 1(1):77\u201389.\n[Lohman2010] Margaret C Lohman. 2010. An unex-\namined triumvirate: dogmatism, problem solving, and\nhrd. Human Resource Development Review .\n[Martin et al.2011] Matthew M Martin, Sydney M Stag-\ngers, and Carolyn M Anderson. 2011. The relation-\nships between cognitive \ufb02exibility with dogmatism,\nintellectual \ufb02exibility, preference for consistency, and\nself-compassion. Communication Research Reports ,\n28(3):275\u2013280.\n[McCluskey and Hmielowski2012] Michael McCluskey\nand Jay Hmielowski. 2012. Opinion expression dur-\ning social con\ufb02ict: Comparing online reader com-\nments and letters to the editor. Journalism , 13(3):303\u2013\n319.", "start_char_idx": 0, "end_char_idx": 3430, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2c48808-43db-4927-a36a-d1a013f1479c": {"__data__": {"id_": "f2c48808-43db-4927-a36a-d1a013f1479c", "embedding": null, "metadata": {"page_label": "9", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e7bc5b4-0b67-4c5a-a893-268c550a3abb", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "0b4a04a119bee58a19a623664c84678fccde968f35129a08f92b88202da694a5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8162bf79-863e-49f9-8d40-b0329bd68b6a", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a90619a32620ca23911f9fb1c5df92cee7c4ab6ec4e122786a201ff7030662d8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ef6d38f-9c00-4c55-993b-fa5bef23c4a2", "node_type": "1", "metadata": {}, "hash": "ed4e8213c80a107672dc3df513e37f4f32ce224fe36823414de7d22c9c1b9158", "class_name": "RelatedNodeInfo"}}, "text": "[Lohman2010] Margaret C Lohman. 2010. An unex-\namined triumvirate: dogmatism, problem solving, and\nhrd. Human Resource Development Review .\n[Martin et al.2011] Matthew M Martin, Sydney M Stag-\ngers, and Carolyn M Anderson. 2011. The relation-\nships between cognitive \ufb02exibility with dogmatism,\nintellectual \ufb02exibility, preference for consistency, and\nself-compassion. Communication Research Reports ,\n28(3):275\u2013280.\n[McCluskey and Hmielowski2012] Michael McCluskey\nand Jay Hmielowski. 2012. Opinion expression dur-\ning social con\ufb02ict: Comparing online reader com-\nments and letters to the editor. Journalism , 13(3):303\u2013\n319.\n[McKenny2005] John McKenny. 2005. Content analy-\nsis of dogmatism compared with corpus analysis of\nepistemic stance in student essays. Information De-\nsign Journal & Document Design , 13(1).\n[Ott et al.2011] Myle Ott, Yejin Choi, Claire Cardie, and\nJeffrey T Hancock. 2011. Finding deceptive opin-\nion spam by any stretch of the imagination. In Pro-\nceedings of the 49th Annual Meeting of the Associa-\ntion for Computational Linguistics: Human Language\nTechnologies-Volume 1 , pages 309\u2013319. Association\nfor Computational Linguistics.\n[Oxford Dictionary2016] English Oxford Dictionary.\n2016. De\ufb01nition of dogmatism.\n[Pennebaker and Francis1996] James W Pennebaker and\nMartha E Francis. 1996. Cognitive, emotional, and\nlanguage processes in disclosure. Cognition & Emo-\ntion, 10(6):601\u2013626.\n[Pennebaker et al.2001] James W Pennebaker, Martha E\nFrancis, and Roger J Booth. 2001. Linguistic inquiry\nand word count: Liwc 2001. Mahway: Lawrence Erl-\nbaum Associates , 71:2001.\n[Pennebaker2011] James W Pennebaker. 2011. The se-\ncret life of pronouns. New Scientist , 211(2828):42\u201345.\n[Rokeach1954] Milton Rokeach. 1954. The nature and\nmeaning of dogmatism.", "start_char_idx": 2805, "end_char_idx": 4582, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ef6d38f-9c00-4c55-993b-fa5bef23c4a2": {"__data__": {"id_": "2ef6d38f-9c00-4c55-993b-fa5bef23c4a2", "embedding": null, "metadata": {"page_label": "10", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5e98a887-73f9-4e55-a4ca-96e985805783", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "1d3562eeb507abc2065617edbbbe501a76bd14ed486b0e2b088a8d5377bdc54a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f2c48808-43db-4927-a36a-d1a013f1479c", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "c846b183ed7dd4bc0bc7ebfb1ebd3a5c1355bdb2a2930ea2adf1dee32b97bf54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b99d716-ecdf-4e0f-968b-85a3f7535f64", "node_type": "1", "metadata": {}, "hash": "205d94cc751f62aaa0175944da9157acb8a48dc5fefdce17b4eecd1e4cc70427", "class_name": "RelatedNodeInfo"}}, "text": "[Shearman and Levine2006] Sachiyo M Shearman and\nTimothy R Levine. 2006. Dogmatism updated: A\nscale revision and validation. Communication Quar-\nterly, 54(3):275\u2013291.\n[Sheng et al.2008] Victor S Sheng, Foster Provost, and\nPanagiotis G Ipeirotis. 2008. Get another label? im-\nproving data quality and data mining using multiple,\nnoisy labelers. Proceedings of the 14th ACM SIGKDD\ninternational conference on Knowledge discovery and\ndata mining , pages 614\u2013622.\n[Swink2011] Nathan Swink. 2011. Dogmatism and\nmoral conviction in individuals: Injustice for all.\n[Tan et al.2016] Chenhao Tan, Vlad Niculae, Cristian\nDanescu-Niculescu-Mizil, and Lillian Lee. 2016.\nWinning arguments: Interaction dynamics and persua-\nsion strategies in good-faith online discussions. In\nProceedings of WWW .\n[Van Hiel et al.2010] Alain Van Hiel, Emma Onraet, and\nSarah De Pauw. 2010. The relationship between\nsocial-cultural attitudes and behavioral measures of\ncognitive style: A meta-analytic integration of studies.\nJournal of personality , 78(6):1765\u20131800.\n[Wolf et al.2007] Markus Wolf, Jan Sedway, Cynthia M\nBulik, and Hans Kordy. 2007. Linguistic analyses of\nnatural written language: Unobtrusive assessment of\ncognitive style in eating disorders. International Jour-\nnal of Eating Disorders , 40(8):711\u2013717.\n[Yoo and Gretzel2009] Kyung-Hyan Yoo and Ulrike\nGretzel. 2009. Comparison of deceptive and truthful\ntravel reviews. Information and communication\ntechnologies in tourism 2009 , pages 37\u201347.\n[Zafar et al.2016] Muhammad Bilal Zafar, Krishna P\nGummadi, and Cristian Danescu-Niculescu-Mizil.\n2016. Message impartiality in social media discus-\nsions. In Tenth International AAAI Conference on Web\nand Social Media .", "start_char_idx": 0, "end_char_idx": 1703, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b99d716-ecdf-4e0f-968b-85a3f7535f64": {"__data__": {"id_": "3b99d716-ecdf-4e0f-968b-85a3f7535f64", "embedding": null, "metadata": {"page_label": "1", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "31165a1b-a78a-4f9e-bc74-e87582a28c61", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "ea7be0664a49b7b1602758f1bc5cee2610d2e8f105241f858013174894661e5c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2ef6d38f-9c00-4c55-993b-fa5bef23c4a2", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "1d3562eeb507abc2065617edbbbe501a76bd14ed486b0e2b088a8d5377bdc54a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7688cb58-efb8-4253-b488-d6a6acb150f4", "node_type": "1", "metadata": {}, "hash": "3749b9fc0be1037b1abd143e2aff4788efc6a1b68d7d46940a84eab1f4ec1d9f", "class_name": "RelatedNodeInfo"}}, "text": "Improved Neural Relation Detection for Knowledge Base Question\nAnswering\nMo Yu\u2020Wenpeng Yin\u22c6Kazi Saidul Hasan\u2021Cicero dos Santos\u2020\nBing Xiang\u2021Bowen Zhou\u2020\n\u2020AI Foundations, IBM Research, USA\n\u22c6Center for Information and Language Processing, LMU Munich\n\u2021IBM Watson, USA\n{yum,kshasan,cicerons,bingxia,zhou }@us.ibm.com, wenpeng@cis.lmu.de\nAbstract\nRelation detection is a core component of\nmany NLP applications including Knowl-\nedge Base Question Answering (KBQA).\nIn this paper, we propose a hierarchi-\ncal recurrent neural network enhanced by\nresidual learning which detects KB re-\nlations given an input question. Our\nmethod uses deep residual bidirectional\nLSTMs to compare questions and rela-\ntion names via different levels of abstrac-\ntion. Additionally, we propose a sim-\nple KBQA system that integrates entity\nlinking and our proposed relation detec-\ntor to make the two components enhance\neach other. Our experimental results show\nthat our approach not only achieves out-\nstanding relation detection performance,\nbut more importantly, it helps our KBQA\nsystem achieve state-of-the-art accuracy\nfor both single-relation (SimpleQuestions)\nand multi-relation (WebQSP) QA bench-\nmarks.\n1 Introduction\nKnowledge Base Question Answering (KBQA)\nsystems answer questions by obtaining informa-\ntion from KB tuples (Berant et al., 2013; Yao et al.,\n2014; Bordes et al., 2015; Bast and Haussmann,\n2015; Yih et al., 2015; Xu et al., 2016). For an\ninput question, these systems typically generate a\nKB query, which can be executed to retrieve the\nanswers from a KB. Figure 1 illustrates the process\nused to parse two sample questions in a KBQA\nsystem: (a) a single-relation question, which can\nbe answered with a single <head-entity, relation,\ntail-entity>KB tuple (Fader et al., 2013; Yih et al.,\n2014; Bordes et al., 2015); and (b) a more complex\ncase, where some constraints need to be handledfor multiple entities in the question. The KBQA\nsystem in the \ufb01gure performs two key tasks: (1)\nentity linking , which links n-grams in questions\nto KB entities, and (2) relation detection , which\nidenti\ufb01es the KB relation(s) a question refers to.\nThe main focus of this work is to improve the\nrelation detection subtask and further explore how\nit can contribute to the KBQA system. Although\ngeneral relation detection1methods are well stud-\nied in the NLP community, such studies usually\ndo not take the end task of KBQA into considera-\ntion. As a result, there is a signi\ufb01cant gap between\ngeneral relation detection studies and KB-speci\ufb01c\nrelation detection. First, in most general relation\ndetection tasks, the number of target relations is\nlimited, normally smaller than 100. In contrast, in\nKBQA even a small KB, like Freebase2M (Bor-\ndes et al., 2015), contains more than 6,000 relation\ntypes. Second, relation detection for KBQA often\nbecomes a zero-shot learning task, since some test\ninstances may have unseen relations in the training\ndata. For example, the SimpleQuestions (Bordes\net al., 2015) data set has 14% of the golden test\nrelations not observed in golden training tuples.\nThird, as shown in Figure 1(b), for some KBQA\ntasks like WebQuestions (Berant et al., 2013), we\nneed to predict a chain of relations instead of a\nsingle relation. This increases the number of tar-\nget relation types and the sizes of candidate rela-\ntion pools, further increasing the dif\ufb01culty of KB\nrelation detection. Owing to these reasons, KB re-\nlation detection is signi\ufb01cantly more challenging\ncompared to general relation detection tasks.\nThis paper improves KB relation detection to\ncope with the problems mentioned above. First, in\norder to deal with the unseen relations, we propose\nto break the relation names into word sequences\nfor question-relation matching. Second, noticing\n1In the information extraction \ufb01eld such tasks are usually\ncalled relation extraction orrelation classi\ufb01cation .arXiv:1704.06194v2  [cs.CL]  27 May 2017", "start_char_idx": 0, "end_char_idx": 3921, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7688cb58-efb8-4253-b488-d6a6acb150f4": {"__data__": {"id_": "7688cb58-efb8-4253-b488-d6a6acb150f4", "embedding": null, "metadata": {"page_label": "2", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cdb55b56-a520-4e68-a7f3-a410dabf27d4", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "6744922e00dba0eaf3f7b724fd20b59f2b5839a1279a74fbcffe50ba7102b62c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b99d716-ecdf-4e0f-968b-85a3f7535f64", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "ea7be0664a49b7b1602758f1bc5cee2610d2e8f105241f858013174894661e5c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2dbf0002-0e09-4c42-b9a9-133a614a3409", "node_type": "1", "metadata": {}, "hash": "1c2ba49f9f3e2cecd729a85d08595541ba758dc283c6374e1035a1d57d80303c", "class_name": "RelatedNodeInfo"}}, "text": "Question: what episode was mike kelleythe writer of\nKnowledgeBaseMike\tKelley(American\ttelevision\twriter/producer)Mike\tKelley(American\tbaseball\tplayer)\u2026Entity Linking \nLove\tWill\tFind\ta\tWayUSA\u2026First\tbaseman\u2026episodes_written\nposition_playedRelation Detection(a)(b)Question: what tvshow did grantshowplay onin2008Mike\tKelley?episodes_writtenEntity Linking RelationDetectionGrantShow?starring_rolesseries(date)from2008ConstraintDetectionGrantShow\t(American\tactor)SwingTown\nBigLoveepisodesScoundrelsseries2011from20102008Figure 1: KBQA examples and its three key components. (a) A single relation example. We \ufb01rst identify the topic entity with\nentity linking and then detect the relation asked by the question with relation detection (from all relations connecting the topic\nentity). Based on the detected entity and relation, we form a query to search the KB for the correct answer \u201c Love Will Find a\nWay\u201d. (b) A more complex question containing two entities. By using \u201c Grant Show \u201d as the topic entity, we could detect a chain\nof relations \u201c starring roles-series \u201d pointing to the answer. An additional constraint detection takes the other entity \u201c 2008 \u201d as\na constraint, to \ufb01lter the correct answer \u201c SwingTown \u201d from all candidates found by the topic entity and relation.\nthat original relation names can sometimes help\nto match longer question contexts, we propose\nto build both relation-level and word-level rela-\ntion representations. Third, we use deep bidirec-\ntional LSTMs ( BiLSTM s) to learn different levels\nof question representations in order to match the\ndifferent levels of relation information. Finally, we\npropose a residual learning method for sequence\nmatching, which makes the model training easier\nand results in more abstract (deeper) question rep-\nresentations, thus improves hierarchical matching.\nIn order to assess how the proposed improved\nrelation detection could bene\ufb01t the KBQA end\ntask, we also propose a simple KBQA implemen-\ntation composed of two-step relation detection .\nGiven an input question and a set of candidate enti-\nties retrieved by an entity linker based on the ques-\ntion, our proposed relation detection model plays a\nkey role in the KBQA process: (1) Re-ranking the\nentity candidates according to whether they con-\nnect to high con\ufb01dent relations detected from the\nraw question text by the relation detection model.\nThis step is important to deal with the ambigui-\nties normally present in entity linking results. (2)\nFinding the core relation (chains) for each topic\nentity2selection from a much smaller candidate\nentity set after re-ranking. The above steps are\nfollowed by an optional constraint detection step,\nwhen the question cannot be answered by single\nrelations (e.g., multiple entities in the question).\nFinally the highest scored query from the above\n2Following Yih et al. (2015), here topic entity refers to\nthe root of the (directed) query tree; and core-chain is the\ndirected path of relation from root to the answer node.steps is used to query the KB for answers.\nOur main contributions include: (i) An im-\nproved relation detection model by hierarchical\nmatching between questions and relations with\nresidual learning; (ii) We demonstrate that the im-\nproved relation detector enables our simple KBQA\nsystem to achieve state-of-the-art results on both\nsingle-relation and multi-relation KBQA tasks.\n2 Related Work\nRelation Extraction Relation extraction (RE) is\nan important sub-\ufb01eld of information extraction.\nGeneral research in this \ufb01eld usually works on a\n(small) pre-de\ufb01ned relation set, where given a text\nparagraph and two target entities, the goal is to\ndetermine whether the text indicates any types of\nrelations between the entities or not. As a result\nRE is usually formulated as a classi\ufb01cation task .\nTraditional RE methods rely on large amount of\nhand-crafted features (Zhou et al., 2005; Rink and\nHarabagiu, 2010; Sun et al., 2011).", "start_char_idx": 0, "end_char_idx": 3913, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2dbf0002-0e09-4c42-b9a9-133a614a3409": {"__data__": {"id_": "2dbf0002-0e09-4c42-b9a9-133a614a3409", "embedding": null, "metadata": {"page_label": "2", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cdb55b56-a520-4e68-a7f3-a410dabf27d4", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "6744922e00dba0eaf3f7b724fd20b59f2b5839a1279a74fbcffe50ba7102b62c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7688cb58-efb8-4253-b488-d6a6acb150f4", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a160715f7685af5d7b044ff6dc16e1e768796fed3b992139dd752f311e0a346a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4e912775-8bfe-438d-a92a-a411df0aef65", "node_type": "1", "metadata": {}, "hash": "30a5a396065ddc286def3d7c29bc5be9b4d78b40b2d03730d0a010f4c8a2a626", "class_name": "RelatedNodeInfo"}}, "text": "2 Related Work\nRelation Extraction Relation extraction (RE) is\nan important sub-\ufb01eld of information extraction.\nGeneral research in this \ufb01eld usually works on a\n(small) pre-de\ufb01ned relation set, where given a text\nparagraph and two target entities, the goal is to\ndetermine whether the text indicates any types of\nrelations between the entities or not. As a result\nRE is usually formulated as a classi\ufb01cation task .\nTraditional RE methods rely on large amount of\nhand-crafted features (Zhou et al., 2005; Rink and\nHarabagiu, 2010; Sun et al., 2011). Recent re-\nsearch bene\ufb01ts a lot from the advancement of deep\nlearning: from word embeddings (Nguyen and Gr-\nishman, 2014; Gormley et al., 2015) to deep net-\nworks like CNNs and LSTMs (Zeng et al., 2014;\ndos Santos et al., 2015; Vu et al., 2016) and atten-\ntion models (Zhou et al., 2016; Wang et al., 2016).\nThe above research assumes there is a \ufb01xed\n(closed) set of relation types, thus no zero-shot\nlearning capability is required. The number\nof relations is usually not large: The widely\nused ACE2005 has 11/32 coarse/\ufb01ne-grained rela-\ntions; SemEval2010 Task8 has 19 relations; TAC-", "start_char_idx": 3365, "end_char_idx": 4500, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e912775-8bfe-438d-a92a-a411df0aef65": {"__data__": {"id_": "4e912775-8bfe-438d-a92a-a411df0aef65", "embedding": null, "metadata": {"page_label": "3", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2797e83c-4e17-4161-b0e8-4670879fd46d", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a9a90dd247dbf0ec9575d99e9facbe36965f3ab97cd30c1a98b52bde1e986ebb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2dbf0002-0e09-4c42-b9a9-133a614a3409", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "1049093f22f77219759c56ba8fa5f5befb1b538285bdf33064072590e6b0c7c2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "007664a8-83f5-4530-b6d1-d4fd70c0488e", "node_type": "1", "metadata": {}, "hash": "1e0dbac22a64a16203aafa51fc2beea431dd35c4d5357c61e7165e7e4454fe9b", "class_name": "RelatedNodeInfo"}}, "text": "KBP2015 has 74 relations although it considers\nopen-domain Wikipedia relations. All are much\nfewer than thousands of relations in KBQA. As a\nresult, few work in this \ufb01eld focuses on dealing\nwith large number of relations or unseen relations.\nYu et al. (2016) proposed to use relation embed-\ndings in a low-rank tensor method. However their\nrelation embeddings are still trained in supervised\nway and the number of relations is not large in the\nexperiments.\nRelation Detection in KBQA Systems Rela-\ntion detection for KBQA also starts with feature-\nrich approaches (Yao and Van Durme, 2014; Bast\nand Haussmann, 2015) towards usages of deep\nnetworks (Yih et al., 2015; Xu et al., 2016; Dai\net al., 2016) and attention models (Yin et al., 2016;\nGolub and He, 2016). Many of the above re-\nlation detection research could naturally support\nlarge relation vocabulary and open relation sets\n(especially for QA with OpenIE KB like ParaLex\n(Fader et al., 2013)), in order to \ufb01t the goal of\nopen-domain question answering.\nDifferent KBQA data sets have different levels\nof requirement about the above open-domain ca-\npacity. For example, most of the gold test relations\nin WebQuestions can be observed during train-\ning, thus some prior work on this task adopted the\nclose domain assumption like in the general RE re-\nsearch. While for data sets like SimpleQuestions\nand ParaLex, the capacity to support large relation\nsets and unseen relations becomes more necessary.\nTo the end, there are two main solutions: (1) use\npre-trained relation embeddings (e.g. from TransE\n(Bordes et al., 2013)), like (Dai et al., 2016); (2)\nfactorize the relation names to sequences and for-\nmulate relation detection as a sequence match-\ning and ranking task. Such factorization works\nbecause that the relation names usually comprise\nmeaningful word sequences. For example, Yin\net al. (2016) split relations to word sequences for\nsingle-relation detection. Liang et al. (2016) also\nachieve good performance on WebQSP with word-\nlevel relation representation in an end-to-end neu-\nral programmer model. Yih et al. (2015) use char-\nacter tri-grams as inputs on both question and rela-\ntion sides. Golub and He (2016) propose a gener-\native framework for single-relation KBQA which\npredicts relation with a character-level sequence-\nto-sequence model.\nAnother difference between relation detection\nin KBQA and general RE is that general RE re-search assumes that the two argument entities\nare both available. Thus it usually bene\ufb01ts from\nfeatures (Nguyen and Grishman, 2014; Gormley\net al., 2015) or attention mechanisms (Wang et al.,\n2016) based on the entity information (e.g. entity\ntypes or entity embeddings). For relation detec-\ntion in KBQA, such information is mostly missing\nbecause: (1) one question usually contains single\nargument (the topic entity) and (2) one KB entity\ncould have multiple types (type vocabulary size\nlarger than 1,500). This makes KB entity typing\nitself a dif\ufb01cult problem so no previous used en-\ntity information in the relation detection model.3\n3 Background: Different Granularity in\nKB Relations\nPrevious research (Yih et al., 2015; Yin et al.,\n2016) formulates KB relation detection as a se-\nquence matching problem. However, while the\nquestions are natural word sequences, how to rep-\nresent relations as sequences remains a challeng-\ning problem. Here we give an overview of two\ntypes of relation sequence representations com-\nmonly used in previous work.\n(1) Relation Name as a Single Token (relation-\nlevel). In this case, each relation name is treated\nas a unique token. The problem with this ap-\nproach is that it suffers from the low relation cov-\nerage due to limited amount of training data, thus\ncannot generalize well to large number of open-\ndomain relations.", "start_char_idx": 0, "end_char_idx": 3778, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "007664a8-83f5-4530-b6d1-d4fd70c0488e": {"__data__": {"id_": "007664a8-83f5-4530-b6d1-d4fd70c0488e", "embedding": null, "metadata": {"page_label": "3", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2797e83c-4e17-4161-b0e8-4670879fd46d", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a9a90dd247dbf0ec9575d99e9facbe36965f3ab97cd30c1a98b52bde1e986ebb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4e912775-8bfe-438d-a92a-a411df0aef65", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "0804163e59fad7a3c96a5db867ff72d408800dd48b6381077d2a87cfdebdc2cf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8dac8b99-7844-4832-8190-64046c95b28f", "node_type": "1", "metadata": {}, "hash": "9770cb0cc5b199339079559a9ba38b49b37556c8aba0e0114f4eedf37f91cb29", "class_name": "RelatedNodeInfo"}}, "text": "This makes KB entity typing\nitself a dif\ufb01cult problem so no previous used en-\ntity information in the relation detection model.3\n3 Background: Different Granularity in\nKB Relations\nPrevious research (Yih et al., 2015; Yin et al.,\n2016) formulates KB relation detection as a se-\nquence matching problem. However, while the\nquestions are natural word sequences, how to rep-\nresent relations as sequences remains a challeng-\ning problem. Here we give an overview of two\ntypes of relation sequence representations com-\nmonly used in previous work.\n(1) Relation Name as a Single Token (relation-\nlevel). In this case, each relation name is treated\nas a unique token. The problem with this ap-\nproach is that it suffers from the low relation cov-\nerage due to limited amount of training data, thus\ncannot generalize well to large number of open-\ndomain relations. For example, in Figure 1, when\ntreating relation names as single tokens, it will be\ndif\ufb01cult to match the questions to relation names\n\u201cepisodes written \u201d and \u201c starring roles \u201d if these\nnames do not appear in training data \u2013 their rela-\ntion embeddings hrs will be random vectors thus\nare not comparable to question embeddings hqs.\n(2) Relation as Word Sequence (word-level ). In\nthis case, the relation is treated as a sequence of\nwords from the tokenized relation name. It has\nbetter generalization, but suffers from the lack\nof global information from the original relation\nnames. For example in Figure 1(b), when doing\nonly word-level matching, it is dif\ufb01cult to rank the\ntarget relation \u201c starring roles \u201d higher compared\nto the incorrect relation \u201c plays produced \u201d. This\nis because the incorrect relation contains word\n\u201cplays \u201d, which is more similar to the question\n3Such entity information has been used in KBQA systems\nas features for the \ufb01nal answer re-rankers.", "start_char_idx": 2921, "end_char_idx": 4751, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8dac8b99-7844-4832-8190-64046c95b28f": {"__data__": {"id_": "8dac8b99-7844-4832-8190-64046c95b28f", "embedding": null, "metadata": {"page_label": "4", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "32caede3-2d9c-4c3f-a2b8-ef9608a8efda", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "0152c2b3390b663d9c9eaaa253fff1cb38f31df021da3eebe895c192b9e3a7a6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "007664a8-83f5-4530-b6d1-d4fd70c0488e", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "f2e08066463632cc42207851edfffc2938ac237b218ddf22e5dfc6d64c141597", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "311df028-ffba-48d2-87eb-7544267a4b83", "node_type": "1", "metadata": {}, "hash": "3480dd02d61e547f2f456dfa355bd0ce8659858980752de94d9fc7d85d0af613", "class_name": "RelatedNodeInfo"}}, "text": "Relation TokenQuestion 1 Question 2\nwhat tv episodes were <e>the writer of what episode was written by <e>\nrelation-level episodes written tv episodes were <e>the writer of episode was written by <e>\nword-levelepisodes tv episodes episode\nwritten the writer of written\nTable 1: An example of KB relation ( episodes written ) with two types of relation tokens (relation names\nand words), and two questions asking this relation. The topic entity is replaced with token <e>which\ncould give the position information to the deep networks. The italics show the evidence phrase for each\nrelation token in the question.\n(containing word \u201c play\u201d) in the embedding space.\nOn the other hand, if the target relation co-occurs\nwith questions related to \u201c tv appearance \u201d in train-\ning, by treating the whole relation as a token (i.e.\nrelation id), we could better learn the correspon-\ndence between this token and phrases like \u201c tv\nshow \u201d and \u201c play on \u201d.\nThe two types of relation representation con-\ntain different levels of abstraction. As shown\nin Table 1, the word-level focuses more on lo-\ncal information (words and short phrases), and\nthe relation-level focus more on global informa-\ntion (long phrases and skip-grams) but suffer from\ndata sparsity. Since both these levels of granu-\nlarity have their own pros and cons, we propose\na hierarchical matching approach for KB relation\ndetection: for a candidate relation, our approach\nmatches the input question to both word-level and\nrelation-level representations to get the \ufb01nal rank-\ning score. Section 4 gives the details of our pro-\nposed approach.\n4 Improved KB Relation Detection\nThis section describes our hierarchical sequence\nmatching with residual learning approach for rela-\ntion detection. In order to match the question to\ndifferent aspects of a relation (with different ab-\nstraction levels), we deal with three problems as\nfollows on learning question/relation representa-\ntions.\n4.1 Relation Representations from Different\nGranularity\nWe provide our model with both types of re-\nlation representation: word-level and relation-\nlevel. Therefore, the input relation becomes r=\n{rword\n1,\u00b7\u00b7\u00b7,rword\nM1}\u222a{rrel\n1,\u00b7\u00b7\u00b7,rrel\nM2}, where the\n\ufb01rstM1tokens are words (e.g. {episode, writ-\nten}), and the last M2tokens are relation names,\ne.g.,{episode written}or{starring roles, series}\n(when the target is a chain like in Figure 1(b)).\nWe transform each token above to its word embed-ding then use two BiLSTMs (with shared parame-\nters) to get their hidden representations [Bword\n1:M1:\nBrel\n1:M2](each row vector \u03b2iis the concatena-\ntion between forward/backward representations at\ni). We initialize the relation sequence LSTMs\nwith the \ufb01nal state representations of the word se-\nquence, as a back-off for unseen relations. We ap-\nplyonemax-pooling on these two sets of vectors\nand get the \ufb01nal relation representation hr.\n4.2 Different Abstractions of Questions\nRepresentations\nFrom Table 1, we can see that different parts of a\nrelation could match different contexts of question\ntexts. Usually relation names could match longer\nphrases in the question and relation words could\nmatch short phrases. Yet different words might\nmatch phrases of different lengths.\nAs a result, we hope the question representa-\ntions could also comprise vectors that summa-\nrize various lengths of phrase information (differ-\nent levels of abstraction), in order to match rela-\ntion representations of different granularity. We\ndeal with this problem by applying deep BiL-\nSTMs on questions. The \ufb01rst-layer of BiLSTM\nworks on the word embeddings of question words\nq={q1,\u00b7\u00b7\u00b7,qN}and gets hidden representations\n\u0393(1)\n1:N= [\u03b3(1)\n1;\u00b7\u00b7\u00b7;\u03b3(1)\nN]. The second-layer BiL-\nSTM works on \u0393(1)\n1:Nto get the second set of hid-\nden representations \u0393(2)\n1:N. Since the second BiL-\nSTM starts with the hidden vectors from the \ufb01rst\nlayer, intuitively it could learn more general and\nabstract information compared to the \ufb01rst layer.", "start_char_idx": 0, "end_char_idx": 3940, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "311df028-ffba-48d2-87eb-7544267a4b83": {"__data__": {"id_": "311df028-ffba-48d2-87eb-7544267a4b83", "embedding": null, "metadata": {"page_label": "4", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "32caede3-2d9c-4c3f-a2b8-ef9608a8efda", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "0152c2b3390b663d9c9eaaa253fff1cb38f31df021da3eebe895c192b9e3a7a6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8dac8b99-7844-4832-8190-64046c95b28f", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "89f318de3122455af7d0604ebe6aba1e391efd927dd55ef1d3a21f70d36744c7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7a7412b-fd26-4785-88ae-5d4f1ce3d9d1", "node_type": "1", "metadata": {}, "hash": "16787a9b5bef8e97cae5bde87dcd92c7c2f4f8e1d5795836df9dd45db8a9a4ca", "class_name": "RelatedNodeInfo"}}, "text": "As a result, we hope the question representa-\ntions could also comprise vectors that summa-\nrize various lengths of phrase information (differ-\nent levels of abstraction), in order to match rela-\ntion representations of different granularity. We\ndeal with this problem by applying deep BiL-\nSTMs on questions. The \ufb01rst-layer of BiLSTM\nworks on the word embeddings of question words\nq={q1,\u00b7\u00b7\u00b7,qN}and gets hidden representations\n\u0393(1)\n1:N= [\u03b3(1)\n1;\u00b7\u00b7\u00b7;\u03b3(1)\nN]. The second-layer BiL-\nSTM works on \u0393(1)\n1:Nto get the second set of hid-\nden representations \u0393(2)\n1:N. Since the second BiL-\nSTM starts with the hidden vectors from the \ufb01rst\nlayer, intuitively it could learn more general and\nabstract information compared to the \ufb01rst layer.\nNote that the \ufb01rst(second)-layer of question rep-\nresentations does not necessarily correspond to the\nword(relation)-level relation representations, in-\nstead either layer of question representations could\npotentially match to either level of relation repre-\nsentations. This raises the dif\ufb01culty of matching\nbetween different levels of relation/question rep-\nresentations; the following section gives our pro-\nposal to deal with such problem.", "start_char_idx": 3209, "end_char_idx": 4384, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7a7412b-fd26-4785-88ae-5d4f1ce3d9d1": {"__data__": {"id_": "e7a7412b-fd26-4785-88ae-5d4f1ce3d9d1", "embedding": null, "metadata": {"page_label": "5", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "92db31fd-64b7-4603-9417-8743b5e0ce44", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "1d2d58f6b49d40199104a17b389efc3c29692755ce3e8abc854b8677c7b04cae", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "311df028-ffba-48d2-87eb-7544267a4b83", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a68f1c1323ad60b34c83661189265bbc69d596d1c824e7faa7bc9257a71c8b42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3b072c93-b875-4d91-8399-67e5d39ce8b7", "node_type": "1", "metadata": {}, "hash": "25b5b80f2ecdaaada30cbb4ab77a73ec98fc34d5d6e340a2682129e7a8c36e2f", "class_name": "RelatedNodeInfo"}}, "text": "\u2026\u2026\u2026max-pooling\nmax-poolingQuestionRepresentationRelationRepresentation(cosinesimilarity)\nShortcutconnectionsPoint-wisesummationBi-LSTM 2Bi-LSTM 1\ud835\udf38\ud835\udc8a\ud835\udfcf\ud835\udf38\ud835\udc8a\ud835\udfd0Relation-LevelWord-Levelwhat\ttvshow\t\t\t\t\t\tdid\t\t\t\t\t\t<e>\t\t\t\t\t\t\t\t\u2026\t\t\t\tstarring_roleseriesstarring\t\t\t\troleseries\ud835\udf37\ud835\udc8b\ud835\udc98\ud835\udf37\ud835\udc8c\ud835\udc93\u2026.\u2026....\u2026...QuestionRelationFigure 2: The proposed Hierarchical Residual BiLSTM (HR-BiLSTM) model for relation detection.\nNote that without the dotted arrows of shortcut connections between two layers, the model will only\ncompute the similarity between the second-layer of questions representations and the relation, thus is not\ndoing hierarchical matching.\n4.3 Hierarchical Matching between Relation\nand Question\nNow we have question contexts of different\nlengths encoded in \u0393(1)\n1:Nand\u0393(2)\n1:N. Unlike the\nstandard usage of deep BiLSTMs that employs\nthe representations in the \ufb01nal layer for prediction,\nhere we expect that two layers of question repre-\nsentations can be complementary to each other and\nboth should be compared to the relation represen-\ntation space ( Hierarchical Matching ). This is im-\nportant for our task since each relation token can\ncorrespond to phrases of different lengths, mainly\nbecause of syntactic variations. For example in Ta-\nble 1, the relation word written could be matched\nto either the same single word in the question or a\nmuch longer phrase be the writer of .\nWe could perform the above hierarchical match-\ning by computing the similarity between each\nlayer of \u0393andhrseparately and doing the\n(weighted) sum between the two scores. How-\never this does not give signi\ufb01cant improvement\n(see Table 2). Our analysis in Section 6.2 shows\nthat this naive method suffers from the training\ndif\ufb01culty, evidenced by that the converged train-\ning loss of this model is much higher than that\nof a single-layer baseline model. This is mainly\nbecause (1) Deep BiLSTMs do not guarantee that\nthe two-levels of question hidden representations\nare comparable, the training usually falls to local\noptima where one layer has good matching scores\nand the other always has weight close to 0. (2)The training of deeper architectures itself is more\ndif\ufb01cult.\nTo overcome the above dif\ufb01culties, we adopt the\nidea from Residual Networks (He et al., 2016) for\nhierarchical matching by adding shortcut connec-\ntions between two BiLSTM layers. We proposed\ntwo ways of such Hierarchical Residual Match-\ning: (1) Connecting each \u03b3(1)\niand\u03b3(2)\ni, resulting\nin a\u03b3\u2032\ni=\u03b3(1)\ni+\u03b3(2)\nifor each position i. Then the\n\ufb01nal question representation hqbecomes a max-\npooling over all \u03b3\u2032\nis,1\u2264i\u2264N. (2) Applying max-\npooling on \u0393(1)\n1:Nand\u0393(2)\n1:Nto get h(1)\nmaxandh(2)\nmax,\nrespectively, then setting hq=h(1)\nmax+h(2)\nmax. Fi-\nnally we compute the matching score of rgiven q\nassrel(r;q) =cos(hr,hq).\nIntuitively, the proposed method should bene\ufb01t\nfrom hierarchical training since the second layer is\n\ufb01tting the residues from the \ufb01rst layer of matching,\nso the two layers of representations are more likely\nto be complementary to each other. This also en-\nsures the vector spaces of two layers are compara-\nble and makes the second-layer training easier.\nDuring training we adopt a ranking loss to max-\nimizing the margin between the gold relation r+\nand other relations r\u2212in the candidate pool R.\nlrel= max{0,\u03b3\u2212srel(r+;q) +srel(r\u2212;q)}\nwhere\u03b3is a constant parameter. Fig 2 sum-\nmarizes the above Hierarchical Residual BiLSTM\n(HR-BiLSTM )model.", "start_char_idx": 0, "end_char_idx": 3401, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3b072c93-b875-4d91-8399-67e5d39ce8b7": {"__data__": {"id_": "3b072c93-b875-4d91-8399-67e5d39ce8b7", "embedding": null, "metadata": {"page_label": "6", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "30b8bf60-b0a3-4d25-ba1d-0665188f6124", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "8e4f5877b18cde8f429cc21f6554566e76279d6b00dcd575ef5e78c62993a6f6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7a7412b-fd26-4785-88ae-5d4f1ce3d9d1", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "1d2d58f6b49d40199104a17b389efc3c29692755ce3e8abc854b8677c7b04cae", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd9f4dab-d6c8-4fca-a754-61b399002eef", "node_type": "1", "metadata": {}, "hash": "465843824e64fcadf679f882ed9513f4e12811c42a3c1f59d7f5ffdd5d003aeb", "class_name": "RelatedNodeInfo"}}, "text": "Remark: Another way of hierarchical matching\nconsists in relying on attention mechanism , e.g.\n(Parikh et al., 2016), to \ufb01nd the correspondence\nbetween different levels of representations. This\nperforms below the HR-BiLSTM (see Table 2).\n5 KBQA Enhanced by Relation\nDetection\nThis section describes our KBQA pipeline system.\nWe make minimal efforts beyond the training of\nthe relation detection model, making the whole\nsystem easy to build.\nFollowing previous work (Yih et al., 2015; Xu\net al., 2016), our KBQA system takes an existing\nentity linker to produce the top- Klinked entities,\nELK(q), for a question q(\u201cinitial entity linking \u201d).\nThen we generate the KB queries for qfollowing\nthe four steps illustrated in Algorithm 1.\nAlgorithm 1: KBQA with two-step relation detection\nInput : Question q, Knowledge Base KB, the initial\ntop-Kentity candidates ELK(q)\nOutput: Top query tuple (\u02c6e,\u02c6r,{(c, rc)})\n1Entity Re-Ranking (\ufb01rst-step relation detection ): Use\ntheraw question text as input for a relation detector to\nscore all relations in the KB that are associated to the\nentities in ELK(q); use the relation scores to re-rank\nELK(q)and generate a shorter list EL\u2032\nK\u2032(q)\ncontaining the top- K\u2032entity candidates (Section 5.1)\n2Relation Detection : Detect relation(s) using the\nreformatted question text in which the topic entity is\nreplaced by a special token <e>(Section 5.2)\n3Query Generation : Combine the scores from step 1\nand 2, and select the top pair (\u02c6e,\u02c6r)(Section 5.3)\n4Constraint Detection (optional): Compute similarity\nbetween qand any neighbor entity cof the entities\nalong \u02c6r(connecting by a relation rc) , add the high\nscoring candrcto the query (Section 5.4).\nCompared to previous approaches, the main dif-\nference is that we have an additional entity re-\nranking step after the initial entity linking . We\nhave this step because we have observed that entity\nlinking sometimes becomes a bottleneck in KBQA\nsystems. For example, on SimpleQuestions the\nbest reported linker could only get 72.7% top-1\naccuracy on identifying topic entities. This is usu-\nally due to the ambiguities of entity names, e.g. in\nFig 1(a), there are TV writer andbaseball player\n\u201cMike Kelley \u201d, which is impossible to distinguish\nwith only entity name matching.\nHaving observed that different entity candidates\nusually connect to different relations, here we pro-\npose to help entity disambiguation in the initial en-\ntity linking with relations detected in questions.Sections 5.1 and 5.2 elaborate how our relation\ndetection help to re-rank entities in the initial en-\ntity linking, and then those re-ranked entities en-\nable more accurate relation detection. The KBQA\nend task, as a result, bene\ufb01ts from this process.\n5.1 Entity Re-Ranking\nIn this step, we use the raw question text as input\nfor a relation detector to score all relations in the\nKB with connections to at least one of the entity\ncandidates in ELK(q). We call this step relation\ndetection on entity set since it does not work on\na single topic entity as the usual settings. We use\nthe HR-BiLSTM as described in Sec. 4. For each\nquestionq, after generating a score srel(r;q)for\neach relation using HR-BiLSTM, we use the top\nlbest scoring relations ( Rl\nq) to re-rank the origi-\nnal entity candidates. Concretely, for each entity\neand its associated relations Re, given the origi-\nnal entity linker score slinker , and the score of the\nmost con\ufb01dent relation r\u2208Rl\nq\u2229Re, we sum these\ntwo scores to re-rank the entities:\nsrerank (e;q) =\u03b1\u00b7slinker(e;q)\n+(1\u2212\u03b1)\u00b7max\nr\u2208Rlq\u2229Resrel(r;q).\nFinally, we select top K\u2032<K entities according to\nscoresrerank to form the re-ranked list EL\u2032\nK\u2032(q).\nWe use the same example in Fig 1(a) to illustrate\nthe idea.", "start_char_idx": 0, "end_char_idx": 3700, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd9f4dab-d6c8-4fca-a754-61b399002eef": {"__data__": {"id_": "bd9f4dab-d6c8-4fca-a754-61b399002eef", "embedding": null, "metadata": {"page_label": "6", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "30b8bf60-b0a3-4d25-ba1d-0665188f6124", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "8e4f5877b18cde8f429cc21f6554566e76279d6b00dcd575ef5e78c62993a6f6", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3b072c93-b875-4d91-8399-67e5d39ce8b7", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "d44acbbe71d290479e6584e7b09a2a80d22e143e4c3384f60f1ddaca4f89ece7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c39c02d5-3afb-4bdc-b84f-ab5259c1b5da", "node_type": "1", "metadata": {}, "hash": "a98a2310ca5dab2db0abb77463ca6ec11341cb69a8e39b1d46bf59e2394ecc65", "class_name": "RelatedNodeInfo"}}, "text": "Concretely, for each entity\neand its associated relations Re, given the origi-\nnal entity linker score slinker , and the score of the\nmost con\ufb01dent relation r\u2208Rl\nq\u2229Re, we sum these\ntwo scores to re-rank the entities:\nsrerank (e;q) =\u03b1\u00b7slinker(e;q)\n+(1\u2212\u03b1)\u00b7max\nr\u2208Rlq\u2229Resrel(r;q).\nFinally, we select top K\u2032<K entities according to\nscoresrerank to form the re-ranked list EL\u2032\nK\u2032(q).\nWe use the same example in Fig 1(a) to illustrate\nthe idea. Given the input question in the exam-\nple, a relation detector is very likely to assign high\nscores to relations such as \u201c episodes written \u201d,\n\u201cauthor of\u201d and \u201c profession \u201d. Then, according\nto the connections of entity candidates in KB,\nwe \ufb01nd that the TV writer \u201c Mike Kelley \u201d will\nbe scored higher than the baseball player \u201c Mike\nKelley \u201d, because the former has the relations\n\u201cepisodes written \u201d and \u201c profession \u201d. This method\ncan be viewed as exploiting entity-relation collo-\ncation for entity linking.\n5.2 Relation Detection\nIn this step, for each candidate entity e\u2208\nEL\u2032\nK(q), we use the question text as the input to a\nrelation detector to score all the relations r\u2208Re\nthat are associated to the entity ein the KB.4Be-\ncause we have a single topic entity input in this\nstep, we do the following question reformatting:\nwe replace the the candidate e\u2019s entity mention in\n4Note that the number of entities and the number of rela-\ntion candidates will be much smaller than those in the previ-\nous step.", "start_char_idx": 3263, "end_char_idx": 4710, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c39c02d5-3afb-4bdc-b84f-ab5259c1b5da": {"__data__": {"id_": "c39c02d5-3afb-4bdc-b84f-ab5259c1b5da", "embedding": null, "metadata": {"page_label": "7", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d80b32d-19ee-46cc-9d84-38d1d98ff2e7", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "b34a74462d9524571681422e5dbcc0610ae141a9b5ac27149e14a56445cf7930", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd9f4dab-d6c8-4fca-a754-61b399002eef", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "d27952f3618d2685237b7a4aa99f0802cfa59de699ca3b15b58a6179857d19dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbcd8385-2908-4832-a9b9-16e9bb32d580", "node_type": "1", "metadata": {}, "hash": "ae0f3c063d23fb664fb30b9e5b6d4cb404524b3176ddf1ca5f8c90b4e6342298", "class_name": "RelatedNodeInfo"}}, "text": "qwith a token \u201c <e>\u201d. This helps the model bet-\nter distinguish the relative position of each word\ncompared to the entity. We use the HR-BiLSTM\nmodel to predict the score of each relation r\u2208Re:\nsrel(r;e,q).\n5.3 Query Generation\nFinally, the system outputs the <entity, relation (or\ncore-chain)>pair(\u02c6e,\u02c6r)according to:\ns(\u02c6e,\u02c6r;q) = max\ne\u2208EL\u2032\nK\u2032(q),r\u2208Re(\u03b2\u00b7srerank (e;q)\n+(1\u2212\u03b2)\u00b7srel(r;e,q)),\nwhere\u03b2is a hyperparameter to be tuned.\n5.4 Constraint Detection\nSimilar to (Yih et al., 2015), we adopt an ad-\nditional constraint detection step based on text\nmatching. Our method can be viewed as entity-\nlinking on a KB sub-graph. It contains two steps:\n(1)Sub-graph generation : given the top scored\nquery generated by the previous 3 steps5, for each\nnodev(answer node or the CVT node like in Fig-\nure 1(b)), we collect all the nodes cconnecting to\nv(with relation rc) with any relation, and generate\na sub-graph associated to the original query. (2)\nEntity-linking on sub-graph nodes : we compute\na matching score between each n-gram in the input\nquestion (without overlapping the topic entity) and\nentity name of c(except for the node in the orig-\ninal query) by taking into account the maximum\noverlapping sequence of characters between them\n(see Appendix A for details and B for special rules\ndealing with date/answer type constraints). If the\nmatching score is larger than a threshold \u03b8(tuned\non training set), we will add the constraint entity c\n(andrc) to the query by attaching it to the corre-\nsponding node von the core-chain.\n6 Experiments\n6.1 Task Introduction & Settings\nWe use the SimpleQuestions (Bordes et al., 2015)\nand WebQSP (Yih et al., 2016) datasets. Each\nquestion in these datasets is labeled with the gold\nsemantic parse. Hence we can directly evaluate\nrelation detection performance independently as\nwell as evaluate on the KBQA end task.\n5Starting with the top-1 query suffers more from error\npropagation. However we still achieve state-of-the-art on We-\nbQSP in Sec.6, showing the advantage of our relation detec-\ntion model. We leave in future work beam-search and feature\nextraction on beam for \ufb01nal answer re-ranking like in previ-\nous research.SimpleQuestions (SQ): It is a single-relation\nKBQA task. The KB we use consists of a Freebase\nsubset with 2M entities (FB2M) (Bordes et al.,\n2015), in order to compare with previous research.\nYin et al. (2016) also evaluated their relation ex-\ntractor on this data set and released their proposed\nquestion-relation pairs, so we run our relation de-\ntection model on their data set. For the KBQA\nevaluation, we also start with their entity linking\nresults6. Therefore, our results can be compared\nwith their reported results on both tasks.\nWebQSP (WQ): A multi-relation KBQA task.\nWe use the entire Freebase KB for evaluation\npurposes. Following Yih et al. (2016), we use\nS-MART (Yang and Chang, 2015) entity-linking\noutputs.7In order to evaluate the relation detec-\ntion models, we create a new relation detection\ntask from the WebQSP data set.8For each ques-\ntion and its labeled semantic parse: (1) we \ufb01rst\nselect the topic entity from the parse; and then (2)\nselect all the relations and relation chains (length\n\u22642) connected to the topic entity, and set the core-\nchain labeled in the parse as the positive label and\nall the others as the negative examples.\nWe tune the following hyper-parameters on de-\nvelopment sets: (1) the size of hidden states for\nLSTMs ({50, 100, 200, 400})9; (2) learning rate\n({0.1, 0.5, 1.0, 2.0}); (3) whether the shortcut\nconnections are between hidden states or between\nmax-pooling results (see Section 4.3); and (4) the\nnumber of training epochs.", "start_char_idx": 0, "end_char_idx": 3649, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbcd8385-2908-4832-a9b9-16e9bb32d580": {"__data__": {"id_": "bbcd8385-2908-4832-a9b9-16e9bb32d580", "embedding": null, "metadata": {"page_label": "7", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6d80b32d-19ee-46cc-9d84-38d1d98ff2e7", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "b34a74462d9524571681422e5dbcc0610ae141a9b5ac27149e14a56445cf7930", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c39c02d5-3afb-4bdc-b84f-ab5259c1b5da", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a746ea52c380b6d4a098b6e9566e04e0ad950c414c28916e56c3a793bcdb0fb8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0a8184b8-1064-421b-a59b-1101caff8be7", "node_type": "1", "metadata": {}, "hash": "7667116b4801f0ed967517e09f3de372104e5f38baf428c204682d9a18146a13", "class_name": "RelatedNodeInfo"}}, "text": "We tune the following hyper-parameters on de-\nvelopment sets: (1) the size of hidden states for\nLSTMs ({50, 100, 200, 400})9; (2) learning rate\n({0.1, 0.5, 1.0, 2.0}); (3) whether the shortcut\nconnections are between hidden states or between\nmax-pooling results (see Section 4.3); and (4) the\nnumber of training epochs.\nFor both the relation detection experiments and\nthe second-step relation detection in KBQA, we\nhave entity replacement \ufb01rst (see Section 5.2\nand Figure 1). All word vectors are initialized\nwith 300-dpretrained word embeddings (Mikolov\net al., 2013). The embeddings of relation names\nare randomly initialized, since existing pre-trained\nrelation embeddings (e.g. TransE) usually support\nlimited sets of relation names. We leave the usage\nof pre-trained relation embeddings to future work.\n6.2 Relation Detection Results\nTable 2 shows the results on two relation detec-\ntion tasks. The AMPCNN result is from (Yin\net al., 2016), which yielded state-of-the-art scores\nby outperforming several attention-based meth-\n6The two resources have been downloaded from https:\n//github.com/Gorov/SimpleQuestions-EntityLinking\n7https://github.com/scottyih/STAGG\n8The dataset is available at https://github.com/Gorov/\nKBQA_RE_data .\n9For CNNs we double the size for fair comparison.", "start_char_idx": 3330, "end_char_idx": 4616, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a8184b8-1064-421b-a59b-1101caff8be7": {"__data__": {"id_": "0a8184b8-1064-421b-a59b-1101caff8be7", "embedding": null, "metadata": {"page_label": "8", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a322bbac-55d8-4024-aafb-3a18236b59e2", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "51d528a36ddcb23ecc562a39b1dab8a7de660e27a9c4f6e78fd4688649232298", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbcd8385-2908-4832-a9b9-16e9bb32d580", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "e16b6dae19ce7a2e7d41be7490e98c9984c383889d625b16b67ff3a5a9f1c82f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1366e121-f984-43b4-a772-dbcb39632bda", "node_type": "1", "metadata": {}, "hash": "dbb5bdd69146b37af11498242b9158ae11fd9d6204b0370b7677cc54baee249f", "class_name": "RelatedNodeInfo"}}, "text": "Accuracy\nModel Relation Input Views SimpleQuestions WebQSP\nAMPCNN (Yin et al., 2016) words 91.3 -\nBiCNN (Yih et al., 2015) char-3-gram 90.0 77.74\nBiLSTM w/ words words 91.2 79.32\nBiLSTM w/ relation names rel names 88.9 78.96\nHier-Res-BiLSTM (HR-BiLSTM) words + rel names 93.3 82.53\nw/o rel name words 91.3 81.69\nw/o rel words rel names 88.8 79.68\nw/o residual learning (weighted sum on two layers) words + rel names 92.5 80.65\nreplacing residual with attention (Parikh et al., 2016) words + rel names 92.6 81.38\nsingle-layer BiLSTM question encoder words + rel names 92.8 78.41\nreplacing BiLSTM with CNN (HR-CNN) words + rel names 92.9 79.08\nTable 2: Accuracy on the SimpleQuestions and WebQSP relation detection tasks (test sets). The top\nshows performance of baselines. On the bottom we give the results of our proposed model together with\nthe ablation tests.\nods. We re-implemented the BiCNN model from\n(Yih et al., 2015), where both questions and rela-\ntions are represented with the word hash trick on\ncharacter tri-grams. The baseline BiLSTM with\nrelation word sequence appears to be the best base-\nline on WebQSP and is close to the previous best\nresult of AMPCNN on SimpleQuestions. Our pro-\nposed HR-BiLSTM outperformed the best base-\nlines on both tasks by margins of 2-3% (p <0.001\nand 0.01 compared to the best baseline BiLSTM w/\nwords on SQ and WQ respectively).\nNote that using only relation names instead\nof words results in a weaker baseline BiLSTM\nmodel. The model yields a signi\ufb01cant per-\nformance drop on SimpleQuestions (91.2% to\n88.9%). However, the drop is much smaller on\nWebQSP, and it suggests that unseen relations\nhave a much bigger impact on SimpleQuestions.\nAblation Test: The bottom of Table 2 shows ab-\nlation results of the proposed HR-BiLSTM. First,\nhierarchical matching between questions and both\nrelation names and relation words yields improve-\nment on both datasets, especially for SimpleQues-\ntions (93.3% vs. 91.2/88.8%). Second, residual\nlearning helps hierarchical matching compared to\nweighted-sum and attention-based baselines (see\nSection 4.3). For the attention-based baseline,\nwe tried the model from (Parikh et al., 2016) and\nits one-way variations, where the one-way model\ngives better results10. Note that residual learn-\ning signi\ufb01cantly helps on WebQSP (80.65% to\n10We also tried to apply the same attention method on deep\nBiLSTM with residual connections, but it does not lead to\nbetter results compared to HR-BiLSTM. We hypothesize that\nthe idea of hierarchical matching with attention mechanism\nmay work better for long sequences, and the new advanced\nattention mechanisms (Wang and Jiang, 2016; Wang et al.,\n2017) might help hierarchical matching. We leave the above\ndirections to future work.82.53%), while it does not help as much on Sim-\npleQuestions. On SimpleQuestions, even remov-\ning the deep layers only causes a small drop in per-\nformance. WebQSP bene\ufb01ts more from residual\nand deeper architecture, possibly because in this\ndataset it is more important to handle larger scope\nof context matching.\nFinally, on WebQSP, replacing BiLSTM with\nCNN in our hierarchical matching framework re-\nsults in a large performance drop. Yet on Sim-\npleQuestions the gap is much smaller. We believe\nthis is because the LSTM relation encoder can bet-\nter learn the composition of chains of relations in\nWebQSP, as it is better at dealing with longer de-\npendencies.\nAnalysis Next, we present empirical evidences,\nwhich show why our HR-BiLSTM model achieves\nthe best scores. We use WebQSP for the analy-\nsis purposes.", "start_char_idx": 0, "end_char_idx": 3564, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1366e121-f984-43b4-a772-dbcb39632bda": {"__data__": {"id_": "1366e121-f984-43b4-a772-dbcb39632bda", "embedding": null, "metadata": {"page_label": "8", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a322bbac-55d8-4024-aafb-3a18236b59e2", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "51d528a36ddcb23ecc562a39b1dab8a7de660e27a9c4f6e78fd4688649232298", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a8184b8-1064-421b-a59b-1101caff8be7", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "4bd9acc4f0db6dcc28ad4ed43904b021286f98b5c9755bec147ef29ec7f8976a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30ac4b62-3548-47e5-961c-8f2d4c86f360", "node_type": "1", "metadata": {}, "hash": "7fa33ec49311e175865d9a6d03ad6beb2df91612259dafcd8d7db3ac6bff8252", "class_name": "RelatedNodeInfo"}}, "text": "On SimpleQuestions, even remov-\ning the deep layers only causes a small drop in per-\nformance. WebQSP bene\ufb01ts more from residual\nand deeper architecture, possibly because in this\ndataset it is more important to handle larger scope\nof context matching.\nFinally, on WebQSP, replacing BiLSTM with\nCNN in our hierarchical matching framework re-\nsults in a large performance drop. Yet on Sim-\npleQuestions the gap is much smaller. We believe\nthis is because the LSTM relation encoder can bet-\nter learn the composition of chains of relations in\nWebQSP, as it is better at dealing with longer de-\npendencies.\nAnalysis Next, we present empirical evidences,\nwhich show why our HR-BiLSTM model achieves\nthe best scores. We use WebQSP for the analy-\nsis purposes. First, we have the hypothesis that\ntraining of the weighted-sum model usually falls\nto local optima, since deep BiLSTMs do not guar-\nantee that the two-levels of question hidden rep-\nresentations are comparable . This is evidenced\nby that during training one layer usually gets a\nweight close to 0 thus is ignored. For exam-\nple, one run gives us weights of -75.39/0.14 for\nthe two layers (we take exponential for the \ufb01nal\nweighted sum). It also gives much lower train-\ning accuracy (91.94%) compared to HR-BiLSTM\n(95.67%), suffering from training dif\ufb01culty.\nSecond, compared to our deep BiLSTM with\nshortcut connections, we have the hypothesis that\nfor KB relation detection, training deep BiLSTMs\nis more dif\ufb01cult without shortcut connections . Our\nexperiments suggest that deeper BiLSTM does not\nalways result in lower training accuracy. In the\nexperiments a two-layer BiLSTM converges to\n94.99%, even lower than the 95.25% achieved by a", "start_char_idx": 2811, "end_char_idx": 4505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30ac4b62-3548-47e5-961c-8f2d4c86f360": {"__data__": {"id_": "30ac4b62-3548-47e5-961c-8f2d4c86f360", "embedding": null, "metadata": {"page_label": "9", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76e6460c-f850-4db3-84a3-c90d0d4d108c", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "96ed761d0ffd63ff7a08c63941ff15ff5690688075c2f725d75e88fc364e85c1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1366e121-f984-43b4-a772-dbcb39632bda", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "0c822df6108085f4ff49e9e6c15032a45978d07dac0b23df591530667ada135d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "108db81e-6951-468c-a52c-8bace41261d4", "node_type": "1", "metadata": {}, "hash": "02fa5d3bf48338c7c8e3ed626f53663f21592f5886d128013b0761aace6ab380", "class_name": "RelatedNodeInfo"}}, "text": "single-layer BiLSTM. Under our setting the two-\nlayer model captures the single-layer model as a\nspecial case (so it could potentially better \ufb01t the\ntraining data), this result suggests that the deep\nBiLSTM without shortcut connections might suf-\nfers more from training dif\ufb01culty.\nFinally, we hypothesize that HR-BiLSTM is\nmore than combination of two BiLSTMs with\nresidual connections, because it encourages the\nhierarchical architecture to learn different levels\nof abstraction . To verify this, we replace the deep\nBiLSTM question encoder with two single-layer\nBiLSTMs (both on words) with shortcut connec-\ntions between their hidden states. This decreases\ntest accuracy to 76.11%. It gives similar training\naccuracy compared to HR-BiLSTM, indicating a\nmore serious over-\ufb01tting problem. This proves\nthat the residual and deep structures both con-\ntribute to the good performance of HR-BiLSTM.\n6.3 KBQA End-Task Results\nTable 3 compares our system with two published\nbaselines (1) STAGG (Yih et al., 2015), the state-\nof-the-art on WebQSP11and (2) AMPCNN (Yin\net al., 2016), the state-of-the-art on SimpleQues-\ntions. Since these two baselines are specially de-\nsigned/tuned for one particular dataset, they do not\ngeneralize well when applied to the other dataset.\nIn order to highlight the effect of different rela-\ntion detection models on the KBQA end-task, we\nalso implemented another baseline that uses our\nKBQA system but replaces HR-BiLSTM with our\nimplementation of AMPCNN (for SimpleQues-\ntions) or the char-3-gram BiCNN (for WebQSP)\nrelation detectors (second block in Table 3).\nCompared to the baseline relation detector (3rd\nrow of results), our method, which includes an im-\nproved relation detector (HR-BiLSTM), improves\nthe KBQA end task by 2-3% (4th row). Note that\nin contrast to previous KBQA systems, our sys-\ntem does not use joint-inference or feature-based\nre-ranking step, nevertheless it still achieves better\nor comparable results to the state-of-the-art.\nThe third block of the table details two ablation\ntests for the proposed components in our KBQA\nsystems: (1) Removing the entity re-ranking step\nsigni\ufb01cantly decreases the scores. Since the re-\nranking step relies on the relation detection mod-\nels, this shows that our HR-BiLSTM model con-\ntributes to the good performance in multiple ways.\n11The STAGG score on SQ is from (Bao et al., 2016).Accuracy\nSystem SQ WQ\nSTAGG 72.8 63.9\nAMPCNN (Yin et al., 2016) 76.4 -\nBaseline: Our Method w/75.1 60.0baseline relation detector\nOur Method 77.0 63.0\nw/o entity re-ranking 74.9 60.6\nw/o constraints - 58.0\nOur Method (multi-detectors) 78.7 63.9\nTable 3: KBQA results on SimpleQuestions (SQ)\nand WebQSP (WQ) test sets. The numbers in\ngreen color are directly comparable to our results\nsince we start with the same entity linking results.\nAppendix C gives the detailed performance of the\nre-ranking step. (2) In contrast to the conclusion\nin (Yih et al., 2015), constraint detection is crucial\nfor our system12. This is probably because our\njoint performance on topic entity and core-chain\ndetection is more accurate (77.5% top-1 accuracy),\nleaving a huge potential (77.5% vs. 58.0%) for the\nconstraint detection module to improve.\nFinally, like STAGG, which uses multiple rela-\ntion detectors (see Yih et al. (2015) for the three\nmodels used), we also try to use the top-3 rela-\ntion detectors from Section 6.2. As shown on the\nlast row of Table 3, this gives a signi\ufb01cant perfor-\nmance boost, resulting in a new state-of-the-art re-\nsult on SimpleQuestions and a result comparable\nto the state-of-the-art on WebQSP.\n7 Conclusion\nKB relation detection is a key step in KBQA and\nis signi\ufb01cantly different from general relation ex-\ntraction tasks.", "start_char_idx": 0, "end_char_idx": 3722, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "108db81e-6951-468c-a52c-8bace41261d4": {"__data__": {"id_": "108db81e-6951-468c-a52c-8bace41261d4", "embedding": null, "metadata": {"page_label": "9", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "76e6460c-f850-4db3-84a3-c90d0d4d108c", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "96ed761d0ffd63ff7a08c63941ff15ff5690688075c2f725d75e88fc364e85c1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "30ac4b62-3548-47e5-961c-8f2d4c86f360", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "c542904cae04b8672cfd7852f6d1aade2ca044c52f09433f8dd676a8fa79b8ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5375fc8f-5497-4fb2-a4be-9c58efc07577", "node_type": "1", "metadata": {}, "hash": "97983ee17fe483063e18a20de533f3f2d67de9ab858ff67822ffd03cf7a5fe07", "class_name": "RelatedNodeInfo"}}, "text": "58.0%) for the\nconstraint detection module to improve.\nFinally, like STAGG, which uses multiple rela-\ntion detectors (see Yih et al. (2015) for the three\nmodels used), we also try to use the top-3 rela-\ntion detectors from Section 6.2. As shown on the\nlast row of Table 3, this gives a signi\ufb01cant perfor-\nmance boost, resulting in a new state-of-the-art re-\nsult on SimpleQuestions and a result comparable\nto the state-of-the-art on WebQSP.\n7 Conclusion\nKB relation detection is a key step in KBQA and\nis signi\ufb01cantly different from general relation ex-\ntraction tasks. We propose a novel KB relation\ndetection model, HR-BiLSTM, that performs hier-\narchical matching between questions and KB rela-\ntions. Our model outperforms the previous meth-\nods on KB relation detection tasks and allows our\nKBQA system to achieve state-of-the-arts. For fu-\nture work, we will investigate the integration of\nour HR-BiLSTM into end-to-end systems. For ex-\nample, our model could be integrated into the de-\ncoder in (Liang et al., 2016), to provide better se-\nquence prediction. We will also investigate new\nemerging datasets like GraphQuestions (Su et al.,\n2016) and ComplexQuestions (Bao et al., 2016) to\nhandle more characteristics of general QA.\n12Note that another reason is that we are evaluating on ac-\ncuracy here. When evaluating on F1 the gap will be smaller.", "start_char_idx": 3153, "end_char_idx": 4508, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5375fc8f-5497-4fb2-a4be-9c58efc07577": {"__data__": {"id_": "5375fc8f-5497-4fb2-a4be-9c58efc07577", "embedding": null, "metadata": {"page_label": "10", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4bd4c3a1-61f8-4f02-86a6-017c0f023645", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "29a3c7729207736e7dd6f5750fe217673401f77d3f48bc001e988e53582fd08c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "108db81e-6951-468c-a52c-8bace41261d4", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "cb3f53096770359a093324df4286b5689b981f29f602f0be86bf1f8bb5c70566", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2102027-836c-407c-ba7c-0c5ee0492fb6", "node_type": "1", "metadata": {}, "hash": "0472a1e7bce553a2288e8a1bcdd8db1adf726a2607061f31549140ef310e482c", "class_name": "RelatedNodeInfo"}}, "text": "References\nJunwei Bao, Nan Duan, Zhao Yan, Ming Zhou, and\nTiejun Zhao. 2016. Constraint-based question an-\nswering with knowledge graph. In Proceedings of\nCOLING 2016, the 26th International Conference\non Computational Linguistics: Technical Papers .\nThe COLING 2016 Organizing Committee, Osaka,\nJapan, pages 2503\u20132514.\nHannah Bast and Elmar Haussmann. 2015. More ac-\ncurate question answering on freebase. In Proceed-\nings of the 24th ACM International on Conference\non Information and Knowledge Management . ACM,\npages 1431\u20131440.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on Freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing . Association for Computational\nLinguistics, Seattle, Washington, USA, pages 1533\u2013\n1544.\nAntoine Bordes, Nicolas Usunier, Sumit Chopra, and\nJason Weston. 2015. Large-scale simple question\nanswering with memory networks. arXiv preprint\narXiv:1506.02075 .\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Advances in Neural Information\nProcessing Systems . pages 2787\u20132795.\nZihang Dai, Lei Li, and Wei Xu. 2016. Cfo: Condi-\ntional focused neural question answering with large-\nscale knowledge bases. In Proceedings of the 54th\nAnnual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers) . Asso-\nciation for Computational Linguistics, Berlin, Ger-\nmany, pages 800\u2013810.\nCicero dos Santos, Bing Xiang, and Bowen Zhou.\n2015. Classifying relations by ranking with con-\nvolutional neural networks. In Proceedings of the\n53rd Annual Meeting of the Association for Compu-\ntational Linguistics and the 7th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers) . Association for Computa-\ntional Linguistics, Beijing, China, pages 626\u2013634.\nAnthony Fader, Luke S Zettlemoyer, and Oren Etzioni.\n2013. Paraphrase-driven learning for open question\nanswering. In ACL (1) . Citeseer, pages 1608\u20131618.\nDavid Golub and Xiaodong He. 2016. Character-level\nquestion answering with attention. arXiv preprint\narXiv:1604.00727 .\nMatthew R. Gormley, Mo Yu, and Mark Dredze. 2015.\nImproved relation extraction with feature-rich com-\npositional embedding models. In Proceedings of\nthe 2015 Conference on Empirical Methods in Nat-\nural Language Processing . Association for Compu-\ntational Linguistics, Lisbon, Portugal, pages 1774\u2013\n1784.Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition . pages\n770\u2013778.\nChen Liang, Jonathan Berant, Quoc Le, Kenneth D\nForbus, and Ni Lao. 2016. Neural symbolic ma-\nchines: Learning semantic parsers on freebase with\nweak supervision. arXiv preprint arXiv:1611.00020\n.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems . pages 3111\u20133119.\nThien Huu Nguyen and Ralph Grishman. 2014. Em-\nploying word representations and regularization for\ndomain adaptation of relation extraction. In Pro-\nceedings of the 52nd Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers) . Association for Computational Linguistics,\nBaltimore, Maryland, pages 68\u201374.\nAnkur Parikh, Oscar T \u00a8ackstr \u00a8om, Dipanjan Das, and\nJakob Uszkoreit.", "start_char_idx": 0, "end_char_idx": 3576, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2102027-836c-407c-ba7c-0c5ee0492fb6": {"__data__": {"id_": "e2102027-836c-407c-ba7c-0c5ee0492fb6", "embedding": null, "metadata": {"page_label": "10", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4bd4c3a1-61f8-4f02-86a6-017c0f023645", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "29a3c7729207736e7dd6f5750fe217673401f77d3f48bc001e988e53582fd08c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5375fc8f-5497-4fb2-a4be-9c58efc07577", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "4d37b177857a622d1e67507bf4ca4464f01dedaf2d51941d4ff9616a63b0755d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b692bd4f-765e-4503-a82c-8a2fb897c0d5", "node_type": "1", "metadata": {}, "hash": "b0f58a388ce31a224b1b68a77556625ad8c521f0c9a7e357b00b6dceb7841c9b", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:1611.00020\n.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems . pages 3111\u20133119.\nThien Huu Nguyen and Ralph Grishman. 2014. Em-\nploying word representations and regularization for\ndomain adaptation of relation extraction. In Pro-\nceedings of the 52nd Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 2: Short\nPapers) . Association for Computational Linguistics,\nBaltimore, Maryland, pages 68\u201374.\nAnkur Parikh, Oscar T \u00a8ackstr \u00a8om, Dipanjan Das, and\nJakob Uszkoreit. 2016. A decomposable attention\nmodel for natural language inference. In Proceed-\nings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing . Association\nfor Computational Linguistics, Austin, Texas, pages\n2249\u20132255.\nBryan Rink and Sanda Harabagiu. 2010. Utd: Clas-\nsifying semantic relations by combining lexical and\nsemantic resources. In Proceedings of the 5th Inter-\nnational Workshop on Semantic Evaluation . Associ-\nation for Computational Linguistics, Uppsala, Swe-\nden, pages 256\u2013259.\nYu Su, Huan Sun, Brian Sadler, Mudhakar Sri-\nvatsa, Izzeddin Gur, Zenghui Yan, and Xifeng Yan.\n2016. On generating characteristic-rich question\nsets for qa evaluation. In Proceedings of the\n2016 Conference on Empirical Methods in Natu-\nral Language Processing . Association for Compu-\ntational Linguistics, Austin, Texas, pages 562\u2013572.\nhttps://aclweb.org/anthology/D16-1054.\nAng Sun, Ralph Grishman, and Satoshi Sekine. 2011.\nSemi-supervised relation extraction with large-scale\nword clustering. In Proceedings of the 49th Annual\nMeeting of the Association for Computational Lin-\nguistics: Human Language Technologies . Associa-\ntion for Computational Linguistics, Portland, Ore-\ngon, USA, pages 521\u2013529.\nNgoc Thang Vu, Heike Adel, Pankaj Gupta, and Hin-\nrich Sch \u00a8utze. 2016. Combining recurrent and con-\nvolutional neural networks for relation classi\ufb01ca-\ntion. In Proceedings of the 2016 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies . Association for Computational Linguis-\ntics, San Diego, California, pages 534\u2013539.", "start_char_idx": 2900, "end_char_idx": 5189, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b692bd4f-765e-4503-a82c-8a2fb897c0d5": {"__data__": {"id_": "b692bd4f-765e-4503-a82c-8a2fb897c0d5", "embedding": null, "metadata": {"page_label": "11", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da6b4d72-fd10-4b13-89f5-8c3604e4fefa", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "af1a275b993bd2f396c73303bfb9b0605183b250c6373d8130eb760099e37de3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2102027-836c-407c-ba7c-0c5ee0492fb6", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "c3458af8c76935f37cc2570fde63d6a8669c0132ef0744c695c86dd26c32cd7f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "197d7bd5-e401-4506-8bd7-ac68b66276bf", "node_type": "1", "metadata": {}, "hash": "983cbe9c35e63f65d373f81f20f39b6bcd8f4da0f38b37a276103177e4da15fd", "class_name": "RelatedNodeInfo"}}, "text": "Linlin Wang, Zhu Cao, Gerard de Melo, and Zhiyuan\nLiu. 2016. Relation classi\ufb01cation via multi-level at-\ntention cnns. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) . Association for\nComputational Linguistics, Berlin, Germany, pages\n1298\u20131307.\nShuohang Wang and Jing Jiang. 2016. Learning\nnatural language inference with lstm. In Pro-\nceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Com-\nputational Linguistics: Human Language Tech-\nnologies . Association for Computational Linguis-\ntics, San Diego, California, pages 1442\u20131451.\nhttp://www.aclweb.org/anthology/N16-1170.\nZhiguo Wang, Wael Hamza, and Radu Florian. 2017.\nBilateral multi-perspective matching for natural lan-\nguage sentences. arXiv preprint arXiv:1702.03814\n.\nKun Xu, Siva Reddy, Yansong Feng, Songfang Huang,\nand Dongyan Zhao. 2016. Question answering on\nfreebase via relation extraction and textual evidence.\nInProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers) . Association for Computational Lin-\nguistics, Berlin, Germany, pages 2326\u20132336.\nYi Yang and Ming-Wei Chang. 2015. S-mart: Novel\ntree-based structured learning algorithms applied to\ntweet entity linking. In Proceedings of the 53rd An-\nnual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers) . Association for Computational Lin-\nguistics, Beijing, China, pages 504\u2013513.\nXuchen Yao, Jonathan Berant, and Benjamin\nVan Durme. 2014. Freebase qa: Information\nextraction or semantic parsing? ACL 2014 page 82.\nXuchen Yao and Benjamin Van Durme. 2014. Infor-\nmation extraction over structured data: Question an-\nswering with freebase. In ACL (1) . Citeseer, pages\n956\u2013966.\nWen-tau Yih, Ming-Wei Chang, Xiaodong He, and\nJianfeng Gao. 2015. Semantic parsing via staged\nquery graph generation: Question answering with\nknowledge base. In Association for Computational\nLinguistics (ACL) .\nWen-tau Yih, Xiaodong He, and Christopher Meek.\n2014. Semantic parsing for single-relation ques-\ntion answering. In Proceedings of the 52nd An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers) . Association\nfor Computational Linguistics, Baltimore, Mary-\nland, pages 643\u2013648.\nWen-tau Yih, Matthew Richardson, Chris Meek, Ming-\nWei Chang, and Jina Suh. 2016. The value of se-\nmantic parse labeling for knowledge base questionanswering. In Proceedings of the 54th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers) . Association for Computa-\ntional Linguistics, Berlin, Germany, pages 201\u2013206.\nWenpeng Yin, Mo Yu, Bing Xiang, Bowen Zhou, and\nHinrich Sch \u00a8utze. 2016. Simple question answering\nby attentive convolutional neural network. In Pro-\nceedings of COLING 2016, the 26th International\nConference on Computational Linguistics: Techni-\ncal Papers . The COLING 2016 Organizing Commit-\ntee, Osaka, Japan, pages 1746\u20131756.\nMo Yu, Mark Dredze, Raman Arora, and Matthew R.\nGormley. 2016. Embedding lexical features via low-\nrank tensors. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies . Association for Computational Lin-\nguistics, San Diego, California, pages 1019\u20131029.\nhttp://www.aclweb.org/anthology/N16-1117.\nDaojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,\nand Jun Zhao. 2014. Relation classi\ufb01cation via con-\nvolutional deep neural network.", "start_char_idx": 0, "end_char_idx": 3603, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "197d7bd5-e401-4506-8bd7-ac68b66276bf": {"__data__": {"id_": "197d7bd5-e401-4506-8bd7-ac68b66276bf", "embedding": null, "metadata": {"page_label": "11", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "da6b4d72-fd10-4b13-89f5-8c3604e4fefa", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "af1a275b993bd2f396c73303bfb9b0605183b250c6373d8130eb760099e37de3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b692bd4f-765e-4503-a82c-8a2fb897c0d5", "node_type": "1", "metadata": {"page_label": "11", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "29d3dd72b8bf61c3bb092d05e4d3498ad8c1c228bc07e04edd39b35caffb6c71", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "df5099fc-9b9f-4dcf-9e7a-13837bacc5a1", "node_type": "1", "metadata": {}, "hash": "651bb4463944ff3d286899f1e589351d542443356ede85b21d840d05b9b88d72", "class_name": "RelatedNodeInfo"}}, "text": "The COLING 2016 Organizing Commit-\ntee, Osaka, Japan, pages 1746\u20131756.\nMo Yu, Mark Dredze, Raman Arora, and Matthew R.\nGormley. 2016. Embedding lexical features via low-\nrank tensors. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies . Association for Computational Lin-\nguistics, San Diego, California, pages 1019\u20131029.\nhttp://www.aclweb.org/anthology/N16-1117.\nDaojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,\nand Jun Zhao. 2014. Relation classi\ufb01cation via con-\nvolutional deep neural network. In Proceedings of\nCOLING 2014, the 25th International Conference\non Computational Linguistics: Technical Papers .\nDublin City University and Association for Com-\nputational Linguistics, Dublin, Ireland, pages 2335\u2013\n2344.\nGuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.\n2005. Exploring various knowledge in relation ex-\ntraction. In Association for Computational Linguis-\ntics. pages 427\u2013434.\nPeng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen\nLi, Hongwei Hao, and Bo Xu. 2016. Attention-\nbased bidirectional long short-term memory net-\nworks for relation classi\ufb01cation. In Proceedings of\nthe 54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 2: Short Papers) .\nAssociation for Computational Linguistics, Berlin,\nGermany, pages 207\u2013212.", "start_char_idx": 3011, "end_char_idx": 4368, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "df5099fc-9b9f-4dcf-9e7a-13837bacc5a1": {"__data__": {"id_": "df5099fc-9b9f-4dcf-9e7a-13837bacc5a1", "embedding": null, "metadata": {"page_label": "12", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d94be76f-1a2c-46e7-979b-389c19fd3b82", "node_type": "4", "metadata": {"page_label": "12", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "19e8dd881f1e52da70848874b44a290e4b58fb4981fe672b538bf5d5e5753bfb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "197d7bd5-e401-4506-8bd7-ac68b66276bf", "node_type": "1", "metadata": {"page_label": "11", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "1d2f3005f7a3e566382e6ceaf8aba67e5f3f8b60e486b638899a3b9cc19537a3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9dd4669d-2859-44dc-8333-8c5efe456dbc", "node_type": "1", "metadata": {}, "hash": "21131df89714f747e99a1237fb428b2b90aba97c4e044d7126f0b320ce702cbe", "class_name": "RelatedNodeInfo"}}, "text": "Appendix A: Detailed Score Computation\nfor Constraint Detection\nGiven an input question qand an entity name ein\nKB, we denote the lengths of the question and the\nentity name as|q|and|ne|. For a mention mof\nthe entityewhich is an n-gram inq, we compute\nthe longest consecutive common sub-sequence be-\ntweenmande, and denote its length as |m\u2229e|.\nAll the lengths above are measured by the number\nof characters.\nBased on the above numbers we compute the\nproportions of the length of the overlap between\nentity mention and entity name (in characters) in\nthe entity name|m\u2229e|\n|e|and in the question|m\u2229e|\n|q|;\nThe \ufb01nal score for the question has a mention link-\ning toeis\nslinker (e;q) = max\nm|m\u2229e|\n|q|+|m\u2229e|\n|e|\nAppendix B: Special Rules for Constraint\nDetection\n1. Special threshold for date constraints. The\ntime stamps in KB usually follow the year-\nmonth-day format, while the time in We-\nbQSP are usually years. This makes the over-\nlap between the date entities in questions and\nthe KB entity names smaller (length of over-\nlap is usually 4). To deal with this, we only\ncheck whether the dates in questions could\nmatch the years in KB, thus have a special\nthreshold of \u03b8= 1for date constraints.\n2. Filtering the constraints for answer nodes.\nSometimes the answer node could connect\nto huge number of other nodes, e.g. when\nthe question is asking for a country and we\nhave an answer candidate the U.S. . From\nthe observation on the WebQSP datasets,\nwe found that for most of the time, the\ngold constraints on answers are their en-\ntity types (e.g., whether the question is ask-\ning for a country or a city). Based on\nthis observation, in the constraint detection\nstep, for the answer nodes we only keep\nthe tuples with type relations (i.e. the\nrelation name contains the word \u201c type\u201d),\nsuch as common.topic.notable types, educa-\ntion.educational institution.school type etc.Appendix C: Effects of Entity Re-Ranking\non SimpleQuestions\nRemoving entity re-ranking step results in signi\ufb01-\ncant performance drop (see Table 3, the row of w/o\nentity re-ranking ). Table 4 evaluates our re-ranker\nas an separate task. Our re-ranker results in large\nimprovement, especially when the beam sizes are\nsmaller than 10. This is indicating another impor-\ntant usage of our proposed improved relation de-\ntection model on entity linking re-ranking.\nTop K FreeBase API (Golub & He, 2016)\n1 40.9 52.9\n10 64.3 74.0\n20 69.7 77.8\n50 75.7 82.0\nTop K (Yin et al., 2016) Our Re-Ranker\n1 72.7 79.0\n10 86.9 89.5\n20 88.4 90.9\n50 90.2 92.5\nTable 4: Entity re-ranking on SimpleQuestions\n(test set).", "start_char_idx": 0, "end_char_idx": 2568, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9dd4669d-2859-44dc-8333-8c5efe456dbc": {"__data__": {"id_": "9dd4669d-2859-44dc-8333-8c5efe456dbc", "embedding": null, "metadata": {"page_label": "1", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a73e442e-f027-42b5-b16a-3cf06321c6df", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "bdaf3f06e9bfe17eb1282df628e40450c76fd0bcee8abcfe0822fbd117cec25b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "df5099fc-9b9f-4dcf-9e7a-13837bacc5a1", "node_type": "1", "metadata": {"page_label": "12", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "19e8dd881f1e52da70848874b44a290e4b58fb4981fe672b538bf5d5e5753bfb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0d3408db-ac49-43ce-8b3b-8724c62e525c", "node_type": "1", "metadata": {}, "hash": "2d99342905f777fdcf88e399426b501a1a6bb14c14555d985413e8bc9159536d", "class_name": "RelatedNodeInfo"}}, "text": "Massively Multilingual Neural Grapheme-to-Phoneme Conversion\nBen Peters\nSaarland University\nSaarbr \u00a8ucken, Germany\nbenzurdopeters@gmail.comJon Dehdari andJosef van Genabith\nDFKI & Saarland University\nSaarbr \u00a8ucken, Germany\nfirstname.lastname@dfki.de\nAbstract\nGrapheme-to-phoneme conversion (g2p)\nis necessary for text-to-speech and auto-\nmatic speech recognition systems. Most\ng2p systems are monolingual: they require\nlanguage-speci\ufb01c data or handcrafting of\nrules. Such systems are dif\ufb01cult to ex-\ntend to low resource languages, for which\ndata and handcrafted rules are not avail-\nable. As an alternative, we present a neu-\nral sequence-to-sequence approach to g2p\nwhich is trained on spelling\u2013pronunciation\npairs in hundreds of languages. The sys-\ntem shares a single encoder and decoder\nacross all languages, allowing it to utilize\nthe intrinsic similarities between different\nwriting systems. We show an 11% im-\nprovement in phoneme error rate over an\napproach based on adapting high-resource\nmonolingual g2p models to low-resource\nlanguages. Our model is also much more\ncompact relative to previous approaches.\n1 Introduction\nAccurate grapheme-to-phoneme conversion (g2p)\nis important for any application that depends on\nthe sometimes inconsistent relationship between\nspoken and written language. Most prominently,\nthis includes text-to-speech and automatic speech\nrecognition. Most work on g2p has focused on\na few languages for which extensive pronuncia-\ntion data is available (Bisani and Ney, 2008; No-\nvak et al., 2016; Rao et al., 2015; Yao and Zweig,\n2015, inter alia) . Most languages lack these re-\nsources. However, a low resource language\u2019s writ-\ning system is likely to be similar to the writing sys-\ntems of languages that do have suf\ufb01cient pronun-\nciation data. Therefore g2p may be possible for\nlow resource languages if this high resource datacan be properly utilized.\nWe attempt to leverage high resource data by\ntreating g2p as a multisource neural machine\ntranslation (NMT) problem. The source sequences\nfor our system are words in the standard orthogra-\nphy in any language. The target sequences are the\ncorresponding representation in the International\nPhonetic Alphabet (IPA). Our results show that the\nparameters learned by the shared encoder\u2013decoder\nare able to exploit the orthographic and phonemic\nsimilarities between the various languages in our\ndata.\n2 Related Work\n2.1 Low Resource g2p\nOur approach is similar in goal to Deri and Knight\n(2016)\u2019s model for adapting high resource g2p\nmodels for low resource languages. They trained\nweighted \ufb01nite state transducer (wFST) models on\na variety of high resource languages, then trans-\nferred those models to low resource languages, us-\ning a language distance metric to choose which\nhigh resource models to use and a phoneme dis-\ntance metric to map the high resource language\u2019s\nphonemes to the low resource language\u2019s phoneme\ninventory. These distance metrics are computed\nbased on data from Phoible (Moran et al., 2014)\nand URIEL (Littell et al., 2017).\nOther low resource g2p systems have used a\nstrategy of combining multiple models. Schlippe\net al. (2014) trained several data-driven g2p sys-\ntems on varying quantities of monolingual data\nand combined their outputs with a phoneme-level\nvoting scheme. This led to improvements over the\nbest-performing single system for small quantities\nof data in some languages. Jyothi and Hasegawa-\nJohnson (2017) trained recurrent neural networks\nfor small data sets and found that a version of their\nsystem that combined the neural network outputarXiv:1708.01464v1  [cs.CL]  4 Aug 2017", "start_char_idx": 0, "end_char_idx": 3612, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0d3408db-ac49-43ce-8b3b-8724c62e525c": {"__data__": {"id_": "0d3408db-ac49-43ce-8b3b-8724c62e525c", "embedding": null, "metadata": {"page_label": "2", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb90669e-9444-4916-b9a7-101e2ea7d96b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "d188fb5b764273f3483ee6a9967a01ed3cd1bc64809a44e38fda68a62ce4a4d7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9dd4669d-2859-44dc-8333-8c5efe456dbc", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "bdaf3f06e9bfe17eb1282df628e40450c76fd0bcee8abcfe0822fbd117cec25b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "13c17320-1a7b-4e6e-bf5f-172679e963f0", "node_type": "1", "metadata": {}, "hash": "72030c3e8c36b7c06d9d5c78e7c9140d5f1e6b7c74356c462b74e223f7965e82", "class_name": "RelatedNodeInfo"}}, "text": "with the output of the wFST-based Phonetisaurus\nsystem (Novak et al., 2016) did better than either\nsystem alone.\nA different approach came from Kim and Sny-\nder (2012), who used supervised learning with\nan undirected graphical model to induce the\ngrapheme\u2013phoneme mappings for languages writ-\nten in the Latin alphabet. Given a short text in\na language, the model predicts the language\u2019s or-\nthographic rules. To create phonemic context fea-\ntures from the short text, the model na \u00a8\u0131vely maps\ngraphemes to IPA symbols written with the same\ncharacter, and uses the features of these symbols\nto learn an approximation of the phonotactic con-\nstraints of the language. In their experiments,\nthese phonotactic features proved to be more valu-\nable than geographical and genetic features drawn\nfrom WALS (Dryer and Haspelmath, 2013).\n2.2 Multilingual Neural NLP\nIn recent years, neural networks have emerged as\na common way to use data from several languages\nin a single system. Google\u2019s zero-shot neural ma-\nchine translation system (Johnson et al., 2016)\nshares an encoder and decoder across all language\npairs. In order to facilitate this multi-way transla-\ntion, they prepend an arti\ufb01cial token to the begin-\nning of each source sentence at both training and\ntranslation time. The token identi\ufb01es what lan-\nguage the sentence should be translated to. This\napproach has three bene\ufb01ts: it is far more ef\ufb01cient\nthan building a separate model for each language\npair; it allows for translation between languages\nthat share no parallel data; and it improves re-\nsults on low-resource languages by allowing them\nto implicitly share parameters with high-resource\nlanguages. Our g2p system is inspired by this ap-\nproach, although it differs in that there is only one\ntarget \u201clanguage\u201d, IPA, and the arti\ufb01cial tokens\nidentify the language of the source instead of the\nlanguage of the target.\nOther work has also made use of multilingually-\ntrained neural networks. Phoneme-level polyglot\nlanguage models (Tsvetkov et al., 2016) train a\nsingle model on multiple languages and addition-\nally condition on externally constructed typolog-\nical data about the language. \u00a8Ostling and Tiede-\nmann (2017) used a similar approach, in which\na character-level neural language model is trained\non a massively multilingual corpus. A language\nembedding vector is concatenated to the input ateach time step. The language embeddings their\nsystem learned correlate closely to the genetic re-\nlationships between languages. However, neither\nof these models was applied to g2p.\n3 Grapheme-to-Phoneme\ng2p is the problem of converting the orthographic\nrepresentation of a word into a phonemic repre-\nsentation. A phoneme is an abstract unit of sound\nwhich may have different realizations in different\ncontexts. For example, the English phoneme /p/\nhas two phonetic realizations (or allophones):\n\u2022[ph], as in the word \u2018pain\u2019 [pheI n]\n\u2022[p], as in the word \u2018Spain\u2019 [s p eI n]\nEnglish speakers without linguistic training of-\nten struggle to perceive any difference between\nthese sounds. Writing systems usually do not dis-\ntinguish between allophones: [ph]and[p]are both\nwritten as\u27e8p\u27e9in English. The sounds are written\ndifferently in languages where they contrast, such\nas Hindi and Eastern Armenian.\nMost writing systems in use today are glot-\ntographic, meaning that their symbols encode\nsolely phonological information1. But despite\nbeing glottographic, in few writing systems do\ngraphemes correspond one-to-one with phonemes.\nThere are cases in which multiple graphemes rep-\nresent a single phoneme, as in the word thein En-\nglish:\nth e\nD @\nThere are cases in which a single grapheme rep-\nresents multiple phonemes, such as syllabaries, in\nwhich each symbol represents a syllable.\nIn many languages, there are silent letters, as in\nthe word hora in Spanish:\nh o r a\n-o R a\nThere are more complicated correspondences,\nsuch as the silent ein English that affects the pro-\nnunciation of the previous vowel, as seen in the\npair of words cape andcap.\nIt is possible for an orthographic system to have\nany or all of the above phenomena while remain-\ning unambiguous.", "start_char_idx": 0, "end_char_idx": 4131, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "13c17320-1a7b-4e6e-bf5f-172679e963f0": {"__data__": {"id_": "13c17320-1a7b-4e6e-bf5f-172679e963f0", "embedding": null, "metadata": {"page_label": "2", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cb90669e-9444-4916-b9a7-101e2ea7d96b", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "d188fb5b764273f3483ee6a9967a01ed3cd1bc64809a44e38fda68a62ce4a4d7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0d3408db-ac49-43ce-8b3b-8724c62e525c", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "260202c3f899243c8857f32228708f3517b45cdfba93836814b2efc326ac5096", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c037ee30-8133-4839-b08b-7c652fb85248", "node_type": "1", "metadata": {}, "hash": "f4033bedb4b0a2cb15b7082ad20e19a044fef4ffbaede082bb24617dec00954d", "class_name": "RelatedNodeInfo"}}, "text": "But despite\nbeing glottographic, in few writing systems do\ngraphemes correspond one-to-one with phonemes.\nThere are cases in which multiple graphemes rep-\nresent a single phoneme, as in the word thein En-\nglish:\nth e\nD @\nThere are cases in which a single grapheme rep-\nresents multiple phonemes, such as syllabaries, in\nwhich each symbol represents a syllable.\nIn many languages, there are silent letters, as in\nthe word hora in Spanish:\nh o r a\n-o R a\nThere are more complicated correspondences,\nsuch as the silent ein English that affects the pro-\nnunciation of the previous vowel, as seen in the\npair of words cape andcap.\nIt is possible for an orthographic system to have\nany or all of the above phenomena while remain-\ning unambiguous. However, some orthographic\n1The Chinese script, in which characters have both phono-\nlogical form and semantic meaning, is the best-known excep-\ntion.", "start_char_idx": 3391, "end_char_idx": 4282, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c037ee30-8133-4839-b08b-7c652fb85248": {"__data__": {"id_": "c037ee30-8133-4839-b08b-7c652fb85248", "embedding": null, "metadata": {"page_label": "3", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6dcddd5f-dfe2-47f8-8106-bc3416d45f0f", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "e26bc83b70f8446d4c284430df89420030e629e58a481281ad552053e58499bb", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "13c17320-1a7b-4e6e-bf5f-172679e963f0", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "05656a424d16f9f9b63a7ddbbc2e87fdd231164b03b2808116cf7f31b5ef9b31", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "76b58edd-ef42-4c84-be01-57af7fda0024", "node_type": "1", "metadata": {}, "hash": "a2d67a1215308ae3cc98e60cc2c0f156ce45b769164bed2fa9723c4cce5c4272", "class_name": "RelatedNodeInfo"}}, "text": "systems contain ambiguities. English is well-\nknown for its spelling ambiguities. Abjads, used\nfor Arabic and Hebrew, do not give full represen-\ntation to vowels.\nConsequently, g2p is harder than simply replac-\ning each grapheme symbol with a corresponding\nphoneme symbol. It is the problem of replacing a\ngrapheme sequence\nG=g1,g2,...,g m\nwith a phoneme sequence\n\u03a6 =\u03c61,\u03c62,...,\u03c6 n\nwhere the sequences are not necessarily of the\nsame length. Data-driven g2p is therefore the\nproblem of \ufb01nding the phoneme sequence that\nmaximizes the likelihood of the grapheme se-\nquence:\n\u02c6\u03a6 = arg max\n\u03a6\u2032Pr(\u03a6\u2032|G)\nData-driven approaches are especially useful\nfor problems in which the rules that govern them\nare complex and dif\ufb01cult to engineer by hand.\ng2p for languages with ambiguous orthographies\nis such a problem. Multilingual g2p, in which the\nvarious languages have similar but different and\npossibly contradictory spelling rules, can be seen\nas an extreme case of that. Therefore, a data-\ndriven sequence-to-sequence model is a natural\nchoice.\n4 Methods\n4.1 Encoder\u2013Decoder Models\nIn order to \ufb01nd the best phoneme sequence, we\nuse a neural encoder\u2013decoder model with atten-\ntion (Bahdanau et al., 2014). The model consists\nof two main parts: the encoder compresses each\nsource grapheme sequence Ginto a \ufb01xed-length\nvector. The decoder , conditioned on this \ufb01xed-\nlength vector, generates the output phoneme se-\nquence \u03a6.\nThe encoder and decoder are both implemented\nas recurrent neural networks, which have the ad-\nvantage of being able to process sequences of ar-\nbitrary length and use long histories ef\ufb01ciently.\nThey are trained jointly to minimize cross-entropy\non the training data. We had our best results\nwhen using a bidirectional encoder, which consists\nof two separate encoders which process the inputEnc. & dec. model type LSTM\nAttention General\nEnc. & dec. layers 2\nHidden layer size 150\nSource embedding size 150\nTarget embedding size 150\nBatch size 64\nOptimizer SGD\nLearning rate 1.0\nTraining epochs 13\nTable 1: Hyperparameters for multilingual g2p\nmodels\nin forward and reverse directions. We used long\nshort-term memory units (Hochreiter and Schmid-\nhuber, 1997) for both the encoder and decoder.\nFor the attention mechanism, we used the general\nglobal attention architecture described by Luong\net al. (2015).\nWe implemented2all models with OpenNMT\n(Klein et al., 2017). Our hyperparameters, which\nwe determined by experimentation, are listed in\nTable 1.\n4.2 Training Multilingual Models\nPresenting pronunciation data in several languages\nto the network might create problems because dif-\nferent languages have different pronunciation pat-\nterns. For example, the string \u2018real\u2019 is pronounced\ndifferently in English, German, Spanish, and Por-\ntuguese. We solve this problem by prepending\neach grapheme sequence with an arti\ufb01cial token\nconsisting of the language\u2019s ISO 639-3 code en-\nclosed in angle brackets. The English word \u2018real\u2019,\nfor example, would be presented to the system as\n<eng>r e a l\nThe arti\ufb01cial token is treated simply as an element\nof the grapheme sequence. This is similar to the\napproach taken by Johnson et al. (2016) in their\nzero-shot NMT system. However, their source-\nside arti\ufb01cial tokens identify the target language,\nwhereas ours identify the source language. An\nalternative approach, used by \u00a8Ostling and Tiede-\nmann (2017), would be to concatenate a language\nembedding to the input at each time step. They\ndo not evaluate their approach on grapheme-to-\nphoneme conversion.\n5 Data\nIn order to train a neural g2p system, one needs a\nlarge quantity of pronunciation data. A standard\n2https://github.com/bpopeters/mg2p", "start_char_idx": 0, "end_char_idx": 3647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "76b58edd-ef42-4c84-be01-57af7fda0024": {"__data__": {"id_": "76b58edd-ef42-4c84-be01-57af7fda0024", "embedding": null, "metadata": {"page_label": "4", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c402d675-8b25-4e53-9b00-3ddde2cc03c4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "3c617bf1b908f302848c0b0efd52ef96870ff6c3ae1b63c325832b47cc5bcf37", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c037ee30-8133-4839-b08b-7c652fb85248", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "e26bc83b70f8446d4c284430df89420030e629e58a481281ad552053e58499bb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8474b79-dfec-41da-8800-44a185accb65", "node_type": "1", "metadata": {}, "hash": "c9a66c9462f4a2c8fa1120ae739d81f0a057946bcc9b57f51c34b1e6af9397f9", "class_name": "RelatedNodeInfo"}}, "text": "Split Train Test\nLanguages 311 507\nWords 631,828 25,894\nScripts 42 45\nTable 2: Corpus Statistics\ndataset for g2p is the Carnegie Mellon Pronounc-\ning Dictionary (Lenzo, 2007). However, that is a\nmonolingual English resource, so it is unsuitable\nfor our multilingual task. Instead, we use the mul-\ntilingual pronunciation corpus collected by Deri\nand Knight (2016) for all experiments. This cor-\npus consists of spelling\u2013pronunciation pairs ex-\ntracted from Wiktionary. It is already partitioned\ninto training and test sets. Corpus statistics are\npresented in Table 2.\nIn addition to the raw IPA transcriptions ex-\ntracted from Wiktionary, the corpus provides an\nautomatically cleaned version of transcriptions.\nCleaning is a necessary step because web-scraped\ndata is often noisy and may be transcribed at\nan inconsistent level of detail. The data clean-\ning used here attempts to make the transcriptions\nconsistent with the phonemic inventories used in\nPhoible (Moran et al., 2014). When a transcrip-\ntion contains a phoneme that is not in its lan-\nguage\u2019s inventory in Phoible, that phoneme is re-\nplaced by the phoneme with the most similar ar-\nticulatory features that is in the language\u2019s inven-\ntory. Sometimes this cleaning algorithm works\nwell: in the German examples in Table 3, the raw\nGerman symbols /X/and/\u00e7/are both converted\nto/x/. This is useful because the /X/inAns-\nbach and the /\u00e7/inKaninchen are instances of the\nsame phoneme, so their phonemic representations\nshould use the same symbol. However, the clean-\ning algorithm can also have negative effects on the\ndata quality. For example, the phoneme /\u00f4/is not\npresent in the Phoible inventory for German, but it\nisused in several German transcriptions in the cor-\npus. The cleaning algorithm converts /\u00f4/to/l/in\nall German transcriptions, whereas /r/would be\na more reasonable guess. The cleaning algorithm\nalso removes most suprasegmentals, even though\nthese are often an important part of a language\u2019s\nphonology. Developing a more sophisticated pro-\ncedure for cleaning pronunciation data is a direc-\ntion for future work, but in this paper we use the\ncorpus\u2019s provided cleaned transcriptions in order\nto ease comparison to previous results.Lang. Script Spelling Cleaned IPA Raw IPA\ndeu Latin Ansbach a: n s b a: x \"ansbaX\ndeu Latin Kaninchen k a: n I n x @ n ka\"ni:n\u00e7@n\neus Latin untxi u n \u201d t \u201d S I \"un.>tSi\nTable 3: Example entries from the Wiktionary\ntraining corpus\n6 Experiments\nWe present experiments with two versions of our\nsequence-to-sequence model. LangID prepends\neach training, validation, and test sample with\nan arti\ufb01cial token identifying the language of the\nsample. NoLangID omits this token. LangID and\nNoLangID have identical structure otherwise. To\ntranslate the test corpus, we used a beam width of\n100. Although this is an unusually wide beam and\nhad negligible performance effects, it was neces-\nsary to compute our error metrics.\n6.1 Evaluation\nWe use the following three evaluation metrics:\n\u2022Phoneme Error Rate (PER) is the Lev-\nenshtein distance between the predicted\nphoneme sequences and the gold standard\nphoneme sequences, divided by the length of\nthe gold standard phoneme sequences.\n\u2022Word Error Rate (WER) is the percentage of\nwords in which the predicted phoneme se-\nquence does not exactly match the gold stan-\ndard phoneme sequence.\n\u2022Word Error Rate 100 (WER 100) is the per-\ncentage of words in the test set for which the\ncorrect guess is not in the \ufb01rst 100 guesses of\nthe system.\nIn system evaluations, WER, WER 100, and\nPER numbers presented for multiple languages are\naveraged, weighting each language equally (fol-\nlowing Deri and Knight, 2016).\nIt would be interesting to compute error metrics\nthat incorporate phoneme similarity, such as those\nproposed by Hixon et al. (2011).", "start_char_idx": 0, "end_char_idx": 3791, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8474b79-dfec-41da-8800-44a185accb65": {"__data__": {"id_": "c8474b79-dfec-41da-8800-44a185accb65", "embedding": null, "metadata": {"page_label": "4", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c402d675-8b25-4e53-9b00-3ddde2cc03c4", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "3c617bf1b908f302848c0b0efd52ef96870ff6c3ae1b63c325832b47cc5bcf37", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "76b58edd-ef42-4c84-be01-57af7fda0024", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "15d296dabfc74e77b41a70189029100b924344a096ef3a475d3dc4302a33b47e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fd6b9d1b-3ce9-470f-8b1d-0ef4283b90fb", "node_type": "1", "metadata": {}, "hash": "48a06ba9a51c2474c2adf2a58faafeb83874b11b16697a09061261088234a673", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Word Error Rate (WER) is the percentage of\nwords in which the predicted phoneme se-\nquence does not exactly match the gold stan-\ndard phoneme sequence.\n\u2022Word Error Rate 100 (WER 100) is the per-\ncentage of words in the test set for which the\ncorrect guess is not in the \ufb01rst 100 guesses of\nthe system.\nIn system evaluations, WER, WER 100, and\nPER numbers presented for multiple languages are\naveraged, weighting each language equally (fol-\nlowing Deri and Knight, 2016).\nIt would be interesting to compute error metrics\nthat incorporate phoneme similarity, such as those\nproposed by Hixon et al. (2011). PER weights all\nphoneme errors the same, even though some errors\nare more harmful than others: /d/and/k/are usu-\nally contrastive, whereas /d/and/d \u201d/almost never\nare. Such statistics would be especially interest-\ning for evaluating a multilingual system, because\ndifferent languages often map the same grapheme\nto phonemes that are only subtly different from", "start_char_idx": 3187, "end_char_idx": 4151, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fd6b9d1b-3ce9-470f-8b1d-0ef4283b90fb": {"__data__": {"id_": "fd6b9d1b-3ce9-470f-8b1d-0ef4283b90fb", "embedding": null, "metadata": {"page_label": "5", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d2814921-d22c-45db-8c20-209e09adc6c2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "30ba51b8495aad853bfff9664b2e56530470631bfad84cbcd604c2cf82286c0a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8474b79-dfec-41da-8800-44a185accb65", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "8ddd806b4ebfb2faf68d3c64d116e668e87c4eeac66a415781d878937b7ef247", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49b06c9a-2624-4921-9962-bb6f5c36b2a5", "node_type": "1", "metadata": {}, "hash": "a0a452db81f226a23337c97add582396889c5e811a4ace4bcc05eec643908c13", "class_name": "RelatedNodeInfo"}}, "text": "each other. However, these statistics have not been\nwidely reported for other g2p systems, so we omit\nthem here.\n6.2 Baseline\nResults on LangID and NoLangID are compared\nto the system presented by Deri and Knight\n(2016), which is identi\ufb01ed in our results as wFST.\nTheir results can be divided into two parts:\n\u2022High resource results, computed with wFSTs\ntrained on a combination of Wiktionary pro-\nnunciation data and g2p rules extracted from\nWikipedia IPA Help pages. They report high\nresource results for 85 languages.\n\u2022Adapted results, where they apply various\nmapping strategies in order to adapt high re-\nsource models to other languages. The \ufb01nal\nadapted results they reported include most of\nthe 85 languages with high resource results,\nas well as the various languages they were\nable to adapt them for, for a total of 229 lan-\nguages. This test set omits 23 of the high re-\nsource languages that are written in unique\nscripts or for which language distance met-\nrics could not be computed.\n6.3 Training\nWe train the LangID and NoLangID versions of\nour model each on three subsets of the Wiktionary\ndata:\n\u2022LangID-High and NoLangID-High: Trained\non data from the 85 languages for which Deri\nand Knight (2016) used non-adapted wFST\nmodels.\n\u2022LangID-Adapted and NoLangID-Adapted:\nTrained on data from any of the 229 lan-\nguages for which they built adapted mod-\nels. Because many of these languages had\nno training data at all, the model is actually\nonly trained on data in 157 languages. As is\nnoted above, the Adapted set omits 23 lan-\nguages which are in the High test set.\n\u2022LangID-All and NoLangID-All: Trained on\ndata in all 311 languages in the Wiktionary\ntraining corpus.\nIn order to ease comparison to Deri and\nKnight\u2019s system, we limited our use of the training\ncorpus to 10,000 words per language. We set aside10 percent of the data in each language for valida-\ntion, so the maximum number of training words\nfor any language is 9000 for our systems.\n6.4 Adapted Results\nOn the 229 languages for which Deri and Knight\n(2016) presented their \ufb01nal results, the LangID\nversion of our system outperforms the baseline by\na wide margin. The best performance came with\nthe version of our model that was trained on data\nin all available languages, not just the languages\nit was tested on. Using a language ID token im-\nproves results considerably, but even NoLangID\nbeats the baseline in WER and WER 100. Full\nresults are presented in Table 4.\nModel WER WER 100 PER\nwFST 88.04 69.80 48.01\nLangID-High 74.99 46.18 42.64\nLangID-Adapted 75.06 46.39 41.77\nLangID-All 74.10 43.23 37.85\nNoLangID-High 82.14 50.17 54.05\nNoLangID-Adapted 85.11 48.24 55.93\nNoLangID-All 83.65 47.13 51.87\nTable 4: Adapted Results\n6.5 High Resource Results\nHaving shown that our model exceeds the perfor-\nmance of the wFST-adaptation approach, we next\ncompare it to the baseline models for just high\nresource languages. The wFST models here are\npurely monolingual \u2013 they do not use data adap-\ntation because there is suf\ufb01cient training data for\neach of them. Full results are presented in Table 5.\nWe omit models trained on the Adapted languages\nbecause they were not trained on high resource\nlanguages with unique writing systems, such as\nGeorgian and Greek, and consequently performed\nvery poorly on them.\nIn contrast to the larger-scale Adapted results,\nin the High Resource experiments none of the\nsequence-to-sequence approaches equal the per-\nformance of the wFST model in WER and PER,\nalthough LangID-High does come close. The\nLangID models do beat wFST in WER 100.", "start_char_idx": 0, "end_char_idx": 3548, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49b06c9a-2624-4921-9962-bb6f5c36b2a5": {"__data__": {"id_": "49b06c9a-2624-4921-9962-bb6f5c36b2a5", "embedding": null, "metadata": {"page_label": "5", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d2814921-d22c-45db-8c20-209e09adc6c2", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "30ba51b8495aad853bfff9664b2e56530470631bfad84cbcd604c2cf82286c0a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fd6b9d1b-3ce9-470f-8b1d-0ef4283b90fb", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "f33e1a38d61c4e776ff24dfc83a473912226ed9b236b63e1233c8660615f8d3a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d527e945-a7e4-42dd-b463-2d3b427514da", "node_type": "1", "metadata": {}, "hash": "6b3cb6b069860d1b7062fc0781babfc3570dcbf34e770fdabee6568e9a03bf44", "class_name": "RelatedNodeInfo"}}, "text": "The wFST models here are\npurely monolingual \u2013 they do not use data adap-\ntation because there is suf\ufb01cient training data for\neach of them. Full results are presented in Table 5.\nWe omit models trained on the Adapted languages\nbecause they were not trained on high resource\nlanguages with unique writing systems, such as\nGeorgian and Greek, and consequently performed\nvery poorly on them.\nIn contrast to the larger-scale Adapted results,\nin the High Resource experiments none of the\nsequence-to-sequence approaches equal the per-\nformance of the wFST model in WER and PER,\nalthough LangID-High does come close. The\nLangID models do beat wFST in WER 100. A\npossible explanation is that a monolingual wFST\nmodel will never generate phonemes that are not\npart of the language\u2019s inventory. A multilingual\nmodel, on the other hand, could potentially gener-", "start_char_idx": 2896, "end_char_idx": 3746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d527e945-a7e4-42dd-b463-2d3b427514da": {"__data__": {"id_": "d527e945-a7e4-42dd-b463-2d3b427514da", "embedding": null, "metadata": {"page_label": "6", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c3ead361-c540-4751-afbe-ccfe6f550959", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "7fb2201138492ea3abf3fc1186ce77984b6b75b42875fd85ca58c857f3c666ef", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49b06c9a-2624-4921-9962-bb6f5c36b2a5", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "cf4206d7a7618c54d5096b4ecd43f06b27e07d9afe1a6bdd0bb1f4aa207f14fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77a13371-9836-4f53-9a3f-f252b9716849", "node_type": "1", "metadata": {}, "hash": "69c95125547f60662873bc072bfc4d642ced9bed926ac3d132f64bf491c28c61", "class_name": "RelatedNodeInfo"}}, "text": "ate phonemes from the inventories of any language\nit has been trained on.\nEven if LangID-High does not present a more\naccurate result, it does present a more compact\none: LangID-High is 15.4 MB, while the com-\nbined wFST high resource models are 197.5 MB.\nModel WER WER 100 PER\nwFST 44.17 21.97 14.70\nLangID-High 47.88 15.50 16.89\nLangID-All 48.76 15.78 17.35\nNoLangID-High 69.72 29.24 35.16\nNoLangID-All 69.82 29.27 35.47\nTable 5: High Resource Results\n6.6 Results on Unseen Languages\nFinally, we report our models\u2019 results on unseen\nlanguages in Table 6. The unseen languages are\nany that are present in the test corpus but absent\nfrom the training data. Deri and Knight did not\nreport results speci\ufb01cally on these languages. Al-\nthough the NoLangID models sometimes do better\non WER 100, even here the LangID models have a\nslight advantage in WER and PER. This is some-\nwhat surprising because the LangID models have\nnot learned embeddings for the language ID to-\nkens of unseen languages. Perhaps negative asso-\nciations are also being learned, driving the model\ntowards predicting more common pronunciations\nfor unseen languages.\nModel WER WER 100 PER\nLangID-High 85.94 58.10 53.06\nLangID-Adapted 87.78 68.40 65.62\nLangID-All 86.27 62.31 54.33\nNoLangID-High 88.52 58.21 62.02\nNoLangID-Adapted 91.27 57.61 74.07\nNoLangID-All 89.96 56.29 62.79\nTable 6: Results on languages not in the training\ncorpus\n7 Discussion\n7.1 Language ID Tokens\nAdding a language ID token always improves\nresults in cases where an embedding has been\nlearned for that token. The power of these em-\nbeddings is demonstrated by what happens whenone feeds the same input word to the model with\ndifferent language tokens, as is seen in Table 7.\nImpressively, this even works when the source se-\nquence is in the wrong script for the language, as\nis seen in the entry for Arabic.\nLanguage Pronunciation\nEnglish d Z u: \u00e6I s\nGerman j U t s @\nSpanish x w i T e\ufb02\nItalian d Z u i t S e\nPortuguese Z w i s \u02dci\nTurkish Z U I d \u201d Z E\nArabic j u: i s\nTable 7: The word \u2018juice\u2019 translated by the\nLangID-All model with various language ID to-\nkens. The incorrect English pronunciation rhymes\nwith the system\u2019s result for \u2018ice\u2019\n7.2 Language Embeddings\nBecause these language ID tokens are so useful,\nit would be good if they could be effectively es-\ntimated for unseen languages. \u00a8Ostling and Tiede-\nmann (2017) found that the language vectors their\nmodels learned correlated well to genetic relation-\nships, so it would be interesting to see if the em-\nbeddings our source encoder learned for the lan-\nguage ID tokens showed anything similar. In a few\ncases they do (the languages closest to German\nin the vector space are Luxembourgish, Bavarian,\nand Yiddish, all close relatives). However, for the\nmost part the structure of these vectors is not in-\nterpretable. Therefore, it would be dif\ufb01cult to esti-\nmate the embedding for an unseen language, or to\n\u201cborrow\u201d the language ID token of a similar lan-\nguage. A more promising way forward is to \ufb01nd a\nmodel that uses an externally constructed typolog-\nical representation of the language.\n7.3 Phoneme Embeddings\nIn contrast to the language embeddings, the\nphoneme embeddings appear to show many reg-\nularities (see Table 8). This is a sign that our\nmultilingual model learns similar embeddings for\nphonemes that are written with the same grapheme\nin different languages. These phonemes tend to be\nphonetically similar to each other.\nPerhaps the structure of the phoneme embed-\nding space is what leads to our models\u2019 very\ngood performance on WER 100. Even when the", "start_char_idx": 0, "end_char_idx": 3575, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77a13371-9836-4f53-9a3f-f252b9716849": {"__data__": {"id_": "77a13371-9836-4f53-9a3f-f252b9716849", "embedding": null, "metadata": {"page_label": "7", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c50c7bb-d1a3-470a-92f9-df245329242c", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a6e2e8d912d924891dedcd7b6743e1d96d6b58fe26abd90ea2886a69e5ee4adc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d527e945-a7e4-42dd-b463-2d3b427514da", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "7fb2201138492ea3abf3fc1186ce77984b6b75b42875fd85ca58c857f3c666ef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1de9c534-ed62-4759-9689-25566bf91a84", "node_type": "1", "metadata": {}, "hash": "a6fae57ab9a4e6cd6153b71f0bd6702a837a8b5a3bb89a28840b0ca1f9abc748", "class_name": "RelatedNodeInfo"}}, "text": "model\u2019s \ufb01rst predicted pronunciation is not cor-\nrect, it tends to assign more probability mass to\nguesses that are more similar to the correct one.\nApplying some sort of \ufb01ltering or reranking of the\nsystem output might therefore lead to better per-\nformance.\nPhoneme Closest phonemes\nb ph,B,F\n@ \u02dc a,\u02d8 e,W\ntht:,t,t \u201d\nx X,G,\u00e8\ny y:,Y,I\n\u00f4 RG,r \u201d,R\nTable 8: Selected phonemes and the most similar\nphonemes, measured by the cosine similarity of\nthe embeddings learned by the LangID-All model\n7.4 Future Work\nBecause the language ID token is so bene\ufb01cial to\nperformance, it would be very interesting to \ufb01nd\nways to extend a similar bene\ufb01t to unseen lan-\nguages. One possible way to do so is with tokens\nthat identify something other than the language,\nsuch as typological features about the language\u2019s\nphonemic inventory. This could enable better\nsharing of resources among languages. Such typo-\nlogical knowledge is readily available in databases\nlike Phoible and WALS for a wide variety of lan-\nguages. It would be interesting to explore if any of\nthese features is a good predictor of a language\u2019s\northographic rules.\nIt would also be interesting to apply the arti-\n\ufb01cial token approach to other problems besides\nmultilingual g2p. One closely related application\nis monolingual English g2p. Some of the ambi-\nguity of English spelling is due to the wide vari-\nety of loanwords in the language, many of which\nhave unassimilated spellings. Knowing the origins\nof these loanwords could provide a useful hint for\n\ufb01guring out their pronunciations. The etymology\nof a word could be tagged in an analogous way to\nhow language ID is tagged in multilingual g2p.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473 .\nMaximilian Bisani and Hermann Ney. 2008. Joint-\nsequence models for grapheme-to-phoneme conver-\nsion. Speech communication 50(5):434\u2013451.Aliya Deri and Kevin Knight. 2016. Grapheme-to-\nphoneme models for (almost) any language. In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics . volume 1, pages\n399\u2013408.\nMatthew S. Dryer and Martin Haspelmath, editors.\n2013. WALS Online . Max Planck Institute for Evo-\nlutionary Anthropology, Leipzig. http://wals.info/.\nBen Hixon, Eric Schneider, and Susan L Epstein. 2011.\nPhonemic similarity metrics to compare pronunci-\nation methods. In Twelfth Annual Conference of\nthe International Speech Communication Associa-\ntion (INTERSPEECH) . Florence, Italy, pages 825\u2013\n828.\nSepp Hochreiter and J \u00a8urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation\n9(8):1735\u20131780.\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Vi \u00b4egas, Martin Wattenberg, Greg Corrado,\net al. 2016. Google\u2019s multilingual neural machine\ntranslation system: Enabling zero-shot translation.\narXiv preprint arXiv:1611.04558 .\nPreethi Jyothi and Mark Hasegawa-Johnson. 2017.\nLow-resource grapheme-to-phoneme conversion us-\ning recurrent neural networks. In Proc. ICASSP .\nYoung-Bum Kim and Benjamin Snyder. 2012. Uni-\nversal grapheme-to-phoneme prediction over Latin\nalphabets. In Proceedings of the 2012 Joint Con-\nference on Empirical Methods in Natural Language\nProcessing and Computational Natural Language\nLearning . Association for Computational Linguis-\ntics, pages 332\u2013343.\nG. Klein, Y . Kim, Y . Deng, J. Senellart, and A. M.\nRush. 2017. OpenNMT: Open-Source Toolkit\nfor Neural Machine Translation.", "start_char_idx": 0, "end_char_idx": 3549, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1de9c534-ed62-4759-9689-25566bf91a84": {"__data__": {"id_": "1de9c534-ed62-4759-9689-25566bf91a84", "embedding": null, "metadata": {"page_label": "7", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c50c7bb-d1a3-470a-92f9-df245329242c", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a6e2e8d912d924891dedcd7b6743e1d96d6b58fe26abd90ea2886a69e5ee4adc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77a13371-9836-4f53-9a3f-f252b9716849", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "f1287f1a91e961286dc57c890a4301b6f10fa10fc3acaa4ad522fca268c337cc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04cebd34-d844-4bde-9e44-55afa023d9e8", "node_type": "1", "metadata": {}, "hash": "731a54e2b6e5db2eda7bfb08ad909c420ba26415af3d8ac3b233e33408fc479d", "class_name": "RelatedNodeInfo"}}, "text": "Google\u2019s multilingual neural machine\ntranslation system: Enabling zero-shot translation.\narXiv preprint arXiv:1611.04558 .\nPreethi Jyothi and Mark Hasegawa-Johnson. 2017.\nLow-resource grapheme-to-phoneme conversion us-\ning recurrent neural networks. In Proc. ICASSP .\nYoung-Bum Kim and Benjamin Snyder. 2012. Uni-\nversal grapheme-to-phoneme prediction over Latin\nalphabets. In Proceedings of the 2012 Joint Con-\nference on Empirical Methods in Natural Language\nProcessing and Computational Natural Language\nLearning . Association for Computational Linguis-\ntics, pages 332\u2013343.\nG. Klein, Y . Kim, Y . Deng, J. Senellart, and A. M.\nRush. 2017. OpenNMT: Open-Source Toolkit\nfor Neural Machine Translation. arXiv preprint\narXiv:1701.02810 .\nKevin Lenzo. 2007. The CMU pronouncing dictionary.\nPatrick Littell, David Mortensen, Ke Lin, Katherine\nKairis, Carlisle Turner, and Lori Levin. 2017. Uriel\nand lang2vec: Representing languages as typolog-\nical, geographical, and phylogenetic vectors. In\nEACL 2017 . Valencia, Spain, pages 8\u201314.\nMinh-Thang Luong, Hieu Pham, and Christopher D.\nManning. 2015. Effective approaches to attention-\nbased neural machine translation. arXiv preprint\narXiv:1508.04025 .\nSteven Moran, Daniel McCloy, and Richard Wright,\neditors. 2014. PHOIBLE Online . Max Planck\nInstitute for Evolutionary Anthropology, Leipzig.\nhttp://phoible.org .\nJosef Robert Novak, Nobuaki Minematsu, and Keikichi\nHirose. 2016. Phonetisaurus: Exploring grapheme-\nto-phoneme conversion with joint n-gram models in", "start_char_idx": 2846, "end_char_idx": 4358, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04cebd34-d844-4bde-9e44-55afa023d9e8": {"__data__": {"id_": "04cebd34-d844-4bde-9e44-55afa023d9e8", "embedding": null, "metadata": {"page_label": "8", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c322d402-f3b8-4588-9192-a7a9d0f09457", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "23530eb8d4dab612355d2eda6f1e5e5ab123a8e5d02904e5c5d8a478ce58f5ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1de9c534-ed62-4759-9689-25566bf91a84", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "bbaf218f2fbf951c32899fea4b5d6b7aeb2083a92ea06144cb435b2cd0c50b1f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "693cfe0b-d036-44a9-b7f2-868501c09665", "node_type": "1", "metadata": {}, "hash": "1f0321f6b12feb6bff9c4fa936b0e829c44aaed179dfd1ac5b1967022aaee6a9", "class_name": "RelatedNodeInfo"}}, "text": "the WFST framework. Natural Language Engineer-\ning22(6):907\u2013938.\nRobert \u00a8Ostling and J \u00a8org Tiedemann. 2017. Continuous\nmultilinguality with language vectors. EACL 2017\npage 644.\nKanishka Rao, Fuchun Peng, Has \u00b8im Sak, and\nFranc \u00b8oise Beaufays. 2015. Grapheme-to-phoneme\nconversion using long short-term memory recurrent\nneural networks. In International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) .\npages 4225\u20134229.\nTim Schlippe, Wolf Quaschningk, and Tanja Schultz.\n2014. Combining grapheme-to-phoneme converter\noutputs for enhanced pronunciation generation in\nlow-resource scenarios. In International Work-\nshop on Spoken Language Technologies for Under-\nresourced Languages (SLTU) . pages 139\u2013145.\nYulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui,\nGuillaume Lample, Patrick Littell, David\nMortensen, Alan W Black, Lori Levin, and Chris\nDyer. 2016. Polyglot neural language models: A\ncase study in cross-lingual phonetic representation\nlearning. In NAACL 2016 . San Diego, California,\npages 1357\u20131366.\nKaisheng Yao and Geoffrey Zweig. 2015. Sequence-\nto-sequence neural net models for grapheme-\nto-phoneme conversion. arXiv preprint\narXiv:1506.00196 .", "start_char_idx": 0, "end_char_idx": 1174, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "693cfe0b-d036-44a9-b7f2-868501c09665": {"__data__": {"id_": "693cfe0b-d036-44a9-b7f2-868501c09665", "embedding": null, "metadata": {"page_label": "1", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd2b9134-d1eb-4afb-b4a2-13c7dfd106e1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "6dda72cf52aad94a19a9d1e78eed710336f03e92a1f98179ea518d76e48594df", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04cebd34-d844-4bde-9e44-55afa023d9e8", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "23530eb8d4dab612355d2eda6f1e5e5ab123a8e5d02904e5c5d8a478ce58f5ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c99b79c7-0fb0-4f10-8dc9-330276e166bf", "node_type": "1", "metadata": {}, "hash": "b3ff7d9d556e684db1dd3fddd05523c8bbd62e65a52f3aff5a16483db88a2588", "class_name": "RelatedNodeInfo"}}, "text": "Adversarial Learning for Chinese NER from Crowd Annotations\u2217\nYaosheng Yang1, Meishan Zhang4, Wenliang Chen1\nWei Zhang2, Haofen Wang3, Min Zhang1\n1School of Computer Science and Technology, Soochow University, China\n2Alibaba Group and3Shenzhen Gowild Robotics Co. Ltd\n4School of Computer Science and Technology, Heilongjiang University, China\n1ysyang@stu.suda.edu.cn, {wlchen, minzhang}@suda.edu.cn\n4mason.zms@gmail.com,2lantu.zw@alibaba-inc.com,3wang haofen@gowild.cn\nAbstract\nTo quickly obtain new labeled data, we can choose crowd-\nsourcing as an alternative way at lower cost in a short time.\nBut as an exchange, crowd annotations from non-experts may\nbe of lower quality than those from experts. In this paper, we\npropose an approach to performing crowd annotation learning\nfor Chinese Named Entity Recognition (NER) to make full\nuse of the noisy sequence labels from multiple annotators. In-\nspired by adversarial learning, our approach uses a common\nBi-LSTM and a private Bi-LSTM for representing annotator-\ngeneric and -speci\ufb01c information. The annotator-generic in-\nformation is the common knowledge for entities easily mas-\ntered by the crowd. Finally, we build our Chinese NE tagger\nbased on the LSTM-CRF model. In our experiments, we cre-\nate two data sets for Chinese NER tasks from two domains.\nThe experimental results show that our system achieves better\nscores than strong baseline systems.\nIntroduction\nThere has been signi\ufb01cant progress on Named Entity Recog-\nnition (NER) in recent years using models based on machine\nlearning algorithms (Zhao and Kit 2008; Collobert et al.\n2011; Lample et al. 2016). As with other Natural Language\nProcessing (NLP) tasks, building NER systems typically re-\nquires a massive amount of labeled training data which are\nannotated by experts. In real applications, we often need to\nconsider new types of entities in new domains where we do\nnot have existing annotated data. For such new types of en-\ntities, however, it is very hard to \ufb01nd experts to annotate the\ndata within short time limits and hiring experts is costly and\nnon-scalable, both in terms of time and money.\nIn order to quickly obtain new training data, we can use\ncrowdsourcing as one alternative way at lower cost in a\nshort time. But as an exchange, crowd annotations from non-\nexperts may be of lower quality than those from experts. It\nis one biggest challenge to build a powerful NER system on\nsuch a low quality annotated data. Although we can obtain\nhigh quality annotations for each input sentence by majority\nvoting, it can be a waste of human labors to achieve such\na goal, especially for some ambiguous sentences which may\nrequire a number of annotations to reach an agreement. Thus\n\u2217The corresponding author is Wenliang Chen.\nCopyright c\u20dd2018, Association for the Advancement of Arti\ufb01cial\nIntelligence (www.aaai.org). All rights reserved.majority work directly build models on crowd annotations,\ntrying to model the differences among annotators, for exam-\nple, some of the annotators may be more trustful (Rodrigues,\nPereira, and Ribeiro 2014; Nguyen et al. 2017).\nHere we focus mainly on the Chinese NER, which is more\ndif\ufb01cult than NER for other languages such as English for\nthe lack of morphological variations such as capitalization\nand in particular the uncertainty in word segmentation. The\nChinese NE taggers trained on news domain often perform\npoor in other domains. Although we can alleviate the prob-\nlem by using character-level tagging to resolve the problem\nof poor word segmentation performances (Peng and Dredze\n2015), still there exists a large gap when the target domain\nchanges, especially for the texts of social media. Thus, in\norder to get a good tagger for new domains and also for the\nconditions of new entity types, we require large amounts of\nlabeled data. Therefore, crowdsourcing is a reasonable solu-\ntion for these situations.\nIn this paper, we propose an approach to training a Chi-\nnese NER system on the crowd-annotated data. Our goal is\nto extract additional annotator independent features by ad-\nversarial training, alleviating the annotation noises of non-\nexperts.", "start_char_idx": 0, "end_char_idx": 4130, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c99b79c7-0fb0-4f10-8dc9-330276e166bf": {"__data__": {"id_": "c99b79c7-0fb0-4f10-8dc9-330276e166bf", "embedding": null, "metadata": {"page_label": "1", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cd2b9134-d1eb-4afb-b4a2-13c7dfd106e1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "6dda72cf52aad94a19a9d1e78eed710336f03e92a1f98179ea518d76e48594df", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "693cfe0b-d036-44a9-b7f2-868501c09665", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "ed5a3f45ca9cbd6acf1cb37fc433124ba4b99842abc0cf2bd40a0040d31da905", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "04d7545b-4492-4144-b8fb-820ce2afaa3c", "node_type": "1", "metadata": {}, "hash": "85307863626b4aee76fdf6831316dbdd6e4d64e9d541754fcbb62172a0c753cf", "class_name": "RelatedNodeInfo"}}, "text": "The\nChinese NE taggers trained on news domain often perform\npoor in other domains. Although we can alleviate the prob-\nlem by using character-level tagging to resolve the problem\nof poor word segmentation performances (Peng and Dredze\n2015), still there exists a large gap when the target domain\nchanges, especially for the texts of social media. Thus, in\norder to get a good tagger for new domains and also for the\nconditions of new entity types, we require large amounts of\nlabeled data. Therefore, crowdsourcing is a reasonable solu-\ntion for these situations.\nIn this paper, we propose an approach to training a Chi-\nnese NER system on the crowd-annotated data. Our goal is\nto extract additional annotator independent features by ad-\nversarial training, alleviating the annotation noises of non-\nexperts. The idea of adversarial training in neural networks\nhas been used successfully in several NLP tasks, such as\ncross-lingual POS tagging (Kim et al. 2017) and cross-\ndomain POS tagging (Gui et al. 2017). They use it to reduce\nthe negative in\ufb02uences of the input divergences among dif-\nferent domains or languages, while we use adversarial train-\ning to reduce the negative in\ufb02uences brought by different\ncrowd annotators. To our best knowledge, we are the \ufb01rst to\napply adversarial training for crowd annotation learning.\nIn the learning framework, we perform adversarial train-\ning between the basic NER and an additional worker dis-\ncriminator. We have a common Bi-LSTM for representing\nannotator-generic information and a private Bi-LSTM for\nrepresenting annotator-speci\ufb01c information. We build an-\nother label Bi-LSTM by the crowd-annotated NE label se-\nquence which re\ufb02ects the mind of the crowd annotators who\nlearn entity de\ufb01nitions by reading the annotation guidebook.\nThe common and private Bi-LSTMs are used for NER,\nwhile the common and label Bi-LSTMs are used as inputs\nfor the worker discriminator. The parameters of the com-\nmon Bi-LSTM are learned by adversarial training, maximiz-\ning the worker discriminator loss and meanwhile minimiz-arXiv:1801.05147v1  [cs.CL]  16 Jan 2018", "start_char_idx": 3322, "end_char_idx": 5422, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04d7545b-4492-4144-b8fb-820ce2afaa3c": {"__data__": {"id_": "04d7545b-4492-4144-b8fb-820ce2afaa3c", "embedding": null, "metadata": {"page_label": "2", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "289a0fc7-c14f-4200-906d-9991f20c2cab", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "ddf3110e3f4a3b37776ab3e4928ab989d4909a25add9b06c13459b48de554e28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c99b79c7-0fb0-4f10-8dc9-330276e166bf", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "2f22f4a895e251bbcd6d3fc91c1ae721bb5b10668467b1b72e36572e8f68fe54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78335aaf-5fe7-4da6-8e9f-1d46ccc6bc3b", "node_type": "1", "metadata": {}, "hash": "3b97874e710e3f117cc0721b3baabfeba50ebc090c8f5c7cbd87fe1b044b0930", "class_name": "RelatedNodeInfo"}}, "text": "ing the NER loss. Thus the resulting features of the common\nBi-LSTM are worker invariant and NER sensitive.\nFor evaluation, we create two Chinese NER datasets in\ntwo domains: dialog and e-commerce. We require the crowd\nannotators to label the types of entities, including person,\nsong, brand, product, and so on. Identifying these entities\nis useful for chatbot and e-commerce platforms (Kl \u00a8uwer\n2011). Then we conduct experiments on the newly created\ndatasets to verify the effectiveness of the proposed adversar-\nial neural network model. The results show that our system\noutperforms very strong baseline systems. In summary, we\nmake the following contributions:\n\u2022We propose a crowd-annotation learning model based on\nadversarial neural networks. The model uses labeled data\ncreated by non-experts to train a NER classi\ufb01er and simul-\ntaneously learns the common and private features among\nthe non-expert annotators.\n\u2022We create two data sets in dialog and e-commerce do-\nmains by crowd annotations. The experimental results\nshow that the proposed approach performs the best among\nall the comparison systems.\nRelated Work\nOur work is related to three lines of research: Sequence la-\nbeling, Adversarial training, and Crowdsourcing.\nSequence labeling. NER is widely treated as a sequence la-\nbeling problem, by assigning a unique label over each sen-\ntential word (Ratinov and Roth 2009). Early studies on se-\nquence labeling often use the models of HMM, MEMM,\nand CRF (Lafferty et al. 2001) based on manually-crafted\ndiscrete features, which can suffer the feature sparsity prob-\nlem and require heavy feature engineering. Recently, neural\nnetwork models have been successfully applied to sequence\nlabeling (Collobert et al. 2011; Huang, Xu, and Yu 2015;\nLample et al. 2016). Among these work, the model which\nuses Bi-LSTM for feature extraction and CRF for decoding\nhas achieved state-of-the-art performances (Huang, Xu, and\nYu 2015; Lample et al. 2016), which is exploited as the base-\nline model in our work.\nAdversarial Training. Adversarial Networks have achieved\ngreat success in computer vision such as image genera-\ntion (Denton et al. 2015; Ganin et al. 2016). In the NLP\ncommunity, the method is mainly exploited under the set-\ntings of domain adaption (Zhang, Barzilay, and Jaakkola\n2017; Gui et al. 2017), cross-lingual (Chen et al. 2016;\nKim et al. 2017) and multi-task learning (Chen et al. 2017;\nLiu, Qiu, and Huang 2017). All these settings involve the\nfeature divergences between the training and test examples,\nand aim to learn invariant features across the divergences by\nan additional adversarial discriminator, such as domain dis-\ncriminator. Our work is similar to these work but is applies\non crowdsourcing learning, aiming to \ufb01nd invariant features\namong different crowdsourcing workers.\nCrowdsourcing. Most NLP tasks require a massive amount\nof labeled training data which are annotated by experts.\nHowever, hiring experts is costly and non-scalable, both in\nterms of time and money. Instead, crowdsourcing is anothersolution to obtain labeled data at a lower cost but with rela-\ntive lower quality than those from experts. Snow et al. (2008)\ncollected labeled results for several NLP tasks from Amazon\nMechanical Turk and demonstrated that non-experts annota-\ntions were quite useful for training new systems. In recent\nyears, a series of work have focused on how to use crowd-\nsourcing data ef\ufb01ciently in tasks such as classi\ufb01cation (Felt\net al. 2015; Bi et al. 2014), and compare quality of crowd\nand expert labels (Dumitrache, Aroyo, and Welty 2017).\nIn sequence labeling tasks, Dredze, Talukdar, and Cram-\nmer (2009) viewed this task as a multi-label problem while\nRodrigues, Pereira, and Ribeiro (2014) took workers iden-\ntities into account by assuming that each sentential word\nwas tagged correctly by one of the crowdsourcing workers\nand proposed a CRF-based model with multiple annotators.\nNguyen et al.", "start_char_idx": 0, "end_char_idx": 3939, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78335aaf-5fe7-4da6-8e9f-1d46ccc6bc3b": {"__data__": {"id_": "78335aaf-5fe7-4da6-8e9f-1d46ccc6bc3b", "embedding": null, "metadata": {"page_label": "2", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "289a0fc7-c14f-4200-906d-9991f20c2cab", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "ddf3110e3f4a3b37776ab3e4928ab989d4909a25add9b06c13459b48de554e28", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "04d7545b-4492-4144-b8fb-820ce2afaa3c", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "1266cdf83819efad3ffe2fdf56f09ede89aa73661daca9be539eac9ce080478f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f3860c03-49d7-4dd6-b527-a4760b19a7c3", "node_type": "1", "metadata": {}, "hash": "488bf8e4a6672e9460df8678c65bbaf30753e528800a416f25fb22636816f6c1", "class_name": "RelatedNodeInfo"}}, "text": "In recent\nyears, a series of work have focused on how to use crowd-\nsourcing data ef\ufb01ciently in tasks such as classi\ufb01cation (Felt\net al. 2015; Bi et al. 2014), and compare quality of crowd\nand expert labels (Dumitrache, Aroyo, and Welty 2017).\nIn sequence labeling tasks, Dredze, Talukdar, and Cram-\nmer (2009) viewed this task as a multi-label problem while\nRodrigues, Pereira, and Ribeiro (2014) took workers iden-\ntities into account by assuming that each sentential word\nwas tagged correctly by one of the crowdsourcing workers\nand proposed a CRF-based model with multiple annotators.\nNguyen et al. (2017) introduced a crowd representation in\nwhich the crowd vectors were added into the LSTM-CRF\nmodel at train time, but ignored them at test time. In this\npaper, we apply adversarial training on crowd annotations\non Chinese NER in new domains, and achieve better perfor-\nmances than previous studies on crowdsourcing learning.\nBaseline: LSTM-CRF\nWe use a neural CRF model as the baseline system (Ratinov\nand Roth 2009), treating NER as a sequence labeling prob-\nlem over Chinese characters, which has achieved state-of-\nthe-art performances (Peng and Dredze 2015). To this end,\nwe explore the BIEO schema to convert NER into sequence\nlabeling, following Lample et al. (2016), where sentential\ncharacter is assigned with one unique tag. Concretely, we tag\nthe non-entity character by label \u201cO\u201d, the beginning charac-\nter of an entity by \u201cB-XX\u201d, the ending character of an entity\nby \u201cE-XX\u201d and the other character of an entity by \u201cI-XX\u201d,\nwhere \u201cXX\u201d denotes the entity type.\nWe build high-level neural features from the input char-\nacter sequence by a bi-directional LSTM (Lample et al.\n2016). The resulting features are combined and then are\nfed into an output CRF layer for decoding. In summary, the\nbaseline model has three main components. First, we make\nvector representations for sentential characters x1x2\u00b7\u00b7\u00b7xn,\ntransforming the discrete inputs into low-dimensional neu-\nral inputs. Second, feature extraction is performed to obtain\nhigh-level features hner\n1hner\n2\u00b7\u00b7\u00b7hner\nn, by using a bi-directional\nLSTM (Bi-LSTM) structure together with a linear trans-\nformation over x1x2\u00b7\u00b7\u00b7xn. Third, we apply a CRF tag-\nging module over hner\n1hner\n2\u00b7\u00b7\u00b7hner\nn, obtaining the \ufb01nal output\nNE labels. The overall framework of the baseline model is\nshown by the right part of Figure 1.\nVector Representation of Characters\nTo represent Chinese characters, we simply exploit a neu-\nral embedding layer to map discrete characters into the low-\ndimensional vector representations. The goal is achieved\nby a looking-up table EW, which is a model parameter\nand will be \ufb01ne-tuned during training. The looking-up ta-\nble can be initialized either by random or by using a pre-\ntrained embeddings from large scale raw corpus. For a given\nChinese character sequence c1c2\u00b7\u00b7\u00b7cn, we obtain the vec-", "start_char_idx": 3337, "end_char_idx": 6215, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f3860c03-49d7-4dd6-b527-a4760b19a7c3": {"__data__": {"id_": "f3860c03-49d7-4dd6-b527-a4760b19a7c3", "embedding": null, "metadata": {"page_label": "3", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3b815434-5509-48f3-a18a-47d3cf16db98", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "2b2bcde186dc8d0b5b829f500d6fec0683717b9aec150d1b23d1201f8d530511", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78335aaf-5fe7-4da6-8e9f-1d46ccc6bc3b", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "b2be700e25e530a2807542c21c8988173c6ddce37ab61c76317e93308055006a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e7943d8d-04a3-4ec7-bc75-66dd8b15fefb", "node_type": "1", "metadata": {}, "hash": "9fe87b9e8e15fa96143ed7a8e316665fc4fab49112836726da7f201eaed054d2", "class_name": "RelatedNodeInfo"}}, "text": "x1 x2 ...... xn\u22121 xnhprivate\n1 hprivate\n2...... hprivate\nn\u22121hprivate\nnhcommon\nn hcommon\nn\u22121...... hcommon\n2 hcommon\n1 hlabel\nn hlabel\nn\u22121...... hlabel\n2 hlabel\n1\nx\u20321 x\u20322...... x\u2032n\u22121x\u2032n...... hner\n2 hner\n1 hner\nn\u22121hner\nn\n\u2a01...... oner\n1 oner\n2 oner\nn\u22121oner\nn...... L-PER B-PER O O\nw1 w2 ...... wn\u22121 wn \u00afy1 \u00afy2 ...... \u00afyn\u22121 \u00afyn...... hworker\n2 hworker\n1 hworker\nn\u22121hworker\nn\n\u2a01hworkeroworkerworker\nBi-LSTMBi-LSTMBi-LSTMCNNBaseline Worker-AdversarialFigure 1: The framework of the proposed model, which consists of two parts.\ntor representation of each sentential character by: xt=\nlook-up (ct,EW), t\u2208[1, n].\nFeature Extraction\nBased on the vector sequence x1x2\u00b7\u00b7\u00b7xn, we extract\nhigher-level features hner\n1hner\n2\u00b7\u00b7\u00b7hner\nnby using a bidirec-\ntional LSTM module and a simple feed-forward neural layer,\nwhich are then used for CRF tagging at the next step.\nLSTM is a type of recurrent neural network (RNN),\nwhich is designed for solving the exploding and dimin-\nishing gradients of basic RNNs (Graves and Schmidhu-\nber 2005). It has been widely used in a number of NLP\ntasks, including POS-tagging (Huang, Xu, and Yu 2015;\nMa and Hovy 2016), parsing (Dyer et al. 2015) and machine\ntranslation (Wu et al. 2016), because of its strong capabili-\nties of modeling natural language sentences.\nBy traversing x1x2\u00b7\u00b7\u00b7xnby order and reversely, we ob-\ntain the output features hprivate\n1hprivate\n2\u00b7\u00b7\u00b7hprivate\nn of the bi-\nLSTM, where hprivate\nt =\u2212 \u2192ht\u2295\u2190 \u2212ht. Here we refer this Bi-\nLSTM as private in order to differentiate it with the com-\nmon Bi-LSTM over the same character inputs which will be\nintroduced in the next section.\nFurther we make an integration of the output vectors of\nbi-directional LSTM by a linear feed-forward neural layer,\nresulting in the features hner\n1hner\n2\u00b7\u00b7\u00b7hner\nnby equation:\nhner\nt=Whprivate\nt +b, (1)\nwhere Wandbare both model parameters.\nCRF Tagging\nFinally we feed the resulting features hner\nt, t\u2208[1, n]into a\nCRF layer directly for NER decoding. CRF tagging is one\nglobally normalized model, aiming to \ufb01nd the best output\nsequence considering the dependencies between successive\nlabels. In the sequence labeling setting for NER, the output\nlabel of one position has a strong dependency on the label of\nthe previous position. For example, the label before \u201cI-XX\u201dmust be either \u201cB-XX\u201d or \u201cI-XX\u201d, where \u201cXX\u201d should be\nexactly the same.\nCRF involves two parts for prediction. First we should\ncompute the scores for each label based hner\nt, resulting in\noner\nt, whose dimension is the number of output labels. The\nother part is a transition matrix Twhich de\ufb01nes the scores\nof two successive labels. Tis also a model parameter. Based\nononer\ntandT, we use the Viterbi algorithm to \ufb01nd the best-\nscoring label sequence.\nWe can formalize the CRF tagging process as follows:\noner\nt=Wnerhner\nt, t\u2208[1, n]\nscore (X,y) =n\u2211\nt=1(ot,yt+Tyt\u22121,yt)\nyner= arg max\ny(\nscore (X,y)))\n,(2)\nwhere score (\u00b7)is the scoring function for a given output la-\nbel sequence y=y1y2\u00b7\u00b7\u00b7ynbased on input X,yneris the\nresulting label sequence, Wneris a model parameter.\nTraining\nTo train model parameters, we exploit a negative log-\nlikelihood objective as the loss function.", "start_char_idx": 0, "end_char_idx": 3154, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e7943d8d-04a3-4ec7-bc75-66dd8b15fefb": {"__data__": {"id_": "e7943d8d-04a3-4ec7-bc75-66dd8b15fefb", "embedding": null, "metadata": {"page_label": "3", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3b815434-5509-48f3-a18a-47d3cf16db98", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "2b2bcde186dc8d0b5b829f500d6fec0683717b9aec150d1b23d1201f8d530511", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f3860c03-49d7-4dd6-b527-a4760b19a7c3", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "73688bbeead6941c308ac258c613935b02066e670c86a5af77385b19453dcfb4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2da53847-a6f9-4db8-b7e5-9a3b832ca833", "node_type": "1", "metadata": {}, "hash": "f68dd0d52371919cc42e34fac403e2504939e4be92b430f8a8885344c68ada72", "class_name": "RelatedNodeInfo"}}, "text": "The\nother part is a transition matrix Twhich de\ufb01nes the scores\nof two successive labels. Tis also a model parameter. Based\nononer\ntandT, we use the Viterbi algorithm to \ufb01nd the best-\nscoring label sequence.\nWe can formalize the CRF tagging process as follows:\noner\nt=Wnerhner\nt, t\u2208[1, n]\nscore (X,y) =n\u2211\nt=1(ot,yt+Tyt\u22121,yt)\nyner= arg max\ny(\nscore (X,y)))\n,(2)\nwhere score (\u00b7)is the scoring function for a given output la-\nbel sequence y=y1y2\u00b7\u00b7\u00b7ynbased on input X,yneris the\nresulting label sequence, Wneris a model parameter.\nTraining\nTo train model parameters, we exploit a negative log-\nlikelihood objective as the loss function. We apply softmax\nover all candidate output label sequences, thus the probabil-\nity of the crowd-annotated label sequence is computed by:\np(\u00af y|X) =exp(\nscore (X,\u00af y))\n\u2211\ny\u2208YXexp(\nscore (X,y)), (3)\nwhere \u00af yis the crowd-annotated label sequences and YXis\nall candidate label sequence of input X.\nBased on the above formula, the loss function of our base-\nline model is:\nloss(\u0398,X,\u00af y) =\u2212logp(\u00af y|X), (4)\nwhere \u0398is the set of all model parameters. We use standard\nback-propagation method to minimize the loss function of\nthe baseline CRF model.", "start_char_idx": 2523, "end_char_idx": 3695, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2da53847-a6f9-4db8-b7e5-9a3b832ca833": {"__data__": {"id_": "2da53847-a6f9-4db8-b7e5-9a3b832ca833", "embedding": null, "metadata": {"page_label": "4", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "336e4992-5ac6-4372-9099-e6c746f70250", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "678bad8dfe8c377fba0b87b092a07dc6cc971d9b5432ed65546b5d1e911da2d3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e7943d8d-04a3-4ec7-bc75-66dd8b15fefb", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "fa0cdf7a68aafe6f62c7fc33930e789e6f03d3f59eb8ded9634a75bfe6575f07", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6195407-1203-4178-8fbc-990540c57f42", "node_type": "1", "metadata": {}, "hash": "7799263dfb5b2996d2e767a1010bf07ac64a52bbc0117ca4c1e0d677c31d40c1", "class_name": "RelatedNodeInfo"}}, "text": "Worker Adversarial\nAdversarial learning has been an effective mechanism to re-\nsolve the problem of the input features between the training\nand test examples having large divergences (Goodfellow et\nal. 2014; Ganin et al. 2016). It has been successfully applied\non domain adaption (Gui et al. 2017), cross-lingual learn-\ning (Chen et al. 2016) and multi-task learning (Liu, Qiu, and\nHuang 2017). All settings involve feature shifting between\nthe training and testing.\nIn this paper, our setting is different. We are using the\nannotations from non-experts, which are noise and can in-\n\ufb02uence the \ufb01nal performances if they are not properly pro-\ncessed. Directly learning based on the resulting corpus may\nadapt the neural feature extraction into the biased annota-\ntions. In this work, we assume that individual workers have\ntheir own guidelines in mind after short training. For exam-\nple, a perfect worker can annotate highly consistently with\nan expert, while common crowdsourcing workers may be\nconfused and have different understandings on certain con-\ntexts. Based on the assumption, we make an adaption for the\noriginal adversarial neural network to our setting.\nOur adaption is very simple. Brie\ufb02y speaking, the original\nadversarial learning adds an additional discriminator to clas-\nsify the type of source inputs, for example, the domain cate-\ngory in the domain adaption setting, while we add a discrim-\ninator to classify the annotation workers. Solely the features\nfrom the input sentence is not enough for worker classi\ufb01-\ncation. The annotation result of the worker is also required.\nThus the inputs of our discriminator are different. Here we\nexploit both the source sentences and the crowd-annotated\nNE labels as basic inputs for the worker discrimination.\nIn the following, we describe the proposed adversarial\nlearning module, including both the submodels and the train-\ning method. As shown by the left part of Figure 1, the\nsubmodel consists of four parts: (1) a common Bi-LSTM\nover input characters; (2) an additional Bi-LSTM to en-\ncode crowd-annotated NE label sequence; (3) a convolu-\ntional neural network (CNN) to extract features for worker\ndiscriminator; (4) output and prediction.\nCommon Bi-LSTM over Characters\nTo build the adversarial part, \ufb01rst we create a new bi-\ndirectional LSTM, named by the common Bi-LSTM:\nhcommon\n1hcommon\n2\u00b7\u00b7\u00b7hcommon\nn =Bi-LSTM (x1x2\u00b7\u00b7\u00b7xn).(5)\nAs shown in Figure 1, this Bi-LSTM is constructed over\nthe same input character representations of the private Bi-\nLSTM, in order to extract worker independent features.\nThe resulting features of the common Bi-LSTM are used\nfor both NER and the worker discriminator, different with\nthe features of private Bi-LSTM which are used for NER\nonly. As shown in Figure 1, we concatenate the outputs of\nthe common and private Bi-LSTMs together, and then feed\nthe results into the feed-forward combination layer of the\nNER part. Thus Formula 1 can be rewritten as:\nhner\nt=W(hcommon\nt\u2295hprivate\nt) +b, (6)\nwhere Wis wider than the original combination because the\nnewly-added hcommon\nt .Noticeably, although the resulting common features are\nused for the worker discriminator, they actually have no ca-\npability to distinguish the workers. Because this part is ex-\nploited to maximize the loss of the worker discriminator, it\nwill be interpreted in the later training subsection. These fea-\ntures are invariant among different workers, thus they can\nhave less noises for NER. This is the goal of adversarial\nlearning, and we hope the NER being able to \ufb01nd useful\nfeatures from these worker independent features.\nAdditional Bi-LSTM over Annotated NER Labels\nIn order to incorporate the annotated NE labels to predict the\nexact worker, we build another bi-directional LSTM (named\nby label Bi-LSTM) based on the crowd-annotated NE label\nsequence. This Bi-LSTM is used for worker discriminator\nonly. During the decoding of the testing phase, we will never\nhave this Bi-LSTM, because the worker discriminator is no\nlonger required.", "start_char_idx": 0, "end_char_idx": 4012, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6195407-1203-4178-8fbc-990540c57f42": {"__data__": {"id_": "c6195407-1203-4178-8fbc-990540c57f42", "embedding": null, "metadata": {"page_label": "4", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "336e4992-5ac6-4372-9099-e6c746f70250", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "678bad8dfe8c377fba0b87b092a07dc6cc971d9b5432ed65546b5d1e911da2d3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2da53847-a6f9-4db8-b7e5-9a3b832ca833", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "dfb63b3a5321ee22c0ce6e040b407704cec833afaed8be4d3570710bc812b139", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c8862aa2-a8b0-4bd3-9c2f-895f1e4ca56a", "node_type": "1", "metadata": {}, "hash": "e9d8f979e35d4a384efcc956b78d7d60a0711f6220033fbd7226c93ab6a63a91", "class_name": "RelatedNodeInfo"}}, "text": "Because this part is ex-\nploited to maximize the loss of the worker discriminator, it\nwill be interpreted in the later training subsection. These fea-\ntures are invariant among different workers, thus they can\nhave less noises for NER. This is the goal of adversarial\nlearning, and we hope the NER being able to \ufb01nd useful\nfeatures from these worker independent features.\nAdditional Bi-LSTM over Annotated NER Labels\nIn order to incorporate the annotated NE labels to predict the\nexact worker, we build another bi-directional LSTM (named\nby label Bi-LSTM) based on the crowd-annotated NE label\nsequence. This Bi-LSTM is used for worker discriminator\nonly. During the decoding of the testing phase, we will never\nhave this Bi-LSTM, because the worker discriminator is no\nlonger required.\nAssuming the crowd-annotated NE label sequence an-\nnotated by one worker is \u00af y= \u00afy1\u00afy2\u00b7\u00b7\u00b7\u00afyn, we exploit a\nlooking-up table ELto obtain the corresponding sequence\nof their vector representations x\u20321x\u20322\u00b7\u00b7\u00b7x\u2032n, similar to the\nmethod that maps characters into their neural representa-\ntions. Concretely, for one NE label \u00afyt(t\u2208[1, n]), we obtain\nits neural vector by: x\u2032t=look-up (\u00afyt,EL).\nNext step we apply bi-directional LSTM over the se-\nquence x\u20321x\u20322\u00b7\u00b7\u00b7x\u2032n, which can be formalized as:\nhlabel\n1hlabel\n2\u00b7\u00b7\u00b7hlabel\nn=Bi-LSTM (x\u2032\n1x\u2032\n2\u00b7\u00b7\u00b7x\u2032\nn).(7)\nThe resulting feature sequence is concatenated with the out-\nputs of the common Bi-LSTM, and further be used for\nworker classi\ufb01cation.\nCNN\nFollowing, we add a convolutional neural network (CNN)\nmodule based on the concatenated outputs of the common\nBi-LSTM and the label Bi-LSTM, to produce the \ufb01nal fea-\ntures for worker discriminator. A convolutional operator\nwith window size 5 is used, and then max pooling strategy\nis applied over the convolution sequence to obtain the \ufb01nal\n\ufb01xed-dimensional feature vector. The whole process can be\ndescribed by the following equations:\nhworker\nt =hcommon\nt\u2295hlabel\nt\n\u02dchworker\nt = tanh( Wcnn[hworker\nt\u22122,hworker\nt\u22121,\u00b7\u00b7\u00b7,hworker\nt+2])\nhworker=max-pooling (\u02dchworker\n1\u02dchworker\n2\u00b7\u00b7\u00b7\u02dchworker\nn)(8)\nwhere t\u2208[1, n]andWcnnis one model parameter. We ex-\nploit zero vector to paddle the out-of-index vectors.\nOutput and Prediction\nAfter obtaining the \ufb01nal feature vector for the worker dis-\ncriminator, we use it to compute the output vector, which\nscores all the annotation workers. The score function is de-\n\ufb01ned by:\noworker=Wworkerhworker, (9)\nwhere Wworkeris one model parameter and the output di-\nmension equals the number of total non-expert annotators.", "start_char_idx": 3226, "end_char_idx": 5747, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8862aa2-a8b0-4bd3-9c2f-895f1e4ca56a": {"__data__": {"id_": "c8862aa2-a8b0-4bd3-9c2f-895f1e4ca56a", "embedding": null, "metadata": {"page_label": "5", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d5bb378b-222b-4cef-a97b-09e5b54acfa7", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "978b485835d5b691e8b5c44eff326f37ebcd41ee1c27a3251d777c71aa7116c3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6195407-1203-4178-8fbc-990540c57f42", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "5749fc1050c11411746f156a42c5452ac15e9d7e29d12f9f19b6573b062f5163", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d3894d84-648a-42c2-93cb-ebff7d3dbc0f", "node_type": "1", "metadata": {}, "hash": "3b29a8c9913ebc5e1752c2a882aad7dd8f1398db5d017fe3d4403ac8424517d8", "class_name": "RelatedNodeInfo"}}, "text": "The prediction is to \ufb01nd the worker which is responsible for\nthis annotation.\nAdversarial Training\nThe training objective with adversarial neural network is\ndifferent from the baseline model, as it includes the ex-\ntra worker discriminator. Thus the new objective includes\ntwo parts, one being the negative log-likelihood from NER\nwhich is the same as the baseline, and the other being the\nnegative the log-likelihood from the worker discriminator.\nIn order to obtain the negative log-likelihood of the\nworker discriminator, we use softmax to compute the prob-\nability of the actual worker \u00afzas well, which is de\ufb01ned by:\np(\u00afz|X,\u00af y) =exp(oworker\n\u00afz)\u2211\nzexp(oworkerz), (10)\nwhere zshould enumerate all workers.\nBased on the above de\ufb01nition of probability, our new ob-\njective is de\ufb01ned as follows:\nR(\u0398,\u0398\u2032,X,\u00af y,\u00afz) =loss(\u0398,X,\u00af y)\u2212loss(\u0398,\u0398\u2032,X)\n=\u2212logp(\u00af y|X) + log p(\u00afz|X,\u00af y),(11)\nwhere \u0398is the set of all model parameters related to NER,\nand\u0398\u2032is the set of the remaining parameters which are only\nrelated to the worker discriminator, X,\u00af yand\u00afzare the in-\nput sentence, the crowd-annotated NE labels and the cor-\nresponding annotator for this annotation, respectively. It is\nworth noting that the parameters of the common Bi-LSTM\nare included in the set of \u0398by de\ufb01nition.\nIn particular, our goal is not to simply minimize the new\nobjective. Actually, we aim for a saddle point, \ufb01nding the\nparameters \u0398and\u0398\u2032satisfying the following conditions:\n\u02c6\u0398 = arg min\n\u0398R(\u0398,\u0398\u2032,X,\u00af y,\u00afz)\n\u02c6\u0398\u2032= arg max\n\u0398\u2032R(\u02c6\u0398,\u0398\u2032,X,\u00af y,\u00afz)(12)\nwhere the \ufb01rst equation aims to \ufb01nd one \u0398that minimizes\nour new objective R (\u00b7), and the second equation aims to \ufb01nd\none\u0398\u2032maximizing the same objective.\nIntuitively, the \ufb01rst equation of Formula 12 tries to min-\nimize the NER loss, but at the same time maximize the\nworker discriminator loss by the shared parameters of the\ncommon Bi-LSTM. Thus the resulting features of common\nBi-LSTM actually attempt to hurt the worker discrimina-\ntor, which makes these features worker independent since\nthey are unable to distinguish different workers. The second\nequation tries to minimize the worker discriminator loss by\nits own parameter \u0398\u2032.\nWe use the standard back-propagation method to train the\nmodel parameters, the same as the baseline model. In order\nto incorporate the term of the argmax part of Formula 12 ,\nwe follow the previous work of adversarial training (Ganin\net al. 2016; Chen et al. 2016; Liu, Qiu, and Huang 2017),\nby introducing a gradient reverse layer between the com-\nmon Bi-LSTM and the CNN module, whose forward does\nnothing but the backward simply negates the gradients.#Sent AvgLen Kappa\nDL-PS 16,948 9.21 0.6033\nUC-MT 2,337 34.97 0.7437\nUC-UQ 2,300 7.69 0.7529\nTable 1: Statistics of labeled datasets.\nExperiments\nData Sets\nWith the purpose of obtaining evaluation datasets from\ncrowd annotators, we collect the sentences from two do-\nmains: Dialog and E-commerce domain. We hire undergrad-\nuate students to annotate the sentences. They are required to\nidentify the prede\ufb01ned types of entities in the sentences. To-\ngether with the guideline document, the annotators are edu-\ncated some tips in \ufb01fteen minutes and also provided with 20\nexemplifying sentences.\nLabeled Data: DL-PS. In Dialog domain (DL), we collect\nraw sentences from a chatbot application. And then we ran-\ndomly select 20K sentences as our pool and hire 43 students\nto annotate the sentences. We ask the annotators to label two\ntypes of entities: Person-Name and Song-Name. The anno-\ntators label the sentences independently.", "start_char_idx": 0, "end_char_idx": 3522, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d3894d84-648a-42c2-93cb-ebff7d3dbc0f": {"__data__": {"id_": "d3894d84-648a-42c2-93cb-ebff7d3dbc0f", "embedding": null, "metadata": {"page_label": "5", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d5bb378b-222b-4cef-a97b-09e5b54acfa7", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "978b485835d5b691e8b5c44eff326f37ebcd41ee1c27a3251d777c71aa7116c3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c8862aa2-a8b0-4bd3-9c2f-895f1e4ca56a", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "d4d960d667113a426aff448c8f123047382be0a2dc92574f1d532c7182ec5380", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f5ac39b8-8089-44df-86fd-91fb77f031d1", "node_type": "1", "metadata": {}, "hash": "4b84a2b97b0eb4870dbf04a4113b9c807a302c066b4b0252fa528e1243cea2b0", "class_name": "RelatedNodeInfo"}}, "text": "Experiments\nData Sets\nWith the purpose of obtaining evaluation datasets from\ncrowd annotators, we collect the sentences from two do-\nmains: Dialog and E-commerce domain. We hire undergrad-\nuate students to annotate the sentences. They are required to\nidentify the prede\ufb01ned types of entities in the sentences. To-\ngether with the guideline document, the annotators are edu-\ncated some tips in \ufb01fteen minutes and also provided with 20\nexemplifying sentences.\nLabeled Data: DL-PS. In Dialog domain (DL), we collect\nraw sentences from a chatbot application. And then we ran-\ndomly select 20K sentences as our pool and hire 43 students\nto annotate the sentences. We ask the annotators to label two\ntypes of entities: Person-Name and Song-Name. The anno-\ntators label the sentences independently. In particular, each\nsentence is assigned to three annotators for this data. Al-\nthough the setting can be wasteful of labor, we can use the\nresulting dataset to test several well-known baselines such\nas majority voting.\nAfter annotation, we remove some illegal sentences re-\nported by the annotators. Finally, we have 16,948 sentences\nannotated by the students. Table 1 shows the information of\nannotated data. The average Kappa value among the anno-\ntators is 0.6033, indicating that the crowd annotators have\nmoderate agreement on identifying entities on this data.\nIn order to evaluate the system performances, we create\na set of corpus with gold annotations. Concretely, we ran-\ndomly select 1,000 sentences from the \ufb01nal dataset and let\ntwo experts generate the gold annotations. Among them, we\nuse 300 sentences as the development set and the remain-\ning 700 as the test set. The rest sentences with only student\nannotations are used as the training set.\nLabeled data: EC-MT and EC-UQ. In E-commerce do-\nmain (EC), we collect raw sentences from two types of texts:\none is titles of merchandise entries (EC-MT) and another\nis user queries (EC-UQ). The annotators label \ufb01ve types\nof entities: Brand, Product, Model, Material, and Speci\ufb01ca-\ntion. These \ufb01ve types of entities are very important for E-\ncommerce platform, for example building knowledge graph\nof merchandises. Five students participate the annotations\nfor this domain since the number of sentences is small. We\nuse the similar strategy as DL-PS to annotate the sentences,\nexcept that only two annotators are assigned for each sen-\ntence, because we aim to test the system performances under\nvery small duplicated annotations.\nFinally, we obtain 2,337 sentences for EC-MT and 2,300\nfor EC-UQ. Table 1 shows the information of annotated\nresults. Similarly, we produce the development and test\ndatasets for system evaluation, by randomly selecting 400\nsentences and letting two experts to generate the groundtruth", "start_char_idx": 2731, "end_char_idx": 5500, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5ac39b8-8089-44df-86fd-91fb77f031d1": {"__data__": {"id_": "f5ac39b8-8089-44df-86fd-91fb77f031d1", "embedding": null, "metadata": {"page_label": "6", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0bf2ba39-83a9-45d4-9814-13d20291b2b0", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "3aa5945cfb06eb99cdeb14e0b13c694a1cc36376d1adb1fbce584ba871667164", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d3894d84-648a-42c2-93cb-ebff7d3dbc0f", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "99da92db405e611f8af200163f3a6670acb2b6f1cdd849f737837a4b005693e4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8a8ec2c2-9bdf-4806-843e-6bb42388ae00", "node_type": "1", "metadata": {}, "hash": "449e7968bbd3f26799abad76d42d9441e6ef7232bf1b35f3ba697dd8d45adb0b", "class_name": "RelatedNodeInfo"}}, "text": "annotations. Among them, we use 100 sentences as the de-\nvelopment set and the remaining 300 as the test set. The rest\nsentences with only crowdsourcing annotations are used as\nthe training set.\nUnlabeled data. The vector representations of characters\nare basic inputs of our baseline and proposed models, which\nare obtained by the looking-up table EW. As introduced be-\nfore, we can use pretrained embeddings from large-scale\nraw corpus to initialize the table. In order to pretrain the\ncharacter embeddings, we use one large-scale unlabeled data\nfrom the user-generated content in Internet. Totally, we ob-\ntain a number of 5M sentences. Finally, we use the tool\nword2vec1to pretrain the character embeddings based on\nthe unlabeled dataset in our experiments.\nSettings\nFor evaluation, we use the entity-level metrics of Precision\n(P), Recall (R), and their F1 value in our experiments, treat-\ning one tagged entity as correct only when it matches the\ngold entity exactly.\nThere are several hyper-parameters in the baseline LSTM-\nCRF and our \ufb01nal models. We set them empirically by the\ndevelopment performances. Concretely, we set the dimen-\nsion size of the character embeddings by 100, the dimension\nsize of the NE label embeddings by 50, and the dimension\nsizes of all the other hidden features by 200.\nWe exploit online training with a mini-batch size 128 to\nlearn model parameters. The max-epoch iteration is set by\n200, and the best-epoch model is chosen according to the de-\nvelopment performances. We use RMSprop (Tieleman and\nHinton 2012) with a learning rate 10\u22123to update model pa-\nrameters, and use l2-regularization by a parameter 10\u22125. We\nadopt the dropout technique to avoid over\ufb01tting by a drop\nvalue of 0.2.\nComparison Systems\nThe proposed approach (henceforward referred to as AL-\nCrowd) is compared with the following systems:\n\u2022CRF: We use the Crfsuite2tool to train a model on the\ncrowdsourcing labeled data. As for the feature settings,\nwe use the supervised version of Zhao and Kit (2008).\n\u2022CRF-VT: We use the same settings of the CRF system,\nexcept that the training data is the voted version, whose\ngroundtruths are produced by majority voting at the char-\nacter level for each annotated sentence.\n\u2022CRF-MA: The CRF model proposed by Rodrigues,\nPereira, and Ribeiro (2014), which uses a prior distributa-\ntion to model multiple crowdsourcing annotators. We use\nthe source code provided by the authors.\n\u2022LSTM-CRF: Our baseline system trained on the crowd-\nsourcing labeled data.\n\u2022LSTM-CRF-VT: Our baseline system trained on the voted\ncorpus, which is the same as CRF-VT.\n1https://code.google.com/archive/p/word2vec\n2http://www.chokkan.org/software/crfsuite/Model P R F1\nCRF 89.48 70.38 78.79\nCRF-VT 85.16 65.07 73.77\nCRF-MA 72.83 90.79 80.82\nLSTM-CRF 90.50 79.97 84.91\nLSTM-CRF-VT 88.68 75.51 81.57\nLSTM-Crowd 86.40 83.43 84.89\nALCrowd 89.56 82.70 85.99\nTable 2: Main results on the DL-PS data.\nModelData: EC-MT\nP R F1\nCRF 75.12 66.67 70.64\nLSTM-CRF 75.02 72.84 73.91\nLSTM-Crowd 73.81 75.18 74.49\nALCrowd 76.33 74.00 75.15\nData: EC-UQ\nCRF 65.45 55.33 59.96\nLSTM-CRF 71.96 66.55 69.15\nLSTM-Crowd 67.51 71.10 69.26\nALCrowd 74.72 68.60 71.53\nTable 3: Main results on the EC-MT and EC-UQ datasets.\n\u2022LSTM-Crowd: The LSTM-CRF model with crowd anno-\ntation learning proposed by Nguyen et al. (2017).", "start_char_idx": 0, "end_char_idx": 3315, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a8ec2c2-9bdf-4806-843e-6bb42388ae00": {"__data__": {"id_": "8a8ec2c2-9bdf-4806-843e-6bb42388ae00", "embedding": null, "metadata": {"page_label": "6", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0bf2ba39-83a9-45d4-9814-13d20291b2b0", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "3aa5945cfb06eb99cdeb14e0b13c694a1cc36376d1adb1fbce584ba871667164", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f5ac39b8-8089-44df-86fd-91fb77f031d1", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "8d3be7ef2168a075d65925e719622325595c26791b122623cf08c2857250b193", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "065f5cc5-0336-4413-bf85-42bcadb1c76e", "node_type": "1", "metadata": {}, "hash": "9b9a3c6891b236b7af50fb3b6b2a6c19d457eec7a69435e0ab97afbe8551de90", "class_name": "RelatedNodeInfo"}}, "text": "ModelData: EC-MT\nP R F1\nCRF 75.12 66.67 70.64\nLSTM-CRF 75.02 72.84 73.91\nLSTM-Crowd 73.81 75.18 74.49\nALCrowd 76.33 74.00 75.15\nData: EC-UQ\nCRF 65.45 55.33 59.96\nLSTM-CRF 71.96 66.55 69.15\nLSTM-Crowd 67.51 71.10 69.26\nALCrowd 74.72 68.60 71.53\nTable 3: Main results on the EC-MT and EC-UQ datasets.\n\u2022LSTM-Crowd: The LSTM-CRF model with crowd anno-\ntation learning proposed by Nguyen et al. (2017). We use\nthe source code provided by the authors.\nThe \ufb01rst three systems are based on the CRF model using\ntraditional handcrafted features, and the last three systems\nare based on the neural LSTM-CRF model. Among them,\nCRF-MA, LSTM-Crowd and our system with adversarial\nlearning (ALCrowd) are based on crowd annotation learning\nthat directly trains the model on the crowd-annotations. Five\nsystems, including CRF, CRF-MA, LSTM-CRF, LSTM-\nCrowd, and ALCrowd, are trained on the original version of\nlabeled data, while CRF-VT and LSTM-CRF-VT are trained\non the voted version. Since CRF-VT, CRF-MA and LSTM-\nCRF-VT all require ground-truth answers for each training\nsentence, which are dif\ufb01cult to be produced with only two\nannotations, we do not apply the three models on the two\nEC datasets.\nMain Results\nIn this section, we show the model performances of our\nproposed crowdsourcing learning system (ALCrowd), and\nmeanwhile compare it with the other systems mentioned\nabove. Table 2 shows the experimental results on the DL-\nPS datasets and Table 3 shows the experiment results on the\nEC-MT and EC-UQ datasets, respectively.\nThe results of CRF and LSTM-CRF mean that the crowd\nannotation is an alternative solution with low cost for la-\nbeling data that could be used for training a NER system\neven there are some inconsistencies. Compared with CRF,\nLSTM-CRF achieves much better performances on all the\nthree data, showing +6.12 F1 improvement on DL-PS, +4.51\non EC-MT, and +9.19 on EC-UQ. This indicates that LSTM-", "start_char_idx": 2918, "end_char_idx": 4829, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "065f5cc5-0336-4413-bf85-42bcadb1c76e": {"__data__": {"id_": "065f5cc5-0336-4413-bf85-42bcadb1c76e", "embedding": null, "metadata": {"page_label": "7", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dfe84564-5c00-4edc-998d-fdb7b356cfe0", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "eb5b4d0fffcd97205361632a35f367cc9945528ebdf14bb7c06a3ed3fbf053bc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8a8ec2c2-9bdf-4806-843e-6bb42388ae00", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "9bb1ee98495bb710c3b0da484c99bfcfd3c4b0684e06e936383989143417d4d0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a03da7be-4d95-4004-abff-356c0117a88d", "node_type": "1", "metadata": {}, "hash": "6dc803cb800d0dcb88f4900911616b42a0d6e850d10411d392849810561425e3", "class_name": "RelatedNodeInfo"}}, "text": "Data:DL-PS Data:EC-MT Data:EC-UQ606570758085Random PretrainedFigure 2: Comparisons by using different character embed-\ndings, where the Y-axis shows the F1 values\nCRF is a very strong baseline system, demonstrating the ef-\nfectiveness of neural network.\nInterestingly, when compared with CRF and LSTM-CRF,\nCRF-VT and LSTM-CRF-VT trained on the voted version\nperform worse in the DL-PS dataset. This trend is also men-\ntioned in Nguyen et al. (2017). This fact shows that the ma-\njority voting method might be unsuitable for our task. There\nare two possible reasons accounting for the observation. On\nthe one hand, simple character-level voting based on three\nannotations for each sentence may be still not enough. In the\nDL-PS dataset, even with only two prede\ufb01ned entity types,\none character can have nine NE labels. Thus the majority-\nvoting may be incapable of handling some cases. While the\ncost by adding more annotations for each sentence would\nbe greatly increased. On the other hand, the lost informa-\ntion produced by majority-voting may be important, at least\nthe ambiguous annotations denote that the input sentence is\ndif\ufb01cult for NER. The normal CRF and LSTM-CRF mod-\nels without discard any annotations can differentiate these\ndif\ufb01cult contexts through learning.\nThree crowd-annotation learning systems provide bet-\nter performances than their counterpart systems, (CRF-MA\nVS CRF) and (LSTM-Crowd/ALCrowd VS LSTM-CRF).\nCompared with the strong baseline LSTM-CRF, ALCrowd\nshows its advantage with +1.08 F1 improvements on DL-PS,\n+1.24 on EC-MT, and +2.38 on EC-UQ, respectively. This\nindicates that adding the crowd-annotation learning is quite\nuseful for building NER systems. In addition, ALCrowd also\noutperforms LSTM-Crowd on all the datasets consistently,\ndemonstrating the high effectiveness of ALCrowd in extract-\ning worker independent features. Among all the systems,\nALCrowd performs the best, and signi\ufb01cantly better than\nall the other models (the p-value is below 10\u22125by using\nt-test). The results indicate that with the help of adversarial\ntraining, our system can learn a better feature representation\nfrom crowd annotation.\nDiscussion\nImpact of Character Embeddings. First, we investigate the\neffect of the pretrained character embeddings in our pro-\nposed crowdsourcing learning model. The comparison re-\nsults are shown in Figure 2, where Random refers to the\nrandom initialized character embeddings, and Pretrained\nrefers to the embeddings pretrained on the unlabeled data.", "start_char_idx": 0, "end_char_idx": 2504, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a03da7be-4d95-4004-abff-356c0117a88d": {"__data__": {"id_": "a03da7be-4d95-4004-abff-356c0117a88d", "embedding": null, "metadata": {"page_label": "7", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dfe84564-5c00-4edc-998d-fdb7b356cfe0", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "eb5b4d0fffcd97205361632a35f367cc9945528ebdf14bb7c06a3ed3fbf053bc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "065f5cc5-0336-4413-bf85-42bcadb1c76e", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "3c794a1f78d92e63a4fb2d2337a29e8fd613cf700e69f5981c81a11ba730d363", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e42104b7-2c60-4246-9b39-4afd0d15a53d", "node_type": "1", "metadata": {}, "hash": "06f57e2a59a7296490fd01746f73e72be558101308a71ee61151462a24947f1c", "class_name": "RelatedNodeInfo"}}, "text": "In addition, ALCrowd also\noutperforms LSTM-Crowd on all the datasets consistently,\ndemonstrating the high effectiveness of ALCrowd in extract-\ning worker independent features. Among all the systems,\nALCrowd performs the best, and signi\ufb01cantly better than\nall the other models (the p-value is below 10\u22125by using\nt-test). The results indicate that with the help of adversarial\ntraining, our system can learn a better feature representation\nfrom crowd annotation.\nDiscussion\nImpact of Character Embeddings. First, we investigate the\neffect of the pretrained character embeddings in our pro-\nposed crowdsourcing learning model. The comparison re-\nsults are shown in Figure 2, where Random refers to the\nrandom initialized character embeddings, and Pretrained\nrefers to the embeddings pretrained on the unlabeled data.\nAccording to the results, we \ufb01nd that our model with the\npretrained embeddings signi\ufb01cantly outperforms that using\n\uf07d\uf07d\uf0ec\uf0a9\uf02a\uf0ed\uf0a5\uf099\uf027\n\uf07d \uf07d \uf07d\uf07d \uf07d \uf07d\uf0ec\uf0a9\uf02a\uf0ed\uf0a5\uf099\uf027\n\uf07d\uf07d\uf0ec\uf0a9\uf02a\uf0ed\uf0a5\uf099\uf027\n\uf07d\uf07d\uf0ec\uf0a9\uf02a\uf0ed\uf0a5\uf099\uf027\n\uf07d\uf07d\uf0ec\uf0a9\uf02a\uf0ed\uf0a5\uf099\uf027\n\uf07d\uf07d\uf0ec\uf0a9\uf02a\uf0ed\uf0a5\uf099\uf027\n\uf07d \uf07d \uf07d\uf07d \uf07d \uf07d\uf0ec\uf0a9\uf02a\uf0ed\uf0a5\uf099\uf027\n\uf060\uf0f4\uf022\uf022\uf084\uf0d7\n\uf060\uf0f4\uf022\uf022\uf084\uf0d7\n\uf060\uf0f4\uf022 \uf022 \uf022\uf022 \uf022 \uf022\uf084\uf0d7\n\uf060\uf0f4\uf022\uf022\uf084\uf0d7 \uf0d7 \uf0d7\uf0e5 \uf0e5 \uf0e5\uf0e5 \uf0e5 \uf0e5\n\uf060\uf0f4\uf022\uf022\uf084\uf0d7\n\uf060\uf0f4\uf022\uf022\uf084\uf0d7 \uf0d7 \uf0d7\uf0e5 \uf0e5 \uf0e5\uf0e5 \uf0e5 \uf0e5\n\uf060\uf0f4\uf022 \uf022 \uf022\uf022 \uf022 \uf022\uf084\uf0d7 \uf0d7 \uf0d7\uf0e5 \uf0e5 \uf0e5\uf0e5 \uf0e5 \uf0e5Figure 3: Case studies of different systems, where named\nentities are illustrated by square brackets.\nthe random embeddings, demonstrating that the pretrained\nembeddings successfully provide useful information.\nCase Studies. Second, we present several case studies in or-\nder to study the differences between our baseline and the\nworker adversarial models. We conduct a closed test on the\ntraining set, the results of which can be regarded as modi\ufb01-\ncations of the training corpus, since there exist inconsistent\nannotations for each training sentence among the different\nworkers. Figure 3 shows the two examples from the DL-PS\ndataset, which compares the outputs of the baseline and our\n\ufb01nal models, as well as the majority-voting strategy.\nIn the \ufb01rst case, none of the annotations get the cor-\nrect NER result, but our proposed model can capture it.\nThe result of LSTM-CRF is the same as majority-voting.\nIn the second example, the output of majority-voting is the\nworst, which can account for the reason why the same model\ntrained on the voted corpus performs so badly, as shown in\nTable 2. The model of LSTM-CRF fails to recognize the\nnamed entity \u201cXiexie\u201d because of not trusting the second\nannotation, treating it as one noise annotation. Our proposed\nmodel is able to recognize it, because of its ability of extract-\ning worker independent features.\nConclusions\nIn this paper, we presented an approach to performing crowd\nannotation learning based on the idea of adversarial training\nfor Chinese Named Entity Recognition (NER).", "start_char_idx": 1691, "end_char_idx": 4347, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e42104b7-2c60-4246-9b39-4afd0d15a53d": {"__data__": {"id_": "e42104b7-2c60-4246-9b39-4afd0d15a53d", "embedding": null, "metadata": {"page_label": "7", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dfe84564-5c00-4edc-998d-fdb7b356cfe0", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "eb5b4d0fffcd97205361632a35f367cc9945528ebdf14bb7c06a3ed3fbf053bc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a03da7be-4d95-4004-abff-356c0117a88d", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a9f63ec4124f7a7f183ca2903c19db2561ca0ee2670c5547711b26ebc23605b0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b7a83585-17c0-42cc-a453-d5b8ae67bbd5", "node_type": "1", "metadata": {}, "hash": "85a05551bf531dc5f19e5376de3dddb8cee3a7d7ee726e8556178e5eac21dc09", "class_name": "RelatedNodeInfo"}}, "text": "In the \ufb01rst case, none of the annotations get the cor-\nrect NER result, but our proposed model can capture it.\nThe result of LSTM-CRF is the same as majority-voting.\nIn the second example, the output of majority-voting is the\nworst, which can account for the reason why the same model\ntrained on the voted corpus performs so badly, as shown in\nTable 2. The model of LSTM-CRF fails to recognize the\nnamed entity \u201cXiexie\u201d because of not trusting the second\nannotation, treating it as one noise annotation. Our proposed\nmodel is able to recognize it, because of its ability of extract-\ning worker independent features.\nConclusions\nIn this paper, we presented an approach to performing crowd\nannotation learning based on the idea of adversarial training\nfor Chinese Named Entity Recognition (NER). In our ap-\nproach, we use a common and private Bi-LSTMs for rep-\nresenting annotator-generic and -speci\ufb01c information, and\nlearn a label Bi-LSTM from the crowd-annotated NE label\nsequences. Finally, the proposed approach adopts a LSTM-\nCRF model to perform tagging. In our experiments, we cre-\nate two data sets for Chinese NER tasks in the dialog and e-\ncommerce domains. The experimental results show that the\nproposed approach outperforms strong baseline systems.", "start_char_idx": 3554, "end_char_idx": 4814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b7a83585-17c0-42cc-a453-d5b8ae67bbd5": {"__data__": {"id_": "b7a83585-17c0-42cc-a453-d5b8ae67bbd5", "embedding": null, "metadata": {"page_label": "8", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b639655-4cf4-4fc4-b263-c719dfbefd41", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "54c72eba04c7d8ac49f77cc37ec76b168b98c80d26d387d30456712687ca13cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e42104b7-2c60-4246-9b39-4afd0d15a53d", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "5eb573e831855c983b5665044d44493f39dd033fc33500c0dace254c78ae74ee", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "53a631c9-fa33-42ec-a7d7-61e2f240a66f", "node_type": "1", "metadata": {}, "hash": "5a2f729525e744272791a3607ded929c247688357ca85df1a9fc7eef2a6712d9", "class_name": "RelatedNodeInfo"}}, "text": "Acknowledgments\nThis work is supported by the National Natural Science\nFoundation of China (Grant No. 61572338, 61525205, and\n61602160). This work is also partially supported by the joint\nresearch project of Alibaba and Soochow University. Wen-\nliang is also partially supported by Collaborative Innovation\nCenter of Novel Software Technology and Industrialization.\nReferences\n[2014] Bi, W.; Wang, L.; Kwok, J. T.; and Tu, Z. 2014.\nLearning to predict from crowdsourced data. In UAI, 82\u2013\n91.\n[2016] Chen, X.; Sun, Y .; Athiwaratkun, B.; Cardie, C.; and\nWeinberger, K. 2016. Adversarial deep averaging networks\nfor cross-lingual sentiment classi\ufb01cation. arXiv preprint\narXiv:1606.01614 .\n[2017] Chen, X.; Shi, Z.; Qiu, X.; and Huang, X. 2017. Ad-\nversarial multi-criteria learning for chinese word segmenta-\ntion. arXiv preprint arXiv:1704.07556 .\n[2011] Collobert, R.; Weston, J.; Bottou, L.; Karlen, M.;\nKavukcuoglu, K.; and Kuksa, P. 2011. Natural language\nprocessing (almost) from scratch. The Journal of Machine\nLearning Research 12:2493\u20132537.\n[2015] Denton, E. L.; Chintala, S.; Fergus, R.; et al. 2015.\nDeep generative image models using a laplacian pyramid of\nadversarial networks. In NIPS , 1486\u20131494.\n[2009] Dredze, M.; Talukdar, P. P.; and Crammer, K. 2009.\nSequence learning from data with multiple labels. In Work-\nshop Co-Chairs , 39.\n[2017] Dumitrache, A.; Aroyo, L.; and Welty, C. 2017.\nCrowdsourcing ground truth for medical relation extraction.\narXiv preprint arXiv:1701.02185 .\n[2015] Dyer, C.; Ballesteros, M.; Ling, W.; Matthews, A.;\nand Smith, N. A. 2015. Transition-based dependency pars-\ning with stack long short-term memory. In ACL, 334\u2013343.\n[2015] Felt, P.; Black, K.; Ringger, E. K.; Seppi, K. D.; and\nHaertel, R. 2015. Early gains matter: A case for prefer-\nring generative over discriminative crowdsourcing models.\nInHLT-NAACL , 882\u2013891.\n[2016] Ganin, Y .; Ustinova, E.; Ajakan, H.; Germain, P.;\nLarochelle, H.; Laviolette, F.; Marchand, M.; and Lempit-\nsky, V . 2016. Domain-adversarial training of neural net-\nworks. Journal of Machine Learning Research 17(59):1\u201335.\n[2014] Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y .\n2014. Generative adversarial nets. In NIPS , 2672\u20132680.\n[2005] Graves, A., and Schmidhuber, J. 2005. Framewise\nphoneme classi\ufb01cation with bidirectional lstm and other\nneural network architectures. Neural Networks 18(5):602\u2013\n610.\n[2017] Gui, T.; Zhang, Q.; Huang, H.; Peng, M.; and Huang,\nX. 2017. Part-of-speech tagging for twitter with adversarial\nneural networks. In Proceedings of the 2017 Conference on\nEMNLP , 2401\u20132410. Copenhagen, Denmark: Association\nfor Computational Linguistics.[2015] Huang, Z.; Xu, W.; and Yu, K. 2015. Bidirec-\ntional lstm-crf models for sequence tagging. arXiv preprint\narXiv:1508.01991 .\n[2017] Kim, J.-K.; Kim, Y .-B.", "start_char_idx": 0, "end_char_idx": 2869, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "53a631c9-fa33-42ec-a7d7-61e2f240a66f": {"__data__": {"id_": "53a631c9-fa33-42ec-a7d7-61e2f240a66f", "embedding": null, "metadata": {"page_label": "8", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b639655-4cf4-4fc4-b263-c719dfbefd41", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "54c72eba04c7d8ac49f77cc37ec76b168b98c80d26d387d30456712687ca13cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b7a83585-17c0-42cc-a453-d5b8ae67bbd5", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "7208c5897998717e2a841eb66ff18b58204be2c7d4566a8da39febe1efbff379", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "18aa9c90-7a35-4ca2-8e8c-af96978dc1bc", "node_type": "1", "metadata": {}, "hash": "673196c0134e58258c48b25cac6de2d5b5be05f5f5f7db7bdfb7456c18c8b72d", "class_name": "RelatedNodeInfo"}}, "text": "2005. Framewise\nphoneme classi\ufb01cation with bidirectional lstm and other\nneural network architectures. Neural Networks 18(5):602\u2013\n610.\n[2017] Gui, T.; Zhang, Q.; Huang, H.; Peng, M.; and Huang,\nX. 2017. Part-of-speech tagging for twitter with adversarial\nneural networks. In Proceedings of the 2017 Conference on\nEMNLP , 2401\u20132410. Copenhagen, Denmark: Association\nfor Computational Linguistics.[2015] Huang, Z.; Xu, W.; and Yu, K. 2015. Bidirec-\ntional lstm-crf models for sequence tagging. arXiv preprint\narXiv:1508.01991 .\n[2017] Kim, J.-K.; Kim, Y .-B.; Sarikaya, R.; and Fosler-\nLussier, E. 2017. Cross-lingual transfer learning for pos\ntagging without cross-lingual resources. In Proceedings of\nthe 2017 Conference on EMNLP , 2822\u20132828. Copenhagen,\nDenmark: Association for Computational Linguistics.\n[2011] Kl \u00a8uwer, T. 2011. From chatbots to dialog systems.\nConversational agents and natural language interaction:\nTechniques and Effective Practices 1\u201322.\n[2001] Lafferty, J.; McCallum, A.; Pereira, F.; et al. 2001.\nConditional random \ufb01elds: Probabilistic models for seg-\nmenting and labeling sequence data. In ICML , volume 1,\n282\u2013289.\n[2016] Lample, G.; Ballesteros, M.; Subramanian, S.;\nKawakami, K.; and Dyer, C. 2016. Neural architectures\nfor named entity recognition. In NAACL , 260\u2013270.\n[2017] Liu, P.; Qiu, X.; and Huang, X. 2017. Adversarial\nmulti-task learning for text classi\ufb01cation. In Proceedings of\nthe 55th ACL , 1\u201310. Vancouver, Canada: Association for\nComputational Linguistics.\n[2016] Ma, X., and Hovy, E. 2016. End-to-end sequence\nlabeling via bi-directional lstm-cnns-crf. In Proceedings of\nthe 54th ACL , 1064\u20131074.\n[2017] Nguyen, A. T.; Wallace, B.; Li, J. J.; Nenkova, A.;\nand Lease, M. 2017. Aggregating and predicting sequence\nlabels from crowd annotations. In Proceedings of the 55th\nACL, volume 1, 299\u2013309.\n[2015] Peng, N., and Dredze, M. 2015. Named entity recog-\nnition for chinese social media with jointly trained embed-\ndings. In Proceedings of the EMNLP , 548\u2013554.\n[2009] Ratinov, L., and Roth, D. 2009. Design challenges\nand misconceptions in named entity recognition. In Pro-\nceedings of the CoNLL-2009 , 147\u2013155.\n[2014] Rodrigues, F.; Pereira, F.; and Ribeiro, B. 2014. Se-\nquence labeling with multiple annotators. Machine Learning\n95(2):165\u2013181.\n[2008] Snow, R.; O\u2019Connor, B.; Jurafsky, D.; and Ng, A. Y .\n2008. Cheap and fast\u2014but is it good?: evaluating non-expert\nannotations for natural language tasks. In Proceedings of the\nconference on EMNLP , 254\u2013263. Association for Computa-\ntional Linguistics.\n[2012] Tieleman, T., and Hinton, G. 2012. Lecture 6.5-\nrmsprop: Divide the gradient by a running average of its re-\ncent magnitude. COURSERA: Neural networks for machine\nlearning 4(2):26\u201331.\n[2016] Wu, Y .; Schuster, M.; Chen, Z.; Le, Q. V .; Norouzi,\nM.; Macherey, W.; Krikun, M.; Cao, Y .; Gao, Q.; Macherey,\nK.; et al. 2016. Google\u2019s neural machine translation system:\nBridging the gap between human and machine translation.", "start_char_idx": 2314, "end_char_idx": 5290, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "18aa9c90-7a35-4ca2-8e8c-af96978dc1bc": {"__data__": {"id_": "18aa9c90-7a35-4ca2-8e8c-af96978dc1bc", "embedding": null, "metadata": {"page_label": "8", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6b639655-4cf4-4fc4-b263-c719dfbefd41", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "54c72eba04c7d8ac49f77cc37ec76b168b98c80d26d387d30456712687ca13cd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "53a631c9-fa33-42ec-a7d7-61e2f240a66f", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "b73802415ad657cbdbdd997afb62c60a9456e954514abaa047e552fccf89e69c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bd11d076-87d2-4b16-b62e-a5c94c86b520", "node_type": "1", "metadata": {}, "hash": "656832862b0d272625ae54a3cf8f99a14f551aa2891bc13f7fa14704c0665824", "class_name": "RelatedNodeInfo"}}, "text": "Cheap and fast\u2014but is it good?: evaluating non-expert\nannotations for natural language tasks. In Proceedings of the\nconference on EMNLP , 254\u2013263. Association for Computa-\ntional Linguistics.\n[2012] Tieleman, T., and Hinton, G. 2012. Lecture 6.5-\nrmsprop: Divide the gradient by a running average of its re-\ncent magnitude. COURSERA: Neural networks for machine\nlearning 4(2):26\u201331.\n[2016] Wu, Y .; Schuster, M.; Chen, Z.; Le, Q. V .; Norouzi,\nM.; Macherey, W.; Krikun, M.; Cao, Y .; Gao, Q.; Macherey,\nK.; et al. 2016. Google\u2019s neural machine translation system:\nBridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144 .\n[2017] Zhang, Y .; Barzilay, R.; and Jaakkola, T. 2017.\nAspect-augmented adversarial networks for domain adapta-\ntion. arXiv preprint arXiv:1701.00188 .", "start_char_idx": 4671, "end_char_idx": 5477, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bd11d076-87d2-4b16-b62e-a5c94c86b520": {"__data__": {"id_": "bd11d076-87d2-4b16-b62e-a5c94c86b520", "embedding": null, "metadata": {"page_label": "9", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f40fa86-ec61-4f3a-876b-7076516f73f1", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "67ada805fa6faf29137c99510c9b4875b63e620ad1a18ef003f63e83e44df8a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "18aa9c90-7a35-4ca2-8e8c-af96978dc1bc", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "7374c3e8c724373bc90ec2b4cea914459937d80648245c862bf097416f5e4c87", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "cd90c7a9-4104-4364-8241-984097fc4137", "node_type": "1", "metadata": {}, "hash": "8b9f3daaf01f03dbfb4e7fdaffedbbe4852cf85b957960d8dc0de41dcdb6fd79", "class_name": "RelatedNodeInfo"}}, "text": "[2008] Zhao, H., and Kit, C. 2008. Unsupervised segmenta-\ntion helps supervised learning of character tagging for word\nsegmentation and named entity recognition. In IJCNLP ,\n106\u2013111.", "start_char_idx": 0, "end_char_idx": 182, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cd90c7a9-4104-4364-8241-984097fc4137": {"__data__": {"id_": "cd90c7a9-4104-4364-8241-984097fc4137", "embedding": null, "metadata": {"page_label": "1", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "019bbff9-f260-4c69-a1ff-ac34882dab33", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "001f45543cab8b0419622f0faf22be514590ca090124a0f86a63c7d0745d8386", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bd11d076-87d2-4b16-b62e-a5c94c86b520", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "67ada805fa6faf29137c99510c9b4875b63e620ad1a18ef003f63e83e44df8a0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "302a8d5c-6348-44d5-935b-2a02f23c7f37", "node_type": "1", "metadata": {}, "hash": "0be7b81f3fdfa2a45fe41559b72e2bae3ce7e046c8a9069b097caa28f9478cb9", "class_name": "RelatedNodeInfo"}}, "text": "arXiv:1810.08699v1  [cs.CL]  19 Oct 2018pioNER: Datasets and Baselines for Armenian\nNamed Entity Recognition\nTsolak Ghukasyan1, Garnik Davtyan2, Karen Avetisyan3, Ivan Andrianov4\nIvannikov Laboratory for System Programming at Russian-Ar menian University1,2,3, Yerevan, Armenia\nIvannikov Institute for System Programming of the Russian A cademy of Sciences4, Moscow, Russia\nEmail: {1tsggukasyan,2garnik.davtyan,3karavet,4ivan.andrianov}@ispras.ru\nAbstract \u2014In this work, we tackle the problem of Armenian\nnamed entity recognition, providing silver- and gold-stan dard\ndatasets as well as establishing baseline results on popula r mod-\nels. We present a 163000-token named entity corpus automati cally\ngenerated and annotated from Wikipedia, and another 53400-\ntoken corpus of news sentences with manual annotation of peo ple,\norganization and location named entities. The corpora were used\nto train and evaluate several popular named entity recognit ion\nmodels. Alongside the datasets, we release 50-, 100-, 200-, 300-\ndimensional GloVe word embeddings trained on a collection o f\nArmenian texts from Wikipedia, news, blogs, and encycloped ia.\nIndex Terms \u2014machine learning, deep learning, natural lan-\nguage processing, named entity recognition, word embeddin gs\nI. I NTRODUCTION\nNamed entity recognition is an important task of natural\nlanguage processing, featuring in many popular text proces sing\ntoolkits. This area of natural language processing has been\nactively studied in the latest decades and the advent of deep\nlearning reinvigorated the research on more effective and\naccurate models. However, most of existing approaches requ ire\nlarge annotated corpora. To the best of our knowledge, no suc h\nwork has been done for the Armenian language, and in this\nwork we address several problems, including the creation of a\ncorpus for training machine learning models, the developme nt\nof gold-standard test corpus and evaluation of the effectiv eness\nof established approaches for the Armenian language.\nConsidering the cost of creating manually annotated named\nentity corpus, we focused on alternative approaches. Lack\nof named entity corpora is a common problem for many\nlanguages, thus bringing the attention of many researchers\naround the globe. Projection based transfer schemes have be en\nshown to be very effective (e.g. [1], [2], [3]), using resour ce-\nrich language\u2019s corpora to generate annotated data for the l ow-\nresource language. In this approach, the annotations of hig h-\nresource language are projected over the corresponding tok ens\nof the parallel low-resource language\u2019s texts. This strate gy\ncan be applied for language pairs that have parallel corpora .\nHowever, that approach would not work for Armenian as we\ndid not have access to suf\ufb01ciently large parallel corpus wit h a\nresource-rich language.\nAnother popular approach is using Wikipedia. Klesti\nHoxha and Artur Baxhaku employ gazetteers extracted from\nWikipedia to generate an annotated corpus for Albanian [4],\nand Weber and P\u00f6tzl propose a rule-based system for Germanthat leverages the information from Wikipedia [5]. However ,\nthe latter relies on external tools such as part-of-speech t ag-\ngers, making it nonviable for the Armenian language.\nNothman et al. generated a silver-standard corpus for 9\nlanguages by extracting Wikipedia article texts with outgo ing\nlinks and turning those links into named entity annotations\nbased on the target article\u2019s type [6]. Sysoev and Andrianov\nused a similar approach for the Russian language [7] [8].\nBased on its success for a wide range of languages, our choice\nfell on this model to tackle automated data generation and\nannotation for the Armenian language.\nAside from the lack of training data, we also address the\nabsence of a benchmark dataset of Armenian texts for named\nentity recognition. We propose a gold-standard corpus with\nmanual annotation of CoNLL named entity categories: person ,\nlocation, and organization [9] [10], hoping it will be used t o\nevaluate future named entity recognition models.\nFurthermore, popular entity recognition models were ap-\nplied to the mentioned data in order to obtain baseline resul ts\nfor future research in the area. Along with the datasets, we\ndeveloped GloVe [11] word embeddings to train and evaluate\nthe deep learning models in our experiments.", "start_char_idx": 0, "end_char_idx": 4328, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "302a8d5c-6348-44d5-935b-2a02f23c7f37": {"__data__": {"id_": "302a8d5c-6348-44d5-935b-2a02f23c7f37", "embedding": null, "metadata": {"page_label": "1", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "019bbff9-f260-4c69-a1ff-ac34882dab33", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "001f45543cab8b0419622f0faf22be514590ca090124a0f86a63c7d0745d8386", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cd90c7a9-4104-4364-8241-984097fc4137", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "6f812cb402c92fcbf8a81d24824000c4c64c442668ba05486e2ec2a2b66f4b98", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c6f4b3ee-aed7-4923-9da2-3b35ac9d12b7", "node_type": "1", "metadata": {}, "hash": "22fbc850929d5af7d73a794c7a8163fbd7c93e4962cb6deab7a644d16da2ae64", "class_name": "RelatedNodeInfo"}}, "text": "Sysoev and Andrianov\nused a similar approach for the Russian language [7] [8].\nBased on its success for a wide range of languages, our choice\nfell on this model to tackle automated data generation and\nannotation for the Armenian language.\nAside from the lack of training data, we also address the\nabsence of a benchmark dataset of Armenian texts for named\nentity recognition. We propose a gold-standard corpus with\nmanual annotation of CoNLL named entity categories: person ,\nlocation, and organization [9] [10], hoping it will be used t o\nevaluate future named entity recognition models.\nFurthermore, popular entity recognition models were ap-\nplied to the mentioned data in order to obtain baseline resul ts\nfor future research in the area. Along with the datasets, we\ndeveloped GloVe [11] word embeddings to train and evaluate\nthe deep learning models in our experiments.\nThe contributions of this work are (i) the silver-standard\ntraining corpus, (ii) the gold-standard test corpus, (iii) GloVe\nword embeddings, (iv) baseline results for 3 different mode ls\non the proposed benchmark data set. All aforementioned\nresources are available on GitHub1.\nII. A UTOMATED TRAINING CORPUS GENERATION\nWe used Sysoev and Andrianov\u2019s modi\ufb01cation of the\nNothman et al. approach to automatically generate data for\ntraining a named entity recognizer. This approach uses link s\nbetween Wikipedia articles to generate sequences of named-\nentity annotated tokens.\nA. Dataset extraction\nThe main steps of the dataset extraction system are de-\nscribed in Figure 1.\nFirst, each Wikipedia article is assigned a named entity\nclass (e.g. the article /Armke/armini/armmen /Armke/armayb/armsha/armke/armayb/armsha/armhi/armayb/armnu (Kim Kashkashian) is\nclassi\ufb01ed as PER (person), /Armayb/armza/armgim/armyech/armre/armini /armlyun/armini/armgim/armayb (League of Nations)\nasORG (organization), /Armse/armini/armre/armini/armayb (Syria) as LOC etc). One of the\n1https://github.com/ispras-texterra/pioner\n\u00a9 2018 IEEE. Personal use of this material is permitted. Perm ission from IEEE must be obtained for all other uses, in any cu rrent or future media, including\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for r esale or redistribution to servers or lists, or\nreuse of any copyrighted component of this work in other work s.", "start_char_idx": 3454, "end_char_idx": 5823, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c6f4b3ee-aed7-4923-9da2-3b35ac9d12b7": {"__data__": {"id_": "c6f4b3ee-aed7-4923-9da2-3b35ac9d12b7", "embedding": null, "metadata": {"page_label": "2", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "718b525c-8452-4fb2-a017-735f3fc2725d", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a06d9e348a719f45160a8240f9212fcee76c28ca016345c243c07f1c36165494", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "302a8d5c-6348-44d5-935b-2a02f23c7f37", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "f4e7962e1e5d6c02aa8f0c6b8a58d53e8bf3ad23208262297a09604672fc5190", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "487f951b-7582-44a0-85de-f92bb3b90338", "node_type": "1", "metadata": {}, "hash": "817dccc5c2d987968976b94f8d8bb6efdbd03152412b8fe5f4466b7b3dfc38d3", "class_name": "RelatedNodeInfo"}}, "text": "Fig. 1: Steps of automatic dataset extraction from Wikipedi a\nClassi\ufb01cation of Wikipedia articles into NE types\nLabelling common article aliases to increase coverage\nExtraction of text fragments with outgoing links\nLabelling links according to their target article\u2019s type\nAdjustment of labeled entities\u2019 boundaries\ncore differences between our approach and Nothman\u2019s system\nis that we do not rely on manual classi\ufb01cation of articles and\ndo not use inter-language links to project article classi\ufb01c ations\nacross languages. Instead, our classi\ufb01cation algorithm us es\nonly an article\u2019s Wikidata entry\u2019s \ufb01rst instance of label\u2019s\nparent subclass of labels, which are, incidentally, language\nindependent and thus can be used for any language.\nThen, outgoing links in articles are assigned the article\u2019s\ntype they are leading to. Sentences are included in the train ing\ncorpus only if they contain at least one named entity and\nall contained capitalized words have an outgoing link to an\narticle of known type. Since in Wikipedia articles only the \ufb01 rst\nmention of each entity is linked, this approach becomes very\nrestrictive and in order to include more sentences, additio nal\nlinks are inferred. This is accomplished by compiling a list of\ncommon aliases for articles corresponding to named entitie s,\nand then \ufb01nding text fragments matching those aliases to\nassign a named entity label. An article\u2019s aliases include it s\ntitle, titles of disambiguation pages with the article, and texts\nof links leading to the article (e.g. /Armlyun/armyech/armnu/armini/armnu/armgim/armre/armayb/armda (Leningrad),\n/Armpe/armyech/armtyun/armre/armvo/armgim/armre/armayb/armda (Petrograd), /Armpe/armyech/armtyun/armyech/armre/armben/armvo/armvyun/armre/armgim (Peterburg) are aliases\nfor/Armse/armayb/armnu/armken/armtyun /Armpe/armyech/armtyun/armyech/armre/armben/armvo/armvyun/armre/armgim (Saint Petersburg)). The list of\naliases is compiled for all PER ,ORG ,LOC articles.\nAfter that, link boundaries are adjusted by removing the\nlabels for expressions in parentheses, the text after a comm a,\nand in some cases breaking into separate named entities if th e\nlinked text contains a comma. For example, [LOC/Armayb/armben/armvo/armvev/armhi/armayb/armnu\n/armparenleft/armke/armayb/armghat/armayb/armke/armparenright ](Abovyan (town)) is reworked into [LOC/Armayb/armben/armvo/armvev/armhi/armayb/armnu ]\n/armparenleft/armke/armayb/armghat/armayb/armke/armparenright .\nB. Using Wikidata to classify Wikipedia\nInstead of manually classifying Wikipedia articles as it wa s\ndone in Nothman et al., we developed a rule-based classi\ufb01er\nthat used an article\u2019s Wikidata instance of andsubclass of\nattributes to \ufb01nd the corresponding named entity type.The classi\ufb01cation could be done using solely instance\noflabels, but these labels are unnecessarily speci\ufb01c for the\ntask and building a mapping on it would require a more\ntime-consuming and meticulous work. Therefore, we classi\ufb01 ed\narticles based on their \ufb01rst instance of attribute\u2019s subclass of\nvalues. Table I displays the mapping between these values an d\nnamed entity types. Using higher-level subclass of values was\nnot an option as their values often were too general, making\nit impossible to derive the correct named entity category.\nC. Generated data\nUsing the algorithm described above, we generated 7455\nannotated sentences with 163247 tokens based on 20 February\n2018 dump of Armenian Wikipedia.\nThe generated data is still signi\ufb01cantly smaller than the\nmanually annotated corpora from CoNLL 2002 and 2003.\nFor comparison, the train set of English CoNLL 2003 corpus\ncontains 203621 tokens and the German one 206931, while\nthe Spanish and Dutch corpora from CoNLL 2002 respectively\n273037 and 218737 lines. The smaller size of our generated\ndata can be attributed to the strict selection of candidate\nsentences as well as simply to the relatively small size of\nArmenian Wikipedia.\nThe accuracy of annotation in the generated corpus heavily\nrelies on the quality of links in Wikipedia articles.", "start_char_idx": 0, "end_char_idx": 4031, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "487f951b-7582-44a0-85de-f92bb3b90338": {"__data__": {"id_": "487f951b-7582-44a0-85de-f92bb3b90338", "embedding": null, "metadata": {"page_label": "2", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "718b525c-8452-4fb2-a017-735f3fc2725d", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a06d9e348a719f45160a8240f9212fcee76c28ca016345c243c07f1c36165494", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c6f4b3ee-aed7-4923-9da2-3b35ac9d12b7", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "b5d37fbbf9b68285e75f95b6bb2d775aa25e2ae79d3800fea75e4eab77cefdef", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8b5451c1-4abe-47a3-8da5-f75e644dce5d", "node_type": "1", "metadata": {}, "hash": "2ebca87ef36d5f35079dba2567ca53ef4bf4c32474fe378c98f45e5666f0f1e6", "class_name": "RelatedNodeInfo"}}, "text": "C. Generated data\nUsing the algorithm described above, we generated 7455\nannotated sentences with 163247 tokens based on 20 February\n2018 dump of Armenian Wikipedia.\nThe generated data is still signi\ufb01cantly smaller than the\nmanually annotated corpora from CoNLL 2002 and 2003.\nFor comparison, the train set of English CoNLL 2003 corpus\ncontains 203621 tokens and the German one 206931, while\nthe Spanish and Dutch corpora from CoNLL 2002 respectively\n273037 and 218737 lines. The smaller size of our generated\ndata can be attributed to the strict selection of candidate\nsentences as well as simply to the relatively small size of\nArmenian Wikipedia.\nThe accuracy of annotation in the generated corpus heavily\nrelies on the quality of links in Wikipedia articles. During gen-\neration, we assumed that \ufb01rst mentions of all named entities\nhave an outgoing link to their article, however this was not a l-\nways the case in actual source data and as a result the train se t\ncontained sentences where not all named entities are labele d.\nAnnotation inaccuracies also stemmed from wrongly assigne d\nlink boundaries (for example, in Wikipedia article /Armayb/armre/armto/armvo/armvyun/armre\n/Armvo/armvyun/armyech/armlyun/armse/armlyun/armini /Armvev/armyech/armlyun/armini/armnu/armgim/armto/armvo/armnu (Arthur Wellesley) there is a link to the\nNapoleon article with the text \" /arme/Armnu/armayb/armpe/armvo/armlyun/armyech/armvo/armnu/armat \" (\"Napoleon is\"),\nwhen it should be \" /Armnu/armayb/armpe/armvo/armlyun/armyech/armvo/armnu/armat \" (\"Napoleon\")). Another kind\nof common annotation errors occurred when a named entity\nappeared inside a link not targeting a LOC ,ORG , orPER\narticle (e.g. \" /Armayb/Armmen/Armnu /armnu/armayb/armkhe/armayb/armgim/armayb/armho/armayb/armken/armayb/armnu /armat/armnu/armtyun/armre/armvo/armvyun/armto/armhi/armvo/armvyun/armnu/armnu/armyech/armre/armvo/armvyun/armmen \"\n(\"USA presidential elections\") is linked to the article /Armayb/Armmen/Armnu\n/armnu/armayb/armkhe/armayb/armgim/armayb/armho/armayb/armken/armayb/armnu /armat/armnu/armtyun/armre/armvo/armvyun/armto/armhi/armvo/armvyun/armnu/armnu/armyech/armre 2016 (United States presi-\ndential election, 2016) and as a result [ LOC/Armayb/Armmen/Armnu ] (USA) is\nlost).\nIII. T EST DATASET\nIn order to evaluate the models trained on generated data,\nwe manually annotated a named entities dataset comprising\n53453 tokens and 2566 sentences selected from over 250 news\ntexts from ilur.am2. This dataset is comparable in size with\nthe test sets of other languages (Table II). Included senten ces\nare from political, sports, local and world news (Figures 2,\n3), covering the period between August 2012 and July 2018.\nThe dataset provides annotations for 3 popular named entity\nclasses: people ( PER ), organizations ( ORG ), and locations\n(LOC ), and is released in CoNLL03 format with IOB tagging\nscheme. Tokens and sentences were segmented according to\nthe UD standards for the Armenian language [12].\n2http://ilur.am/news/newsline.html", "start_char_idx": 3269, "end_char_idx": 6295, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8b5451c1-4abe-47a3-8da5-f75e644dce5d": {"__data__": {"id_": "8b5451c1-4abe-47a3-8da5-f75e644dce5d", "embedding": null, "metadata": {"page_label": "3", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1002b06f-4a58-47f5-bada-303fcfdb722b", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "b58610248d2bfea5aecb203068790937f9e724f8929ea642273a530ddf4a5bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "487f951b-7582-44a0-85de-f92bb3b90338", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "f3df8a4a787b3f287562971e3961cd228896d7ded57babb8b01d6fd4c2a6b482", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "72f07dcb-e55f-4d27-b77b-2aa3f9d6cf0c", "node_type": "1", "metadata": {}, "hash": "95136fe2c744fe10765033483a225497af323be7fe2f62d354df1940f5cee62a", "class_name": "RelatedNodeInfo"}}, "text": "TABLE I: The mapping of Wikidata subclass of values to named e ntity types\nsubclass of NE type\ncompany, business enterprise, company, juridical person, air carrier, political organiza-\ntion, government organization, secret service, political party, international organization,\nalliance, armed organization, higher education instituti on, educational institution, uni-\nversity, educational organization, school, \ufb01ctional magi c school, broadcaster, newspaper,\nperiodical literature, religious organization, football club, sports team, musical ensemble,\nmusic organisation, vocal-musical ensemble, sports organ ization, criminal organization,\nmuseum of culture, scienti\ufb01c organisation, non-governmen tal organization, nonpro\ufb01t\norganization, national sports team, legal person, scholar ly publication, academic journal,\nassociation, band, sports club, institution, medical faci lityORG\nstate, disputed territory, country, occupied territory, p olitical territorial entity, city, town,\nvillage, rural area, rural settlement, urban-type settlem ent, geographical object, geo-\ngraphic location, geographic region, community, administ rative territorial entity, former\nadministrative territorial entity, human settlement, cou nty, province, federated state,\ndistrict, county-equivalent, municipal formation, raion , nahiyah, mintaqah, muhafazah,\nrealm, principality, historical country, watercourse, la ke, sea, still waters, body of water,\nlandmass, minor planet, landform, natural geographic obje ct, mountain range, mountain,\nprotected area, national park, geographic region, geograp hic location, arena, bridge,\nairport, stadium, performing arts center, public building , venue, sports venue, church,\ntemple, place of worship, retail buildingLOC\nperson, \ufb01ctional character, \ufb01ctional humanoid, human who m ay be \ufb01ctional, given name,\n\ufb01ctional human, magician in fantasyPER\nFig. 2: Topics distribution in the gold-standard corpus\nsports\n25.4%politics\n13% society\n6.9%of\ufb01cial\n4.6%\nopinion pieces4.6%\nart2.3%\nmiscellaneous43.2%\nTABLE II: Comparison of Armenian, English, German, Span-\nish and Russian test sets: sentence, token, and named entity\ncounts\nTest set Tokens LOC ORG PER\nArmenian 53453 1306 1337 1274\nEnglish CoNLL03 46435 1668 1661 1617\nGerman CoNLL03 51943 1035 773 1195\nSpanish CoNLL02 51533 1084 1400 735\nRussian factRuEval-2016 59382 1239 1595 1353\nDuring annotation, we generally relied on categories and\nguidelines assembled by BBN Technologies for TREC 2002\nquestion answering track3. Only named entities corresponding\nto BBN\u2019s person name category were tagged as PER .\nThose include proper names of people, including \ufb01ctional\npeople, \ufb01rst and last names, family names, unique nicknames .\n3https://catalog.ldc.upenn.edu/docs/LDC2005T33/BBN-T ypes-\nSubtypes.htmlFig. 3: Distribution of examples by location in the gold-\nstandard corpus\nlocal\n40%world\n13.5%\nregional9.6%\nmiscellaneous36.9%\nSimilarly, organization name categories, including company\nnames, government agencies, educational and academic in-\nstitutions, sports clubs, musical ensembles and other grou ps,\nhospitals, museums, newspaper names, were marked as ORG .\nHowever, unlike BBN, we did not mark adjectival forms of\norganization names as named entities. BBN\u2019s gpe name ,\nfacility name ,location name categories were combined and\nannotated as LOC .\nWe ignored entities of other categories (e.g. works of art,\nlaw, or events), including those cases where an ORG ,LOC or\nPER entity was inside an entity of extraneous type (e.g. /Armho/Armho\n(RA) in/Armho/Armho /Armke/armre/armyech/armayb/armken/armayb/armnu /Armo/armre/armyech/armnu/armse/armgim/armini/armre/armke (RA Criminal Code) was\nnot annotated as LOC ).\nQuotation marks around a named entity were not annotated\nunless those quotations were a part of that entity\u2019s full of\ufb01 -\ncial name (e.g.", "start_char_idx": 0, "end_char_idx": 3824, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "72f07dcb-e55f-4d27-b77b-2aa3f9d6cf0c": {"__data__": {"id_": "72f07dcb-e55f-4d27-b77b-2aa3f9d6cf0c", "embedding": null, "metadata": {"page_label": "3", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1002b06f-4a58-47f5-bada-303fcfdb722b", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "b58610248d2bfea5aecb203068790937f9e724f8929ea642273a530ddf4a5bb8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8b5451c1-4abe-47a3-8da5-f75e644dce5d", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "479ac7a48b0c81a8f934f4f6012f6090f1213912abb125a73e851903301c6e0d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "68af7989-bde8-4ce5-a0db-6ff8fc4d94fa", "node_type": "1", "metadata": {}, "hash": "9af1634fb0e1d114c327cf7d82333f83f030061baece4520f9bfe3d93fbc6407", "class_name": "RelatedNodeInfo"}}, "text": "However, unlike BBN, we did not mark adjectival forms of\norganization names as named entities. BBN\u2019s gpe name ,\nfacility name ,location name categories were combined and\nannotated as LOC .\nWe ignored entities of other categories (e.g. works of art,\nlaw, or events), including those cases where an ORG ,LOC or\nPER entity was inside an entity of extraneous type (e.g. /Armho/Armho\n(RA) in/Armho/Armho /Armke/armre/armyech/armayb/armken/armayb/armnu /Armo/armre/armyech/armnu/armse/armgim/armini/armre/armke (RA Criminal Code) was\nnot annotated as LOC ).\nQuotation marks around a named entity were not annotated\nunless those quotations were a part of that entity\u2019s full of\ufb01 -\ncial name (e.g. /armquotleft/Armnu/armayb/armini/armre/armini/armtyun /armgim/armvo/armre/armtsa/armayb/armre/armayb/armnu/armquotright /Armpyur/Armben/Armat (\"Nairit Plant\"\nCJSC)).\nDepending on context, metonyms such as /Armken/armre/armyech/armmen/armlyun (Kremlin),", "start_char_idx": 3136, "end_char_idx": 4077, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "68af7989-bde8-4ce5-a0db-6ff8fc4d94fa": {"__data__": {"id_": "68af7989-bde8-4ce5-a0db-6ff8fc4d94fa", "embedding": null, "metadata": {"page_label": "4", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d578ed6c-fe42-4513-bc2c-a4d2b8cc92c8", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "27ac438b164699c8e3cd925bedbd5415216e41b2e8efc1e3b80ba21731b28ec4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "72f07dcb-e55f-4d27-b77b-2aa3f9d6cf0c", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "d1f4bb94fd7e28e7488f3ff09296e14ea3f922e9050aa6f365207e6cd70a0677", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d34ac9c1-9944-42b2-a752-56325798331e", "node_type": "1", "metadata": {}, "hash": "52c267ea9a2e134cb9f4e5e77d6f756432e881896d1e58f984ccb9c33bebd58d", "class_name": "RelatedNodeInfo"}}, "text": "/Armben/armayb/armghat/armre/armayb/armmen/armhi/armayb/armnu 26 (Baghramyan 26) were annotated as ORG\nwhen referring to respective government agencies. Likewis e,\ncountry or city names were also tagged as ORG when\nreferring to sports teams representing them.\nIV. W ORD EMBEDDINGS\nApart from the datasets, we also developed word embed-\ndings for the Armenian language, which we used in our\nexperiments to train and evaluate named entity recognition\nalgorithms. Considering their ability to capture semantic reg-\nularities, we used GloVe to train word embeddings. We assem-\nbled a dataset of Armenian texts containing 79 million token s\nfrom the articles of Armenian Wikipedia, The Armenian Sovie t\nEncyclopedia, a subcorpus of Eastern Armenian National\nCorpus [13], over a dozen Armenian news websites and blogs.\nIncluded texts covered topics such as economics, politics,\nweather forecast, IT, law, society and politics, coming fro m\nnon-\ufb01ction as well as \ufb01ction genres.\nSimilar to the original embeddings published for the Englis h\nlanguage, we release 50-, 100-, 200- and 300-dimensional\nword vectors for Armenian with a vocabulary size of 400000.\nBefore training, all the words in the dataset were lowercase d.\nFor the \ufb01nal models we used the following training hyperpa-\nrameters: 15 window size and 20 training epochs.\nV. E XPERIMENTS\nIn this section we describe a number of experiments targeted\nto compare the performance of popular named entity recogni-\ntion algorithms on our data. We trained and evaluated Stanfo rd\nNER4, spaCy 2.05, and a recurrent model similar to [14],\n[15] that uses bidirectional LSTM cells for character-base d\nfeature extraction and CRF, described in Guillaume Genthia l\u2019s\nSequence Tagging with Tensor\ufb02ow blog post [16].\nA. Models\nStanford NER is conditional random \ufb01elds (CRF) classi\ufb01er\nbased on lexical and contextual features such as the current\nword, character-level n-grams of up to length 6 at its begin-\nning and the end, previous and next words, word shape and\nsequence features [17].\nspaCy 2.0 uses a CNN-based transition system for named\nentity recognition. For each token, a Bloom embedding is\ncalculated based on its lowercase form, pre\ufb01x, suf\ufb01x and\nshape, then using residual CNNs, a contextual representati on\nof that token is extracted that potentially draws informati on\nfrom up to 4 tokens from each side [18]. Each update of\nthe transition system\u2019s con\ufb01guration is a classi\ufb01cation ta sk\nthat uses the contextual representation of the top token on t he\nstack, preceding and succeeding tokens, \ufb01rst two tokens of t he\nbuffer, and their leftmost, second leftmost, rightmost, se cond\nrightmost children. The valid transition with the highest s core\nis applied to the system. This approach reportedly performs\nwithin 1% of the current state-of-the-art for English6. In our\n4https://nlp.stanford.edu/software/CRF-NER.shtml\n5https://spacy.io/\n6https://spacy.io/usage/v2#features-modelsFig. 4: The neural architecture for extracting contextual r ep-\nresentations of tokens\nInput tokenWord embedding\nCharacter embeddingsbiLSTMbiLSTMContextual representation\nexperiments, we tried out 50-, 100-, 200- and 300-dimension al\npre-trained GloVe embeddings. Due to time constraints, we d id\nnot tune the rest of hyperparameters and used their default\nvalues.\nThe main model that we focused on was the recurrent\nmodel with a CRF top layer, and the above-mentioned methods\nserved mostly as baselines. The distinctive feature of this\napproach is the way contextual word embeddings are formed.\nFor each token separately, to capture its word shape feature s,\ncharacter-based representation is extracted using a bidir ec-\ntional LSTM [19]. This representation gets concatenated wi th\na distributional word vector such as GloVe, forming an inter -\nmediate word embedding. Using another bidirectional LSTM\ncell on these intermediate word embeddings, the contextual\nrepresentation of tokens is obtained (Figure 4). Finally, a CRF\nlayer labels the sequence of these contextual representati ons.\nIn our experiments, we used Guillaume Genthial\u2019s implemen-\ntation7of the algorithm.", "start_char_idx": 0, "end_char_idx": 4102, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d34ac9c1-9944-42b2-a752-56325798331e": {"__data__": {"id_": "d34ac9c1-9944-42b2-a752-56325798331e", "embedding": null, "metadata": {"page_label": "4", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d578ed6c-fe42-4513-bc2c-a4d2b8cc92c8", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "27ac438b164699c8e3cd925bedbd5415216e41b2e8efc1e3b80ba21731b28ec4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "68af7989-bde8-4ce5-a0db-6ff8fc4d94fa", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "1025c5d57f9eca5302e7568886e87f9875421a3d445ccff73b4485657fd5badf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e4690e11-1b8d-47e9-a444-b68d330b092a", "node_type": "1", "metadata": {}, "hash": "c0bc921668fdc3f0186f938c62ba64cf187510668ba6164004c29edadd460503", "class_name": "RelatedNodeInfo"}}, "text": "Due to time constraints, we d id\nnot tune the rest of hyperparameters and used their default\nvalues.\nThe main model that we focused on was the recurrent\nmodel with a CRF top layer, and the above-mentioned methods\nserved mostly as baselines. The distinctive feature of this\napproach is the way contextual word embeddings are formed.\nFor each token separately, to capture its word shape feature s,\ncharacter-based representation is extracted using a bidir ec-\ntional LSTM [19]. This representation gets concatenated wi th\na distributional word vector such as GloVe, forming an inter -\nmediate word embedding. Using another bidirectional LSTM\ncell on these intermediate word embeddings, the contextual\nrepresentation of tokens is obtained (Figure 4). Finally, a CRF\nlayer labels the sequence of these contextual representati ons.\nIn our experiments, we used Guillaume Genthial\u2019s implemen-\ntation7of the algorithm. We set the size of character-based\nbiLSTM to 100 and the size of second biLSTM network to\n300.\nB. Evaluation\nExperiments were carried out using IOB tagging scheme,\nwith a total of 7 class tags: O, B-PER, I-PER, B-LOC, I-LOC,\nB-ORG, I-ORG.\nWe randomly selected 80% of generated annotated sen-\ntences for training and used the other 20% as a development\nset. The models with the best F1 score on the development\nset were tested on the manually annotated gold dataset.\n7https://github.com/guillaumegenthial/sequence_taggi ng", "start_char_idx": 3192, "end_char_idx": 4624, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4690e11-1b8d-47e9-a444-b68d330b092a": {"__data__": {"id_": "e4690e11-1b8d-47e9-a444-b68d330b092a", "embedding": null, "metadata": {"page_label": "5", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d0c966e3-49f6-471d-ab0b-8c874f7e822b", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "9440bda2bc9fd006612c48381d0a859b1c1e920b8fca162bf4e84d59f879d647", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d34ac9c1-9944-42b2-a752-56325798331e", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a3870b4d055456019ee4a4ac52c16c8c832188d6f4a8ee1c39d1e54e40163da1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4c90538c-b368-465a-a8d4-622d4a89eadf", "node_type": "1", "metadata": {}, "hash": "612549d3ecc029bd204bb02a8e3ff5adccec146f4bf98b334b7878fde5ccd593", "class_name": "RelatedNodeInfo"}}, "text": "TABLE III: Evaluation results for named entity recognition algorithms\nAlgorithmdev test\nPrecision Recall F1 Precision Recall F1\nStanford NER 76.86 70.62 73.61 78.46 46.52 58.41\nspaCy 2.0 68.19 71.86 69.98 64.83 55.77 59.96\nChar-biLSTM+biLSTM+CRF 77.21 74.81 75.99 73.27 54.14 62.23\nTABLE IV: Evaluation results for Char-biLSTM+biLSTM+CRF\nWord embeddingstrain embeddings=False train embeddings=True\ndev F1 test F1 dev F1 test F1\nGloVe (dim=50) 74.18 56.46 75.99 62.23\nGloVe (dim=100) 73.94 58.52 74.83 61.54\nGloVe (dim=200) 75.00 58.37 74.97 59.78\nGloVe (dim=300) 74.21 59.66 74.75 59.92\nTABLE V: Evaluation results for spaCy 2.0 NER\nWord embeddingsdev test\nPrecision Recall F1 Precision Recall F1\nGloVe (dim=50) 69.31 71.26 70.27 66.52 51.72 58.20\nGloVe (dim=100) 70.12 72.91 71.49 66.34 53.35 59.14\nGloVe (dim=200) 68.19 71.86 69.98 64.83 55.77 59.96\nGloVe (dim=300) 70.08 71.80 70.93 66.61 52.94 59.00\nVI. D ISCUSSION\nTable III shows the average scores of evaluated models. The\nhighest F1 score was achieved by the recurrent model using\na batch size of 8 and Adam optimizer with an initial learning\nrate of 0.001. Updating word embeddings during training\nalso noticeably improved the performance. GloVe word vecto r\nmodels of four different sizes (50, 100, 200, and 300) were\ntested, with vectors of size 50 producing the best results (T able\nIV).\nFor spaCy 2.0 named entity recognizer, the same word\nembedding models were tested. However, in this case the per-\nformance of 200-dimensional embeddings was highest (Table\nV). Unsurprisingly, both deep learning models outperforme d\nthe feature-based Stanford recognizer in recall, the latte r\nhowever demonstrated noticeably higher precision.\nIt is clear that the development set of automatically gen-\nerated examples was not an ideal indicator of models\u2019 per-\nformance on gold-standard test set. Higher development set\nscores often led to lower test scores as seen in the evalua-\ntion results for spaCy 2.0 and Char-biLSTM+biLSTM+CRF\n(Tables V and IV). Analysis of errors on the development\nset revealed that many were caused by the incomplete-\nness of annotations, when named entity recognizers correct ly\npredicted entities that were absent from annotations (e.g.\n[/Armkhe/Armse/Armho/Armmen/armendash/armini LOC ] (USSR\u2019s), [ /Armda/armini/armnu/armayb/armmen/armvo/armnu ORG ] (the_Dinamo),\n[/Armpe/armini/armre/armyech/armnu/armyech/armhi/armayb/armnu /armto/armyech/armre/armayb/armken/armghat/armza/armvo/armvyun LOC ] (Iberian Peninsula\u2019s) etc).\nSimilarly, the recognizers often correctly ignored non-en tities\nthat are incorrectly labeled in data (e.g. [ /armo/armse/armmen/armayb/armnu/armnu/armyech/armre/armini PER ],\n[/armken/armvo/armnu/armse/armyech/armre/armvev/armayb/armtyun/armvo/armre/armini/armayb/armnu ORG ] etc).\nGenerally, tested models demonstrated relatively high pre -cision of recognizing tokens that started named entities, b ut\nfailed to do so with descriptor words for organizations and,\nto a certain degree, locations. The confusion matrix for one\nof the trained recurrent models illustrates that differenc e\n(Table VI).", "start_char_idx": 0, "end_char_idx": 3104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c90538c-b368-465a-a8d4-622d4a89eadf": {"__data__": {"id_": "4c90538c-b368-465a-a8d4-622d4a89eadf", "embedding": null, "metadata": {"page_label": "5", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d0c966e3-49f6-471d-ab0b-8c874f7e822b", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "9440bda2bc9fd006612c48381d0a859b1c1e920b8fca162bf4e84d59f879d647", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e4690e11-1b8d-47e9-a444-b68d330b092a", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "23b6ea41c7e3ff5efc49acfaeecc36ef5920eb00be626c216ef524bc6dd2873f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5a8a4aae-e59a-45c7-a61f-0ff627d70a42", "node_type": "1", "metadata": {}, "hash": "2ae0a11734b9d53bf983d8f28aae5feacba60a7600bc6cf50ec9408cb4b69173", "class_name": "RelatedNodeInfo"}}, "text": "Similarly, the recognizers often correctly ignored non-en tities\nthat are incorrectly labeled in data (e.g. [ /armo/armse/armmen/armayb/armnu/armnu/armyech/armre/armini PER ],\n[/armken/armvo/armnu/armse/armyech/armre/armvev/armayb/armtyun/armvo/armre/armini/armayb/armnu ORG ] etc).\nGenerally, tested models demonstrated relatively high pre -cision of recognizing tokens that started named entities, b ut\nfailed to do so with descriptor words for organizations and,\nto a certain degree, locations. The confusion matrix for one\nof the trained recurrent models illustrates that differenc e\n(Table VI). This can be partly attributed to the quality of\ngenerated data: descriptor words are sometimes super\ufb02uous ly\nlabeled (e.g. [ /Armho/armayb/armvev/armayb/armhi/armayb/armnu /armken/armghat/armza/armini/armnu/armyech/armre/armini /armtyun/armyech/armghat/armayb/armben/armnu/armini/armken/armnu/armyech/armre/armat LOC ] (the\nindigenous people of Hawaii)), which is likely caused by the\ninconsistent style of linking in Armenian Wikipedia (in the\narticle/Armayb/Armmen/Armnu/armmen/armsha/armayb/armken/armvo/armvyun/armhi/armto (Culture of the United States), its linked\ntext fragment \" /Armho/armayb/armvev/armayb/armhi/armayb/armnu /armken/armghat/armza/armini/armnu/armyech/armre/armini /armtyun/armyech/armghat/armayb/armben/armnu/armini/armken/armnu/armyech/armre/armat \" (\"the\nindigenous people of Hawaii\") leads to the article /Armho/armayb/armvev/armayb/armhi/armayb/armnu\n/armken/armghat/armza/armini/armnu/armyech/armre (Hawaii)).\nTABLE VI: Confusion matrix on the development set\nPredicted\nO B-PER B-ORG B-LOC I-ORG I-PER I-LOCActualO 26707 100 57 249 150 78 129\nB-PER 107 712 6 32 2 4 0\nB-ORG 93 6 259 58 8 0 0\nB-LOC 226 25 32 1535 5 3 20\nI-ORG 67 1 5 3 289 3 19\nI-PER 46 5 0 1 6 660 8\nI-LOC 145 0 1 13 45 11 597\nPrecision (%) 97.5 83.86 71.94 81.17 57.23 86.95 77.23\nVII. C ONCLUSION\nWe release two named-entity annotated datasets for the\nArmenian language: a silver-standard corpus for training N ER\nmodels, and a gold-standard corpus for testing. It is worth t o\nunderline the importance of the latter corpus, as we aim it\nto serve as a benchmark for future named entity recognition\nsystems designed for the Armenian language. Along with the", "start_char_idx": 2505, "end_char_idx": 4760, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5a8a4aae-e59a-45c7-a61f-0ff627d70a42": {"__data__": {"id_": "5a8a4aae-e59a-45c7-a61f-0ff627d70a42", "embedding": null, "metadata": {"page_label": "6", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef2ffb70-a742-4e82-8f25-5953600ab075", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "56b81422a0e14035fcbf1047055a1cd9ede0285a2fe023772e80aae66de07d23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4c90538c-b368-465a-a8d4-622d4a89eadf", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a811e9b87fa4dfb75e67d74277321919851c28f5fc260f9e274930e7af8ed049", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fd8f3b7-f1fb-413f-b074-f4f719395c99", "node_type": "1", "metadata": {}, "hash": "b78ed1236652a187176694ebd53eb9e92237474dbe232d2a570121c6122ac258", "class_name": "RelatedNodeInfo"}}, "text": "corpora, we publish GloVe word vector models trained on a\ncollection of Armenian texts.\nAdditionally, to establish the applicability of Wikipedia -\nbased approaches for the Armenian language, we provide eval -\nuation results for 3 different named entity recognition sys tems\ntrained and tested on our datasets. The results reinforce th e\nability of deep learning approaches in achieving relativel y high\nrecall values for this speci\ufb01c task, as well as the power of\nusing character-extracted embeddings alongside conventi onal\nword embeddings.\nThere are several avenues of future work. Since Nothman\net al. 2013, more ef\ufb01cient methods of exploiting Wikipedia\nhave been proposed, namely WiNER [20], which could help\nincrease both the quantity and quality of the training corpu s.\nAnother potential area of work is the further enrichment of\nthe benchmark test set with additional annotation of other\nclasses such as MISC or more \ufb01ne-grained types (e.g. CITY ,\nCOUNTRY ,REGION etc instead of LOC ).\nREFERENCES\n[1] David Yarowsky, Grace Ngai, Richard Wicentowski. Inducing multilin-\ngual text analysis tools via robust projection across align ed corpora .\nIn Proceedings of the First International Conference on Hum an Lan-\nguage Technology Research. Association for Computational Linguistics,\nStroudsburg, PA, USA, HLT\u201901, pp. 1\u20138. 2001.\n[2] Imed Zitouni, Radu Florian. Mention detection crossing the language\nbarrier . In Proceedings of the 2008 Conference on Empirical Methods in\nNatural Language Processing. Association for Computation al Linguistics,\npp. 600\u2013609. 2008.\n[3] Maud Ehrmann, Marco Turchi, Ralf Steinberger. Building a multilingual\nnamed entity-annotated corpus using annotation projectio n. In Proceed-\nings of Recent Advances in Natural Language Processing. Ass ociation\nfor Computational Linguistics, pp. 118\u2013124. 2011.\n[4] Klesti Hoxha, Artur Baxhaku. An Automatically Generated Annotated\nCorpus for Albanian Named Entity Recognition CYBERNETICS AND\nINFORMATION TECHNOLOGIES, vol. 18, No 1. 2018.\n[5] Weber and P\u00f6tzl. NERU: Named Entity Recognition for German . Pro-\nceedings of GermEval 2014 Named Entity Recognition Shared T ask, pp.\n157-162. 2014.\n[6] Nothman J., Ringland N., Radford W., Murphy T., Curran J. R.Learning\nmultilingual named entity recognition from Wikipedia . Arti\ufb01cial Intelli-\ngence, vol. 194, pp. 151\u2013175. 2013.\n[7] Sysoev A. A., Andrianov I. A. Named Entity Recognition in Russian: the\nPower of Wiki-Based Approach . Computational Linguistics and Intellec-\ntual Technologies: Proceedings of the International Confe rence \"Dialogue\n2016\". 2016.\n[8] Turdakov D., Astrakhantsev N., Nedumov Y ., Sysoev A., An drianov I.,\nMayorov V ., Fedorenko D., Korshunov A., Kuznetsov S. Texterra: A\nFramework for Text Analysis . Proceedings of the Institute for System\nProgramming of RAS, vol. 26, Issue 1, pp. 421\u2013438. 2014.\n[9] Erik F. Tjong Kim Sang. Introduction to the CoNLL-2002 Shared\nTask: Language-Independent Named Entity Recognition . Proceedings of\nCoNLL-2002, pp. 155\u2013158. Taipei, Taiwan. 2002.\n[10] Erik F. Tjong Kim Sang, Fien De Meulder. Introduction to the CoNLL-\n2003 Shared Task: LanguageIndependent Named Entity Recogn ition.\nProceedings of the CoNLL-2003, vol. 4, pp. 142\u2013147. Associa tion for\nComputational Linguistics. 2003.\n[11] Jeffrey Pennington, Richard Socher, Christopher D. Ma nning. GloVe:\nGlobal Vectors for Word Representation. . Empirical Methods in Natural\nLanguage Processing (EMNLP) 2014, pp. 1532-1543. 2014.", "start_char_idx": 0, "end_char_idx": 3470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3fd8f3b7-f1fb-413f-b074-f4f719395c99": {"__data__": {"id_": "3fd8f3b7-f1fb-413f-b074-f4f719395c99", "embedding": null, "metadata": {"page_label": "6", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ef2ffb70-a742-4e82-8f25-5953600ab075", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "56b81422a0e14035fcbf1047055a1cd9ede0285a2fe023772e80aae66de07d23", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5a8a4aae-e59a-45c7-a61f-0ff627d70a42", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "ed36cd27afd8a4ff686c7e29dd878275fdb7e6b03964f10bc14f42f41ac66cff", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2730dc0-dcd0-4abf-8968-c1328d855cd7", "node_type": "1", "metadata": {}, "hash": "c5359f84fd3bd1e1ca913051d447baf564ba42bcb2fe91cd0a58cda73bc72526", "class_name": "RelatedNodeInfo"}}, "text": "Introduction to the CoNLL-2002 Shared\nTask: Language-Independent Named Entity Recognition . Proceedings of\nCoNLL-2002, pp. 155\u2013158. Taipei, Taiwan. 2002.\n[10] Erik F. Tjong Kim Sang, Fien De Meulder. Introduction to the CoNLL-\n2003 Shared Task: LanguageIndependent Named Entity Recogn ition.\nProceedings of the CoNLL-2003, vol. 4, pp. 142\u2013147. Associa tion for\nComputational Linguistics. 2003.\n[11] Jeffrey Pennington, Richard Socher, Christopher D. Ma nning. GloVe:\nGlobal Vectors for Word Representation. . Empirical Methods in Natural\nLanguage Processing (EMNLP) 2014, pp. 1532-1543. 2014.\n[12] Marat M. Yavrumyan, Hrant H. Khachatrian, Anna S. Danie lyan, Gor\nD. Arakelyan. ArmTDP: Eastern Armenian Treebank and Dependency\nParser . XI International Conference on Armenian Linguistics, Abs tracts.\nYerevan. 2017.\n[13] Khurshudian V .G., Daniel M.A., Levonian D.V ., Plungia n V .A.,\nPolyakov A.E., Rubakov S.V . EASTERN ARMENIAN NATIONAL COR-\nPUS. \"Dialog 2009\". 2009.[14] Guillaume Lample, Miguel Ballesteros, Sandeep Subram anian, Kazuya\nKawakami, Chris Dyer. Neural Architectures for Named Entity Recogni-\ntion Proceedings of NAACL-2016, San Diego, California, USA. 201 6.\n[15] Xuezhe Ma, Eduard Hovy. End-to-end Sequence Labeling via Bi-\ndirectional LSTM-CNNs-CRF . Proceedings of ACL. 2016.\n[16] Guillaume Genthial. Sequence Tagging with Tensor\ufb02ow . 2017.\nhttps://guillaumegenthial.github.io/\n[17] Jenny Rose Finkel, Trond Grenager, Christopher Mannin g.Incorporat-\ning Non-local Information into Information Extraction Sys tems by Gibbs\nSampling . Proceedings of the 43nd Annual Meeting of the Association\nfor Computational Linguistics (ACL 2005), pp. 363-370. 200 5.\n[18] Emma Strubell, Patrick Verga, David Belanger, Andrew M cCallum.\nFast and Accurate Entity Recognition with Iterated Dilated Convolutions .\n2017.https://arxiv.org/pdf/1702.02098.pdf\n[19] Wang Ling, Tiago L\u00fa\u0131s, L\u00fa\u0131s Marujo, Ram\u00f3n Fernandez Ast udillo,\nSilvio Amir, Chris Dyer, Alan W Black, Isabel Trancoso. Finding function\nin form: Compositional character models for open vocabular y word\nrepresentation . Proceedings of EMNLP. 2015.\n[20] Abbas Ghaddar, Philippe Langlais. WiNER: A Wikipedia Annotated\nCorpus for Named Entity Recognition . Proceedings of the The 8th Inter-\nnational Joint Conference on Natural Language Processing, pp. 413\u2013422.\n2017.\n[21] Toldova S. Y . Starostin A. S., Bocharov V . V ., Alexeeva S . V ., Bodrova\nA. A., Chuchunkov A. S., Dzhumaev S. S., E\ufb01menko I. V ., Granov sky\nD. V ., Khoroshevsky V . F., Krylova I. V ., Nikolaeva M. A., Smu rov\nI. M. FactRuEval 2016: Evaluation of Named Entity Recognition an d\nFact Extraction Systems for Russian . Computational Linguistics and\nIntellectual Technologies: Proceedings of the Internatio nal Conference\n\"Dialogue 2016\". 2016.", "start_char_idx": 2878, "end_char_idx": 5658, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2730dc0-dcd0-4abf-8968-c1328d855cd7": {"__data__": {"id_": "a2730dc0-dcd0-4abf-8968-c1328d855cd7", "embedding": null, "metadata": {"page_label": "1", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d1e02e8-6587-4553-acb5-c0f5437307a3", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a3ee109f9ddf8cf85db2a56d348975385a06487b860d995f7769f940ca517372", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3fd8f3b7-f1fb-413f-b074-f4f719395c99", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "b63b192bcca82d396a841904c58e274d52379f7cbc3c78e02910106ef9c9ad1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e9a66b0d-7bd4-48df-8d91-ab0a2eb0297c", "node_type": "1", "metadata": {}, "hash": "d6e27dfe6db702592bbcc783e68bfcb65313c9b387024791cafce6fd69e3ce92", "class_name": "RelatedNodeInfo"}}, "text": "Addressing Word-order Divergence in Multilingual Neural Machine\nTranslation for extremely Low Resource Languages\nRudra Murthy V\u2020, Anoop Kunchukuttan\u2021, Pushpak Bhattacharyya\u2020\n\u2020Center for Indian Language Technology (CFILT)\nDepartment of Computer Science and Engineering\nIIT Bombay, India.\n\u2021Microsoft AI & Research, Hyderabad, India.\n{rudra,pb }@cse.iitb.ac.in, ankunchu@microsoft.com\nAbstract\nTransfer learning approaches for Neural Ma-\nchine Translation (NMT) trains a NMT model\non an assisting language-target language pair\n(parent model) which is later \ufb01ne-tuned for\nthe source language-target language pair of in-\nterest (child model), with the target language\nbeing the same. In many cases, the assist-\ning language has a different word order from\nthe source language. We show that divergent\nword order adversely limits the bene\ufb01ts from\ntransfer learning when little to no parallel cor-\npus between the source and target language is\navailable. To bridge this divergence, we pro-\npose to pre-order the assisting language sen-\ntences to match the word order of the source\nlanguage and train the parent model. Our ex-\nperiments on many language pairs show that\nbridging the word order gap leads to major\nimprovements in the translation quality in ex-\ntremely low-resource scenarios.\n1 Introduction\nTransfer learning for multilingual Neural Machine\nTranslation (NMT) (Zoph et al., 2016; Dabre et al.,\n2017; Nguyen and Chiang, 2017) attempts to im-\nprove the NMT performance on the source to\ntarget language pair (child task) using an assist-\ning source language (assisting to target language\ntranslation is the parent task). Here, the parent\nmodel is trained on the assisting and target lan-\nguage parallel corpus and the trained weights are\nused to initialize the child model. If source-target\nlanguage pair parallel corpus is available, the child\nmodel can further be \ufb01ne-tuned. The weight ini-\ntialization reduces the requirement on the training\ndata for the source-target language pair by trans-\nferring knowledge from the parent task, thereby\nimproving the performance on the child task.\nHowever, the divergence between the source\nand the assisting language can adversely impactthe bene\ufb01ts obtained from transfer learning. Mul-\ntiple studies have shown that transfer learning\nworks best when the languages are related (Zoph\net al., 2016; Nguyen and Chiang, 2017; Dabre\net al., 2017). Zoph et al. (2016) studied the in-\n\ufb02uence of language divergence between languages\nchosen for training the parent and the child model,\nand showed that choosing similar languages for\ntraining the parent and the child model leads to\nbetter improvements from transfer learning.\nSeveral studies have tried to address the lex-\nical divergence between the source and the tar-\nget languages either by using Byte Pair Encoding\n(BPE) as basic input representation units (Nguyen\nand Chiang, 2017) or character-level NMT sys-\ntem (Lee et al., 2017) or bilingual embeddings\n(Gu et al., 2018). However, the effect of word\norder divergence and its mitigation has not been\nexplored. In a practical setting, it is not uncom-\nmon to have source and assisting languages with\ndifferent word order. For instance, it is possible to\n\ufb01nd parallel corpora between English (SVO word\norder) and some Indian (SOV word order) lan-\nguages, but very little parallel corpora between In-\ndian languages. Hence, it is natural to use English\nas an assisting language for inter-Indian language\ntranslation.\nTo address the word order divergence, we pro-\npose to pre-order the assisting language sentences\n(SVO) to match the word order of the source lan-\nguage (SOV). We consider an extremely resource-\nconstrained scenario, where there is no parallel\ncorpus for the child task. From our experiments,\nwe show that there is a signi\ufb01cant increase in the\ntranslation accuracy for the unseen source-target\nlanguage pair.arXiv:1811.00383v2  [cs.CL]  10 Apr 2019", "start_char_idx": 0, "end_char_idx": 3907, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e9a66b0d-7bd4-48df-8d91-ab0a2eb0297c": {"__data__": {"id_": "e9a66b0d-7bd4-48df-8d91-ab0a2eb0297c", "embedding": null, "metadata": {"page_label": "2", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "066aa014-8322-42ee-ab37-5b2d37726129", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "35530197eb27b85cc89a701cebbfcd516bd066f0ff4284cf37d7fd9589fc8286", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2730dc0-dcd0-4abf-8968-c1328d855cd7", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a3ee109f9ddf8cf85db2a56d348975385a06487b860d995f7769f940ca517372", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "71297b9c-7a19-42dd-a820-4fad38649288", "node_type": "1", "metadata": {}, "hash": "7e48717ac7be3ce3f853455c0b11feb61683d285c91b2693e2a95f9c57be3462", "class_name": "RelatedNodeInfo"}}, "text": "2 Related Work\nTo the best of our knowledge, no work has ad-\ndressed word order divergence in transfer learn-\ning for multilingual NMT. However, some work\nexists for other NLP tasks in a multilingual set-\nting. For Named Entity Recognition (NER), Xie\net al. (2018) use a self-attention layer after the\nBi-LSTM layer to address word-order divergence\nfor Named Entity Recognition (NER) task. The\napproach does not show any signi\ufb01cant improve-\nments, possibly because the divergence has to be\naddressed before/during construction of the con-\ntextual embeddings in the Bi-LSTM layer. Joty\net al. (2017) use adversarial training for cross-\nlingual question-question similarity ranking. The\nadversarial training tries to force the sentence rep-\nresentation generated by the encoder of similar\nsentences from different input languages to have\nsimilar representations.\nPre-ordering the source language sentences to\nmatch the target language word order has been\nfound useful in addressing word-order divergence\nfor Phrase-Based SMT (Collins et al., 2005; Ra-\nmanathan et al., 2008; Navratil et al., 2012; Chat-\nterjee et al., 2014). For NMT, Ponti et al. (2018)\nand Kawara et al. (2018) have explored pre-\nordering. Ponti et al. (2018) demonstrated that\nby reducing the syntactic divergence between the\nsource and the target languages, consistent im-\nprovements in NMT performance can be obtained.\nOn the contrary, Kawara et al. (2018) reported\ndrop in NMT performance due to pre-ordering.\nNote that these works address source-target diver-\ngence, not divergence between source languages\nin multilingual NMT scenario.\n3 Proposed Solution\nConsider the task of translating for an extremely\nlow-resource language pair. The parallel corpus\nbetween the two languages, if available may be\ntoo small to train an NMT model. Similar to Zoph\net al. (2016), we use transfer learning to over-\ncome data sparsity between the source and the\ntarget languages. We choose English as the as-\nsisting language in all our experiments. In our\nresource-scarce scenario, we have no parallel cor-\npus for training the child model. Hence, at test\ntime, the source language sentence is translated\nusing the parent model after performing a word-\nby-word translation from source to the assisting\nlanguage using a bilingual dictionary.Before Reordering After Reordering\nS\nNP0 VP\nV NP 1S\nNP0 VP\nNP1V\nS\nNP\nNNP\nAnuragVP\nMD\nwillVP\nVB\nmeetNP\nNNP\nThakurS\nNP\nNNP\nAnuragVP\nNP\nNNP\nThakurVP\nMD\nwillVP\nVB\nmeet\nTable 1: Example showing transitive verb before and\nafter reordering (Adapted from Chatterjee et al. (2014))\nSince the source language and the assisting lan-\nguage (English) have different word order, we hy-\npothesize that it leads to inconsistencies in the\ncontextual representations generated by the en-\ncoder for the two languages. Speci\ufb01cally, given an\nEnglish sentence (SVO word order) and its transla-\ntion in the source language (SOV word order), the\nencoder representations for words in the two sen-\ntences will be different due to different contexts\nof synonymous words. This could lead to the at-\ntention and the decoder layers generating different\ntranslations from the same (parallel) sentence in\nthe source or assisting language. This is undesir-\nable as we want the knowledge to be transferred\nfrom the parent model (assisting source \u2192target)\nto the child model (source \u2192target).\nIn this paper, we propose to pre-order English\nsentences (assisting language sentences) to match\nthe source language word-order and train the par-\nent model on the pre-ordered corpus. Table 1\nshows one of the pre-ordering rules (Ramanathan\net al., 2008) used along with an example sentence\nillustrating the effect of pre-ordering. This will en-\nsure that context of words in the parallel source\nand assisting language sentences are similar, lead-\ning to consistent contextual representations across\nthe source languages.", "start_char_idx": 0, "end_char_idx": 3875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "71297b9c-7a19-42dd-a820-4fad38649288": {"__data__": {"id_": "71297b9c-7a19-42dd-a820-4fad38649288", "embedding": null, "metadata": {"page_label": "2", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "066aa014-8322-42ee-ab37-5b2d37726129", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "35530197eb27b85cc89a701cebbfcd516bd066f0ff4284cf37d7fd9589fc8286", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e9a66b0d-7bd4-48df-8d91-ab0a2eb0297c", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "8b53093b6d061b2aef13e92e6a2a36f851a54157027cdaa753ea0c1549e3634b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1a95dc3-c4dd-4997-a3c8-a4837df5b320", "node_type": "1", "metadata": {}, "hash": "cfc8fd5380a5c3cbc43a69a3fa1a70b2cc5635a2bbbe4cfe3de9fbee48afa47c", "class_name": "RelatedNodeInfo"}}, "text": "This could lead to the at-\ntention and the decoder layers generating different\ntranslations from the same (parallel) sentence in\nthe source or assisting language. This is undesir-\nable as we want the knowledge to be transferred\nfrom the parent model (assisting source \u2192target)\nto the child model (source \u2192target).\nIn this paper, we propose to pre-order English\nsentences (assisting language sentences) to match\nthe source language word-order and train the par-\nent model on the pre-ordered corpus. Table 1\nshows one of the pre-ordering rules (Ramanathan\net al., 2008) used along with an example sentence\nillustrating the effect of pre-ordering. This will en-\nsure that context of words in the parallel source\nand assisting language sentences are similar, lead-\ning to consistent contextual representations across\nthe source languages. Pre-ordering may also be\nbene\ufb01cial for other word order divergence scenar-\nios (e.g., SOV to SVO), but we leave veri\ufb01cation\nof these additional scenarios for future work.\n4 Experimental Setup\nIn this section, we describe the languages exper-\nimented with, datasets used, the network hyper-", "start_char_idx": 3041, "end_char_idx": 4165, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1a95dc3-c4dd-4997-a3c8-a4837df5b320": {"__data__": {"id_": "e1a95dc3-c4dd-4997-a3c8-a4837df5b320", "embedding": null, "metadata": {"page_label": "3", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2735b0a1-d30e-45a0-99be-1e60cd9830f5", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "bf30e38748c7d811ab7a679755f17de9c18cbc7aa18a56b2332b15ed35e31254", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "71297b9c-7a19-42dd-a820-4fad38649288", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "8c4c5ab8bb27e468fb49c9ac155c85135b681426769202b40d1e7a6b1c4d3262", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "55e09196-11fa-412e-91c1-26ad4d6b0dbf", "node_type": "1", "metadata": {}, "hash": "0930da67dfd37ccd01a3aaf048e9eff63106029750c07b16b52f0430f3f155ee", "class_name": "RelatedNodeInfo"}}, "text": "parameters used in our experiments.\nLanguages : We experimented with English \u2192\nHindi translation as the parent task. English is\nthe assisting source language. Bengali, Gujarati,\nMarathi, Malayalam and Tamil are the source lan-\nguages, and translation from these to Hindi consti-\ntute the child tasks. Hindi, Bengali, Gujarati and\nMarathi are Indo-Aryan languages, while Malay-\nalam and Tamil are Dravidian languages. All these\nlanguages have a canonical SOV word order.\nDatasets : For training English-Hindi NMT sys-\ntems, we use the IITB English-Hindi parallel cor-\npus (Kunchukuttan et al., 2018) ( 1.46Msentences\nfrom the training set) and the ILCI English-Hindi\nparallel corpus ( 44.7Ksentences). The ILCI\n(Indian Language Corpora Initiative) multilingual\nparallel corpus (Jha, 2010)1spans multiple Indian\nlanguages from the health and tourism domains.\nWe use the 520-sentence dev-set of the IITB par-\nallel corpus for validation. For each child task, we\nuse2Ksentences from ILCI corpus as test set.\nNetwork : We use OpenNMT-Torch (Klein et al.,\n2018) to train the NMT system. We use the stan-\ndard encoder-attention-decoder architecture (Bah-\ndanau et al., 2015) with input-feeding approach\n(Luong et al., 2015). The encoder has two lay-\ners of bidirectional LSTMs with 500 neurons each\nand the decoder contains two LSTM layers with\n500 neurons each. We use a mini-batch of size\n50and a dropout layer. We begin with an initial\nlearning rate of 1.0and continue training with ex-\nponential decay till the learning rate falls below\n0.001. The English input is initialized with pre-\ntrained fastText embeddings (Grave et al., 2018)2.\nEnglish and Hindi vocabularies consists of\n0.27Mand50Ktokens appearing at least 2and\n5times in the English and Hindi training corpus\nrespectively. For representing English and other\nsource languages into a common space, we trans-\nlate each word in the source language into En-\nglish using a bilingual dictionary (we used Google\nTranslate to get single word translations). In an\nend-to-end solution, it would be ideal to use bilin-\ngual embeddings or obtain word-by-word transla-\ntions viabilingual embeddings (Xie et al., 2018).\nHowever, publicly available bilingual embeddings\nfor English-Indian languages are not good enough\n1The corpus is available on request from http://\ntdil-dc.in/index.php?lang=en\n2https://github.com/facebookresearch/\nfastText/blob/master/docs/crawl-vectors.\nmdLanguage BLEU LeBLEU\nNo\nPre-OrderPre-Ordered No\nPre-OrderPre-Ordered\nHT G HT G\nBengali 6.72 8.83 9.19 37.10 41.50 42.01\nGujarati 9.81 14.34 13.90 43.21 47.36 47.60\nMarathi 8.77 10.18 10.30 40.21 41.49 42.22\nMalayalam 5.73 6.49 6.95 33.27 33.69 35.09\nTamil 4.86 6.04 6.00 29.38 30.77 31.33\nTable 2: Transfer learning results for X-Hindi pair,\ntrained on English -Hindi corpus and sentences from X\nword translated to English.\nLanguage No\nPre-OrderPre-Ordered\nHT G\nBengali 1324 1139 1146\nGujarati 1337 1190 1194\nMarathi 1414 1185 1178\nMalayalam 1251 1067 1059\nTamil 1488 1280 1252\nTable 3: Number of UNK tokens generated by each\nmodel on the test set.\nfor obtaining good-quality, bilingual representa-\ntions (Smith et al., 2017; Jawanpuria et al., 2019)\nand publicly available bilingual dictionaries have\nlimited coverage. The focus of our study is the in-\n\ufb02uence of word-order divergence on Multilingual\nNMT.", "start_char_idx": 0, "end_char_idx": 3327, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55e09196-11fa-412e-91c1-26ad4d6b0dbf": {"__data__": {"id_": "55e09196-11fa-412e-91c1-26ad4d6b0dbf", "embedding": null, "metadata": {"page_label": "3", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2735b0a1-d30e-45a0-99be-1e60cd9830f5", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "bf30e38748c7d811ab7a679755f17de9c18cbc7aa18a56b2332b15ed35e31254", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e1a95dc3-c4dd-4997-a3c8-a4837df5b320", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "d373080d944e4db206dc2633aaa851d35cdedccc8dc1da7a4ed0ef98ede0a774", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e13c8d83-73dc-424b-9924-4b2105aee24d", "node_type": "1", "metadata": {}, "hash": "c62e1d17fc1dcfdd8d557b7f8fa379bda06220a281b3319d3f2bfd8b9be0387b", "class_name": "RelatedNodeInfo"}}, "text": "Language No\nPre-OrderPre-Ordered\nHT G\nBengali 1324 1139 1146\nGujarati 1337 1190 1194\nMarathi 1414 1185 1178\nMalayalam 1251 1067 1059\nTamil 1488 1280 1252\nTable 3: Number of UNK tokens generated by each\nmodel on the test set.\nfor obtaining good-quality, bilingual representa-\ntions (Smith et al., 2017; Jawanpuria et al., 2019)\nand publicly available bilingual dictionaries have\nlimited coverage. The focus of our study is the in-\n\ufb02uence of word-order divergence on Multilingual\nNMT. We do not want bilingual embeddings qual-\nity or bilingual dictionary coverage to in\ufb02uence\nthe experiments, rendering our conclusions unre-\nliable. Hence, we use the above mentioned large-\ncoverage bilingual dictionary.\nPre-ordering : We use CFILT-preorder3for pre-\nreordering English sentences. It contains two pre-\nordering con\ufb01gurations: (1) generic rules (G) that\napply to all Indian languages (Ramanathan et al.,\n2008), and (2) hindi-tuned rules (HT) which im-\nproves generic rules by incorporating improve-\nments found through error analysis of English-\nHindi reordering (Patel et al., 2013). The Hindi-\ntuned rules improve translation for other English\nto Indian language pairs too (Kunchukuttan et al.,\n2014).\n5 Results\nWe experiment with two scenarios: (a) an ex-\ntremely resource scarce scenario with no parallel\ncorpus for child tasks, (b) varying amounts of par-\nallel corpora available for child task.\n3https://github.com/anoopkunchukuttan/\ncfilt_preorder", "start_char_idx": 2845, "end_char_idx": 4296, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e13c8d83-73dc-424b-9924-4b2105aee24d": {"__data__": {"id_": "e13c8d83-73dc-424b-9924-4b2105aee24d", "embedding": null, "metadata": {"page_label": "4", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f45cb23-bfea-4041-bdba-f6e9a790db2a", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "7f94cac26a1948279422622a7b09d200262219d8c73dddf4cd4764299b5c3ff8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55e09196-11fa-412e-91c1-26ad4d6b0dbf", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "4630917f6d485f0ac0098056cd25de462f11827edd970d2c08f72cfb9be47e38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "925effbc-39d1-463b-a41b-378982fe6119", "node_type": "1", "metadata": {}, "hash": "c10271fa669a101375137bc3dd3a539310b90da3ac5c74c55fde299bd94a29a0", "class_name": "RelatedNodeInfo"}}, "text": "English the treatment of migraine is done in two ways\nGujarati\n(Original)\u0aae\u0abe\u0a88\u00e6\u0ac7\u0aa8\u0aa8\u0ac0 \u0ab8\u0abe\u0ab0\u0ab5\u0abe\u0ab0 \u0aac\u0ac7 \u0ab0\u0ac0\u0aa4\u0ac7 \u0a95\u0ab0\u0ac0 \u0ab6\u0a95\u0abe\u0aaf \u0a9b \u0ac7.\nGujarati\n(Word Translate )migraine treatment two the way doing be done is there .\nHindi\n(Reference)\u092e\u093e\u0907\u0917\u0930\u094d\u0947\u0928 \u0915\u093e \u091f\u0930\u094d\u0940\u091f\u092e\u0947\u0902\u091f \u0926\u094b \u0924\u0930\u0939 \u0938\u0947 \u093f\u0915\u092f\u093e \u091c\u093e\u0924\u093e \u0939 \u0948\u0964\n(Word Translate) migraine of treatment two kind from did go is .\nNo Pre-Order <unk> \u0909\u092a\u091a\u093e\u0930 \u0926\u094b \u092a\u0930\u094d\u0915\u093e\u0930 \u0938\u0947 \u093f\u0915\u092f\u093e \u091c\u093e\u0924\u093e \u0939 \u0948\u0964\nupachAra do prakAra se kiyA jAtA hai .\n<unk> treatment two kind from did go is .\nPre-ordered (HT) \u092e\u093e\u0907\u0917\u0930\u094d\u0947\u0928 \u0915\u093e \u0909\u092a\u091a\u093e\u0930 \u0926\u094b \u0924\u0930\u0939 \u0938\u0947 \u093f\u0915\u092f\u093e \u091c\u093e\u0924\u093e \u0939 \u0948\u0964\nmAigrena kA upachAra do prakAra se kiyA jAtA hai.\nmigraine of treatment two kind from did go is .\nTable 5: Sample Hindi translation generated by the Gujarati-Hindi NMT model. Text\nin red indicates phrase dropped by the no pre-ordered model.\n15.1 No Parallel Corpus for Child Task\nThe results from our experiments are presented\nin the Table 2. We report BLEU scores and\nLeBLEU4scores. We observe that both the pre-\nordering models signi\ufb01cantly improve the trans-\nlation quality over the no-preordering models for\nall the language pairs. The results support our hy-\npothesis that word-order divergence can limit the\nbene\ufb01ts of multilingual translation. Thus, reduc-\ning the word order divergence improves transla-\ntion in extremely low-resource scenarios.\nAn analysis of the outputs revealed that pre-\nordering signi\ufb01cantly reduced the number of UNK\ntokens (placeholder for unknown words) in the test\noutput (Table 3). We hypothesize that due to word\norder divergence between English and Indian lan-\nguages, the encoder representation generated is\nnot consistent leading to decoder generating un-\nknown words. However, the pre-ordered models\ngenerate better encoder representations leading to\nlesser number of UNK tokens and better transla-\ntion, which is also re\ufb02ected in the BLEU scores\nand Table ??.\n5.2 Parallel Corpus for Child Task\nWe study the impact of child task parallel cor-\npus on pre-ordering. To this end, we \ufb01ne-\ntune the parent task model with the child task\nparallel corpus. Table 4 shows the results for\nBengali-Hindi ,Gujarati-Hindi ,Marathi-Hindi ,\n4LeBLEU (Levenshtein Edit BLEU) is a variant of BLEU\nthat does a soft-match of reference and output words based\non edit distance, hence it can handle morphological variations\nand cognates (Virpioja and Gr \u00a8onroos, 2015).Malayalam-Hindi , and Tamil-Hindi translation.\nWe observe that pre-ordering is bene\ufb01cial when\nalmost no child task corpus is available. As the\nchild task corpus increases, the model learns the\nword order of the source language; hence, the non\npre-ordering models perform almost as good as or\nsometimes better than the pre-ordered ones. The\nnon pre-ordering model is able to forget the word-\norder of English and learn the word order of Indian\nlanguages. We attribute this behavior of the non\npre-ordered model to the phenomenon of catas-\ntrophic forgetting (McCloskey and Cohen, 1989;\nFrench, 1999) which enables the model to learn\nthe word-order of the source language when suf\ufb01-\ncient child task parallel corpus is available.\nWe also compare the performance of the \ufb01ne-\ntuned model with the model trained only on the\navailable source-target parallel corpus with ran-\ndomly initialized weights (No Transfer Learning).\nTransfer learning, with and without pre-ordering,\nis better compared to training only on the small\nsource-target parallel corpus.", "start_char_idx": 0, "end_char_idx": 3351, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "925effbc-39d1-463b-a41b-378982fe6119": {"__data__": {"id_": "925effbc-39d1-463b-a41b-378982fe6119", "embedding": null, "metadata": {"page_label": "4", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1f45cb23-bfea-4041-bdba-f6e9a790db2a", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "7f94cac26a1948279422622a7b09d200262219d8c73dddf4cd4764299b5c3ff8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e13c8d83-73dc-424b-9924-4b2105aee24d", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "1ed0c1d4920153a4c86ee2e09c5cad92be4bc10ab02defa0a4c1c9d370f10f53", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5c4c75a7-d59e-46b7-b13e-aa538ac70c33", "node_type": "1", "metadata": {}, "hash": "6327cccc828b862d8a35dfa34584980fd8d03eab7c2a0bbe752f09a17edb1f1b", "class_name": "RelatedNodeInfo"}}, "text": "The\nnon pre-ordering model is able to forget the word-\norder of English and learn the word order of Indian\nlanguages. We attribute this behavior of the non\npre-ordered model to the phenomenon of catas-\ntrophic forgetting (McCloskey and Cohen, 1989;\nFrench, 1999) which enables the model to learn\nthe word-order of the source language when suf\ufb01-\ncient child task parallel corpus is available.\nWe also compare the performance of the \ufb01ne-\ntuned model with the model trained only on the\navailable source-target parallel corpus with ran-\ndomly initialized weights (No Transfer Learning).\nTransfer learning, with and without pre-ordering,\nis better compared to training only on the small\nsource-target parallel corpus.\n6 Conclusion\nIn this paper, we show that handling word-order\ndivergence between the source and assisting lan-\nguages is crucial for the success of multilingual\nNMT in an extremely low-resource setting. We\nshow that pre-ordering the assisting language to\nmatch the word order of the source language sig-\nni\ufb01cantly improves translation quality in an ex-\ntremely low-resource setting. If pre-ordering is\nnot possible, \ufb01ne-tuning on a small source-target", "start_char_idx": 2639, "end_char_idx": 3802, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5c4c75a7-d59e-46b7-b13e-aa538ac70c33": {"__data__": {"id_": "5c4c75a7-d59e-46b7-b13e-aa538ac70c33", "embedding": null, "metadata": {"page_label": "5", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cbe35ac3-764d-4dc3-a82d-79683d795e2a", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "fcb58eec8053f924ed3b9275fb5aec3dcfee7b1e124c62a226fe6390601c852e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "925effbc-39d1-463b-a41b-378982fe6119", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "f36dcd6a9b945f84cee18cb35fd6468fa6a7abde9b249aa4e92a316af4303dc2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "797609bd-80f4-432a-9a6a-ef0f3be2b5d4", "node_type": "1", "metadata": {}, "hash": "0b3de31341dd0120f7ff594426da224f49eb71ad36b60a6f42978ede61600c61", "class_name": "RelatedNodeInfo"}}, "text": "Corpus\nSizeNo\nTransfer\nLearningNo\nPre-OrderPre-Ordered\nHT G\nBengali\n- - 6.72 8.83 9.19\n500 0.0 11.40 11.49 11.00\n1000 0.0 13.71 13.84 13.62\n2000 0.0 16.41 16.79 16.01\n3000 0.0 17.44 18.42 \u2020 17.82\n4000 0.0 18.86 19.17 18.66\n5000 0.07 19.58 20.15 \u2020 19.82\n10000 1.87 22.50 22.92 22.53\nGujarati\n- - 9.81 14.34 13.90\n500 0.0 17.27 17.11 17.75\n1000 0.0 21.68 22.12 21.45\n2000 0.0 25.34 25.73 25.63\n3000 0.29 27.48 27.77 27.83\n4000 0.82 29.20 29.49 29.51\n5000 0.0 29.87 31.09 \u202030.58 \u2020\n10000 1.52 33.97 34.25 34.08\nMarathi\n- - 8.77 10.18 10.30\n500 0.0 12.84 13.61 \u2020 12.97\n1000 0.0 15.62 15.75 16.10 \u2020\n2000 0.0 18.59 19.10 18.67\n3000 0.0 20.51 20.76 20.29\n4000 0.24 21.78 21.77 21.39\n5000 0.29 22.21 22.41 22.73 \u2020\n10000 7.90 25.16 25.88 25.36\nMalayalam\n- - 5.73 6.49 6.95\n500 0.0 5.40 5.54 6.17 \u2020\n1000 0.0 7.34 7.36 7.63\n2000 0.0 8.24 8.66 \u2020 8.31\n3000 0.0 9.11 9.30 9.31\n4000 0.0 9.65 9.91 9.87\n5000 0.03 10.26 10.47 10.28\n10000 0.0 11.96 11.85 11.63\nTamil\n- - 4.86 6.04 6.00\n500 0.0 5.49 5.85 \u2020 5.59\n1000 0.0 7.04 7.23 7.44 \u2020\n2000 0.0 8.83 8.84 9.24\n3000 0.0 9.80 10.04 9.56\n4000 0.0 9.69 10.59 \u202010.25 \u2020\n5000 0.03 10.84 10.93 10.69\n10000 0.0 12.71 13.05 12.69\nTable 4: Transfer learning results (BLEU) for In-\ndian Language -Hindi pair, \ufb01ne-tuned with varying\nnumber of Indian Language -Hindi parallel sentences.\n\u2020Indicates statistically signi\ufb01cant difference between\nPre-ordered andNo Pre-ordered results using paired\nbootstrap resampling (Koehn, 2004) for a p-value less\nthan 0.05.No Transfer Learning model refers to\ntraining the model on varying number of Indian Lan-\nguage -Hindi parallel sentences with randomly initial-\nized weights.\nparallel corpus is suf\ufb01cient to overcome word or-\nder divergence. While the current work focusedon Indian languages, we would like to validate the\nhypothesis on a more diverse set of languages. We\nwould also like to explore alternative methods to\naddress word-order divergence which do not re-\nquire expensive parsing of the assisting language\ncorpus.", "start_char_idx": 0, "end_char_idx": 1984, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "797609bd-80f4-432a-9a6a-ef0f3be2b5d4": {"__data__": {"id_": "797609bd-80f4-432a-9a6a-ef0f3be2b5d4", "embedding": null, "metadata": {"page_label": "5", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cbe35ac3-764d-4dc3-a82d-79683d795e2a", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "fcb58eec8053f924ed3b9275fb5aec3dcfee7b1e124c62a226fe6390601c852e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5c4c75a7-d59e-46b7-b13e-aa538ac70c33", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "321491c9529124e935d25cfb0ed365ccef2601940377095d0f670610e221e5c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ae8561aa-972f-4c77-a63a-66579ee07ed2", "node_type": "1", "metadata": {}, "hash": "6a7bf5e223f6e996b3fae31135fdb402d282027ff6062ebc4aeb382ca00303c9", "class_name": "RelatedNodeInfo"}}, "text": "\u2020Indicates statistically signi\ufb01cant difference between\nPre-ordered andNo Pre-ordered results using paired\nbootstrap resampling (Koehn, 2004) for a p-value less\nthan 0.05.No Transfer Learning model refers to\ntraining the model on varying number of Indian Lan-\nguage -Hindi parallel sentences with randomly initial-\nized weights.\nparallel corpus is suf\ufb01cient to overcome word or-\nder divergence. While the current work focusedon Indian languages, we would like to validate the\nhypothesis on a more diverse set of languages. We\nwould also like to explore alternative methods to\naddress word-order divergence which do not re-\nquire expensive parsing of the assisting language\ncorpus. Further, use of pre-ordering to address\nword-order divergence for multilingual training of\nother NLP tasks can be explored.\nAcknowledgements\nWe would like to thank Raj Dabre for his helpful\nsuggestions and comments.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2015. Neural Machine Translation by\nJointly Learning to Align and Translate. In Inter-\nnational Conference on Learning Representations,\nICLR 2015 .\nRajen Chatterjee, Anoop Kunchukuttan, and Pushpak\nBhattacharyya. 2014. Supertag Based Pre-ordering\nin Machine Translation. In Proceedings of the 11th\nInternational Conference on Natural Language Pro-\ncessing, ICON 2014 .\nMichael Collins, Philipp Koehn, and Ivona Kucerova.\n2005. Clause Restructuring for Statistical Machine\nTranslation. In Proceedings of the 43rd Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2005 .\nRaj Dabre, Tetsuji Nakagawa, and Hideto Kazawa.\n2017. An Empirical Study of Language Relatedness\nfor Transfer Learning in Neural Machine Transla-\ntion. In Proceedings of the 31st Paci\ufb01c Asia Confer-\nence on Language, Information and Computation .\nRobert M. French. 1999. Catastrophic forgetting in\nconnectionist networks. Trends in Cognitive Sci-\nences , 3(4):128 \u2013 135.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. 2018. Learning\nWord Vectors for 157 Languages. In Proceedings of\nthe Eleventh International Conference on Language\nResources and Evaluation, LREC 2018 .\nJiatao Gu, Hany Hassan, Jacob Devlin, and Victor O.K.\nLi. 2018. Universal Neural Machine Translation for\nExtremely Low Resource Languages. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), NAACL 2018 .\nPratik Jawanpuria, Arjun Balgovind, Anoop\nKunchukuttan, and Bamdev Mishra. 2019. Learning\nMultilingual Word Embeddings in Latent Metric\nSpace: A Geometric Approach. Transactions of the\nAssociation for Computational Linguistics, TACL .", "start_char_idx": 1305, "end_char_idx": 4023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae8561aa-972f-4c77-a63a-66579ee07ed2": {"__data__": {"id_": "ae8561aa-972f-4c77-a63a-66579ee07ed2", "embedding": null, "metadata": {"page_label": "6", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0fe866c-0595-42c6-932a-a1c0cca353ec", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "1611d467edc1e2d42f9f806f0ed18150f91850277955a71656c8764deffd6afa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "797609bd-80f4-432a-9a6a-ef0f3be2b5d4", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "d7a955b01efedf6b092d57b42d75a5decfbf180c0976d14590c9ca95bb6cc2f1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5463b287-7526-47dd-8d98-de5361182a08", "node_type": "1", "metadata": {}, "hash": "2fe05e5bb391d083a6d46986f5609a892f75a53e75f8dc17cacc8b77546426b3", "class_name": "RelatedNodeInfo"}}, "text": "Girish Nath Jha. 2010. The TDIL program and the In-\ndian Langauge Corpora Intitiative (ILCI). In Pro-\nceedings of the Seventh conference on International\nLanguage Resources and Evaluation, LREC 2010 .\nSha\ufb01q Joty, Preslav Nakov, Llu \u00b4\u0131s M `arquez, and Is-\nraa Jaradat. 2017. Cross-language Learning with\nAdversarial Neural Networks. In Proceedings of\nthe 21st Conference on Computational Natural Lan-\nguage Learning, CoNLL 2017 .\nYuki Kawara, Chenhui Chu, and Yuki Arase. 2018.\nRecursive Neural Network Based Preordering for\nEnglish-to-Japanese Machine Translation. In Pro-\nceedings of ACL 2018, Student Research Workshop .\nGuillaume Klein, Yoon Kim, Yuntian Deng, Vincent\nNguyen, Jean Senellart, and Alexander Rush. 2018.\nOpenNMT: Neural Machine Translation Toolkit. In\nProceedings of the 13th Conference of the Associa-\ntion for Machine Translation in the Americas (Vol-\nume 1: Research Papers) .\nPhilipp Koehn. 2004. Statistical Signi\ufb01cance Tests for\nMachine Translation Evaluation. In Proceedings of\nthe 2004 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2004 .\nAnoop Kunchukuttan, Pratik Mehta, and Pushpak\nBhattacharyya. 2018. The IIT Bombay English-\nHindi Parallel Corpus. In Proceedings of the\nEleventh International Conference on Language Re-\nsources and Evaluation, LREC 2018 .\nAnoop Kunchukuttan, Abhijit Mishra, Rajen Chatter-\njee, Ritesh Shah, and Pushpak Bhattacharyya. 2014.\nShata-Anuvadak: Tackling Multiway Translation of\nIndian Languages. In Proceedings of the Ninth In-\nternational Conference on Language Resources and\nEvaluation, LREC 2014 .\nJason Lee, Kyunghyun Cho, and Thomas Hofmann.\n2017. Fully Character-Level Neural Machine Trans-\nlation without Explicit Segmentation. Transactions\nof the Association for Computational Linguistics,\nTACL , 5.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective Approaches to Attention-\nbased Neural Machine Translation. In Proceedings\nof the 2015 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2015 .\nMichael McCloskey and Neal J. Cohen. 1989. Catas-\ntrophic Interference in Connectionist Networks: The\nSequential Learning Problem. volume 24 of Psy-\nchology of Learning and Motivation , pages 109 \u2013\n165. Academic Press.\nJiri Navratil, Karthik Visweswariah, and Ananthakr-\nishnan Ramanathan. 2012. A Comparison of Syn-\ntactic Reordering Methods for English-German Ma-\nchine Translation. In Proceedings of COLING 2012,\nThe 24th International Conference on Computa-\ntional Linguistics, COLING 2012 .Toan Q. Nguyen and David Chiang. 2017. Trans-\nfer learning across low-resource, related languages\nfor neural machine translation. In Proceedings of\nthe Eighth International Joint Conference on Natu-\nral Language Processing (Volume 2: Short Papers),\nIJCNLP 2017 .\nRaj Nath Patel, Rohit Gupta, Prakash B. Pimpale, and\nSasikumar M. 2013. Reordering rules for English-\nHindi SMT. In Proceedings of the Second Workshop\non Hybrid Approaches to Translation .\nEdoardo Maria Ponti, Roi Reichart, Anna Korhonen,\nand Ivan Vuli \u00b4c. 2018. Isomorphic transfer of syn-\ntactic structures in cross-lingual nlp. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), ACL 2018 .\nAnanthakrishnan Ramanathan, Jayprasad Hegde,\nRitesh M. Shah, Pushpak Bhattacharyya, and\nSasikumar M. 2008. Simple Syntactic and Mor-\nphological Processing Can Help English-Hindi\nStatistical Machine Translation. In Proceedings of\nthe Third International Joint Conference on Natural\nLanguage Processing: Volume-I .", "start_char_idx": 0, "end_char_idx": 3559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5463b287-7526-47dd-8d98-de5361182a08": {"__data__": {"id_": "5463b287-7526-47dd-8d98-de5361182a08", "embedding": null, "metadata": {"page_label": "6", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e0fe866c-0595-42c6-932a-a1c0cca353ec", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "1611d467edc1e2d42f9f806f0ed18150f91850277955a71656c8764deffd6afa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ae8561aa-972f-4c77-a63a-66579ee07ed2", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "73257d521dad3ec9fdd39b58dabae7c08b3d145b15ef9f00a9d71224e4a4455d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9bf05942-d94d-4c6c-8ce8-ba879a98bef7", "node_type": "1", "metadata": {}, "hash": "66c4c37a09159d45d216acd5c36851c0e4e8b9b1dd03bd5050ebd40d839c4d12", "class_name": "RelatedNodeInfo"}}, "text": "2013. Reordering rules for English-\nHindi SMT. In Proceedings of the Second Workshop\non Hybrid Approaches to Translation .\nEdoardo Maria Ponti, Roi Reichart, Anna Korhonen,\nand Ivan Vuli \u00b4c. 2018. Isomorphic transfer of syn-\ntactic structures in cross-lingual nlp. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), ACL 2018 .\nAnanthakrishnan Ramanathan, Jayprasad Hegde,\nRitesh M. Shah, Pushpak Bhattacharyya, and\nSasikumar M. 2008. Simple Syntactic and Mor-\nphological Processing Can Help English-Hindi\nStatistical Machine Translation. In Proceedings of\nthe Third International Joint Conference on Natural\nLanguage Processing: Volume-I .\nSamuel L. Smith, David H. P. Turban, Steven Hamblin,\nand Nils Y . Hammerla. 2017. Aligning the fastText\nvectors of 78 languages.\nSami Virpioja and Stig-Arne Gr \u00a8onroos. 2015.\nLeBLEU: N-gram-based Translation Evaluation\nScore for Morphologically Complex Languages. In\nProceedings of the Tenth Workshop on Statistical\nMachine Translation .\nJ. Xie, Z. Yang, G. Neubig, N. A. Smith, and J. Car-\nbonell. 2018. Neural Cross-Lingual Named Entity\nRecognition with Minimal Resources. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2018 .\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\nKnight. 2016. Transfer Learning for Low-Resource\nNeural Machine Translation. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2016 .", "start_char_idx": 2851, "end_char_idx": 4371, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9bf05942-d94d-4c6c-8ce8-ba879a98bef7": {"__data__": {"id_": "9bf05942-d94d-4c6c-8ce8-ba879a98bef7", "embedding": null, "metadata": {"page_label": "1", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5ddd3602-df76-409c-9975-77e97fb1d52d", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "400a8045cabc885c9e9e4ab7c98da65bea7c5b3bde6c64d34fe0ee6c22251530", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5463b287-7526-47dd-8d98-de5361182a08", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "19412164c515dff922f0f4b4748b21c13d0df3d17e4fee32f0abcf7788713778", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97aa10c0-d7a4-490f-af32-2aa692bbbfaf", "node_type": "1", "metadata": {}, "hash": "d028134725c95c5cd35762254e8e0003051bb6e223ba7ae2fd17b8be176d2f15", "class_name": "RelatedNodeInfo"}}, "text": "How Contextual are Contextualized Word Representations?\nComparing the Geometry of BERT, ELMo, and GPT-2 Embeddings\nKawin Ethayarajh\u2217\nStanford University\nkawin@stanford.edu\nAbstract\nReplacing static word embeddings with con-\ntextualized word representations has yielded\nsigni\ufb01cant improvements on many NLP tasks.\nHowever, just how contextual are the contex-\ntualized representations produced by models\nsuch as ELMo and BERT? Are there in\ufb01nitely\nmany context-speci\ufb01c representations for each\nword, or are words essentially assigned one of\na \ufb01nite number of word-sense representations?\nFor one, we \ufb01nd that the contextualized rep-\nresentations of all words are not isotropic in\nany layer of the contextualizing model. While\nrepresentations of the same word in differ-\nent contexts still have a greater cosine simi-\nlarity than those of two different words, this\nself-similarity is much lower in upper layers.\nThis suggests that upper layers of contextu-\nalizing models produce more context-speci\ufb01c\nrepresentations, much like how upper layers\nof LSTMs produce more task-speci\ufb01c repre-\nsentations. In all layers of ELMo, BERT, and\nGPT-2, on average, less than 5% of the vari-\nance in a word\u2019s contextualized representa-\ntions can be explained by a static embedding\nfor that word, providing some justi\ufb01cation for\nthe success of contextualized representations.\n1 Introduction\nThe application of deep learning methods to NLP\nis made possible by representing words as vec-\ntors in a low-dimensional continuous space. Tradi-\ntionally, these word embeddings were static : each\nword had a single vector, regardless of context\n(Mikolov et al., 2013a; Pennington et al., 2014).\nThis posed several problems, most notably that\nall senses of a polysemous word had to share the\nsame representation. More recent work, namely\ndeep neural language models such as ELMo (Pe-\nters et al., 2018) and BERT (Devlin et al., 2018),\n\u2217Work partly done at the University of Toronto.have successfully created contextualized word rep-\nresentations , word vectors that are sensitive to\nthe context in which they appear. Replacing\nstatic embeddings with contextualized representa-\ntions has yielded signi\ufb01cant improvements on a di-\nverse array of NLP tasks, ranging from question-\nanswering to coreference resolution.\nThe success of contextualized word represen-\ntations suggests that despite being trained with\nonly a language modelling task, they learn highly\ntransferable and task-agnostic properties of lan-\nguage. In fact, linear probing models trained on\nfrozen contextualized representations can predict\nlinguistic properties of words (e.g., part-of-speech\ntags) almost as well as state-of-the-art models (Liu\net al., 2019a; Hewitt and Manning, 2019). Still,\nthese representations remain poorly understood.\nFor one, just how contextual are these contextu-\nalized word representations? Are there in\ufb01nitely\nmany context-speci\ufb01c representations that BERT\nand ELMo can assign to each word, or are words\nessentially assigned one of a \ufb01nite number of\nword-sense representations?\nWe answer this question by studying the geom-\netry of the representation space for each layer of\nELMo, BERT, and GPT-2. Our analysis yields\nsome surprising \ufb01ndings:\n1. In all layers of all three models, the con-\ntextualized word representations of all words\nare not isotropic: they are not uniformly dis-\ntributed with respect to direction. Instead,\nthey are anisotropic , occupying a narrow\ncone in the vector space. The anisotropy in\nGPT-2\u2019s last layer is so extreme that two ran-\ndom words will on average have almost per-\nfect cosine similarity! Given that isotropy\nhas both theoretical and empirical bene\ufb01ts for\nstatic embeddings (Mu et al., 2018), the ex-\ntent of anisotropy in contextualized represen-arXiv:1909.00512v1  [cs.CL]  2 Sep 2019", "start_char_idx": 0, "end_char_idx": 3791, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97aa10c0-d7a4-490f-af32-2aa692bbbfaf": {"__data__": {"id_": "97aa10c0-d7a4-490f-af32-2aa692bbbfaf", "embedding": null, "metadata": {"page_label": "2", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2b2a1ebf-0aa3-4a53-beb3-bf146bd06540", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "fa5280b9dfbfc703f7704f565d3493017f817e007eb1a19b4351d0962575e51d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9bf05942-d94d-4c6c-8ce8-ba879a98bef7", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "400a8045cabc885c9e9e4ab7c98da65bea7c5b3bde6c64d34fe0ee6c22251530", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "11b0eb41-1d7c-4781-9b34-b9de27768f03", "node_type": "1", "metadata": {}, "hash": "2d62e5ac748cd98dfdf70ab4342bb8c1941dbdf8622e0e4dca93943f44c197d5", "class_name": "RelatedNodeInfo"}}, "text": "tations is surprising.\n2. Occurrences of the same word in different\ncontexts have non-identical vector represen-\ntations. Where vector similarity is de\ufb01ned\nas cosine similarity, these representations are\nmore dissimilar to each other in upper lay-\ners. This suggests that, much like how upper\nlayers of LSTMs produce more task-speci\ufb01c\nrepresentations (Liu et al., 2019a), upper lay-\ners of contextualizing models produce more\ncontext-speci\ufb01c representations.\n3. Context-speci\ufb01city manifests very differently\nin ELMo, BERT, and GPT-2. In ELMo,\nrepresentations of words in the same sen-\ntence grow more similar to each other as\ncontext-speci\ufb01city increases in upper layers;\nin BERT, they become more dissimilar to\neach other in upper layers but are still more\nsimilar than randomly sampled words are on\naverage; in GPT-2, however, words in the\nsame sentence are no more similar to each\nother than two randomly chosen words.\n4. After adjusting for the effect of anisotropy,\non average, less than 5% of the variance in a\nword\u2019s contextualized representations can be\nexplained by their \ufb01rst principal component.\nThis holds across all layers of all models.\nThis suggests that contextualized representa-\ntions do not correspond to a \ufb01nite number\nof word-sense representations, and even in\nthe best possible scenario, static embeddings\nwould be a poor replacement for contextual-\nized ones. Still, static embeddings created\nby taking the \ufb01rst principal component of\na word\u2019s contextualized representations out-\nperform GloVe and FastText embeddings on\nmany word vector benchmarks.\nThese insights help justify why the use of contex-\ntualized representations has led to such signi\ufb01cant\nimprovements on many NLP tasks.\n2 Related Work\nStatic Word Embeddings Skip-gram with neg-\native sampling (SGNS) (Mikolov et al., 2013a)\nand GloVe (Pennington et al., 2014) are among\nthe best known models for generating static word\nembeddings. Though they learn embeddings itera-\ntively in practice, it has been proven that in theory,they both implicitly factorize a word-context ma-\ntrix containing a co-occurrence statistic (Levy and\nGoldberg, 2014a,b). Because they create a single\nrepresentation for each word, a notable problem\nwith static word embeddings is that all senses of a\npolysemous word must share a single vector.\nContextualized Word Representations Given\nthe limitations of static word embeddings, recent\nwork has tried to create context-sensitive word\nrepresentations. ELMo (Peters et al., 2018), BERT\n(Devlin et al., 2018), and GPT-2 (Radford et al.,\n2019) are deep neural language models that are\n\ufb01ne-tuned to create models for a wide range of\ndownstream NLP tasks. Their internal representa-\ntions of words are called contextualized word rep-\nresentations because they are a function of the en-\ntire input sentence. The success of this approach\nsuggests that these representations capture highly\ntransferable and task-agnostic properties of lan-\nguage (Liu et al., 2019a).\nELMo creates contextualized representations of\neach token by concatenating the internal states of\na 2-layer biLSTM trained on a bidirectional lan-\nguage modelling task (Peters et al., 2018). In\ncontrast, BERT and GPT-2 are bi-directional and\nuni-directional transformer-based language mod-\nels respectively. Each transformer layer of 12-\nlayer BERT (base, cased) and 12-layer GPT-2 cre-\nates a contextualized representation of each token\nby attending to different parts of the input sentence\n(Devlin et al., 2018; Radford et al., 2019). BERT\n\u2013 and subsequent iterations on BERT (Liu et al.,\n2019b; Yang et al., 2019) \u2013 have achieved state-of-\nthe-art performance on various downstream NLP\ntasks, ranging from question-answering to senti-\nment analysis.\nProbing Tasks Prior analysis of contextualized\nword representations has largely been restricted\nto probing tasks (Tenney et al., 2019; Hewitt and\nManning, 2019).", "start_char_idx": 0, "end_char_idx": 3882, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "11b0eb41-1d7c-4781-9b34-b9de27768f03": {"__data__": {"id_": "11b0eb41-1d7c-4781-9b34-b9de27768f03", "embedding": null, "metadata": {"page_label": "2", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2b2a1ebf-0aa3-4a53-beb3-bf146bd06540", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "fa5280b9dfbfc703f7704f565d3493017f817e007eb1a19b4351d0962575e51d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "97aa10c0-d7a4-490f-af32-2aa692bbbfaf", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "bf56cd5e8e829c9b934f3b9e4973c132f6fe08672d606f6fc2fd80e5428bfcaf", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e2dff16d-89a6-48b8-bd7b-5304655867a2", "node_type": "1", "metadata": {}, "hash": "3278ee073dffc520654abd2321c58d2d42ed2568afcd0640f23dd0df9fea680e", "class_name": "RelatedNodeInfo"}}, "text": "In\ncontrast, BERT and GPT-2 are bi-directional and\nuni-directional transformer-based language mod-\nels respectively. Each transformer layer of 12-\nlayer BERT (base, cased) and 12-layer GPT-2 cre-\nates a contextualized representation of each token\nby attending to different parts of the input sentence\n(Devlin et al., 2018; Radford et al., 2019). BERT\n\u2013 and subsequent iterations on BERT (Liu et al.,\n2019b; Yang et al., 2019) \u2013 have achieved state-of-\nthe-art performance on various downstream NLP\ntasks, ranging from question-answering to senti-\nment analysis.\nProbing Tasks Prior analysis of contextualized\nword representations has largely been restricted\nto probing tasks (Tenney et al., 2019; Hewitt and\nManning, 2019). This involves training linear\nmodels to predict syntactic (e.g., part-of-speech\ntag) and semantic (e.g., word relation) proper-\nties of words. Probing models are based on the\npremise that if a simple linear model can be trained\nto accurately predict a linguistic property, then the\nrepresentations implicitly encode this information\nto begin with. While these analyses have found\nthat contextualized representations encode seman-\ntic and syntactic information, they cannot answer\nhow contextual these representations are, and to", "start_char_idx": 3159, "end_char_idx": 4411, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e2dff16d-89a6-48b8-bd7b-5304655867a2": {"__data__": {"id_": "e2dff16d-89a6-48b8-bd7b-5304655867a2", "embedding": null, "metadata": {"page_label": "3", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e16a3fb-8fc1-4891-a583-21dd8afd8356", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "0c8c0ac50b0d478e54c06b2dcf72d4619b4d6e8f65b5c88571acd4064e90953d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "11b0eb41-1d7c-4781-9b34-b9de27768f03", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "0297bccb6d7d8a6f4ec1174e21e773e4ffd4e1e78837402c712597377db78a67", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a2185e42-ddef-4835-88ae-df806aa54c3e", "node_type": "1", "metadata": {}, "hash": "8d6871c8339a898fc43d95d94485bc0c153e7f78c7456e06d0fbbafa33189f80", "class_name": "RelatedNodeInfo"}}, "text": "what extent they can be replaced with static word\nembeddings, if at all. Our work in this paper is\nthus markedly different from most dissections of\ncontextualized representations. It is more similar\nto Mimno and Thompson (2017), which studied\nthe geometry of static word embedding spaces.\n3 Approach\n3.1 Contextualizing Models\nThe contextualizing models we study in this pa-\nper are ELMo, BERT, and GPT-21. We choose\nthe base cased version of BERT because it is most\ncomparable to GPT-2 with respect to number of\nlayers and dimensionality. The models we work\nwith are all pre-trained on their respective lan-\nguage modelling tasks. Although ELMo, BERT,\nand GPT-2 have 2, 12, and 12 hidden layers re-\nspectively, we also include the input layer of each\ncontextualizing model as its 0thlayer. This is be-\ncause the 0thlayer is not contextualized, making\nit a useful baseline against which to compare the\ncontextualization done by subsequent layers.\n3.2 Data\nTo analyze contextualized word representations,\nwe need input sentences to feed into our pre-\ntrained models. Our input data come from the\nSemEval Semantic Textual Similarity tasks from\nyears 2012 - 2016 (Agirre et al., 2012, 2013, 2014,\n2015). We use these datasets because they contain\nsentences in which the same words appear in dif-\nferent contexts. For example, the word \u2018dog\u2019 ap-\npears in \u201cA panda dog is running on the road. \u201d\nand\u201cA dog is trying to get bacon off his back. \u201d\nIf a model generated the same representation for\n\u2018dog\u2019 in both these sentences, we could infer that\nthere was no contextualization; conversely, if the\ntwo representations were different, we could infer\nthat they were contextualized to some extent. Us-\ning these datasets, we map words to the list of sen-\ntences they appear in and their index within these\nsentences. We do not consider words that appear\nin less than 5 unique contexts in our analysis.\n3.3 Measures of Contextuality\nWe measure how contextual a word representation\nis using three different metrics: self-similarity ,\nintra-sentence similarity , and maximum explain-\nable variance .\n1We use the pretrained models provided in an earlier ver-\nsion of the PyTorch-Transformers library.De\ufb01nition 1 Letwbe a word that appears in\nsentences{s1,...,sn}at indices{i1,...,in}respec-\ntively, such that w=s1[i1] =...=sn[in]. Let f\u2113(s,i)\nbe a function that maps s[i]to its representation in\nlayer\u2113of model f. The self similarity ofwin layer\n\u2113is\nSelfSim\u2113(w) =1\nn2\u2212n\u2211\nj\u2211\nk\u0338=jcos(f\u2113(sj,ij),f\u2113(sk,ik))\n(1)\nwhere cos denotes the cosine similarity. In other\nwords, the self-similarity of a word win layer \u2113is\nthe average cosine similarity between its contextu-\nalized representations across its nunique contexts.\nIf layer \u2113does not contextualize the representa-\ntions at all, then SelfSim\u2113(w) =1 (i.e., the repre-\nsentations are identical across all contexts). The\nmore contextualized the representations are for w,\nthe lower we would expect its self-similarity to be.\nDe\ufb01nition 2 Letsbe a sentence that is a se-\nquence\u27e8w1,...,wn\u27e9ofnwords. Let f\u2113(s,i)be a\nfunction that maps s[i]to its representation in layer\n\u2113of model f. The intra-sentence similarity ofsin\nlayer \u2113is\nIntraSim \u2113(s) =1\nn\u2211\nicos(\u20d7s\u2113,f\u2113(s,i))\nwhere \u20d7s\u2113=1\nn\u2211\nif\u2113(s,i)(2)\nPut more simply, the intra-sentence similarity of a\nsentence is the average cosine similarity between\nits word representations and the sentence vector,\nwhich is just the mean of those word vectors. This\nmeasure captures how context-speci\ufb01city mani-\nfests in the vector space.", "start_char_idx": 0, "end_char_idx": 3493, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a2185e42-ddef-4835-88ae-df806aa54c3e": {"__data__": {"id_": "a2185e42-ddef-4835-88ae-df806aa54c3e", "embedding": null, "metadata": {"page_label": "3", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e16a3fb-8fc1-4891-a583-21dd8afd8356", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "0c8c0ac50b0d478e54c06b2dcf72d4619b4d6e8f65b5c88571acd4064e90953d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e2dff16d-89a6-48b8-bd7b-5304655867a2", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "25370ba48c145184f18650160342261ee60d009bb07ded8be9d12666122395f9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f1b3d76a-df51-4569-b5b4-e3857b11c872", "node_type": "1", "metadata": {}, "hash": "f46d70408f83178e0f9b66abb81ccb428d376801f25946b24f051665ddd00112", "class_name": "RelatedNodeInfo"}}, "text": "De\ufb01nition 2 Letsbe a sentence that is a se-\nquence\u27e8w1,...,wn\u27e9ofnwords. Let f\u2113(s,i)be a\nfunction that maps s[i]to its representation in layer\n\u2113of model f. The intra-sentence similarity ofsin\nlayer \u2113is\nIntraSim \u2113(s) =1\nn\u2211\nicos(\u20d7s\u2113,f\u2113(s,i))\nwhere \u20d7s\u2113=1\nn\u2211\nif\u2113(s,i)(2)\nPut more simply, the intra-sentence similarity of a\nsentence is the average cosine similarity between\nits word representations and the sentence vector,\nwhich is just the mean of those word vectors. This\nmeasure captures how context-speci\ufb01city mani-\nfests in the vector space. For example, if both\nIntraSim \u2113(s)andSelfSim\u2113(w)are low\u2200w\u2208s, then\nthe model contextualizes words in that layer by\ngiving each one a context-speci\ufb01c representation\nthat is still distinct from all other word represen-\ntations in the sentence. If IntraSim \u2113(s)is high but\nSelfSim\u2113(w)is low, this suggests a less nuanced\ncontextualization, where words in a sentence are\ncontextualized simply by making their representa-\ntions converge in vector space.\nDe\ufb01nition 3 Letwbe a word that appears in\nsentences{s1,...,sn}at indices{i1,...,in}respec-\ntively, such that w=s1[i1] =...=sn[in]. Let f\u2113(s,i)\nbe a function that maps s[i]to its representation in\nlayer \u2113of model f. Where [f\u2113(s1,i1)...f\u2113(sn,in)]\nis the occurrence matrix ofwand\u03c31...\u03c3mare the", "start_char_idx": 2953, "end_char_idx": 4232, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1b3d76a-df51-4569-b5b4-e3857b11c872": {"__data__": {"id_": "f1b3d76a-df51-4569-b5b4-e3857b11c872", "embedding": null, "metadata": {"page_label": "4", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "87d2e3c6-4452-4b9e-bbfa-65a394905e4e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "199251803da515f3a7d840fd84420ad6184390a7136c53d0045f8ddb9e9b0005", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a2185e42-ddef-4835-88ae-df806aa54c3e", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "4f7a4adf60881fc647517be37931e298e765b110e7ba86a5daa8dfc5f2d8d5c1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1b2fb2d3-a1ac-4536-97b7-3a5399acc488", "node_type": "1", "metadata": {}, "hash": "4f6cee2bd019d36f303b950a538629e19d1cba8682a44d98c7fb0db5a1542702", "class_name": "RelatedNodeInfo"}}, "text": "\ufb01rstmsingular values of this matrix, the maximum\nexplainable variance is\nMEV \u2113(w) =\u03c32\n1\n\u2211i\u03c32\ni(3)\nMEV \u2113(w)is the proportion of variance in w\u2019s con-\ntextualized representations for a given layer that\ncan be explained by their \ufb01rst principal compo-\nnent. It gives us an upper bound on how well a\nstatic embedding could replace a word\u2019s contex-\ntualized representations. The closer MEV \u2113(w)is\nto 0, the poorer a replacement a static embedding\nwould be; if MEV \u2113(w) =1, then a static embed-\nding would be a perfect replacement for the con-\ntextualized representations.\n3.4 Adjusting for Anisotropy\nIt is important to consider isotropy (or the lack\nthereof) when discussing contextuality. For ex-\nample, if word vectors were perfectly isotropic\n(i.e., directionally uniform), then SelfSim\u2113(w) =\n0.95 would suggest that w\u2019s representations were\npoorly contextualized. However, consider the sce-\nnario where word vectors are so anisotropic that\nany two words have on average a cosine similar-\nity of 0.99. Then SelfSim\u2113(w) =0.95 would actu-\nally suggest the opposite \u2013 that w\u2019s representations\nwere well contextualized. This is because repre-\nsentations of win different contexts would on av-\nerage be more dissimilar to each other than two\nrandomly chosen words.\nTo adjust for the effect of anisotropy, we use\nthree anisotropic baselines , one for each of our\ncontextuality measures. For self-similarity and\nintra-sentence similarity, the baseline is the aver-\nage cosine similarity between the representations\nof uniformly randomly sampled words from dif-\nferent contexts. The more anisotropic the word\nrepresentations are in a given layer, the closer this\nbaseline is to 1. For maximum explainable vari-\nance (MEV), the baseline is the proportion of vari-\nance in uniformly randomly sampled word repre-\nsentations that is explained by their \ufb01rst principal\ncomponent. The more anisotropic the representa-\ntions in a given layer, the closer this baseline is\nto 1: even for a random assortment of words, the\nprincipal component would be able to explain a\nlarge proportion of the variance.\nSince contextuality measures are calculated for\neach layer of a contextualizing model, we cal-\nculate separate baselines for each layer as well.We then subtract from each measure its respective\nbaseline to get the anisotropy-adjusted contexual-\nity measure . For example, the anisotropy-adjusted\nself-similarity is\nBaseline (f\u2113) =Ex,y\u223cU(O)[cos(f\u2113(x),f\u2113(y))]\nSelfSim\u2217\n\u2113(w) =SelfSim\u2113(w)\u2212Baseline (f\u2113)(4)\nwhereOis the set of all word occurrences and\nf\u2113(\u00b7)maps a word occurrence to its representation\nin layer \u2113of model f. Unless otherwise stated, ref-\nerences to contextuality measures in the rest of the\npaper refer to the anisotropy-adjusted measures,\nwhere both the raw measure and baseline are esti-\nmated with 1K uniformly randomly sampled word\nrepresentations.\n4 Findings\n4.1 (An)Isotropy\nContextualized representations are anisotropic\nin all non-input layers. If word representations\nfrom a particular layer were isotropic (i.e., direc-\ntionally uniform), then the average cosine similar-\nity between uniformly randomly sampled words\nwould be 0 (Arora et al., 2017). The closer this\naverage is to 1, the more anisotropic the represen-\ntations. The geometric interpretation of anisotropy\nis that the word representations all occupy a nar-\nrow cone in the vector space rather than being uni-\nform in all directions; the greater the anisotropy,\nthe narrower this cone (Mimno and Thompson,\n2017). As seen in Figure 1, this implies that in\nalmost all layers of BERT, ELMo and GPT-2, the\nrepresentations of all words occupy a narrow cone\nin the vector space. The only exception is ELMo\u2019s\ninput layer, which produces static character-level\nembeddings without using contextual or even po-\nsitional information (Peters et al., 2018).", "start_char_idx": 0, "end_char_idx": 3810, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1b2fb2d3-a1ac-4536-97b7-3a5399acc488": {"__data__": {"id_": "1b2fb2d3-a1ac-4536-97b7-3a5399acc488", "embedding": null, "metadata": {"page_label": "4", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "87d2e3c6-4452-4b9e-bbfa-65a394905e4e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "199251803da515f3a7d840fd84420ad6184390a7136c53d0045f8ddb9e9b0005", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f1b3d76a-df51-4569-b5b4-e3857b11c872", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "b91615926f8f95eb53cc6c44acc0c3771ac29a01ab767d62c52fa6e2c04eb241", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7a52b1d1-909c-4ead-a4d9-73ebfbd581b9", "node_type": "1", "metadata": {}, "hash": "8922fb41fe594a2af76170ff9e348170b442e95128ea2dc754a0bea95680edf3", "class_name": "RelatedNodeInfo"}}, "text": "The closer this\naverage is to 1, the more anisotropic the represen-\ntations. The geometric interpretation of anisotropy\nis that the word representations all occupy a nar-\nrow cone in the vector space rather than being uni-\nform in all directions; the greater the anisotropy,\nthe narrower this cone (Mimno and Thompson,\n2017). As seen in Figure 1, this implies that in\nalmost all layers of BERT, ELMo and GPT-2, the\nrepresentations of all words occupy a narrow cone\nin the vector space. The only exception is ELMo\u2019s\ninput layer, which produces static character-level\nembeddings without using contextual or even po-\nsitional information (Peters et al., 2018). It should\nbe noted that not all static embeddings are neces-\nsarily isotropic, however; Mimno and Thompson\n(2017) found that skipgram embeddings, which\nare also static, are not isotropic.\nContextualized representations are generally\nmore anisotropic in higher layers. As seen in\nFigure 1, for GPT-2, the average cosine similarity\nbetween uniformly randomly words is roughly 0.6\nin layers 2 through 8 but increases exponentially\nfrom layers 8 through 12. In fact, word represen-\ntations in GPT-2\u2019s last layer are so anisotropic that\nany two words have on average an almost perfect\ncosine similarity! This pattern holds for BERT and", "start_char_idx": 3153, "end_char_idx": 4441, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7a52b1d1-909c-4ead-a4d9-73ebfbd581b9": {"__data__": {"id_": "7a52b1d1-909c-4ead-a4d9-73ebfbd581b9", "embedding": null, "metadata": {"page_label": "5", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "05017b20-5666-4cd9-9a86-29ca466e2e33", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "0437bc2cbc0228c7b0b11be06c8b7a13da2c059d9dd11a0680a2f3632577782d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1b2fb2d3-a1ac-4536-97b7-3a5399acc488", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "984969c18c93cfbebf5e78c6eb39072e3d112046582d98bfd03359e627ff4aad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "478eee57-a17d-4ed1-b12b-eec7ee47dcc0", "node_type": "1", "metadata": {}, "hash": "ea6f273c325f364c67299f30b15aedbbf73ced03b40c079a1dbe8537813bb501", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1: In almost all layers of BERT, ELMo, and GPT-2, the word representations are anisotropic (i.e., not\ndirectionally uniform): the average cosine similarity between uniformly randomly sampled words is non-zero.\nThe one exception is ELMo\u2019s input layer; this is not surprising given that it generates character-level embeddings\nwithout using context. Representations in higher layers are generally more anisotropic than those in lower ones.\nELMo as well, though there are exceptions: for ex-\nample, the anisotropy in BERT\u2019s penultimate layer\nis much higher than in its \ufb01nal layer.\nIsotropy has both theoretical and empirical ben-\ne\ufb01ts for static word embeddings. In theory, it\nallows for stronger \u201cself-normalization\u201d during\ntraining (Arora et al., 2017), and in practice, sub-\ntracting the mean vector from static embeddings\nleads to improvements on several downstream\nNLP tasks (Mu et al., 2018). Thus the extreme\ndegree of anisotropy seen in contextualized word\nrepresentations \u2013 particularly in higher layers \u2013\nis surprising. As seen in Figure 1, for all three\nmodels, the contextualized hidden layer represen-\ntations are almost all more anisotropic than the in-\nput layer representations, which do not incorpo-\nrate context. This suggests that high anisotropy is\ninherent to, or least a by-product of, the process of\ncontextualization.\n4.2 Context-Speci\ufb01city\nContextualized word representations are more\ncontext-speci\ufb01c in higher layers. Recall from\nDe\ufb01nition 1 that the self-similarity of a word, in\na given layer of a given model, is the average co-\nsine similarity between its representations in dif-\nferent contexts, adjusted for anisotropy. If the\nself-similarity is 1, then the representations are\nnot context-speci\ufb01c at all; if the self-similarity is\n0, that the representations are maximally context-\nspeci\ufb01c. In Figure 2, we plot the average self-\nsimilarity of uniformly randomly sampled wordsin each layer of BERT, ELMo, and GPT-2. For\nexample, the self-similarity is 1.0 in ELMo\u2019s in-\nput layer because representations in that layer are\nstatic character-level embeddings.\nIn all three models, the higher the layer, the\nlower the self-similarity is on average. In other\nwords, the higher the layer, the more context-\nspeci\ufb01c the contextualized representations. This\n\ufb01nding makes intuitive sense. In image classi\ufb01ca-\ntion models, lower layers recognize more generic\nfeatures such as edges while upper layers recog-\nnize more class-speci\ufb01c features (Yosinski et al.,\n2014). Similarly, upper layers of LSTMs trained\non NLP tasks learn more task-speci\ufb01c represen-\ntations (Liu et al., 2019a). Therefore, it fol-\nlows that upper layers of neural language mod-\nels learn more context-speci\ufb01c representations, so\nas to predict the next word for a given context\nmore accurately. Of all three models, representa-\ntions in GPT-2 are the most context-speci\ufb01c, with\nthose in GPT-2\u2019s last layer being almost maxi-\nmally context-speci\ufb01c.\nStopwords (e.g., \u2018the\u2019, \u2018of\u2019, \u2018to\u2019 ) have among the\nmost context-speci\ufb01c representations. Across\nall layers, stopwords have among the lowest self-\nsimilarity of all words, implying that their con-\ntextualized representations are among the most\ncontext-speci\ufb01c. For example, the words with the\nlowest average self-similarity across ELMo\u2019s lay-\ners are \u2018and\u2019, \u2018of\u2019, \u2018\u2019s\u2019, \u2018the\u2019 , and \u2018to\u2019. This is rel-\natively surprising, given that these words are not\npolysemous. This \ufb01nding suggests that the variety", "start_char_idx": 0, "end_char_idx": 3446, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "478eee57-a17d-4ed1-b12b-eec7ee47dcc0": {"__data__": {"id_": "478eee57-a17d-4ed1-b12b-eec7ee47dcc0", "embedding": null, "metadata": {"page_label": "6", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "04167c03-1197-4caa-993f-1db41fed7f7d", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "4088d62b03013003c53e04268c8cc9f22af3d47c28b1c58eeafa821439f15dbc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7a52b1d1-909c-4ead-a4d9-73ebfbd581b9", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "0437bc2cbc0228c7b0b11be06c8b7a13da2c059d9dd11a0680a2f3632577782d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25b2116a-4dde-4a3e-8633-7d486d102a01", "node_type": "1", "metadata": {}, "hash": "71f82097269a33ab139d7f99806f2c199c1526617f3aef61be55c2b6f23159eb", "class_name": "RelatedNodeInfo"}}, "text": "Figure 2: The average cosine similarity between representations of the same word in different contexts is called\nthe word\u2019s self-similarity (see De\ufb01nition 1). Above, we plot the average self-similarity of uniformly randomly\nsampled words after adjusting for anisotropy (see section 3.4). In all three models, the higher the layer, the lower\nthe self-similarity, suggesting that contextualized word representations are more context-speci\ufb01c in higher layers.\nof contexts a word appears in, rather than its inher-\nent polysemy, is what drives variation in its con-\ntextualized representations. This answers one of\nthe questions we posed in the introduction: ELMo,\nBERT, and GPT-2 are not simply assigning one of\na \ufb01nite number of word-sense representations to\neach word; otherwise, there would not be so much\nvariation in the representations of words with so\nfew word senses.\nContext-speci\ufb01city manifests very differently in\nELMo, BERT, and GPT-2. As noted earlier,\ncontextualized representations are more context-\nspeci\ufb01c in upper layers of ELMo, BERT, and GPT-\n2. However, how does this increased context-\nspeci\ufb01city manifest in the vector space? Do word\nrepresentations in the same sentence converge to a\nsingle point, or do they remain distinct from one\nanother while still being distinct from their repre-\nsentations in other contexts? To answer this ques-\ntion, we can measure a sentence\u2019s intra-sentence\nsimilarity. Recall from De\ufb01nition 2 that the intra-\nsentence similarity of a sentence, in a given layer\nof a given model, is the average cosine similarity\nbetween each of its word representations and their\nmean, adjusted for anisotropy. In Figure 3, we plot\nthe average intra-sentence similarity of 500 uni-\nformly randomly sampled sentences.\nIn ELMo, words in the same sentence are more\nsimilar to one another in upper layers. As\nword representations in a sentence become more\ncontext-speci\ufb01c in upper layers, the intra-sentencesimilarity also rises. This suggests that, in prac-\ntice, ELMo ends up extending the intuition behind\nFirth\u2019s (1957) distributional hypothesis to the sen-\ntence level: that because words in the same sen-\ntence share the same context, their contextualized\nrepresentations should also be similar.\nIn BERT, words in the same sentence are more\ndissimilar to one another in upper layers. As\nword representations in a sentence become more\ncontext-speci\ufb01c in upper layers, they drift away\nfrom one another, although there are exceptions\n(see layer 12 in Figure 3). However, in all lay-\ners, the average similarity between words in the\nsame sentence is still greater than the average sim-\nilarity between randomly chosen words (i.e., the\nanisotropy baseline). This suggests a more nu-\nanced contextualization than in ELMo, with BERT\nrecognizing that although the surrounding sen-\ntence informs a word\u2019s meaning, two words in the\nsame sentence do not necessarily have a similar\nmeaning because they share the same context.\nIn GPT-2, word representations in the same\nsentence are no more similar to each other than\nrandomly sampled words. On average, the un-\nadjusted intra-sentence similarity is roughly the\nsame as the anisotropic baseline, so as seen in Fig-\nure 3, the anisotropy-adjusted intra-sentence simi-\nlarity is close to 0 in most layers of GPT-2. In fact,\nthe intra-sentence similarity is highest in the input\nlayer, which does not contextualize words at all.\nThis is in contrast to ELMo and BERT, where the", "start_char_idx": 0, "end_char_idx": 3447, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25b2116a-4dde-4a3e-8633-7d486d102a01": {"__data__": {"id_": "25b2116a-4dde-4a3e-8633-7d486d102a01", "embedding": null, "metadata": {"page_label": "7", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "558aabdc-ac0a-4131-87fb-e9351c01caca", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "6515c2a8074f771ccf2ac3a92e22883c54a89c5d05f7c1498d216a648dacfdc9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "478eee57-a17d-4ed1-b12b-eec7ee47dcc0", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "4088d62b03013003c53e04268c8cc9f22af3d47c28b1c58eeafa821439f15dbc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "49597c31-bc55-41dc-b18a-946066c88ae2", "node_type": "1", "metadata": {}, "hash": "3a8e01e9c1237f6bbaa842980533c24be7527b3e2152faaaa70f5bd31959a0fc", "class_name": "RelatedNodeInfo"}}, "text": "Figure 3: The intra-sentence similarity is the average cosine similarity between each word representation in a\nsentence and their mean (see De\ufb01nition 2). Above, we plot the average intra-sentence similarity of uniformly\nrandomly sampled sentences, adjusted for anisotropy. This statistic re\ufb02ects how context-speci\ufb01city manifests in\nthe representation space, and as seen above, it manifests very differently for ELMo, BERT, and GPT-2.\naverage intra-sentence similarity is above 0.20 for\nall but one layer.\nAs noted earlier when discussing BERT, this be-\nhavior still makes intuitive sense: two words in the\nsame sentence do not necessarily have a similar\nmeaning simply because they share the same con-\ntext. The success of GPT-2 suggests that unlike\nanisotropy, which accompanies context-speci\ufb01city\nin all three models, a high intra-sentence similar-\nity is not inherent to contextualization. Words in\nthe same sentence can have highly contextualized\nrepresentations without those representations be-\ning any more similar to each other than two ran-\ndom word representations. It is unclear, however,\nwhether these differences in intra-sentence simi-\nlarity can be traced back to differences in model\narchitecture; we leave this question as future work.\n4.3 Static vs. Contextualized\nOn average, less than 5% of the variance in\na word\u2019s contextualized representations can be\nexplained by a static embedding. Recall from\nDe\ufb01nition 3 that the maximum explainable vari-\nance (MEV) of a word, for a given layer of a given\nmodel, is the proportion of variance in its con-\ntextualized representations that can be explained\nby their \ufb01rst principal component. This gives us\nan upper bound on how well a static embedding\ncould replace a word\u2019s contextualized representa-\ntions. Because contextualized representations are\nanisotropic (see section 4.1), much of the varia-\ntion across all words can be explained by a sin-gle vector. We adjust for anisotropy by calculating\nthe proportion of variance explained by the \ufb01rst\nprincipal component of uniformly randomly sam-\npled word representations and subtracting this pro-\nportion from the raw MEV . In Figure 4, we plot\nthe average anisotropy-adjusted MEV across uni-\nformly randomly sampled words.\nIn no layer of ELMo, BERT, or GPT-2 can more\nthan 5% of the variance in a word\u2019s contextual-\nized representations be explained by a static em-\nbedding, on average. Though not visible in Figure\n4, the raw MEV of many words is actually below\nthe anisotropy baseline: i.e., a greater proportion\nof the variance across all words can be explained\nby a single vector than can the variance across\nall representations of a single word. Note that\nthe 5% threshold represents the best-case scenario,\nand there is no theoretical guarantee that a word\nvector obtained using GloVe, for example, would\nbe similar to the static embedding that maximizes\nMEV . This suggests that contextualizing models\nare not simply assigning one of a \ufb01nite number of\nword-sense representations to each word \u2013 other-\nwise, the proportion of variance explained would\nbe much higher. Even the average raw MEV is be-\nlow 5% for all layers of ELMo and BERT; only\nfor GPT-2 is the raw MEV non-negligible, being\naround 30% on average for layers 2 to 11 due to\nextremely high anisotropy.\nPrincipal components of contextualized repre-\nsentations in lower layers outperform GloVe\nand FastText on many benchmarks. As noted", "start_char_idx": 0, "end_char_idx": 3416, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "49597c31-bc55-41dc-b18a-946066c88ae2": {"__data__": {"id_": "49597c31-bc55-41dc-b18a-946066c88ae2", "embedding": null, "metadata": {"page_label": "8", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b0bb577b-65bf-4e62-a71c-0720fe801dcf", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "62d45f1e5ba414b01574ee15c5ceb8037112d221909bb6fbc2bad96b5b1e895d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "25b2116a-4dde-4a3e-8633-7d486d102a01", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "6515c2a8074f771ccf2ac3a92e22883c54a89c5d05f7c1498d216a648dacfdc9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da73fe1c-4c99-4a8c-ba60-a90e371e014c", "node_type": "1", "metadata": {}, "hash": "46a3e4e63bfdc5cdc6bcb629d2ba916e4cb9b617046d31a705f9f511c894ba8b", "class_name": "RelatedNodeInfo"}}, "text": "Figure 4: The maximum explainable variance (MEV) of a word is the proportion of variance in its contextualized\nrepresentations that can be explained by their \ufb01rst principal component (see De\ufb01nition 3). Above, we plot the\naverage MEV of uniformly randomly sampled words after adjusting for anisotropy. In no layer of any model can\nmore than 5% of the variance in a word\u2019s contextualized representations be explained by a static embedding.\nStatic Embedding SimLex999 MEN WS353 RW Google MSR SemEval2012(2) BLESS AP\nGloVe 0.194 0.216 0.339 0.127 0.189 0.312 0.097 0.390 0.308\nFastText 0.239 0.239 0.432 0.176 0.203 0.289 0.104 0.375 0.291\nELMo, Layer 1 0.276 0.167 0.317 0.148 0.170 0.326 0.114 0.410 0.308\nELMo, Layer 2 0.215 0.151 0.272 0.133 0.130 0.268 0.132 0.395 0.318\nBERT, Layer 1 0.315 0.200 0.394 0.208 0.236 0.389 0.166 0.365 0.321\nBERT, Layer 2 0.320 0.166 0.383 0.188 0.230 0.385 0.149 0.365 0.321\nBERT, Layer 11 0.221 0.076 0.319 0.135 0.175 0.290 0.149 0.370 0.289\nBERT, Layer 12 0.233 0.082 0.325 0.144 0.184 0.307 0.144 0.360 0.294\nGPT-2, Layer 1 0.174 0.012 0.176 0.183 0.052 0.081 0.033 0.220 0.184\nGPT-2, Layer 2 0.135 0.036 0.171 0.180 0.045 0.062 0.021 0.245 0.184\nGPT-2, Layer 11 0.126 0.034 0.165 0.182 0.031 0.038 0.045 0.270 0.189\nGPT-2, Layer 12 0.140 -0.009 0.113 0.163 0.020 0.021 0.014 0.225 0.172\nTable 1: The performance of various static embeddings on word embedding benchmark tasks. The best result for\neach task is in bold. For the contextualizing models (ELMo, BERT, GPT-2), we use the \ufb01rst principal component\nof a word\u2019s contextualized representations in a given layer as its static embedding. The static embeddings created\nusing ELMo and BERT\u2019s contextualized representations often outperform GloVe and FastText vectors.\nearlier, we can create static embeddings for each\nword by taking the \ufb01rst principal component (PC)\nof its contextualized representations in a given\nlayer. In Table 1, we plot the performance of\nthese PC static embeddings on several benchmark\ntasks2. These tasks cover semantic similarity,\nanalogy solving, and concept categorization: Sim-\nLex999 (Hill et al., 2015), MEN (Bruni et al.,\n2014), WS353 (Finkelstein et al., 2002), RW (Lu-\nong et al., 2013), SemEval-2012 (Jurgens et al.,\n2012), Google analogy solving (Mikolov et al.,\n2013a) MSR analogy solving (Mikolov et al.,\n2013b), BLESS (Baroni and Lenci, 2011) and AP\n(Almuhareb and Poesio, 2004). We leave out lay-\ners 3 - 10 in Table 1 because their performance is\n2The Word Embeddings Benchmarks package was used\nfor evaluation.between those of Layers 2 and 11.", "start_char_idx": 0, "end_char_idx": 2574, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da73fe1c-4c99-4a8c-ba60-a90e371e014c": {"__data__": {"id_": "da73fe1c-4c99-4a8c-ba60-a90e371e014c", "embedding": null, "metadata": {"page_label": "8", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b0bb577b-65bf-4e62-a71c-0720fe801dcf", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "62d45f1e5ba414b01574ee15c5ceb8037112d221909bb6fbc2bad96b5b1e895d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "49597c31-bc55-41dc-b18a-946066c88ae2", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "e8caa2afcf337fe5df5ac7d35cd7c6a20381cc6a14f4bec98c1f46dbb7591587", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d14f2c6f-635b-4e07-b44a-bfb346c93977", "node_type": "1", "metadata": {}, "hash": "89059349c4d6ba30634010b690aa8813744906eda9d782a04840636738be111b", "class_name": "RelatedNodeInfo"}}, "text": "These tasks cover semantic similarity,\nanalogy solving, and concept categorization: Sim-\nLex999 (Hill et al., 2015), MEN (Bruni et al.,\n2014), WS353 (Finkelstein et al., 2002), RW (Lu-\nong et al., 2013), SemEval-2012 (Jurgens et al.,\n2012), Google analogy solving (Mikolov et al.,\n2013a) MSR analogy solving (Mikolov et al.,\n2013b), BLESS (Baroni and Lenci, 2011) and AP\n(Almuhareb and Poesio, 2004). We leave out lay-\ners 3 - 10 in Table 1 because their performance is\n2The Word Embeddings Benchmarks package was used\nfor evaluation.between those of Layers 2 and 11.\nThe best-performing PC static embeddings be-\nlong to the \ufb01rst layer of BERT, although those\nfrom the other layers of BERT and ELMo also out-\nperform GloVe and FastText on most benchmarks.\nFor all three contextualizing models, PC static em-\nbeddings created from lower layers are more effec-\ntive those created from upper layers. Those cre-\nated using GPT-2 also perform markedly worse\nthan their counterparts from ELMo and BERT.\nGiven that upper layers are much more context-\nspeci\ufb01c than lower layers, and given that GPT-\n2\u2019s representations are more context-speci\ufb01c than\nELMo and BERT\u2019s (see Figure 2), this suggests\nthat the PCs of highly context-speci\ufb01c representa-\ntions are less effective on traditional benchmarks.\nThose derived from less context-speci\ufb01c represen-", "start_char_idx": 2007, "end_char_idx": 3346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d14f2c6f-635b-4e07-b44a-bfb346c93977": {"__data__": {"id_": "d14f2c6f-635b-4e07-b44a-bfb346c93977", "embedding": null, "metadata": {"page_label": "9", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0521f782-b619-4268-92e9-cde7d75f6fc2", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "d99fe071f00aff07bf78d292e9a574a35c22d069f52f6f72b42f9ee0742d974c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da73fe1c-4c99-4a8c-ba60-a90e371e014c", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "b8f29d0b1537b732eddff5582e6540a992699f5b9dd69f08bf43767ff33f5164", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc09df64-ca00-4506-ac63-d172b4445dd8", "node_type": "1", "metadata": {}, "hash": "ca7f7fde7f0b40b90989e043cf0667685688dda63c41d445ec39a278d1f4dab1", "class_name": "RelatedNodeInfo"}}, "text": "tations, such as those from Layer 1 of BERT, are\nmuch more effective.\n5 Future Work\nOur \ufb01ndings offer some new directions for future\nwork. For one, as noted earlier in the paper, Mu\net al. (2018) found that making static embeddings\nmore isotropic \u2013 by subtracting their mean from\neach embedding \u2013 leads to surprisingly large im-\nprovements in performance on downstream tasks.\nGiven that isotropy has bene\ufb01ts for static embed-\ndings, it may also have bene\ufb01ts for contextual-\nized word representations, although the latter have\nalready yielded signi\ufb01cant improvements despite\nbeing highly anisotropic. Therefore, adding an\nanisotropy penalty to the language modelling ob-\njective \u2013 to encourage the contextualized represen-\ntations to be more isotropic \u2013 may yield even better\nresults.\nAnother direction for future work is generat-\ning static word representations from contextual-\nized ones. While the latter offer superior per-\nformance, there are often challenges to deploying\nlarge models such as BERT in production, both\nwith respect to memory and run-time. In contrast,\nstatic representations are much easier to deploy.\nOur work in section 4.3 suggests that not only it is\npossible to extract static representations from con-\ntextualizing models, but that these extracted vec-\ntors often perform much better on a diverse array\nof tasks compared to traditional static embeddings\nsuch as GloVe and FastText. This may be a means\nof extracting some use from contextualizing mod-\nels without incurring the full cost of using them in\nproduction.\n6 Conclusion\nIn this paper, we investigated how contextual con-\ntextualized word representations truly are. For\none, we found that upper layers of ELMo, BERT,\nand GPT-2 produce more context-speci\ufb01c rep-\nresentations than lower layers. This increased\ncontext-speci\ufb01city is always accompanied by in-\ncreased anisotropy. However, context-speci\ufb01city\nalso manifests differently across the three models;\nthe anisotropy-adjusted similarity between words\nin the same sentence is highest in ELMo but al-\nmost non-existent in GPT-2. We ultimately found\nthat after adjusting for anisotropy, on average, less\nthan 5% of the variance in a word\u2019s contextual-\nized representations could be explained by a staticembedding. This means that even in the best-case\nscenario, in all layers of all models, static word\nembeddings would be a poor replacement for con-\ntextualized ones. These insights help explain some\nof the remarkable success that contextualized rep-\nresentations have had on a diverse array of NLP\ntasks.\nAcknowledgments\nWe thank the anonymous reviewers for their in-\nsightful comments. We thank the Natural Sciences\nand Engineering Research Council of Canada\n(NSERC) for their \ufb01nancial support.\nReferences\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel M\nCer, Mona T Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada\nMihalcea, et al. 2015. Semeval-2015 task 2: Seman-\ntic textual similarity, English, Spanish and pilot on\ninterpretability. In Proceedings SemEval@ NAACL-\nHLT . pages 252\u2013263.\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel M\nCer, Mona T Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Rada Mihalcea, German Rigau, and Janyce\nWiebe. 2014. Semeval-2014 task 10: Multilin-\ngual semantic textual similarity. In Proceedings Se-\nmEval@ COLING . pages 81\u201391.\nEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-\nAgirre, and Weiwei Guo. 2013. Sem 2013 shared\ntask: Semantic textual similarity, including a pilot\non typed-similarity. In SEM 2013: The Second Joint\nConference on Lexical and Computational Seman-\ntics. Association for Computational Linguistics.\nEneko Agirre, Mona Diab, Daniel Cer, and Aitor\nGonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-\nlot on semantic textual similarity.", "start_char_idx": 0, "end_char_idx": 3773, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc09df64-ca00-4506-ac63-d172b4445dd8": {"__data__": {"id_": "fc09df64-ca00-4506-ac63-d172b4445dd8", "embedding": null, "metadata": {"page_label": "9", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0521f782-b619-4268-92e9-cde7d75f6fc2", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "d99fe071f00aff07bf78d292e9a574a35c22d069f52f6f72b42f9ee0742d974c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d14f2c6f-635b-4e07-b44a-bfb346c93977", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "d3c15ff5d5df9a1f2011fe0cab82ab0000460deb0f134ff25aea72a8751c93a9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1d518517-d758-4ce6-bbe9-30fad3f5aa68", "node_type": "1", "metadata": {}, "hash": "79cdcb4a87c9628ca79fe4f8ae99bf1419b3255ab9337d38898666494f95b94e", "class_name": "RelatedNodeInfo"}}, "text": "2014. Semeval-2014 task 10: Multilin-\ngual semantic textual similarity. In Proceedings Se-\nmEval@ COLING . pages 81\u201391.\nEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-\nAgirre, and Weiwei Guo. 2013. Sem 2013 shared\ntask: Semantic textual similarity, including a pilot\non typed-similarity. In SEM 2013: The Second Joint\nConference on Lexical and Computational Seman-\ntics. Association for Computational Linguistics.\nEneko Agirre, Mona Diab, Daniel Cer, and Aitor\nGonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-\nlot on semantic textual similarity. In Proceedings\nof the First Joint Conference on Lexical and Com-\nputational Semantics-Volume 1: Proceedings of the\nmain conference and the shared task, and Volume\n2: Proceedings of the Sixth International Workshop\non Semantic Evaluation . Association for Computa-\ntional Linguistics, pages 385\u2013393.\nAbdulrahman Almuhareb and Massimo Poesio. 2004.\nAttribute-based and value-based clustering: An\nevaluation. In Proceedings of the 2004 conference\non empirical methods in natural language process-\ning.\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.\nA simple but tough-to-beat baseline for sentence em-\nbeddings. In International Conference on Learning\nRepresentations .", "start_char_idx": 3222, "end_char_idx": 4443, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d518517-d758-4ce6-bbe9-30fad3f5aa68": {"__data__": {"id_": "1d518517-d758-4ce6-bbe9-30fad3f5aa68", "embedding": null, "metadata": {"page_label": "10", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2eaf068-12a3-4a47-b796-c9d13306cab9", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "234ad0a891840fb38df76207c897d2163b2c96106ab89a8bad52ceb398c8cd93", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc09df64-ca00-4506-ac63-d172b4445dd8", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "9a756e08ef4567833d7d703ace58ac02674df54b78dde1376ea2b1ccea4f81a1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79eeb578-880a-42c4-81c6-d5d0ffafbd0d", "node_type": "1", "metadata": {}, "hash": "3378b03d872f3c73e65bf37ed9b98d500c10fc143c9a7149f335ad396cf80b35", "class_name": "RelatedNodeInfo"}}, "text": "Marco Baroni and Alessandro Lenci. 2011. How we\nblessed distributional semantic evaluation. In Pro-\nceedings of the GEMS 2011 Workshop on GEomet-\nrical Models of Natural Language Semantics . Asso-\nciation for Computational Linguistics, pages 1\u201310.\nElia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014.\nMultimodal distributional semantics. Journal of Ar-\nti\ufb01cial Intelligence Research 49:1\u201347.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805 .\nLev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,\nEhud Rivlin, Zach Solan, Gadi Wolfman, and Ey-\ntan Ruppin. 2002. Placing search in context: The\nconcept revisited. ACM Transactions on informa-\ntion systems 20(1):116\u2013131.\nJohn R Firth. 1957. A synopsis of linguistic theory,\n1930-1955. Studies in linguistic analysis .\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for \ufb01nding syntax in word represen-\ntations. In North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies . Association for Computational\nLinguistics.\nFelix Hill, Roi Reichart, and Anna Korhonen. 2015.\nSimlex-999: Evaluating semantic models with (gen-\nuine) similarity estimation. Computational Linguis-\ntics41(4):665\u2013695.\nDavid A Jurgens, Peter D Turney, Saif M Mohammad,\nand Keith J Holyoak. 2012. Semeval-2012 task 2:\nMeasuring degrees of relational similarity. In Pro-\nceedings of the First Joint Conference on Lexical\nand Computational Semantics-Volume 1: Proceed-\nings of the main conference and the shared task, and\nVolume 2: Proceedings of the Sixth International\nWorkshop on Semantic Evaluation . Association for\nComputational Linguistics, pages 356\u2013364.\nOmer Levy and Yoav Goldberg. 2014a. Linguistic reg-\nularities in sparse and explicit word representations.\nInProceedings of the eighteenth conference on com-\nputational natural language learning . pages 171\u2013\n180.\nOmer Levy and Yoav Goldberg. 2014b. Neural word\nembedding as implicit matrix factorization. In Ad-\nvances in Neural Information Processing Systems .\npages 2177\u20132185.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies .Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692 .\nThang Luong, Richard Socher, and Christopher D\nManning. 2013. Better word representations with\nrecursive neural networks for morphology. In\nSIGNLL Conference on Computational Natural\nLanguage Learning (CoNLL) . pages 104\u2013113.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013a. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems . pages 3111\u20133119.\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.\n2013b. Linguistic regularities in continuous space\nword representations. In Proceedings of the 2013\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies . pages 746\u2013751.\nDavid Mimno and Laure Thompson. 2017. The strange\ngeometry of skip-gram with negative sampling.", "start_char_idx": 0, "end_char_idx": 3544, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79eeb578-880a-42c4-81c6-d5d0ffafbd0d": {"__data__": {"id_": "79eeb578-880a-42c4-81c6-d5d0ffafbd0d", "embedding": null, "metadata": {"page_label": "10", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a2eaf068-12a3-4a47-b796-c9d13306cab9", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "234ad0a891840fb38df76207c897d2163b2c96106ab89a8bad52ceb398c8cd93", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1d518517-d758-4ce6-bbe9-30fad3f5aa68", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "7e9448643680bd7a7a1160b00426749ec72c65716faab015cb7fcd5ce4f7a16b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "188c676f-50c0-45bc-81a1-48eed91d1866", "node_type": "1", "metadata": {}, "hash": "40e34d4185a62e76fb264f0a67a4346aab257289d2f6fe413fd1382c186b3a5f", "class_name": "RelatedNodeInfo"}}, "text": "Better word representations with\nrecursive neural networks for morphology. In\nSIGNLL Conference on Computational Natural\nLanguage Learning (CoNLL) . pages 104\u2013113.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013a. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems . pages 3111\u20133119.\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.\n2013b. Linguistic regularities in continuous space\nword representations. In Proceedings of the 2013\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies . pages 746\u2013751.\nDavid Mimno and Laure Thompson. 2017. The strange\ngeometry of skip-gram with negative sampling. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing . pages\n2873\u20132878.\nJiaqi Mu, Suma Bhat, and Pramod Viswanath. 2018.\nAll-but-the-top: Simple and effective postprocess-\ning for word representations. In Proceedings of the\n7th International Conference on Learning Represen-\ntations (ICLR) .\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP) . pages 1532\u20131543.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers) . pages\n2227\u20132237.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners .\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R. Bowman, Dipan-\njan Das, and Ellie Pavlick. 2019. What do you\nlearn from context? probing for sentence structure\nin contextualized word representations. In Inter-\nnational Conference on Learning Representations .\nhttps://openreview.net/forum?id=SJzSgnRcKX.", "start_char_idx": 2767, "end_char_idx": 4981, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "188c676f-50c0-45bc-81a1-48eed91d1866": {"__data__": {"id_": "188c676f-50c0-45bc-81a1-48eed91d1866", "embedding": null, "metadata": {"page_label": "11", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "19004571-ee66-46ad-99d0-eba9866758c3", "node_type": "4", "metadata": {"page_label": "11", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "ae52a553c71c391712f6537f0f4a451d4c72c6bafe70147ca209ce083e945baa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79eeb578-880a-42c4-81c6-d5d0ffafbd0d", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "58ada316950ccecaae4ae71372c79e78e2389f38b5f417b14dc6111c92814d39", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ee6895fb-9704-4772-873a-3a3be7515948", "node_type": "1", "metadata": {}, "hash": "09b1d6ad9609714697472fbaacdb5307914a6a9bbc42eaee8824168ed603748e", "class_name": "RelatedNodeInfo"}}, "text": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. arXiv preprint\narXiv:1906.08237 .\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson. 2014. How transferable are features in deep\nneural networks? In Advances in Neural Informa-\ntion Processing Systems . pages 3320\u20133328.", "start_char_idx": 0, "end_char_idx": 403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ee6895fb-9704-4772-873a-3a3be7515948": {"__data__": {"id_": "ee6895fb-9704-4772-873a-3a3be7515948", "embedding": null, "metadata": {"page_label": "1", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6b1fc99-8048-4656-bc23-b2a6a2bc326c", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "02d7886e3937902df8ad5fd498850171c9f6030b7fb128bd86fea6d9407e1b38", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "188c676f-50c0-45bc-81a1-48eed91d1866", "node_type": "1", "metadata": {"page_label": "11", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "ae52a553c71c391712f6537f0f4a451d4c72c6bafe70147ca209ce083e945baa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c9a4c489-b6e7-4fc2-9aea-7d84fa71703c", "node_type": "1", "metadata": {}, "hash": "a448c2ba71d16d970c01eefdc13e91ee06721ead2868c49cf8129538616a61c9", "class_name": "RelatedNodeInfo"}}, "text": "arXiv:1909.09067v1  [cs.CL]  19 Sep 2019A Corpus for Automatic Readability Assessment and Text Simp li\ufb01cation\nof German\nAlessia Battisti\nInstitute of Computational Linguistics\nUniversity of Zurich\nAndreasstrasse 15, 8050 Zurich\nalessia.battisti@uzh.chSarah Ebling\nInstitute of Computational Linguistics\nUniversity of Zurich\nAndreasstrasse 15, 8050 Zurich\nebling@cl.uzh.ch\nAbstract\nIn this paper, we present a corpus for use in\nautomatic readability assessment and auto-\nmatic text simpli\ufb01cation of German. The\ncorpus is compiled from web sources and\nconsists of approximately 211,000 sen-\ntences. As a novel contribution, it con-\ntains information on text structure, typog-\nraphy, and images, which can be exploited\nas part of machine learning approaches to\nreadability assessment and text simpli\ufb01ca-\ntion. The focus of this publication is on\nrepresenting such information as an exten-\nsion to an existing corpus standard.\n1 Introduction\nSimpli\ufb01ed language is a variety of standard lan-\nguage characterized by reduced lexical and syn-\ntactic complexity, the addition of explanations\nfor dif\ufb01cult concepts, and clearly structured lay-\nout.1Among the target groups of simpli\ufb01ed lan-\nguage commonly mentioned are persons with cog-\nnitive impairment or learning disabilities, prelin-\ngually deaf persons, functionally illiterate persons,\nand foreign language learners (Bredel and Maa\u00df,\n2016).\nTwo natural language processing tasks deal with\nthe concept of simpli\ufb01ed language: automatic\nreadability assessment and automatic text simpli-\n\ufb01cation. Readability assessment refers to the pro-\ncess of determining the level of dif\ufb01culty of a text,\ne.g., along readability measures, school grades, or\nlevels of the Common European Framework of\nReference for Languages (CEFR) (Council of Eu-\nrope, 2009). Readability measures, in their tra-\nditional form, take into account only surface fea-\ntures. For example, the Flesch Reading Ease Score\n1The term plain language is avoided, as it refers to a spe-\nci\ufb01c level of simpli\ufb01cation. Simpli\ufb01ed language subsumes all\nefforts of reducing the complexity of a piece of text.(Flesch, 1948) measures the length of words (in\nsyllables) and sentences (in words). While read-\nability has been shown to correlate with such fea-\ntures to some extent (Just and Carpenter, 1980), a\nconsensus has emerged according to which they\nare not suf\ufb01cient to account for all of the com-\nplexity inherent in a text. As Kauchak et al.\n(2014, p. 2618) state, \u201cthe usability of readabil-\nity formulas is limited and there is little evidence\nthat the output of these tools directly results in\nimproved understanding by readers\u201d. Recently,\nmore sophisticated models employing (deeper) lin-\nguistic features such as lexical, semantic, mor-\nphological, morphosyntactic, syntactic, pragmatic,\ndiscourse, psycholinguistic, and language model\nfeatures have been proposed (Collins-Thompson,\n2014; Heimann M\u00a8 uhlenbock, 2013; Pitler and\nNenkova, 2008; Schwarm and Ostendorf, 2005;\nTanaka et al., 2013).\nAutomatic text simpli\ufb01cation was initiated in\nthe late 1990s (Carroll et al., 1998; Chandrasekar\net al., 1996) and since then has been approached\nby means of rule-based and statistical methods. As\npart of a rule-based approach, the operations car-\nried out typically include replacing complex lex-\nical and syntactic units by simpler ones. A sta-\ntistical approach generally conceptualizes the sim-\npli\ufb01cation task as one of converting a standard-\nlanguage into a simpli\ufb01ed-language text using ma-\nchine translation. Nisioi et al. (2017) introduced\nneural machine translation to automatic text sim-\npli\ufb01cation. Research on automatic text simpli\ufb01-\ncation is comparatively widespread for languages\nsuch as English, Swedish, Spanish, and Brazilian\nPortuguese. To the authors\u2019 knowledge, no pro-\nductive system exists for German. Suter (2015),\nSuter et al.", "start_char_idx": 0, "end_char_idx": 3853, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c9a4c489-b6e7-4fc2-9aea-7d84fa71703c": {"__data__": {"id_": "c9a4c489-b6e7-4fc2-9aea-7d84fa71703c", "embedding": null, "metadata": {"page_label": "1", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e6b1fc99-8048-4656-bc23-b2a6a2bc326c", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "02d7886e3937902df8ad5fd498850171c9f6030b7fb128bd86fea6d9407e1b38", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ee6895fb-9704-4772-873a-3a3be7515948", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "d0cd36c900e39e063be707104feec9b143ae13f170ec38d7ee2e68ec48663c65", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "54098528-d356-4665-919b-d5060a1f6503", "node_type": "1", "metadata": {}, "hash": "4daf6809bd903297933963891dd35b25610b96491b0ebe5e9dada43b7f745a1c", "class_name": "RelatedNodeInfo"}}, "text": "As\npart of a rule-based approach, the operations car-\nried out typically include replacing complex lex-\nical and syntactic units by simpler ones. A sta-\ntistical approach generally conceptualizes the sim-\npli\ufb01cation task as one of converting a standard-\nlanguage into a simpli\ufb01ed-language text using ma-\nchine translation. Nisioi et al. (2017) introduced\nneural machine translation to automatic text sim-\npli\ufb01cation. Research on automatic text simpli\ufb01-\ncation is comparatively widespread for languages\nsuch as English, Swedish, Spanish, and Brazilian\nPortuguese. To the authors\u2019 knowledge, no pro-\nductive system exists for German. Suter (2015),\nSuter et al. (2016) presented a prototype of a rule-\nbased system for German.\nMachine learning approaches to both readabil-\nity assessment and text simpli\ufb01cation rely on\ndata systematically prepared in the form of cor-", "start_char_idx": 3195, "end_char_idx": 4059, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "54098528-d356-4665-919b-d5060a1f6503": {"__data__": {"id_": "54098528-d356-4665-919b-d5060a1f6503", "embedding": null, "metadata": {"page_label": "2", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4037ae60-e748-4652-b18e-67f34a2960dd", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "c1cc1d2611e220f8a326f0c3f74985c79d26a63275ec68557920f49b625c1521", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c9a4c489-b6e7-4fc2-9aea-7d84fa71703c", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "f5de6419100abf8e4a65d178ded54d705069eb9dbb2a1b22e02ef8fe69ddb925", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "993377cd-0f5d-480b-8168-7b6634716d97", "node_type": "1", "metadata": {}, "hash": "6c4b52faa547649e056ae0e29c49ef74ccb852ce57dd2e94c0160d35749b561c", "class_name": "RelatedNodeInfo"}}, "text": "pora. Speci\ufb01cally, for automatic text simpli\ufb01ca-\ntion via machine translation, pairs of standard-\nlanguage/simpli\ufb01ed-language texts aligned at the\nsentence level (i.e., parallel corpora) are needed.\nThe paper at hand introduces a corpus devel-\noped for use in automatic readability assessment\nand automatic text simpli\ufb01cation of German. The\nfocus of this publication is on representing infor-\nmation that is valuable for these tasks but that hith-\nerto has largely been ignored in machine learning\napproaches centering around simpli\ufb01ed language,\nspeci\ufb01cally, text structure (e.g., paragraphs, lines),\ntypography (e.g., font type, font style), and im-\nage (content, position, and dimensions) informa-\ntion. The importance of considering such infor-\nmation has repeatedly been asserted theoretically\n(Arf\u00b4 e et al., 2018; Bock, 2018; Bredel and Maa\u00df,\n2016).\nThe remainder of this paper is structured as fol-\nlows: Section 2 presents previous corpora used for\nautomatic readability assessment and text simpli\ufb01-\ncation. Section 3 describes our corpus, introduc-\ning its novel aspects and presenting the primary\ndata (Section 3.1), the metadata (Section 3.2), the\nsecondary data (Section 3.3), the pro\ufb01le (Section\n3.4), and the results of machine learning experi-\nments carried out on the corpus (Section 3.5).\n2 Previous Corpora for Automatic\nReadability Assessment and Automatic\nText Simpli\ufb01cation\nA number of corpora for use in automatic read-\nability assessment and automatic text simpli\ufb01ca-\ntion exist. The most well-known example is the\nParallel Wikipedia Simpli\ufb01cation Corpus (PWKP)\ncompiled from parallel articles of the English\nWikipedia and Simple English Wikipedia (Zhu et\nal., 2010) and consisting of around 108,000 sen-\ntence pairs. The corpus pro\ufb01le is shown in Table 1.\nWhile the corpus represents the largest dataset in-\nvolving simpli\ufb01ed language to date, its applica-\ntion has been criticized for various reasons (Aman-\ncio and Specia, 2014; Xu et al., 2015; \u02c7Stajner et\nal., 2018); among these, the fact that Simple En-\nglish Wikipedia articles are not necessarily direct\ntranslations of articles from the English Wikipedia\nstands out. Hwang et al. (2015) provided an up-\ndated version of the corpus that includes a total\nof 280,000 full and partial matches between the\ntwo Wikipedia versions. Another frequently used\ndata collection for English is the Newsela Corpus(Xu et al., 2015) consisting of 1,130 news articles,\neach simpli\ufb01ed into four school grade levels by\nprofessional editors. Table 2 shows the pro\ufb01le of\nthe Newsela Corpus. The table obviates that the\ndifference in vocabulary size between the English\nand the simpli\ufb01ed English side of the PWKP Cor-\npus amounts to only 18%, while the corresponding\nnumber for the English side and the level repre-\nsenting the highest amount of simpli\ufb01cation in the\nNewsela Corpus (Simple-4) is 50.8%. V ocabulary\nsize as an indicator of lexical richness is generally\ntaken to correlate positively with complexity (Vaj-\njala and Meurers, 2012).\nGasperin et al. (2010) compiled the PorSimples\nCorpus consisting of Brazilian Portuguese texts\n(2,116 sentences), each with a natural and a strong\nsimpli\ufb01cation, resulting in around 4,500 aligned\nsentences. Drndarevi\u00b4 c and Saggion (2012), Bott\net al. (2012), Bott and Saggion (2012) produced\nthe Simplext Corpus consisting of 200 Span-\nish/simpli\ufb01ed Spanish document pairs, amount-\ning to a total of 1,149 (Spanish)/1,808 (simpli\ufb01ed\nSpanish) sentences (approximately 1,000 aligned\nsentences).\nKlaper et al. (2013) created the \ufb01rst parallel cor-\npus for German/simpli\ufb01ed German, consisting of\n256 parallel texts downloaded from the web (ap-\nproximately 70,000 tokens).", "start_char_idx": 0, "end_char_idx": 3674, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "993377cd-0f5d-480b-8168-7b6634716d97": {"__data__": {"id_": "993377cd-0f5d-480b-8168-7b6634716d97", "embedding": null, "metadata": {"page_label": "2", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4037ae60-e748-4652-b18e-67f34a2960dd", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "c1cc1d2611e220f8a326f0c3f74985c79d26a63275ec68557920f49b625c1521", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "54098528-d356-4665-919b-d5060a1f6503", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "3160b596c92efb1bd776f6aff79681b8bc6b6019edfef75827cb15c0ad24a645", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a215aae4-5dba-42df-b308-20cc0e752ff6", "node_type": "1", "metadata": {}, "hash": "c1942cd47e6e71cf029921c02c13b03ded013be1938900d91bfa2e3d7cf97e8f", "class_name": "RelatedNodeInfo"}}, "text": "(2010) compiled the PorSimples\nCorpus consisting of Brazilian Portuguese texts\n(2,116 sentences), each with a natural and a strong\nsimpli\ufb01cation, resulting in around 4,500 aligned\nsentences. Drndarevi\u00b4 c and Saggion (2012), Bott\net al. (2012), Bott and Saggion (2012) produced\nthe Simplext Corpus consisting of 200 Span-\nish/simpli\ufb01ed Spanish document pairs, amount-\ning to a total of 1,149 (Spanish)/1,808 (simpli\ufb01ed\nSpanish) sentences (approximately 1,000 aligned\nsentences).\nKlaper et al. (2013) created the \ufb01rst parallel cor-\npus for German/simpli\ufb01ed German, consisting of\n256 parallel texts downloaded from the web (ap-\nproximately 70,000 tokens).\n3 Building a Corpus for Automatic\nReadability Assessment and Automatic\nText Simpli\ufb01cation of German\nSection 2 demonstrated that the only corpus\ncontaining simpli\ufb01ed German available is that\nof Klaper et al. (2013). Since its creation,\na number of legal and political developments\nhave spurred the availability of data in simpli-\n\ufb01ed German. Among these developments is\nthe introduction of a set of regulations for ac-\ncessible information technology ( Barrierefreie-\nInformationstechnik-Verordnung, BITV 2.0 ) in\nGermany and the rati\ufb01cation of the United Nations\nConvention on the Rights of Persons with Disabil-\nities (CRPD) in Switzerland. The paper at hand in-\ntroduces a corpus that represents an enhancement\nof the corpus of Klaper et al. (2013) in the follow-\ning ways:\n\u2022The corpus contains more parallel data.\n\u2022The corpus additionally contains\nmonolingual-only data (simpli\ufb01ed Ger-", "start_char_idx": 3022, "end_char_idx": 4563, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a215aae4-5dba-42df-b308-20cc0e752ff6": {"__data__": {"id_": "a215aae4-5dba-42df-b308-20cc0e752ff6", "embedding": null, "metadata": {"page_label": "3", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4c66e2e6-0588-4327-973d-5776b26e67e6", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "191046a1e58fbbcd5c5b8821efdd86e31244b819fd240f42495a0ce01e0d368f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "993377cd-0f5d-480b-8168-7b6634716d97", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "1699eb457471be2bb94b4b342eb763bb00395f91e383f9dc1b9a706d4f830c6d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "157f391d-1774-4ec1-a4a0-63f259bb5ec6", "node_type": "1", "metadata": {}, "hash": "663282756281920ba6cb3c146c946a7e900f69401cc4cb99e8e8bf1b20813931", "class_name": "RelatedNodeInfo"}}, "text": "English Simple English\nNumber of sentences 108,016 114,924\nNumber of tokens 2,645,771 2,175,240\nAvg. no. of words per sentence 24.49 18.93\nV ocabulary size 95,111 78,009\nTable 1: Parallel Wikipedia Simpli\ufb01cation Corpus (PWKP) (Z hu et al., 2010): Pro\ufb01le (from Xu et al.\n(2015))\nOriginal Simple-1 Simple-2 Simple-3 Simple-4\nNumber of sentences 56,037 57,940 63,419 64,035 64,162\nNumber of tokens 1,301,767 1,126,148 1,052,915 903,417 764 ,103\nAvg. no. of sentences per document 49.59 51.27 56.12 56.67 56 .78\nAvg. no. of words per document 1,152.01 996.59 931.78 799.48 676.2\nAvg. no. of words per sentence 23.23 19.44 16.6 14.11 11.91\nV ocabulary size 39,046 19,197\nTable 2: Newsela Corpus (Xu et al., 2015): Pro\ufb01le\nman).\n\u2022The corpus newly contains information on\ntext structure, typography, and images.\nThe simpli\ufb01ed German side of the parallel data\ntogether with the monolingual-only data can be\nused for automatic readability assessment. The\nparallel data in the corpus is useful both for deriv-\ning rules for a rule-based text simpli\ufb01cation sys-\ntem in a data-driven manner and for training a\ndata-driven machine translation system. A data\naugmentation technique such as back-translation\n(Sennrich et al., 2016) can be applied to the\nmonolingual-only data to arrive at additional (syn-\nthetic) parallel data.\n3.1 Primary Data\nThe corpus contains PDFs and webpages col-\nlected from web sources in Germany, Austria, and\nSwitzerland at the end of 2018/beginning of 2019.\nThe web sources mostly consist of websites of\ngovernments, specialised institutions, translation\nagencies, and non-pro\ufb01t organisations (92 differ-\nent domains). The documents cover a range of\ntopics, such as politics (e.g., instructions for vot-\ning), health (e.g., what to do in case of pregnancy),\nand culture (e.g., introduction to art museums).\nFor the webpages, a static dump of all docu-\nments was created. Following this, the documents\nwere manually checked to verify the language.\nThe main content was subsequently extracted, i.e.,HTML markup and boilerplate removed using\nthe Beautiful Soup library for Python.2Informa-\ntion on text structure (e.g., paragraphs, lines) and\ntypography (e.g., boldface, italics) was retained.\nSimilarly, image information (content, position,\nand dimensions of an image) was preserved.\nFor PDFs, the PDFlib Text and Image Extrac-\ntion Toolkit (TET) was used to extract the plain\ntext and record information on text structure, ty-\npography, and images.3The toolkit produces out-\nput in an XML format (TETML).\n3.2 Metadata\nMetadata was collected automatically from the\nHTML (webpages) and TETML (PDFs) \ufb01les,\ncomplemented manually, and recorded in the\nOpen Language Archives Community (OLAC)\nStandard.4OLAC is based on a reduced ver-\nsion of the Dublin Core Metadata Element Set\n(DCMES).5Of the 15 elements of this \u201cSimple\nDublin Core\u201d set, the following 12 were actively\nused along with controlled vocabularies of OLAC\nand Dublin Core:\n\u2022title : title of the document, with the\n2https://pypi.org/project/\nbeautifulsoup4/ (last accessed: February 27, 2019)\n3https://www.pdflib.com/ (last accessed: Febru-\nary 27, 2019)\n4http://www.language-archives.org/\nOLAC/olacms.html (last accessed: February 28, 2019)\n5http://dublincore.org/ (last accessed: Febru-\nary 28, 2019)", "start_char_idx": 0, "end_char_idx": 3273, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "157f391d-1774-4ec1-a4a0-63f259bb5ec6": {"__data__": {"id_": "157f391d-1774-4ec1-a4a0-63f259bb5ec6", "embedding": null, "metadata": {"page_label": "4", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2f9d15aa-5c08-4cde-99bb-a233f1b579f0", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "105d3871d6233132bc681d15c8a04e62bb1f1954102e6beacc6c02ac5e6b6304", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a215aae4-5dba-42df-b308-20cc0e752ff6", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "191046a1e58fbbcd5c5b8821efdd86e31244b819fd240f42495a0ce01e0d368f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32ae6ca5-d98f-4555-80b0-ce5a11ae731e", "node_type": "1", "metadata": {}, "hash": "05496feddbb0afcd6aa19a8c0546a5e6afbfc25c015fc192b580539f81adfbb3", "class_name": "RelatedNodeInfo"}}, "text": "<?xml version=\u20191.0\u2019 encoding=\u2019utf-8\u2019?>\n<olac:olac xmlns:cld=\"http://purl.org/cld/terms/\"\nxmlns:dc=\"http://purl.org/dc/elements/1.1/\"\nxmlns:dcterms=\"http://purl.org/dc/terms/\"\nxmlns:oai=\"http://www.openarchives.org/OAI/2.0/\"\nxmlns:oai_dc=\"http://www.openarchives.org/OAI/2.0/o ai_dc/\"\nxmlns:olac=\"http://www.language-archives.org/OLAC/1 .1/\"\nxmlns:schemaLocation=\"http://www.language-archives. org/OLAC/\n1.1/olac.xsd\"\nxmlns:tei=\"http://www.tei-c.org/ns/1.0\"\nxmlns:xs=\"http://www.w3.org/2001/XMLSchema\"\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instanc e\"\nxmlns=\"http://www.openarchives.org/OAI/2.0/static-r epository\">\n<dc:title xml:lang=\"de\">Maria sagt es weiter...\nEin Bilder-Lese-Buch \u00a8uber sexuelle Gewalt und Hilfe holen.\n</dc:title>\n<dc:language xsi:type=\"olac:language\"\nolac:code=\"de\">A2</dc:language>\n<dc:publisher>Frauenb \u00a8uro der Stadt Linz</dc:publisher>\n<dc:publisher xsi:type=\"dcterms:URI\">www.linz.at/fra uen</dc:publisher>\n<dc:contributor xsi:type=\"olac:role\" olac:code=\"autho r\">\nVerein Hazissa</dc:contributor>\n<dc:contributor xsi:type=\"olac:role\" olac:code=\"trans lator\">\ncapito Ober \u00a8osterreich</dc:contributor>\n<dc:contributor xsi:type=\"olac:role\" olac:code=\"illus trator\">\nM\u00a8uller, Silke</dc:contributor>\n<dc:identifier xsi:type=\"dcterms:URI\">\nhttps://www.linz.at/images/MariaD.pdf</dc:identifie r>\n<dc:date xsi:type=\"dcterms:W3CDTF\">2016</dc:date>\n<dc:format xsi:type=\"dcterms:IMT\">application/pdf</d c:format>\n<dc:type xsi:type=\"dcterms:DCMIType\">Text</dc:type>\n<dc:type xsi:type=\"dcterms:DCMIType\">StillImage</dc: type>\n<dc:type xsi:type=\"olac:linguistic-type\" olac:code=\"p rimary_text\"/>\n<dc:source>mariad.tetml</dc:source>\n<dc:rights/>\n<dcterms:tableOfContents>\nMaria sagt es weiter Seite 7; Informationen zu sexueller Gew alt\nSeite 12; Adressen von Beratungs-Stellen Seite 17; W \u00a8orterbuch\nSeite 32\n</dcterms:tableOfContents>\n</olac:olac>\nFigure 1: Sample metadata in OLAC for a PDF document from the c orpus", "start_char_idx": 0, "end_char_idx": 1936, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32ae6ca5-d98f-4555-80b0-ce5a11ae731e": {"__data__": {"id_": "32ae6ca5-d98f-4555-80b0-ce5a11ae731e", "embedding": null, "metadata": {"page_label": "5", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "724bf392-6e79-4b38-8f3c-0985f0824a0c", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "185a65d7069d60f6935310c079b7e46ad14b774d1ef7faa6d512e527b9770b90", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "157f391d-1774-4ec1-a4a0-63f259bb5ec6", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "105d3871d6233132bc681d15c8a04e62bb1f1954102e6beacc6c02ac5e6b6304", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b33375fc-dbd2-4930-8026-d5c2e5c33c3f", "node_type": "1", "metadata": {}, "hash": "5907a2fdf1548d43152eb282aa1bc2cf85191d16fc267fa1930e1472f5e2be91", "class_name": "RelatedNodeInfo"}}, "text": "language speci\ufb01ed as the value of an\nxml:lang attribute and alternatives to the\noriginal title (e.g., translations) stored as\ndcterms:alternative (cf. Figure 1 for\nan example)\n\u2022contributor : all person entities linked\nto the creation of a document, with an\nolac:code attribute with values from the\nOLAC role vocabulary used to further spec-\nify the role of the contributor, e.g., author ,\neditor ,publisher , ortranslator\n\u2022date : date mentioned in the metadata of the\nHTML or PDF source or, for news and blog\narticles, date mentioned in the body of the\ntext, in W3C date and time format\n\u2022description : value of the description in\nthe metadata of an HTML document or list\nof sections of a PDF document, using the\nDublin Core quali\ufb01er TableOfContents\n\u2022format : distinction between the Internet\nMedia Types (MIME types) text/html\n(for webpages) and application/pdf\n(for PDFs)\n\u2022identifier : URL of the document or In-\nternational Standard Book Number (ISBN)\nfor books or brochures\n\u2022language : language of the document as\nvalue of the attribute olac:code (i.e.,de,\nas conforming to ISO 639), with the CEFR\nlevel as optional element content\n\u2022publisher : organization or person that\nmade the document available\n\u2022relation : used to establish a link between\ndocuments in German and simpli\ufb01ed German\nfor the parallel part of the corpus, using the\nDublin Core quali\ufb01ers hasVersion (for\nthe German text) and isVersionOf (for\nthe simpli\ufb01ed German text)\n\u2022rights : any piece of information about the\nrights of a document, as far as available in the\nsource\n\u2022source : source document, i.e., HTML for\nweb documents and TETML for PDFs\u2022type : nature or genre of the content of\nthe document, which, in accordance with\nthe DCMI Type V ocabulary, is Text in\nall cases and additionally StillImage in\ncases where a document also contains im-\nages. Additionally, the linguistic type is spec-\ni\ufb01ed according to the OLAC Linguistic Data\nType V ocabulary, as either primary text\n(applies to most documents) or lexicon in\ncases where a document represents an entry\nof a simpli\ufb01ed language vocabulary\nThe elements coverage (to denote the spatial\nor temporal scope of the content of a resource),\ncreator (to denote the author of a text, see\ncontributor above), and subject (to denote\nthe topic of the document content) were not used.\nFigure 1 shows an example of OLAC meta-\ndata. The source document described with this\nmetadata record is a PDF structured into chap-\nters, with text corresponding to the CEFR level\nA2 and images. Metadata in OLAC can be con-\nverted into the metadata standard of CLARIN (a\nEuropean research infrastructure for language re-\nsources and technology),6the Component Meta-\nData Infrastructure (CMDI).7The CMDI standard\nwas chosen since it is the supported metadata ver-\nsion of CLARIN, which is speci\ufb01cally popular in\nGerman-speaking countries.\nInformation on the language level of a simpli-\n\ufb01ed German text (typically A1, A2, or B1) is par-\nticularly valuable, as it allows for conducting au-\ntomatic readability assessment and graded auto-\nmatic text simpli\ufb01cation experiments on the data.\n52 websites and 233 PDFs (amounting to approx-\nimately 26,000 sentences) have an explicit lan-\nguage level label.\n3.3 Secondary Data\nAnnotations were added in the Text Corpus\nFormat by WebLicht (TCF)8developed as part\nof CLARIN. TCF supports standoff annotation,\nwhich allows for representation of annotations\nwith con\ufb02icting hierarchies. TCF does not assign\na separate \ufb01le for each annotation layer; instead,\n6https://www.clarin.eu/ (last accessed: Febru-\nary 27, 2019)\n7https://www.clarin.eu/faq/\nhow-can-i-convert-my-dc-or-olac-records-cmdi\n(last accessed: February 28, 2019)\n8https://weblicht.sfs.uni-tuebingen.\nde/weblichtwiki/index.php/The_TCF_Format\n(last accessed: April 11, 2019)", "start_char_idx": 0, "end_char_idx": 3776, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b33375fc-dbd2-4930-8026-d5c2e5c33c3f": {"__data__": {"id_": "b33375fc-dbd2-4930-8026-d5c2e5c33c3f", "embedding": null, "metadata": {"page_label": "6", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f643fb8-b06e-490b-b3e9-909d6292b389", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "8ff71837be83957d13ccadfb0569d6a14b68f9ac0a14afba5bad0b4c1e602b77", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "32ae6ca5-d98f-4555-80b0-ce5a11ae731e", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "185a65d7069d60f6935310c079b7e46ad14b774d1ef7faa6d512e527b9770b90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c154563-728a-4cfe-a93d-33ebee912126", "node_type": "1", "metadata": {}, "hash": "9edbacfe7723205d782204ac7e2a346f4f06c966bb6a86bc4589e9268b1ef8b7", "class_name": "RelatedNodeInfo"}}, "text": "the source text and all annotation layers are stored\njointly in a single \ufb01le. A token layer acts as the\nkey element to which all other annotation layers\nare linked.\nThe following types of annotations were added:\ntext structure, fonts, images, tokens, parts of\nspeech, morphological units, lemmas, sentences,\nand dependency parses. TCF does not readily ac-\ncommodate the incorporation of all of these types\nof information. We therefore extended the format\nin the following ways:\n\u2022Information on the font type and font style\n(e.g., italics, bold print) of a token and its po-\nsition on the physical page (for PDFs only)\nwas speci\ufb01ed as attributes to the token ele-\nments of the tokens layer (cf. Figure 2 for\nan example)\n\u2022Information on physical page segmenta-\ntion (for PDFs only), paragraph segmen-\ntation, and line segmentation was added\nas part of a textspan element in the\ntextstructure layer\n\u2022A separate images layer was introduced to\nholdimage elements that take as attributes\nthe x and y coordinates of the images, their\ndimensions (width and height), and the num-\nber of the page on which they occur\n\u2022A separate fonts layer was introduced to\npreserve detailed information on the font con-\n\ufb01gurations referenced in the tokens layer\nLinguistic annotation was added automatically\nusing the ParZu dependency parser for German\n(Sennrich et al., 2009) (for tokens and depen-\ndency parses), the NLTK toolkit (Bird et al., 2009)\n(for sentences), the TreeTagger (Schmid, 1995)\n(for part-of-speech tags and lemmas), and Zmorge\n(Sennrich and Kunz, 2014) (for morphological\nunits). Figure 2 shows a sample corpus annotation.\nTogether, the metadata shown in Figure 1 and the\nannotations presented in Figure 2 constitute a com-\nplete TCF \ufb01le.\n3.4 Corpus Pro\ufb01le\nThe resulting corpus contains 6,217 documents\n(5,461 monolingual documents plus 378 docu-\nments for each side of the parallel data). Table\n3 shows the corpus pro\ufb01le. The monolingual-\nonly documents on average contain fewer sen-\ntences than the simpli\ufb01ed German side of the par-allel data (average document length in sentences\n31.64 vs. 55.75). The average sentence length\nis almost equal (approx. 11 tokens). Hence, the\nmonolingual-only texts are shorter than the simpli-\n\ufb01ed German texts in the parallel data. Compared\nto their German counterparts, the simpli\ufb01ed Ger-\nman texts in the parallel data have clearly under-\ngone a process of lexical simpli\ufb01cation: The vo-\ncabulary is smaller by 51% (33,384 vs. 16,352\ntypes), which is comparable to the rate of reduc-\ntion reported in Section 2 for the Newsela Corpus\n(50.8%).\n3.5 Empirical validation of the corpus\nBattisti (2019) applied unsupervised machine\nlearning techniques to the simpli\ufb01ed German texts\nof the corpus presented in this paper with the\naim of investigating evidence of multiple complex-\nity levels. While the detailed results are beyond\nthe scope of this paper, the author found features\nbased on the structural information that is a unique\nproperty of this corpus (e.g., number of images,\nnumber of paragraphs, number of lines, number of\nwords of a speci\ufb01c font type, and adherence to a\none-sentence-per-line rule) to be predictive of the\nlevel of dif\ufb01culty of a simpli\ufb01ed German text. To\nour knowledge, this is the \ufb01rst study to deliver em-\npirical proof of the relevance of such features.\n4 Conclusion and Outlook\nWe have introduced a corpus compiled for use\nin automatic readability assessment and automatic\ntext simpli\ufb01cation of German. While such tasks\nhave been addressed for other languages, research\non German is still scarce. The features exploited\nas part of machine learning approaches to read-\nability assessment so far typically include surface\nand/or (deeper) linguistic features. The corpus\npresented in this paper additionally contains infor-\nmation on text structure, typography, and images.\nThese features have been shown to be indicative\nof simple vs. complex texts both theoretically and,\nusing the corpus described in this paper, empiri-\ncally.", "start_char_idx": 0, "end_char_idx": 3989, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c154563-728a-4cfe-a93d-33ebee912126": {"__data__": {"id_": "3c154563-728a-4cfe-a93d-33ebee912126", "embedding": null, "metadata": {"page_label": "6", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f643fb8-b06e-490b-b3e9-909d6292b389", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "8ff71837be83957d13ccadfb0569d6a14b68f9ac0a14afba5bad0b4c1e602b77", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b33375fc-dbd2-4930-8026-d5c2e5c33c3f", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "808b24b012007396a66480bdfbc7bfdbb901273aa851fb3fd3f8e3690df348e5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "714da7de-3625-4641-8b80-17676d0dd35b", "node_type": "1", "metadata": {}, "hash": "71385edd6b6319d851e121fbe174354893e3fbe27105d7ae7af6d49a74ff97e2", "class_name": "RelatedNodeInfo"}}, "text": "To\nour knowledge, this is the \ufb01rst study to deliver em-\npirical proof of the relevance of such features.\n4 Conclusion and Outlook\nWe have introduced a corpus compiled for use\nin automatic readability assessment and automatic\ntext simpli\ufb01cation of German. While such tasks\nhave been addressed for other languages, research\non German is still scarce. The features exploited\nas part of machine learning approaches to read-\nability assessment so far typically include surface\nand/or (deeper) linguistic features. The corpus\npresented in this paper additionally contains infor-\nmation on text structure, typography, and images.\nThese features have been shown to be indicative\nof simple vs. complex texts both theoretically and,\nusing the corpus described in this paper, empiri-\ncally.\nInformation on text structure, typography, and\nimages can also be leveraged as part of a neu-\nral machine translation approach to text simpli-\n\ufb01cation. A set of parallel documents used in\nmachine translation additionally requires sentence\nalignments, which are still missing from our cor-\npus. Hence, as a next step, we will include such", "start_char_idx": 3210, "end_char_idx": 4327, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "714da7de-3625-4641-8b80-17676d0dd35b": {"__data__": {"id_": "714da7de-3625-4641-8b80-17676d0dd35b", "embedding": null, "metadata": {"page_label": "7", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0fb92626-4265-408b-a0c6-ba0be90c2643", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "5d722b8fa7b717e9e745ff1da6781bb16d1e1d360d2577eb4714273efd20672f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c154563-728a-4cfe-a93d-33ebee912126", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "fad87224ae3579265e10043a4fa2856dcc52d4f8b98311a4a097ac3e6fda71d3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "09f86ebd-84e2-465a-a362-8432855eceab", "node_type": "1", "metadata": {}, "hash": "7254edcfe5b30d827a53c5e6410d410c4b454c29b579708724cedca1fecedc0f", "class_name": "RelatedNodeInfo"}}, "text": "<TextCorpus>\n<text>...</text>\n<tokens>\n<token ID=\"t_0\" font=\"F0\">Vorwort</token>\n<token ID=\"t_1\" font=\"F0\">Liebe</token>\n<token ID=\"t_2\" font=\"F0\">Leserinnen</token>\n...\n</tokens>\n<sentences>\n<sentence ID=\"s_0\" tokenIDs=\"t_0 t_1 t_2 t_3\"/>\n...\n</sentences>\n<textstructure>\n<textspan start=\"t_0\" type=\"paragraph\" end=\"t_0\"/>\n<textspan start=\"t_0\" type=\"line\" end=\"t_0\"/>\n<textspan type=\"paragraph\" start=\"t_1\" end=\"t_3\"/>\n<textspan type=\"line\" start=\"t_1\" end=\"t_3\"/>\n<textspan type=\"paragraph\" start=\"t_4\" end=\"t_27\"/>\n...\n</textstructure>\n<lemmas>\n<lemma ID=\"l_0\" tokenIDs=\"t_0\">Vorwort</lemma>\n<lemma ID=\"l_1\" tokenIDs=\"t_1\">lieb</lemma>\n<lemma ID=\"l_2\" tokenIDs=\"t_2\">Leserin</lemma>\n...\n</lemmas>\n<POStags tagset=\"stts\">\n<tag tokenIDs=\"t_0\">NN</tag>\n<tag tokenIDs=\"t_1\">ADJA</tag>\n<tag tokenIDs=\"t_2\">NN</tag>\n...\n</POStags>\n<morphology>\n<analysis tokenIDs=\"t_0\">Neut|_|Sg</analysis>\n<analysis tokenIDs=\"t_1\">Pos|Fem|_|Pl|St|</analysis>\n...\n</morphology>\n<depparsing>\n...\n</depparsing>\n<images>\n<image ID=\"I0\" page=\"1\" x=\"-1.07\" y=\"112.47\"/>\n...\n</images>\n<fonts>\n<font id=\"F0\" name=\"TradeGothic-BoldTwo\"\nfullname=\"UDSPGZ+TradeGothic-BoldTwo\" type=\"Type 1 CFF \"\nembedded=\"true\" ascender=\"977\" capheight=\"722\" italica ngle=\"0\"\ndescender=\"-229\" weight=\"700\" xheight=\"520\"/>\n...\n</fonts>\n</TextCorpus>\nFigure 2: Sample corpus annotation", "start_char_idx": 0, "end_char_idx": 1337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09f86ebd-84e2-465a-a362-8432855eceab": {"__data__": {"id_": "09f86ebd-84e2-465a-a362-8432855eceab", "embedding": null, "metadata": {"page_label": "8", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2cff0bdf-dd24-4e4f-b1e2-934ef0398901", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "71afd0618449464b5487b3dfe3d124ad485b48cb39f512a6a3b33136f66a931a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "714da7de-3625-4641-8b80-17676d0dd35b", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "5d722b8fa7b717e9e745ff1da6781bb16d1e1d360d2577eb4714273efd20672f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5710873f-708e-4d54-a2d8-1b6977cf6a03", "node_type": "1", "metadata": {}, "hash": "0d544b2c9173a28a861bc1b634ab557bb7c78749752187f3e584882e27304cf9", "class_name": "RelatedNodeInfo"}}, "text": "German Simpli\ufb01ed German\nMonolingual\nNumber of documents 5,461\nNumber of sentences 172,773\nNumber of tokens 1,916,045\nAvg. no. of sentences per document 31.64\nAvg. no. of tokens per sentence 11.09\nParallel\nNumber of documents 378 378\nNumber of sentences 17,121 21,072\nNumber of tokens 347,941 246,405\nAvg. no. of sentences per document 45.29 55.75\nAvg. no. of tokens per sentence 20.32 11.69\nV ocabulary size 33,384 16,352\nParallel (total)\nNumber of documents 756\nNumber of sentences 38,193\nNumber of tokens 594,346\nAvg. no. of sentences per document 50.52\nAvg. no. of tokens per sentence 15.56\nMonolingual and parallel (total)\nNumber of documents 6,217\nNumber of sentences 210,966\nNumber of tokens 2,510,391\nAvg. no. of sentences per document 33.93\nAvg. no. of tokens per sentence 11.90\nTable 3: Corpus pro\ufb01le", "start_char_idx": 0, "end_char_idx": 809, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5710873f-708e-4d54-a2d8-1b6977cf6a03": {"__data__": {"id_": "5710873f-708e-4d54-a2d8-1b6977cf6a03", "embedding": null, "metadata": {"page_label": "9", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "208aea5f-885c-49b5-beaf-016acff75fa9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "67fbd21f2709b33d24fa80c8c6e3fd9fac59f5593b9eebe1a809b1cd492fd7e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "09f86ebd-84e2-465a-a362-8432855eceab", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "71afd0618449464b5487b3dfe3d124ad485b48cb39f512a6a3b33136f66a931a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "705780c9-c843-42c7-ad41-94c5988c706e", "node_type": "1", "metadata": {}, "hash": "cdadc7095fbea87576498906236be418d8c43ecfa28fa240c459c47e76e4c175", "class_name": "RelatedNodeInfo"}}, "text": "information using the Customized Alignment for\nText Simpli\ufb01cation (CATS) tool ( \u02c7Stajner et al.,\n2017).\nReferences\nMarcelo Adriano Amancio and Lucia Specia. 2014.\nAn analysis of crowd sourced text simpli\ufb01cations. In\nProceedings of the 3rd Workshop on Predicting and\nImproving Text Readability for Target Reader Pop-\nulations (PITR) , pages 123\u2013130, Gothenburg, Swe-\nden.\nBarbara Arf\u00b4 e, Lucia Mason, and Inmaculada Fa-\njardo. 2018. Simplifying informational text struc-\nture for struggling readers. Reading and Writing ,\n31(9):2191\u20132210.\nAlessia Battisti. 2019. Automatic Cluster Analysis of\nTexts in Simpli\ufb01ed German. Master\u2019s thesis, Univer-\nsity of Zurich.\nSteven Bird, Edward Loper, and Ewan Klein.\n2009. Natural Language Processing with Python .\nO\u2019Reilly Media Inc.\nBettina M. Bock. 2018. Technical report, Universit\u00a8 at\nLeipzig.\nStefan Bott and Horacio Saggion. 2012. Automatic\nsimpli\ufb01cation of Spanish text for e-Accessibility. In\nProceedings of the 13th International Conference on\nComputers Helping People with Special Needs (IC-\nCHP) , pages 527\u2013534, Linz, Austria.\nStefan Bott, Horacio Saggion, and David Figueroa.\n2012. A hybrid system for Spanish text simpli\ufb01-\ncation. In Proceedings of the Third Workshop on\nSpeech and Language Processing for Assistive Tech-\nnologies , pages 75\u201384, Montr\u00b4 eal, Canada.\nUrsula Bredel and Christiane Maa\u00df. 2016. Leichte\nSprache: Theoretische Grundlagen. Orientierung\nf\u00a8ur die Praxis . Duden, Berlin.\nJohn Carroll, Guido Minnen, Yvonne Canning, Siob-\nhan Devlin, and John Tait. 1998. Practical simpli-\n\ufb01cation of English newspaper text to assist aphasic\nreaders. In Proceedings fo the AAAI\u201998 Workshop\non Integrating AI and Assistive Technology , pages\n7\u201310.\nRaman Chandrasekar, Christine Doran, and Bangalore\nSrinivas. 1996. Motivations and methods for text\nsimpli\ufb01cation. In Proceedings of the 16th Con-\nference on Computational Linguistics , pages 1041\u2013\n1044, Copenhagen, Denmark.\nKevyn Collins-Thompson. 2014. Computational as-\nsessment of text readability. A survey of current and\nfuture research. ITL International Journal of Ap-\nplied Linguistics , 165(2):97\u2013135.Council of Europe. 2009. Common European Frame-\nwork of Reference for Languages: Learning, teach-\ning, assessment . Cambridge University Press, Cam-\nbridge.\nBiljana Drndarevi\u00b4 c and Horacio Saggion. 2012. To-\nwards automatic lexical simpli\ufb01cation in Spanish:\nAn empirical study. In Proceedings of the First\nWorkshop on Predicting and Improving Text Read-\nability for Target Reader Populations (PITR) , pages\n8\u201316, Montr\u00b4 eal, Canada.\nRudolph Flesch. 1948. A new readability yardstick.\nJournal of Applied Psychology , 32:221\u2013233.\nCaroline Gasperin, Erick Maziero, and Sandra M.\nAluisio. 2010. Challenging choices for text sim-\npli\ufb01cation. In Computational Processing of the Por-\ntuguese Language. Proceedings of the 9th Interna-\ntional Conference, PROPOR 2010 , pages 40\u201350,\nPorto Alegre, Brazil.\nKatarina Heimann M\u00a8 uhlenbock. 2013. I see what\nyou mean: Assessing readability for speci\ufb01c target\ngroups . Ph.D. thesis, University of Gothenburg.\nWilliam Hwang, Hannaneh Hajishirzi, Mari Ostendorf,\nand Wei Wu. 2015. Aligning Sentences from Stan-\ndard Wikipedia to Simple Wikipedia. In Proceed-\nings of NAACL-HLT , pages 211\u2013217.\nMarcel Just and Patricia Carpenter. 1980. A theory of\nreading.", "start_char_idx": 0, "end_char_idx": 3304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "705780c9-c843-42c7-ad41-94c5988c706e": {"__data__": {"id_": "705780c9-c843-42c7-ad41-94c5988c706e", "embedding": null, "metadata": {"page_label": "9", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "208aea5f-885c-49b5-beaf-016acff75fa9", "node_type": "4", "metadata": {"page_label": "9", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "67fbd21f2709b33d24fa80c8c6e3fd9fac59f5593b9eebe1a809b1cd492fd7e7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5710873f-708e-4d54-a2d8-1b6977cf6a03", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "c66d3bd23ef9e78d8569311ce0091ccd1f1882d99a4519a67b20ae3d98f623b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "085aec09-83e4-4d6b-8e35-06577191b8d9", "node_type": "1", "metadata": {}, "hash": "f65395ef168d278e6d9dfdbff61857bcde40b8a8ab316b6744fe9d3271b73052", "class_name": "RelatedNodeInfo"}}, "text": "2010. Challenging choices for text sim-\npli\ufb01cation. In Computational Processing of the Por-\ntuguese Language. Proceedings of the 9th Interna-\ntional Conference, PROPOR 2010 , pages 40\u201350,\nPorto Alegre, Brazil.\nKatarina Heimann M\u00a8 uhlenbock. 2013. I see what\nyou mean: Assessing readability for speci\ufb01c target\ngroups . Ph.D. thesis, University of Gothenburg.\nWilliam Hwang, Hannaneh Hajishirzi, Mari Ostendorf,\nand Wei Wu. 2015. Aligning Sentences from Stan-\ndard Wikipedia to Simple Wikipedia. In Proceed-\nings of NAACL-HLT , pages 211\u2013217.\nMarcel Just and Patricia Carpenter. 1980. A theory of\nreading. from eye \ufb01xations to comprehension. Psy-\nchological review , 87(4):329\u2013354.\nD. Kauchak, O. Mouradi, C. Pentoney, and G. Leroy.\n2014. Text Simpli\ufb01cation Tools: Using Machine\nLearning to Discover Features that Identify Dif\ufb01cult\nText. In Proceedings of the 47th Hawaii Interna-\ntional Conference on System Sciences , pages 2616\u2013\n2625.\nDavid Klaper, Sarah Ebling, and Martin V olk. 2013.\nBuilding a German/Simple German parallel corpus\nfor automatic text simpli\ufb01cation. In ACL Workshop\non Predicting and Improving Text Readability for\nTarget Reader Populations , pages 11\u201319, So\ufb01a, Bul-\ngaria.\nSergiu Nisioi, Sanja \u02c7Stajner, Simone Paolo Ponzetto,\nand Liviu P. Dinu. 2017. Exploring neural text sim-\npli\ufb01cation models. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics , pages 85\u201391, Vancouver, Canada, July.\nEmily Pitler and Ani Nenkova. 2008. Revisiting Read-\nability: A Uni\ufb01ed Framework for Predicting Text\nQuality. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP \u201908) , pages 186\u2013195, Honolulu, Hawaii.\nHelmut Schmid. 1995. Improvements in part-of-\nspeech tagging with an application to German. In\nProceedings of the EACL\u201995 SIGDAT Workshop ,\npages 47\u201350, Dublin, Ireland.", "start_char_idx": 2701, "end_char_idx": 4566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "085aec09-83e4-4d6b-8e35-06577191b8d9": {"__data__": {"id_": "085aec09-83e4-4d6b-8e35-06577191b8d9", "embedding": null, "metadata": {"page_label": "10", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c7a4f182-d990-4c2c-9a2e-97548e03eb8b", "node_type": "4", "metadata": {"page_label": "10", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "2368d516ae13327a81a0ac2c5b40332b3b538d45b6e1a4eaf2f23d7cf6f31825", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "705780c9-c843-42c7-ad41-94c5988c706e", "node_type": "1", "metadata": {"page_label": "9", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "379f818d44a2712bd14a34c9c98e78f398b39dbce970c32a4330a1fb71682623", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "28edaeb2-8077-4ec4-b006-f8784f939fcd", "node_type": "1", "metadata": {}, "hash": "9f01eeae7bcfd3ce6aa0bee1ae7d5abd65c73081561fa204ea862d8e789f1fde", "class_name": "RelatedNodeInfo"}}, "text": "Sarah E. Schwarm and Mari Ostendorf. 2005. Reading\nlevel assessment using support vector machines and\nstatistical language models. In Proceedings of the\n43rd Annual meeting of the Association for Compu-\ntational Linguistics , pages 523\u2013530.\nRico Sennrich and Beat Kunz. 2014. Zmorge: A Ger-\nman Morphological Lexicon Extracted from Wik-\ntionary. In Proceedings of the 9th International\nConference on Language Resources and Evaluation\n(LREC 2014) .\nRico Sennrich, Gerold Schneider, Martin V olk, and\nMartin Warin. 2009. A new hybrid dependency\nparser for German. Proceedings of the German So-\nciety for Computational Linguistics and Language\nTechnology Conference , pages 115\u2013124.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation mod-\nels with monolingual data. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , pages\n86\u201396, Berlin, Germany, August. Association for\nComputational Linguistics.\nSanja \u02c7Stajner, Marc Franco-Salvador, Simone Paolo\nPonzetto, Paolo Rosso, and Heiner Stuckenschmidt.\n2017. Sentence alignment methods for improving\ntext simpli\ufb01cation systems. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers) , pages\n97\u2013102, Vancouver, Canada, July.\nSanja \u02c7Stajner, Marc Franco-Salvador, Paolo Rosso, and\nSimone Paolo Ponzetto. 2018. CATS: A Tool for\nCustomized Alignment of Text Simpli\ufb01cation Cor-\npora. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018) , pages 3895\u20133903, Miyazaki, Japan.\nJulia Suter, Sarah Ebling, and Martin V olk. 2016.\nRule-based Automatic Text Simpli\ufb01cation for Ger-\nman. In Proceedings of the 13th Conference on Nat-\nural Language Processing (KONVENS 2016) , pages\n279\u2013287, Bochum, Germany.\nJulia Suter. 2015. Rule-based text simpli\ufb01cation for\nGerman. Bachelor\u2019s thesis, University of Zurich.\nS. Tanaka, A. Jatowt, M. P. Kato, and K. Tanaka. 2013.\nEstimating content concreteness for \ufb01nding compre-\nhensible documents. In Proceedings of the 6th ACM\ninternational conference on Web search and data\nmining , pages 475\u2013484.\nSowmya Vajjala and Detmar Meurers. 2012. On im-\nproving the accuracy of readability classi\ufb01cation us-\ning insights from second language acquisition. In\nProceedings of the Seventh Workshop on Building\nEducational Applications Using NLP (NAACL HLT\n\u201912), pages 163\u2013173, Montreal, Canada.\nWei Xu, Chris Callison-Burch, and Courtney Napoles.\n2015. Problems in current text simpli\ufb01cation re-\nsearch: New data can help. Transactions of the Asso-\nciation for Computational Linguistics , 3:283\u2013297.Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.\n2010. A monolingual tree-based translation model\nfor sentence simpli\ufb01cation. In Proceedings of the\nInternational Conference on Computational Linguis-\ntics, pages 1353\u20131361, Beijing, China.", "start_char_idx": 0, "end_char_idx": 2919, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28edaeb2-8077-4ec4-b006-f8784f939fcd": {"__data__": {"id_": "28edaeb2-8077-4ec4-b006-f8784f939fcd", "embedding": null, "metadata": {"page_label": "1", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "53b70b3b-1485-4a7c-9de5-4490c71dd120", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "d1d9360b1367b6bed32a8083c9cd397325b67a0b9413f7112794a590b47df755", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "085aec09-83e4-4d6b-8e35-06577191b8d9", "node_type": "1", "metadata": {"page_label": "10", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "2368d516ae13327a81a0ac2c5b40332b3b538d45b6e1a4eaf2f23d7cf6f31825", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e11d07b4-7bef-485c-908b-1fcff15fa321", "node_type": "1", "metadata": {}, "hash": "8064836af8af161f4e14abd98770a6e2de23328eea345733a684a8dd17927246", "class_name": "RelatedNodeInfo"}}, "text": "Cross-lingual Pre-training Based Transfer for Zero-shot Neural\nMachine Translation\nBaijun Ji\u2021, Zhirui Zhang\u00a7, Xiangyu Duan\u2020\u2021\u2217, Min Zhang\u2020\u2021, Boxing Chen\u00a7and Weihua Luo\u00a7\n\u2020Institute of Arti\ufb01cial Intelligence, Soochow University, Suzhou, China\n\u2021School of Computer Science and Technology, Soochow University, Suzhou, China\n\u00a7Alibaba DAMO Academy, Hangzhou, China\n\u2021bjji@stu.suda.edu.cn\u2020{xiangyuduan, minzhang}@suda.edu.cn\n\u00a7{zhirui.zzr,boxing.cbx,weihua.luowh}@alibaba-inc.com\nAbstract\nTransfer learning between different language pairs has shown\nits effectiveness for Neural Machine Translation (NMT) in\nlow-resource scenario. However, existing transfer methods\ninvolving a common target language are far from success in\nthe extreme scenario of zero-shot translation, due to the lan-\nguage space mismatch problem between transferor (the par-\nent model) and transferee (the child model) on the source\nside. To address this challenge, we propose an effective trans-\nfer learning approach based on cross-lingual pre-training. Our\nkey idea is to make all source languages share the same fea-\nture space and thus enable a smooth transition for zero-shot\ntranslation. To this end, we introduce one monolingual pre-\ntraining method and two bilingual pre-training methods to\nobtain a universal encoder for different languages. Once the\nuniversal encoder is constructed, the parent model built on\nsuch encoder is trained with large-scale annotated data and\nthen directly applied in zero-shot translation scenario. Exper-\niments on two public datasets show that our approach signif-\nicantly outperforms strong pivot-based baseline and various\nmultilingual NMT approaches.\nIntroduction\nAlthough Neural Machine Translation (NMT) has domi-\nnated recent research on translation tasks (Wu et al. 2016;\nVaswani et al. 2017; Hassan et al. 2018), NMT heavily relies\non large-scale parallel data, resulting in poor performance\non low-resource or zero-resource language pairs (Koehn\nand Knowles 2017). Translation between these low-resource\nlanguages (e.g., Arabic \u2192Spanish) is usually accomplished\nwith pivoting through a rich-resource language (such as En-\nglish), i.e., Arabic (source) sentence is translated to En-\nglish (pivot) \ufb01rst which is later translated to Spanish (tar-\nget) (Kauers et al. 2002; de Gispert and Mari\u00f1o 2006).\nHowever, the pivot-based method requires doubled decoding\ntime and suffers from the propagation of translation errors.\nOne common alternative to avoid pivoting in NMT is\ntransfer learning (Zoph et al. 2016; Nguyen and Chiang\n2017; Kocmi and Bojar 2018; Kim et al. 2019) which lever-\nages a high-resource pivot \u2192target model ( parent ) to ini-\n\u2217Corresponding Author.\nCopyright c\u20dd2020, Association for the Advancement of Arti\ufb01cial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The circle and triangle dots represent source sen-\ntences in different language l1andl2, and the square dots\nmeans target sentences in language l3. A sample of transla-\ntion pairs is connected by the dashed line. We would like to\nforce each of the translation pairs has the same latent rep-\nresentation as the right part of the \ufb01gure so as to transfer\nl1\u2192l3model directly to l2\u2192l3model.\ntialize a low-resource source \u2192target model ( child ) that is\nfurther optimized with a small amount of available paral-\nlel data. Although this approach has achieved success in\nsome low-resource language pairs, it still performs very\npoorly in extremely low-resource or zero-resource transla-\ntion scenario. Speci\ufb01cally, Kocmi and Bojar (2018) reports\nthat without any child model training data, the performance\nof the parent model on the child test set is miserable.\nIn this work, we argue that the language space mis-\nmatch problem, also named domain shift problem (Fu et al.\n2015), brings about the zero-shot translation failure in trans-\nfer learning.", "start_char_idx": 0, "end_char_idx": 3840, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e11d07b4-7bef-485c-908b-1fcff15fa321": {"__data__": {"id_": "e11d07b4-7bef-485c-908b-1fcff15fa321", "embedding": null, "metadata": {"page_label": "1", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "53b70b3b-1485-4a7c-9de5-4490c71dd120", "node_type": "4", "metadata": {"page_label": "1", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "d1d9360b1367b6bed32a8083c9cd397325b67a0b9413f7112794a590b47df755", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28edaeb2-8077-4ec4-b006-f8784f939fcd", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "ed51bda83ecdd0d78796fff9c909e0aa636d3e7d20dfa89e6ee8a6f9723e4307", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3002dda1-f9ad-4b77-a875-5fd3cbfb9272", "node_type": "1", "metadata": {}, "hash": "5315ca19789fe59585bc4e8373544cc5928b8082b720ed98e115051f9411ffbb", "class_name": "RelatedNodeInfo"}}, "text": "tialize a low-resource source \u2192target model ( child ) that is\nfurther optimized with a small amount of available paral-\nlel data. Although this approach has achieved success in\nsome low-resource language pairs, it still performs very\npoorly in extremely low-resource or zero-resource transla-\ntion scenario. Speci\ufb01cally, Kocmi and Bojar (2018) reports\nthat without any child model training data, the performance\nof the parent model on the child test set is miserable.\nIn this work, we argue that the language space mis-\nmatch problem, also named domain shift problem (Fu et al.\n2015), brings about the zero-shot translation failure in trans-\nfer learning. It is because transfer learning has no explicit\ntraining process to guarantee that the source and pivot lan-\nguages share the same feature distributions, causing that the\nchild model inherited from the parent model fails in such\na situation. For instance, as illustrated in the left of Fig-\nure 1, the points of the sentence pair with the same seman-\ntics are not overlapping in source space, resulting in that\nthe shared decoder will generate different translations de-\nnoted by different points in target space. Actually, transfer\nlearning for NMT can be viewed as a multi-domain problem\nwhere each source language forms a new domain. Minimiz-\ning the discrepancy between the feature distributions of dif-\nferent source languages, i.e., different domains, will ensure\nthe smooth transition between the parent and child models,arXiv:1912.01214v1  [cs.CL]  3 Dec 2019", "start_char_idx": 3185, "end_char_idx": 4708, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3002dda1-f9ad-4b77-a875-5fd3cbfb9272": {"__data__": {"id_": "3002dda1-f9ad-4b77-a875-5fd3cbfb9272", "embedding": null, "metadata": {"page_label": "2", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "606bf2d5-8b74-490e-82c5-4d7995453e0d", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "4a72894ffe4de17a614f2db53696c647c19623326e684907c8a82097ab073686", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e11d07b4-7bef-485c-908b-1fcff15fa321", "node_type": "1", "metadata": {"page_label": "1", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "7bc14746040dade3571dc36f659fb3fc8577226c2837851ec75654a0bbfd86df", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "366429d4-41bb-4e6e-9177-81ef84a6c571", "node_type": "1", "metadata": {}, "hash": "116c3f1907f81df7bfb14c2be76328c940f46270ed21f0f3292e5d9ef5cb969f", "class_name": "RelatedNodeInfo"}}, "text": "as shown in the right of Figure 1. One way to achieve this\ngoal is the \ufb01ne-tuning technique, which forces the model to\nforget the speci\ufb01c knowledge from parent data and learn new\nfeatures from child data. However, the domain shift problem\nstill exists, and the demand of parallel child data for \ufb01ne-\ntuning heavily hinders transfer learning for NMT towards\nthe zero-resource setting.\nIn this paper, we explore the transfer learning in\na common zero-shot scenario where there are a lot\nof source\u2194pivot and pivot\u2194target parallel data but no\nsource\u2194target parallel data. In this scenario, we propose\na simple but effective transfer approach, the key idea\nof which is to relieve the burden of the domain shift\nproblem by means of cross-lingual pre-training. To this\nend, we \ufb01rstly investigate the performance of two exist-\ning cross-lingual pre-training methods proposed by Lam-\nple and Conneau (2019) in zero-shot translation scenario.\nBesides, a novel pre-training method called BRidge Lan-\nguage Modeling (BRLM) is designed to make full use of the\nsource\u2194pivot bilingual data to obtain a universal encoder\nfor different languages. Once the universal encoder is con-\nstructed, we only need to train the pivot \u2192target model and\nthen test this model in source \u2192target direction directly. The\nmain contributions of this paper are as follows:\n\u2022We propose a new transfer learning approach for NMT\nwhich uses the cross-lingual language model pre-training\nto enable a high performance on zero-shot translation.\n\u2022We propose a novel pre-training method called BRLM,\nwhich can effectively alleviates the distance between dif-\nferent source language spaces.\n\u2022Our proposed approach signi\ufb01cantly improves zero-shot\ntranslation performance, consistently surpassing pivot-\ning and multilingual approaches. Meanwhile, the perfor-\nmance on supervised translation direction remains the\nsame level or even better when using our method.\nRelated Work\nIn recent years, zero-shot translation in NMT has attracted\nwidespread attention in academic research. Existing meth-\nods are mainly divided into four categories: pivot-based\nmethod, transfer learning, multilingual NMT, and unsuper-\nvised NMT.\n\u2022Pivot-based Method is a common strategy to obtain a\nsource\u2192target model by introducing a pivot language.\nThis approach is further divided into pivoting and pivot-\nsynthetic. While the former \ufb01rstly translates a source lan-\nguage into the pivot language which is later translated\nto the target language (Kauers et al. 2002; de Gispert\nand Mari\u00f1o 2006; Utiyama and Isahara 2007), the lat-\nter trains a source\u2192target model with pseudo data gener-\nated from source-pivot or pivot-target parallel data (Chen\net al. 2017; Zheng, Cheng, and Liu 2017). Although the\npivot-based methods can achieve not bad performance, it\nalways falls into a computation-expensive and parameter-\nvast dilemma of quadratic growth in the number of source\nlanguages, and suffers from the error propagation prob-\nlem (Zhu et al. 2013).\u2022Transfer Learning is \ufb01rstly introduced for NMT by\nZoph et al. (2016), which leverages a high-resource par-\nent model to initialize the low-resource child model. On\nthis basis, Nguyen and Chiang (2017) and Kocmi and\nBojar (2018) use shared vocabularies for source/target\nlanguage to improve transfer learning, while Kim, Gao,\nand Ney (2019) relieve the vocabulary mismatch by\nmainly using cross-lingual word embedding. Although\nthese methods are successful in the low-resource scene,\nthey have limited effects in zero-shot translation.\n\u2022Multilingual NMT (MNMT) enables training a single\nmodel that supports translation from multiple source lan-\nguages into multiple target languages, even those unseen\nlanguage pairs (Firat, Cho, and Bengio 2016; Firat et al.\n2016; Johnson et al. 2016; Al-Shedivat and Parikh 2019;\nAharoni, Johnson, and Firat 2019). Aside from sim-\npler deployment, MNMT bene\ufb01ts from transfer learning\nwhere low-resource language pairs are trained together\nwith high-resource ones. However, Gu et al. (2019) point\nout that MNMT for zero-shot translation easily fails, and\nis sensitive to the hyper-parameter setting.", "start_char_idx": 0, "end_char_idx": 4113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "366429d4-41bb-4e6e-9177-81ef84a6c571": {"__data__": {"id_": "366429d4-41bb-4e6e-9177-81ef84a6c571", "embedding": null, "metadata": {"page_label": "2", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "606bf2d5-8b74-490e-82c5-4d7995453e0d", "node_type": "4", "metadata": {"page_label": "2", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "4a72894ffe4de17a614f2db53696c647c19623326e684907c8a82097ab073686", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3002dda1-f9ad-4b77-a875-5fd3cbfb9272", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "7926b6fdcdbe4fddbb2717b2f1c340cface666215ab29702b05f21de91653f4c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "78e626a2-3ae5-46ec-a91d-65af9b82ac33", "node_type": "1", "metadata": {}, "hash": "38212235bdda571afd7c299fa6e1927a0c92d51d40a0c68cf31d61d15e7002b1", "class_name": "RelatedNodeInfo"}}, "text": "Although\nthese methods are successful in the low-resource scene,\nthey have limited effects in zero-shot translation.\n\u2022Multilingual NMT (MNMT) enables training a single\nmodel that supports translation from multiple source lan-\nguages into multiple target languages, even those unseen\nlanguage pairs (Firat, Cho, and Bengio 2016; Firat et al.\n2016; Johnson et al. 2016; Al-Shedivat and Parikh 2019;\nAharoni, Johnson, and Firat 2019). Aside from sim-\npler deployment, MNMT bene\ufb01ts from transfer learning\nwhere low-resource language pairs are trained together\nwith high-resource ones. However, Gu et al. (2019) point\nout that MNMT for zero-shot translation easily fails, and\nis sensitive to the hyper-parameter setting. Also, MNMT\nusually performs worse than the pivot-based method in\nzero-shot translation setting (Arivazhagan et al. 2018).\n\u2022Unsupervised NMT (UNMT) considers a harder setting,\nin which only large-scale monolingual corpora are avail-\nable for training. Recently, many methods have been pro-\nposed to improve the performance of UNMT, including\nusing denoising auto-encoder, statistic machine transla-\ntion (SMT) and unsupervised pre-training (Artetxe et\nal. 2017; Lample et al. 2018; Ren et al. 2019; Lample\nand Conneau 2019). Since UNMT performs well between\nsimilar languages (e.g., English-German translation), its\nperformance between distant languages is still far from\nexpectation.\nOur proposed method belongs to the transfer learning,\nbut it is different from traditional transfer methods which\ntrain a parent model as starting point. Before training a par-\nent model, our approach fully leverages cross-lingual pre-\ntraining methods to make all source languages share the\nsame feature space and thus enables a smooth transition for\nzero-shot translation.\nApproach\nIn this section, we will present a cross-lingual pre-\ntraining based transfer approach. This method is designed\nfor a common zero-shot scenario where there are a lot\nof source\u2194pivot and pivot\u2194target bilingual data but no\nsource\u2194target parallel data, and the whole training process\ncan be summarized as follows step by step:\n\u2022Pre-train a universal encoder with source/pivot monolin-\ngual or source\u2194pivot bilingual data.\n\u2022Train a pivot\u2192target parent model built on the pre-trained\nuniversal encoder with the available parallel data. Dur-\ning the training process, we freeze several layers of the\npre-trained universal encoder to avoid the degeneracy is-\nsue (Howard and Ruder 2018).", "start_char_idx": 3398, "end_char_idx": 5862, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "78e626a2-3ae5-46ec-a91d-65af9b82ac33": {"__data__": {"id_": "78e626a2-3ae5-46ec-a91d-65af9b82ac33", "embedding": null, "metadata": {"page_label": "3", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6137caa2-d008-463d-bbad-6c37b53d7a39", "node_type": "4", "metadata": {"page_label": "3", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "8c37a75377f42b70e413a05541e376329574a251ccdb02efddeb2fb82cffb9fa", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "366429d4-41bb-4e6e-9177-81ef84a6c571", "node_type": "1", "metadata": {"page_label": "2", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a63ff2016185f32dc4218b8159669178cc385990f2d2c44eae7c3f475d2d13af", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "518edc28-ea32-4802-8965-d3f86b569bbc", "node_type": "1", "metadata": {}, "hash": "76cbb2f8b3366bee95bde1f13363c189cc28f72e103663be44ad0f43c541e981", "class_name": "RelatedNodeInfo"}}, "text": "Figure 2: The overview of BRidge Language Modeling (BRLM). The BRLM extends MLM (Lample and Conneau 2019) to\npairs of parallel sentences and leverages explicit alignment information obtained by external aligner tool or additional attention\nlayer to encourage word representation alignment across different languages.\n\u2022Directly translate source sentences into target sentences\nwith the parent model, which bene\ufb01ts from the availabil-\nity of the universal encoder.\nThe key dif\ufb01culty of this method is to ensure the intermedi-\nate representations of the universal encoder are language in-\nvariant. In the rest of this section, we \ufb01rst present two exist-\ning methods yet to be explored in zero-shot translation, and\nthen propose a straightforward but effective cross-lingual\npre-training method. In the end, we present the whole train-\ning and inference protocol for transfer.\nMasked and Translation Language Model\nPretraining\nTwo existing cross-lingual pre-training methods, Masked\nLanguage Modeling (MLM) and Translation Language\nModeling (TLM), have shown their effectiveness on XNLI\ncross-lingual classi\ufb01cation task (Lample and Conneau 2019;\nHuang et al. 2019), but these methods have not been well\nstudied on cross-lingual generation tasks in zero-shot condi-\ntion. We attempt to take advantage of the cross-lingual abil-\nity of the two methods for zero-shot translation.\nSpeci\ufb01cally, MLM adopts the Cloze objective of\nBERT (Devlin et al. 2018) and predicts the masked words\nthat are randomly selected and replaced with [MASK] to-\nken on monolingual corpus. In practice, MLM takes dif-\nferent language monolingual corpora as input to \ufb01nd fea-\ntures shared across different languages. With this method,\nword pieces shared in all languages have been mapped into\na shared space, which makes the sentence representationsacross different languages close (Pires, Schlinger, and Gar-\nrette 2019).\nSince MLM objective is unsupervised and only requires\nmonolingual data, TLM is designed to leverage parallel data\nwhen it is available. Actually, TLM is a simple extension of\nMLM, with the difference that TLM concatenates sentence\npair into a whole sentence, and then randomly masks words\nin both the source and target sentences. In this way, the\nmodel can either attend to surrounding words or to the trans-\nlation sentence, implicitly encouraging the model to align\nthe source and target language representations. Note that al-\nthough each sentence pair is formed into one sentence, the\npositions of the target sentence are reset to count form zero.\nBridge Language Model Pretraining\nAside from MLM and TLM, we propose BRidge Language\nModeling (BRLM) to further obtain word-level representa-\ntion alignment between different languages. This method is\ninspired by the assumption that if the feature spaces of dif-\nferent languages are aligned very well, the masked words\nin the corrupted sentence can also be guessed by the con-\ntext of the correspondingly aligned words on the other side.\nTo achieve this goal, BRLM is designed to strengthen the\nability to infer words across languages based on alignment\ninformation, instead of inferring words within monolingual\nsentence as in MLM or within the pseudo sentence formed\nby concatenating sentence pair as in TLM.\nAs illustrated in Figure 2, BRLM stacks shared encoder\nover both side sentences separately. In particular, we design\ntwo network structures for BRLM, which are divided into", "start_char_idx": 0, "end_char_idx": 3427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "518edc28-ea32-4802-8965-d3f86b569bbc": {"__data__": {"id_": "518edc28-ea32-4802-8965-d3f86b569bbc", "embedding": null, "metadata": {"page_label": "4", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "44d116a9-5dc5-4866-962c-377581814389", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "8d185ee42d301694408a829a8364fda67e409d4b766a88888e5b49223ca193dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "78e626a2-3ae5-46ec-a91d-65af9b82ac33", "node_type": "1", "metadata": {"page_label": "3", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "8c37a75377f42b70e413a05541e376329574a251ccdb02efddeb2fb82cffb9fa", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "540f99c3-e1c8-4e72-9857-86c8f568c94e", "node_type": "1", "metadata": {}, "hash": "58b5118bb67771f397189393ad2d16822533c467a93b5fe16ede600016c39df1", "class_name": "RelatedNodeInfo"}}, "text": "Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-\nSA) according to the way of generating the alignment infor-\nmation. These two structures actually extend MLM into a\nbilingual scenario, with the difference that BRLM leverages\nexternal aligner tool or additional attention layer to explic-\nitly introduce alignment information during model training.\n\u2022Hard Alignment (BRLM-HA). We \ufb01rst use exter-\nnal aligner tool on source \u2194pivot parallel data to ex-\ntract the alignment information of sentence pair. Dur-\ning model training, given source \u2194pivot sentence pair,\nBRLM-HA randomly masks some words in source sen-\ntence and leverages alignment information to obtain the\naligned words in pivot sentence for masked words. Based\non the processed input, BRLM-HA adopts the Trans-\nformer (Vaswani et al. 2017) encoder to gain the hid-\nden states for source and pivot sentences respectively.\nThen the training objective of BRLM-HA is to predict\nthe masked words by not only the surrounding words\nin source sentence but also the encoder outputs of the\naligned words. Note that this training process is also car-\nried out in a symmetric situation, in which we mask some\nwords in pivot sentence and obtain the aligned words in\nthe source sentence.\n\u2022Soft Alignment (BRLM-SA). Instead of using external\naligner tool, BRLM-SA introduces an additional atten-\ntion layer to learn the alignment information together with\nmodel training. In this way, BRLM-SA avoids the effect\ncaused by external wrong alignment information and en-\nables many-to-one soft alignment during model training.\nSimilar with BRLM-HA, the training objective of BRLM-\nSA is to predict the masked words by not only the sur-\nrounding words in source sentence but also the outputs of\nattention layer. In our implementation, the attention layer\nis a multi-head attention layer adopted in Transformer,\nwhere the queries come from the masked source sentence,\nthe keys and values come from the pivot sentence.\nIn principle, MLM and TLM can learn some implicit align-\nment information during model training. However, the align-\nment process in MLM is inef\ufb01cient since the shared word\npieces only account for a small proportion of the whole cor-\npus, resulting in the dif\ufb01culty of expanding the shared in-\nformation to align the whole corpus. TLM also lacks effort\nin alignment between the source and target sentences since\nTLM concatenates the sentence pair into one sequence, mak-\ning the explicit alignment between the source and target in-\nfeasible. BRLM fully utilizes the alignment information to\nobtain better word-level representation alignment between\ndifferent languages, which better relieves the burden of the\ndomain shift problem .\nTransfer Protocol\nWe consider the typical zero-shot translation scenario in\nwhich a high resource pivot language has parallel data with\nboth source and target languages, while source and target\nlanguages has no parallel data between themselves. Our pro-\nposed cross-lingual pretraining based transfer approach for\nsource\u2192target zero-shot translation is mainly divided into\ntwo phrases: the pretraining phase and the transfer phase.Corpus Language Train Dev Test\nEuroparlDe-En,En-Fr 1M,1M 2,000 2,000\nFr-En,En-Es 1M,1M 2,000 2,000\nRo-En,En-De 0.6M,1.5M 2,000 1,000\nMultiUNAr-En,En-Es\nEn-Ru9.7M,11.3M\n11.6M4,000 4,000\nTable 1: Data Statistics.\nIn the pretraining phase, we \ufb01rst pretrain MLM on mono-\nlingual corpora of both source and pivot languages, and con-\ntinue to pretrain TLM or the proposed BRLM on the avail-\nable parallel data between source and pivot languages, in\norder to build a cross-lingual encoder shared by the source\nand pivot languages.\nIn the transfer phase, we train pivot \u2192target NMT model\ninitialized by the cross-lingually pre-trained encoder, and\n\ufb01nally transfer the trained NMT model to source \u2192target\ntranslation thanks to the shared encoder. Note that during\ntraining pivot\u2192target NMT model, we freeze several layers\nof the cross-lingually pre-trained encoder to avoid the de-\ngeneracy issue.", "start_char_idx": 0, "end_char_idx": 4009, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "540f99c3-e1c8-4e72-9857-86c8f568c94e": {"__data__": {"id_": "540f99c3-e1c8-4e72-9857-86c8f568c94e", "embedding": null, "metadata": {"page_label": "4", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "44d116a9-5dc5-4866-962c-377581814389", "node_type": "4", "metadata": {"page_label": "4", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "8d185ee42d301694408a829a8364fda67e409d4b766a88888e5b49223ca193dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "518edc28-ea32-4802-8965-d3f86b569bbc", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "6f6885cba4072c59d5766baad0a5f127a22723ae065d61204777d76ba60fabfe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a63100a3-f57c-457d-ba80-fc482dfd5f56", "node_type": "1", "metadata": {}, "hash": "29ef1b6d18931f84ec8fab6a682cbd415f5f471720b6d77317e10059aa4b731f", "class_name": "RelatedNodeInfo"}}, "text": "In the pretraining phase, we \ufb01rst pretrain MLM on mono-\nlingual corpora of both source and pivot languages, and con-\ntinue to pretrain TLM or the proposed BRLM on the avail-\nable parallel data between source and pivot languages, in\norder to build a cross-lingual encoder shared by the source\nand pivot languages.\nIn the transfer phase, we train pivot \u2192target NMT model\ninitialized by the cross-lingually pre-trained encoder, and\n\ufb01nally transfer the trained NMT model to source \u2192target\ntranslation thanks to the shared encoder. Note that during\ntraining pivot\u2192target NMT model, we freeze several layers\nof the cross-lingually pre-trained encoder to avoid the de-\ngeneracy issue.\nFor the more complicated scenario that either the source\nside or the target side has multiple languages, the encoder\nand the decoder are also shared across each side languages\nfor ef\ufb01cient deployment of translation between multiple lan-\nguages.\nExperiments\nSetup\nWe evaluate our cross-lingual pre-training based transfer ap-\nproach against several strong baselines on two public datat-\nsets, Europarl (Koehn 2005) and MultiUN (Eisele and Chen\n2010), which contain multi-parallel evaluation data to assess\nthe zero-shot performance. In all experiments, we use BLEU\nas the automatic metric for translation evaluation.1\nDatasets. The statistics of Europarl and MultiUN cor-\npora are summarized in Table 1. For Europarl corpus, we\nevaluate on French-English-Spanish (Fr-En-Es), German-\nEnglish-French (De-En-Fr) and Romanian-English-German\n(Ro-En-De), where English acts as the pivot language, its\nleft side is the source language, and its right side is the target\nlanguage. We remove the multi-parallel sentences between\ndifferent training corpora to ensure zero-shot settings. We\nuse the devtest2006 as the validation set and the test2006 as\nthe test set for Fr\u2192Es and De\u2192Fr. For distant language pair\nRo\u2192De, we extract 1,000 overlapping sentences from new-\nstest2016 as the test set and the 2,000 overlapping sentences\nsplit from the training set as the validation set since there is\nno of\ufb01cial validation and test sets. For vocabulary, we use\n60K sub-word tokens based on Byte Pair Encoding (BPE)\n(Sennrich, Haddow, and Birch 2015).\nFor MultiUN corpus, we use four languages: English\n(En) is set as the pivot language, which has parallel data\n1We calculate BLEU scores with the multi-bleu.perl script.", "start_char_idx": 3332, "end_char_idx": 5711, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a63100a3-f57c-457d-ba80-fc482dfd5f56": {"__data__": {"id_": "a63100a3-f57c-457d-ba80-fc482dfd5f56", "embedding": null, "metadata": {"page_label": "5", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4689cc6b-9f9b-46dd-93c1-559c5c1e7f08", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "df0036da89582cbe00d67cab95b10176aab00cd9db9f790106db575b4ae335a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "540f99c3-e1c8-4e72-9857-86c8f568c94e", "node_type": "1", "metadata": {"page_label": "4", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a86e9ede0bae62509696e3bc9472e5dc03971e39a3781bfbae813f074a00e744", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "41e50465-360e-4d54-9b27-e9d2b91f4af0", "node_type": "1", "metadata": {}, "hash": "228b12b73f789aa67ff07e7f1b8c7d8627bda8fd4201c006cca307c2f8ed9f9b", "class_name": "RelatedNodeInfo"}}, "text": "Europarl Fr\u2192En\u2192Es De\u2192En\u2192Fr Ro\u2192En\u2192De\nDirection Fr\u2192Es En\u2192Es De\u2192Fr En\u2192Fr Ro\u2192De En\u2192De\nBaselines\nCross-lingual Transfer (Kim, Gao, and Ney 2019) 18.45 34.01 9.86 34.05 2.02 23.61\nMNMT(Johnson et al. 2016) 27.12 34.69 21.36 33.87 9.31 24.09\nMNMT Agreement (Al-Shedivat and Parikh 2019) 29.91 33.80 24.45 32.55 - -\nPivoting 32.25 34.01 27.79 34.05 14.74 23.61\nProposed Cross-lingual Pretraining Based Transfer\nMLM 35.96 34.83 27.61 35.66 12.64 22.04\nMLM+TLM 36.78 34.73 29.45 35.33 14.39 24.96\nMLM+BRLM-HA 36.30 34.98 29.91 34.99 14.21 24.26\nMLM+BRLM-SA 37.02 34.92 30.66 35.91 15.62 24.95\nTable 2: Results on Europarl test sets. Three pivot settings are conducted in our experiments. In each setting, the left column\npresents the zero-shot performances (source \u2192target), and the right column denotes the performances in the supervised parent\nmodel direction (pivot \u2192target).\nwith other three languages which do not have parallel data\nbetween each other. The three languages are Arabic (Ar),\nSpanish (Es), and Russian (Ru), and mutual translation be-\ntween themselves constitutes six zero-shot translation di-\nrection for evaluation. We use 80K BPE splits as the vo-\ncabulary. Note that all sentences are tokenized by the tok-\nenize.perl2script, and we lowercase all data to avoid a large\nvocabulary for the MultiUN corpus.\nExperimental Details. We use traditional transfer learn-\ning, pivot-based method and multilingual NMT as our base-\nlines. For the fair comparison, the Transformer-big model\nwith 1024 embedding/hidden units, 4096 feed-forward \ufb01lter\nsize, 6 layers and 8 heads per layer is adopted for all trans-\nlation models in our experiments. We set the batch size to\n2400 per batch and limit sentence length to 100 BPE tokens.\nWe set the attn_drop = 0 (a dropout rate on each atten-\ntion head), which is favorable to the zero-shot translation\nand has no effect on supervised translation directions (Gu\net al. 2019). For the model initialization, we use Facebook\u2019s\ncross-lingual pretrained models released by XLM3to initial-\nize the encoder part, and the rest parameters are initialized\nwith xavier uniform. We employ the Adam optimizer with\nlr= 0.0001 ,twarm_up = 4000 and dropout = 0.1. At decod-\ning time, we generate greedily with length penalty \u03b1= 1.0.\nRegarding MLM, TLM and BRLM, as mentioned in the\npre-training phase of transfer protocol, we \ufb01rst pre-train\nMLM on monolingual data of both source and pivot lan-\nguages, then leverage the parameters of MLM to initialize\nTLM and the proposed BRLM, which are continued to be\noptimized with source-pivot bilingual data. In our experi-\nments, we use MLM+TLM, MLM+BRLM to represent this\ntraining process. For the masking strategy during training,\nfollowing Devlin et al. (2018), 15% of BPE tokens are se-\nlected to be masked. Among the selected tokens, 80% of\nthem are replaced with [MASK] token, 10% are replaced\n2https://github.com/moses-smt/mosesdecoder/blob/RELEASE-\n3.0/scripts/tokenizer/tokenizer.perl\n3https://github.com/facebookresearch/XLMwith a random BPE token, and 10% unchanged. The predic-\ntion accuracy of masked words is used as a stopping cri-\nterion in the pre-training stage. Besides, we use fastalign\ntool (Dyer, Chahuneau, and Smith 2013) to extract word\nalignments for BRLM-HA.", "start_char_idx": 0, "end_char_idx": 3254, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "41e50465-360e-4d54-9b27-e9d2b91f4af0": {"__data__": {"id_": "41e50465-360e-4d54-9b27-e9d2b91f4af0", "embedding": null, "metadata": {"page_label": "5", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4689cc6b-9f9b-46dd-93c1-559c5c1e7f08", "node_type": "4", "metadata": {"page_label": "5", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "df0036da89582cbe00d67cab95b10176aab00cd9db9f790106db575b4ae335a0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a63100a3-f57c-457d-ba80-fc482dfd5f56", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "2aeccb7b7a0ee3162ee0574c746a3f945a630eef8dc9735a351210b363fc0f90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "87b1fe1b-0470-4a3d-b9ab-64b7e05e8b5c", "node_type": "1", "metadata": {}, "hash": "92785374ebc16e4ffa4b2cf1d0412b1a689ab3ecb62fca61dacc0ea659dee493", "class_name": "RelatedNodeInfo"}}, "text": "In our experi-\nments, we use MLM+TLM, MLM+BRLM to represent this\ntraining process. For the masking strategy during training,\nfollowing Devlin et al. (2018), 15% of BPE tokens are se-\nlected to be masked. Among the selected tokens, 80% of\nthem are replaced with [MASK] token, 10% are replaced\n2https://github.com/moses-smt/mosesdecoder/blob/RELEASE-\n3.0/scripts/tokenizer/tokenizer.perl\n3https://github.com/facebookresearch/XLMwith a random BPE token, and 10% unchanged. The predic-\ntion accuracy of masked words is used as a stopping cri-\nterion in the pre-training stage. Besides, we use fastalign\ntool (Dyer, Chahuneau, and Smith 2013) to extract word\nalignments for BRLM-HA.\nMain Results\nTable 2 and 3 report zero-shot results on Europarl and\nMulti-UN evaluation sets, respectively. We compare our ap-\nproaches with related approaches of pivoting, multilingual\nNMT (MNMT) (Johnson et al. 2016), and cross-lingual\ntransfer without pretraining (Kim, Gao, and Ney 2019). The\nresults show that our approaches consistently outperform\nother approaches across languages and datasets, especially\nsurpass pivoting, which is a strong baseline in the zero-\nshot scenario that multilingual NMT systems often fail to\nbeat (Johnson et al. 2016; Al-Shedivat and Parikh 2019;\nArivazhagan et al. 2018). Pivoting translates source to\npivot then to target in two steps, causing inef\ufb01cient trans-\nlation process. Our approaches use one encoder-decoder\nmodel to translate between any zero-shot directions, which\nis more ef\ufb01cient than pivoting. Regarding the comparison\nbetween transfer approaches, our cross-lingual pretraining\nbased transfer outperforms transfer method that does not use\npretraining by a large margin.\nResults on Europarl Dataset. Regarding comparison be-\ntween the baselines in table 2, we \ufb01nd that pivoting is the\nstrongest baseline that has signi\ufb01cant advantage over other\ntwo baselines. Cross-lingual transfer for languages without\nshared vocabularies (Kim, Gao, and Ney 2019) manifests the\nworst performance because of not using source \u2194pivot par-\nallel data, which is utilized as bene\ufb01cial supervised signal\nfor the other two baselines.\nOur best approach of MLM+BRLM-SA achieves the sig-\nni\ufb01cant superior performance to all baselines in the zero-\nshot directions, improving by 0.9-4.8 BLEU points over the\nstrong pivoting. Meanwhile, in the supervised direction of\npivot\u2192target, our approaches performs even better than the\noriginal supervised Transformer thanks to the shared en-", "start_char_idx": 2577, "end_char_idx": 5063, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87b1fe1b-0470-4a3d-b9ab-64b7e05e8b5c": {"__data__": {"id_": "87b1fe1b-0470-4a3d-b9ab-64b7e05e8b5c", "embedding": null, "metadata": {"page_label": "6", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7095e65f-fb04-4bf9-897b-614c9d5524ca", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "89a4c84b1d37bb61f4e6fa767b8597bb116bfdf7e578a19ca2e5ceeecefbbc92", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "41e50465-360e-4d54-9b27-e9d2b91f4af0", "node_type": "1", "metadata": {"page_label": "5", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "8c54309d8c1a33736d09173ab80ab05dfa1b4a29c35577164f80c4127f516394", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17aa8243-b803-4f95-90da-a27d33ba8ebe", "node_type": "1", "metadata": {}, "hash": "395255f7522bb1af6b9c47df6ba424ef1601bb128d7afec6f23612b7fbccc606", "class_name": "RelatedNodeInfo"}}, "text": "MultiUN Ar,Es,Ru\u2194En\nDirection Ar\u2192Es Es\u2192Ar Ar\u2192Ru Ru\u2192Ar Es\u2192Ru Ru\u2192Es A-ZST A-ST\nBaselines\nCross-lingual Transfer 10.26 12.44 4.58 4.42 13.80 7.93 8.90 44.73\nMNMT(Johnson et al. 2016) 27.40 20.18 15.12 16.19 17.88 27.93 20.78 43.95\nPivoting m 42.29 30.15 27.23 26.16 29.57 40.08 32.58 43.95\nProposed Cross-lingual Pretraining Based Transfer\nMLM 16.50 23.41 9.61 14.23 22.80 23.66 18.36 44.25\nMLM+TLM 25.98 26.55 16.84 20.07 25.91 29.52 24.14 43.71\nMLM+BRLM-HA 29.05 27.58 18.10 20.42 25.39 30.96 25.25 44.67\nMLM+BRLM-SA 36.01 31.08 25.49 25.06 30.47 36.01 30.68 44.54\nAdding Back Translation\nMNMT* (Gu et al. 2019) 39.72 28.05 24.67 24.43 27.41 38.01 30.38 43.98\nMLM 40.98 31.53 26.06 26.69 31.28 40.02 32.76 44.28\nMLM+TLM 41.15 29.77 27.61 27.74 31.02 40.37 32.39 44.14\nMLM+BRLM-HA 41.74 31.89 27.24 27.54 31.29 40.34 33.35 44.52\nMLM+BRLM-SA 44.17 33.20 29.01 28.91 32.53 41.93 34.95 45.49\nTable 3: Results on MultiUN test sets. The six zero-shot translation directions are evaluated. The column \u201cA-ZST\" reports av-\neraged BLEU of zero-shot translation, while the column \u201cA-ST\" reports averaged BLEU of supervised pivot \u2192target direction.\n(a) Fr-En\n (b) De-En\n (c) Ro-En\nFigure 3: Cosine similarity between sentence representation of each encoder layer across all source-pivot sentence pairs in the\nEuroparl validation set.\ncoder trained on both large-scale monolingual data and par-\nallel data between multiple languages.\nMLM alone that does not use source \u2194pivot parallel data\nperforms much better than the cross-lingual transfer, and\nachieves comparable results to pivoting. When MLM is\ncombined with TLM or the proposed BRLM, the perfor-\nmance is further improved. MLM+BRLM-SA performs the\nbest, and is better than MLM+BRLM-HA indicating that\nsoft alignment is helpful than hard alignment for the cross-\nlingual pretraining.\nResults on MultiUN Dataset. Like experimental results\non Europarl, MLM+BRLM-SA performs the best among\nall proposed cross-lingual pretraining based transfer ap-\nproaches as shown in Table 3. When comparing systems\nconsisting of one encoder-decoder model for all zero-shot\ntranslation, our approaches performs signi\ufb01cantly better\nthan MNMT (Johnson et al. 2016).Although it is challenging for one model to translate all\nzero-shot directions between multiple distant language pairs\nof MultiUN, MLM+BRLM-SA still achieves better perfor-\nmances on Es\u2192Ar and Es\u2192Ru than strong pivoting m,\nwhich uses MNMT to translate source to pivot then to tar-\nget in two separate steps with each step receiving supervised\nsignal of parallel corpora. Our approaches surpass pivoting m\nin all zero-shot directions by adding back translation (Sen-\nnrich, Haddow, and Birch 2015) to generate pseudo parallel\nsentences for all zero-shot directions based on our pretrained\nmodels such as MLM+BRLM-SA, and further training our\nuniversal encoder-decoder model with these pseudo data.\nGu et al.", "start_char_idx": 0, "end_char_idx": 2893, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17aa8243-b803-4f95-90da-a27d33ba8ebe": {"__data__": {"id_": "17aa8243-b803-4f95-90da-a27d33ba8ebe", "embedding": null, "metadata": {"page_label": "6", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7095e65f-fb04-4bf9-897b-614c9d5524ca", "node_type": "4", "metadata": {"page_label": "6", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "89a4c84b1d37bb61f4e6fa767b8597bb116bfdf7e578a19ca2e5ceeecefbbc92", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "87b1fe1b-0470-4a3d-b9ab-64b7e05e8b5c", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "8ec58b9432d3ff0e39c24497e6d8c5eef4d534e429e115984aed6596c40cec17", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "77e6e8bc-49a6-484f-a81a-b35b2521e6e2", "node_type": "1", "metadata": {}, "hash": "682a23e4b67f871b5c36c0c0922810b73e2eda417ff7ebed5353f01aa88eff95", "class_name": "RelatedNodeInfo"}}, "text": "2016).Although it is challenging for one model to translate all\nzero-shot directions between multiple distant language pairs\nof MultiUN, MLM+BRLM-SA still achieves better perfor-\nmances on Es\u2192Ar and Es\u2192Ru than strong pivoting m,\nwhich uses MNMT to translate source to pivot then to tar-\nget in two separate steps with each step receiving supervised\nsignal of parallel corpora. Our approaches surpass pivoting m\nin all zero-shot directions by adding back translation (Sen-\nnrich, Haddow, and Birch 2015) to generate pseudo parallel\nsentences for all zero-shot directions based on our pretrained\nmodels such as MLM+BRLM-SA, and further training our\nuniversal encoder-decoder model with these pseudo data.\nGu et al. (2019) introduces back translation into MNMT,\nwhile we adopt it in our transfer approaches. Finally, our\nbest MLM+BRLM-SA with back translation outperforms\npivoting mby 2.4 BLEU points averagely, and outperforms\nMNMT (Gu et al. 2019) by 4.6 BLEU points averagely.\nAgain, in supervised translation directions, MLM+BRLM-", "start_char_idx": 2181, "end_char_idx": 3212, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77e6e8bc-49a6-484f-a81a-b35b2521e6e2": {"__data__": {"id_": "77e6e8bc-49a6-484f-a81a-b35b2521e6e2", "embedding": null, "metadata": {"page_label": "7", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4946db25-fac9-4260-bd32-94c4b9c5b838", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "ff0252b6919e759383260c4a34c0a483932b2f03c6dad60b954a19c896291e6c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17aa8243-b803-4f95-90da-a27d33ba8ebe", "node_type": "1", "metadata": {"page_label": "6", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "e7b1cb22bd65d605e0b342e4b19241d963c62ed7ecc8a6a3ab84e430f6cedb41", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c81cb8d8-b0ce-46c4-8b19-7e451e36370c", "node_type": "1", "metadata": {}, "hash": "456f5a7ee67a00825207b0492bbbc2250b168e9b6fab8fd2cbe759dccf660e2d", "class_name": "RelatedNodeInfo"}}, "text": "SA with back translation also achieves better performance\nthan the original supervised Transformer.\nAnalysis\nSentence Representation. We \ufb01rst evaluate the represen-\ntational invariance across languages for all cross-lingual pre-\ntraining methods. Following Arivazhagan et al. (2018), we\nadopt max-pooling operation to collect the sentence rep-\nresentation of each encoder layer for all source-pivot sen-\ntence pairs in the Europarl validation sets. Then we calcu-\nlate the cosine similarity for each sentence pair and aver-\nage all cosine scores. As shown in Figure 3, we can ob-\nserve that, MLM+BRLM-SA has the most stable and similar\ncross-lingual representations of sentence pairs on all layers,\nwhile it achieves the best performance in zero-shot transla-\ntion. This demonstrates that better cross-lingual representa-\ntions can bene\ufb01t for the process of transfer learning. Besides,\nMLM+BRLM-HA is not as superior as MLM+BRLM-\nSA and even worse than MLM+TLM on Fr-En, since\nMLM+BRLM-HA may suffer from the wrong alignment\nknowledge from an external aligner tool. We also \ufb01nd an in-\nteresting phenomenon that as the number of layers increases,\nthe cosine similarity decreases.\nContextualized Word Representation. We further sam-\nple an English-Russian sentence pair from the MultiUN\nvalidation sets and visualize the cosine similarity between\nhidden states of the top encoder layer to further investi-\ngate the difference of all cross-lingual pre-training meth-\nods. As shown in Figure 4, the hidden states generated by\nMLM+BRLM-SA have higher similarity for two aligned\nwords. It indicates that MLM+BRLM-SA can gain bet-\nter word-level representation alignment between source and\npivot languages, which better relieves the burden of the do-\nmain shift problem .\nThe Effect of Freezing Parameters. To freeze parame-\nters is a common strategy to avoid catastrophic forgetting in\ntransfer learning (Howard and Ruder 2018). Table 4 shows\nthe performance of transfer learning with freezing different\nlayers on MultiUN test set, in which En \u2192Ru denotes the\nparent model, Ar\u2192Ru and Es\u2192Ru are two child models,\nand all models are based on MLM+BRLM-SA. We can \ufb01nd\nthat updating all parameters during training will cause a no-\ntable drop on the zero-shot direction due to the catastrophic\nforgetting. On the contrary, freezing all the parameters leads\nto the decline on supervised direction because the language\nfeatures extracted during pre-training is not suf\ufb01cient for\nMT task. Freezing the \ufb01rst four layers of the transformer\nshows the best performance and keeps the balance between\npre-training and \ufb01ne-tuning.\nConclusion\nIn this paper, we propose a cross-lingual pretraining based\ntransfer approach for the challenging zero-shot translation\ntask, in which source and target languages have no parallel\ndata, while they both have parallel data with a high resource\n(a) MLM\n (b) MLM+TLM\n(c) MLM+BRLM-HA\n (d) MLM+BRLM-SA\nFigure 4: Cosine similarity visualization at word level given\nan English-Russian sentence pair from the MultiUN valida-\ntion sets. Brighter indicates higher similarity.\nFreezing Layers En\u2192Ru Ar\u2192Ru Es\u2192Ru\nNone 37.80 16.09 19.80\n2 37.79 21.47 28.35\n4 37.55 25.49 30.47\n6 35.31 22.90 28.22\nTable 4: BLEU score of freezing different layers. The num-\nber in Freezing Layers column denotes that the number of\nencoder layers will not be updated.\npivot language. With the aim of building the language in-\nvariant representation between source and pivot languages\nfor smooth transfer of the parent model of pivot \u2192target di-\nrection to the child model of source \u2192target direction, we in-\ntroduce one monolingual pretraining method and two bilin-\ngual pretraining methods to construct an universal encoder\nfor the source and pivot languages. Experiments on public\ndatasets show that our approaches signi\ufb01cantly outperforms\nseveral strong baseline systems, and manifest the language\ninvariance characteristics in both sentence level and word\nlevel neural representations.\nAcknowledgments\nWe would like to thank the anonymous reviewers for the\nhelpful comments. This work was supported by National\nKey R&D Program of China (Grant No. 2016YFE0132100),\nNational Natural Science Foundation of China (Grant No.", "start_char_idx": 0, "end_char_idx": 4209, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c81cb8d8-b0ce-46c4-8b19-7e451e36370c": {"__data__": {"id_": "c81cb8d8-b0ce-46c4-8b19-7e451e36370c", "embedding": null, "metadata": {"page_label": "7", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4946db25-fac9-4260-bd32-94c4b9c5b838", "node_type": "4", "metadata": {"page_label": "7", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "ff0252b6919e759383260c4a34c0a483932b2f03c6dad60b954a19c896291e6c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77e6e8bc-49a6-484f-a81a-b35b2521e6e2", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a4ac54777fb5e935c4f8a8d9a558770ab1e92d2864b8e0d6a8a70427bd470e14", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7c296753-c436-4f67-80c9-b316fdbd5e3a", "node_type": "1", "metadata": {}, "hash": "0b245b4d9e7a961e896d448e7399bee8bf46cbb74f667f77b6b4c2e50ac0cb40", "class_name": "RelatedNodeInfo"}}, "text": "pivot language. With the aim of building the language in-\nvariant representation between source and pivot languages\nfor smooth transfer of the parent model of pivot \u2192target di-\nrection to the child model of source \u2192target direction, we in-\ntroduce one monolingual pretraining method and two bilin-\ngual pretraining methods to construct an universal encoder\nfor the source and pivot languages. Experiments on public\ndatasets show that our approaches signi\ufb01cantly outperforms\nseveral strong baseline systems, and manifest the language\ninvariance characteristics in both sentence level and word\nlevel neural representations.\nAcknowledgments\nWe would like to thank the anonymous reviewers for the\nhelpful comments. This work was supported by National\nKey R&D Program of China (Grant No. 2016YFE0132100),\nNational Natural Science Foundation of China (Grant No.\n61525205, 61673289). This work was also partially sup-\nported by Alibaba Group through Alibaba Innovative Re-\nsearch Program and the Priority Academic Program Devel-\nopment (PAPD) of Jiangsu Higher Education Institutions.", "start_char_idx": 3354, "end_char_idx": 4431, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7c296753-c436-4f67-80c9-b316fdbd5e3a": {"__data__": {"id_": "7c296753-c436-4f67-80c9-b316fdbd5e3a", "embedding": null, "metadata": {"page_label": "8", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc2578e5-f9ca-4125-931f-681cf3b1566a", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "e2e638b41fdc7f3a1079ac1f03bb1b3d199fbc3bcbf94c7577de43482bb26eb0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c81cb8d8-b0ce-46c4-8b19-7e451e36370c", "node_type": "1", "metadata": {"page_label": "7", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "b55ce29140e215b62d373b9ced31738be9eaab8ed4cfbc9a792f88109a25bf90", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc53e336-b57f-4a8d-b408-cd941cdd8ccc", "node_type": "1", "metadata": {}, "hash": "e9ccce4db66f69d94197b7a09b33899ce0bf190097b4f5eca3dab7424ac07ff1", "class_name": "RelatedNodeInfo"}}, "text": "References\nAharoni, R.; Johnson, M.; and Firat, O. 2019. Massively\nmultilingual neural machine translation. In NAACL-HLT .\nAl-Shedivat, M., and Parikh, A. P. 2019. Consistency\nby agreement in zero-shot neural machine translation. In\nNAACL-HLT .\nArivazhagan, N.; Bapna, A.; Firat, O.; Aharoni, R.; Johnson,\nM.; and Macherey, W. 2018. The missing ingredient in zero-\nshot neural machine translation. ArXiv abs/1903.07091.\nArtetxe, M.; Labaka, G.; Agirre, E.; and Cho, K.\n2017. Unsupervised neural machine translation. ArXiv\nabs/1710.11041.\nChen, Y .; Liu, Y . P.; Cheng, Y .; and Li, V . O. K. 2017. A\nteacher-student framework for zero-resource neural machine\ntranslation. In ACL.\nde Gispert, A., and Mari\u00f1o, J. B. 2006. Catalan-english sta-\ntistical machine translation without parallel corpus : Bridg-\ning through spanish.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. In NAACL-HLT .\nDyer, C.; Chahuneau, V .; and Smith, N. A. 2013. A simple,\nfast, and effective reparameterization of ibm model 2. In\nHLT-NAACL .\nEisele, A., and Chen, Y . 2010. Multiun: A multilingual\ncorpus from united nation documents. In LREC .\nFirat, O.; Sankaran, B.; Al-Onaizan, Y .; Yarman-Vural, F. T.;\nand Cho, K. 2016. Zero-resource translation with multi-\nlingual neural machine translation. In EMNLP .\nFirat, O.; Cho, K.; and Bengio, Y . 2016. Multi-way, mul-\ntilingual neural machine translation with a shared attention\nmechanism. In HLT-NAACL .\nFu, Y .; Hospedales, T. M.; Xiang, T. Y .; and Gong, S.\n2015. Transductive multi-view zero-shot learning. IEEE\nTransactions on Pattern Analysis and Machine Intelligence\n37:2332\u20132345.\nGu, J.; Wang, Y .; Cho, K.; and Li, V . O. K. 2019. Improved\nzero-shot neural machine translation via ignoring spurious\ncorrelations. In ACL.\nHassan, H.; Aue, A.; Chen, C.; Chowdhary, V .; Clark, J. R.;\nFedermann, C.; Huang, X.; Junczys-Dowmunt, M.; Lewis,\nW.; Li, M.; Liu, S.; Liu, T. M.; Luo, R.; Menezes, A.; Qin, T.;\nSeide, F.; Tan, X.; Tian, F.; Wu, L.; Wu, S.; Xia, Y .; Zhang,\nD.; Zhang, Z.; and Zhou, M. 2018. Achieving human par-\nity on automatic chinese to english news translation. ArXiv\nabs/1803.05567.\nHoward, J., and Ruder, S. 2018. Universal language model\n\ufb01ne-tuning for text classi\ufb01cation. In ACL.\nHuang, H.; Liang, Y .; Duan, N.; Gong, M.; Shou, L.; Jiang,\nD.; and Zhou, M. 2019. Unicoder: A universal language\nencoder by pre-training with multiple cross-lingual tasks.\nArXiv abs/1909.00964.\nJohnson, M.; Schuster, M.; Le, Q. V .; Krikun, M.; Wu, Y .;\nChen, Z.; Thorat, N.; Vi\u00e9gas, F. B.; Wattenberg, M.; Cor-\nrado, G. S.; Hughes, M.; and Dean, J. 2016. Google\u2019s mul-tilingual neural machine translation system: Enabling zero-\nshot translation. Transactions of the Association for Com-\nputational Linguistics 5:339\u2013351.", "start_char_idx": 0, "end_char_idx": 2845, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc53e336-b57f-4a8d-b408-cd941cdd8ccc": {"__data__": {"id_": "fc53e336-b57f-4a8d-b408-cd941cdd8ccc", "embedding": null, "metadata": {"page_label": "8", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc2578e5-f9ca-4125-931f-681cf3b1566a", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "e2e638b41fdc7f3a1079ac1f03bb1b3d199fbc3bcbf94c7577de43482bb26eb0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7c296753-c436-4f67-80c9-b316fdbd5e3a", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "480d2aa577e70124cff93bfa297f8877c0611862fb2c2a0ee617c5e3f9625813", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1f4b5d80-2026-4b65-b044-571b7c975c31", "node_type": "1", "metadata": {}, "hash": "0bb47b65f68c17e8d69a3aed90b37402b2f98b3a7ec9418272c3306c39a98452", "class_name": "RelatedNodeInfo"}}, "text": "In ACL.\nHuang, H.; Liang, Y .; Duan, N.; Gong, M.; Shou, L.; Jiang,\nD.; and Zhou, M. 2019. Unicoder: A universal language\nencoder by pre-training with multiple cross-lingual tasks.\nArXiv abs/1909.00964.\nJohnson, M.; Schuster, M.; Le, Q. V .; Krikun, M.; Wu, Y .;\nChen, Z.; Thorat, N.; Vi\u00e9gas, F. B.; Wattenberg, M.; Cor-\nrado, G. S.; Hughes, M.; and Dean, J. 2016. Google\u2019s mul-tilingual neural machine translation system: Enabling zero-\nshot translation. Transactions of the Association for Com-\nputational Linguistics 5:339\u2013351.\nKauers, M.; V ogel, S.; F\u00fcgen, C.; and Waibel, A. H. 2002.\nInterlingua based statistical machine translation. In INTER-\nSPEECH .\nKim, Y .; Petrov, P.; Petrushkov, P.; Khadivi, S.; and Ney,\nH. 2019. Pivot-based transfer learning for neural ma-\nchine translation between non-english languages. ArXiv\nabs/1909.09524.\nKim, Y .; Gao, Y .; and Ney, H. 2019. Effective cross-lingual\ntransfer of neural machine translation models without shared\nvocabularies. In ACL.\nKocmi, T., and Bojar, O. 2018. Trivial transfer learning for\nlow-resource neural machine translation. In WMT .\nKoehn, P., and Knowles, R. 2017. Six challenges for neural\nmachine translation. In NMT@ACL .\nKoehn, P. 2005. Europarl: A parallel corpus for statistical\nmachine translation.\nLample, G., and Conneau, A. 2019. Cross-lingual language\nmodel pretraining. ArXiv abs/1901.07291.\nLample, G.; Ott, M.; Conneau, A.; Denoyer, L.; and Ran-\nzato, M. 2018. Phrase-based & neural unsupervised ma-\nchine translation. In EMNLP .\nNguyen, T. Q., and Chiang, D. 2017. Transfer learning\nacross low-resource, related languages for neural machine\ntranslation. In IJCNLP .\nPires, T.; Schlinger, E.; and Garrette, D. 2019. How multi-\nlingual is multilingual bert? In ACL.\nRen, S.; Zhang, Z.; Liu, S.; Zhou, M.; and Ma, S. 2019. Un-\nsupervised neural machine translation with smt as posterior\nregularization. In AAAI .\nSennrich, R.; Haddow, B.; and Birch, A. 2015. Neural ma-\nchine translation of rare words with subword units. In ACL.\nUtiyama, M., and Isahara, H. 2007. A comparison of pivot\nmethods for phrase-based statistical machine translation. In\nHLT-NAACL .\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is all you need. In NIPS .\nWu, Y .; Schuster, M.; Chen, Z.; Le, Q. V .; Norouzi, M.;\nMacherey, W.; Krikun, M.; Cao, Y .; Gao, Q.; Macherey, K.;\net al. 2016. Google\u2019s neural machine translation system:\nBridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144 .\nZheng, H.; Cheng, Y .; and Liu, Y . P. 2017. Maximum ex-\npected likelihood estimation for zero-resource neural ma-\nchine translation. In IJCAI .\nZhu, X.; He, Z.; Wu, H.; Wang, H.; Zhu, C.; and Zhao, T.\n2013. Improving pivot-based statistical machine translation\nusing random walk. In EMNLP .\nZoph, B.; Yuret, D.; May, J.; and Knight, K. 2016.", "start_char_idx": 2315, "end_char_idx": 5216, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1f4b5d80-2026-4b65-b044-571b7c975c31": {"__data__": {"id_": "1f4b5d80-2026-4b65-b044-571b7c975c31", "embedding": null, "metadata": {"page_label": "8", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc2578e5-f9ca-4125-931f-681cf3b1566a", "node_type": "4", "metadata": {"page_label": "8", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "e2e638b41fdc7f3a1079ac1f03bb1b3d199fbc3bcbf94c7577de43482bb26eb0", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc53e336-b57f-4a8d-b408-cd941cdd8ccc", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "1155fa6c3b593c1cf8dc228e2b7289a73102fd093217bf129787c95a34a92614", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e380dea2-bfc0-4b13-9434-8a0d9b58cae7", "node_type": "1", "metadata": {}, "hash": "d242ece40374d12663ee5735160761e6d80d6af00f1e1e7d404c73110ea27b11", "class_name": "RelatedNodeInfo"}}, "text": "; Norouzi, M.;\nMacherey, W.; Krikun, M.; Cao, Y .; Gao, Q.; Macherey, K.;\net al. 2016. Google\u2019s neural machine translation system:\nBridging the gap between human and machine translation.\narXiv preprint arXiv:1609.08144 .\nZheng, H.; Cheng, Y .; and Liu, Y . P. 2017. Maximum ex-\npected likelihood estimation for zero-resource neural ma-\nchine translation. In IJCAI .\nZhu, X.; He, Z.; Wu, H.; Wang, H.; Zhu, C.; and Zhao, T.\n2013. Improving pivot-based statistical machine translation\nusing random walk. In EMNLP .\nZoph, B.; Yuret, D.; May, J.; and Knight, K. 2016. Trans-\nfer learning for low-resource neural machine translation. In\nEMNLP .", "start_char_idx": 4653, "end_char_idx": 5292, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e380dea2-bfc0-4b13-9434-8a0d9b58cae7": {"__data__": {"id_": "e380dea2-bfc0-4b13-9434-8a0d9b58cae7", "embedding": null, "metadata": {"page_label": "1", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e45afb9f-6fed-477f-bcfa-d877f32c9658", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "421d142c686209b3a39328ece0b86502bcfb72060b0a893160d571a15772ef79", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1f4b5d80-2026-4b65-b044-571b7c975c31", "node_type": "1", "metadata": {"page_label": "8", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "7079cef39ac8ee7135db9b3e3d2a0f08814cf27d05dc3b03d56df085f7475b54", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ffd272f7-7798-4852-b1b6-ae3b1b964370", "node_type": "1", "metadata": {}, "hash": "329e5f8396a2b4cb6d7e8e35da98a9024dce19cd8e8b1cc9e134b5086ff96872", "class_name": "RelatedNodeInfo"}}, "text": "Sensitive Data Detection and Classi\ufb01cation in Spanish Clinical Text:\nExperiments with BERT\nAitor Garc \u00b4\u0131a-Pablos, Naiara Perez, Montse Cuadros\nSNLT group at Vicomtech Foundation, Basque Research and Technology Alliance (BRTA)\nDonostia/San-Sebasti \u00b4an, 20009, Spain\n{agarciap, nperez, mcuadros }@vicomtech.org\nAbstract\nMassive digital data processing provides a wide range of opportunities and bene\ufb01ts, but at the cost of endangering personal data privacy.\nAnonymisation consists in removing or replacing sensitive information from data, enabling its exploitation for different purposes\nwhile preserving the privacy of individuals. Over the years, a lot of automatic anonymisation systems have been proposed; however,\ndepending on the type of data, the target language or the availability of training documents, the task remains challenging still. The\nemergence of novel deep-learning models during the last two years has brought large improvements to the state of the art in the \ufb01eld\nof Natural Language Processing. These advancements have been most noticeably led by BERT, a model proposed by Google in 2018,\nand the shared language models pre-trained on millions of documents. In this paper, we use a BERT-based sequence labelling model to\nconduct a series of anonymisation experiments on several clinical datasets in Spanish. We also compare BERT to other algorithms. The\nexperiments show that a simple BERT-based model with general-domain pre-training obtains highly competitive results without any\ndomain speci\ufb01c feature engineering.\nKeywords: Anonymisation, De-identi\ufb01cation, PHI, Clinical Data, BERT\n1. Introduction\nDuring the \ufb01rst two decades of the 21st century, the sharing\nand processing of vast amounts of data has become per-\nvasive. This expansion of data sharing and processing ca-\npabilities is both a blessing and a curse. Data helps build\nbetter information systems for the digital era and enables\nfurther research for advanced data management that ben-\ne\ufb01ts the society in general. But the use of this very data\ncontaining sensitive information con\ufb02icts with private data\nprotection, both from an ethical and a legal perspective.\nThere are several application domains on which this situ-\nation is particularly acute. This is the case of the medical\ndomain (Abouelmehdi et al., 2018). There are plenty of po-\ntential applications for advanced medical data management\nthat can only be researched and developed using real data;\nyet, the use of medical data is severely limited \u2013when not\nentirely prohibited\u2013 due to data privacy protection policies.\nOne way of circumventing this problem is to anonymise\nthe data by removing, replacing or obfuscating the personal\ninformation mentioned, as exempli\ufb01ed in Table 1. This task\ncan be done by hand, having people read and anonymise\nthe documents one by one. Despite being a reliable and\nsimple solution, this approach is tedious, expensive, time\nconsuming and dif\ufb01cult to scale to the potentially thousands\nor millions of documents that need to be anonymised.\nFor this reason, numerous of systems and approaches have\nbeen developed during the last decades to attempt to auto-\nmate the anonymisation of sensitive content, starting with\nthe automatic detection and classi\ufb01cation of sensitive infor-\nmation. Some of these systems rely on rules, patterns and\ndictionaries, while others use more advanced techniques re-\nlated to machine learning and, more recently, deep learning.\nGiven that this paper is concerned with text documents\n(e.g. medical records), the involved techniques are related\nto Natural Language Processing (NLP). When using NLP\napproaches, it is common to pose the problem of documentanonymisation as a sequence labelling problem, i.e. clas-\nsifying each token within a sequence as being sensitive in-\nformation or not. Further, depending on the objective of\nthe anonymisation task, it is also important to determine\nthe type of sensitive information (names of individuals, ad-\ndresses, age, sex, etc.).\nThe anonymisation systems based on NLP techniques per-\nform reasonably well, but are far from perfect. Depend-\ning on the dif\ufb01culty posed by each dataset or the amount\nof available data for training machine learning models,\nthe performance achieved by these methods is not enough\nto fully rely on them in certain situations (Abouelmehdi\net al., 2018).", "start_char_idx": 0, "end_char_idx": 4331, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ffd272f7-7798-4852-b1b6-ae3b1b964370": {"__data__": {"id_": "ffd272f7-7798-4852-b1b6-ae3b1b964370", "embedding": null, "metadata": {"page_label": "1", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e45afb9f-6fed-477f-bcfa-d877f32c9658", "node_type": "4", "metadata": {"page_label": "1", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "421d142c686209b3a39328ece0b86502bcfb72060b0a893160d571a15772ef79", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e380dea2-bfc0-4b13-9434-8a0d9b58cae7", "node_type": "1", "metadata": {"page_label": "1", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "0bbb8a8650c7d8f15850fa5fa5b5fb9e6584180ef5aab55e61e8177464969df2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "15d5497e-73c5-4354-aeee-72b97c7f914e", "node_type": "1", "metadata": {}, "hash": "5690ab0631740b97e438146c9c7f466ea93d86482808211b88f55d95425bfe1d", "class_name": "RelatedNodeInfo"}}, "text": "medical records), the involved techniques are related\nto Natural Language Processing (NLP). When using NLP\napproaches, it is common to pose the problem of documentanonymisation as a sequence labelling problem, i.e. clas-\nsifying each token within a sequence as being sensitive in-\nformation or not. Further, depending on the objective of\nthe anonymisation task, it is also important to determine\nthe type of sensitive information (names of individuals, ad-\ndresses, age, sex, etc.).\nThe anonymisation systems based on NLP techniques per-\nform reasonably well, but are far from perfect. Depend-\ning on the dif\ufb01culty posed by each dataset or the amount\nof available data for training machine learning models,\nthe performance achieved by these methods is not enough\nto fully rely on them in certain situations (Abouelmehdi\net al., 2018). However, in the last two years, the NLP\ncommunity has reached an important milestone thanks to\nthe appearance of the so-called Transformers neural net-\nwork architectures (Wolf et al., 2019). In this paper, we\nconduct several experiments in sensitive information de-\ntection and classi\ufb01cation on Spanish clinical text using\nBERT (from \u2018Bidirectional Encoder Representations from\nTransformers\u2019) (Devlin et al., 2019) as the base for a se-\nquence labelling approach. The experiments are carried\nout on two datasets: the MEDDOCAN: Medical Document\nAnonymization shared task dataset (Marimon et al., 2019),\nand NUB ES(Lima et al., 2019), a corpus of real medical\nreports in Spanish. In these experiments, we compare the\nperformance of BERT with other machine-learning-based\nsystems, some of which use language-speci\ufb01c features. Our\naim is to evaluate how good a BERT-based model performs\nwithout language nor domain specialisation apart from the\ntraining data labelled for the task at hand.\nThe rest of the paper is structured as follows: the next sec-\ntion describes related work about data anonymisation in\ngeneral and clinical data anonymisation in particular; it also\nprovides a more detailed explanation and background about\nthe Transformers architecture and BERT. Section 3. de-\nscribes the data involved in the experiments and the systemsarXiv:2003.03106v2  [cs.CL]  17 Mar 2020", "start_char_idx": 3497, "end_char_idx": 5713, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "15d5497e-73c5-4354-aeee-72b97c7f914e": {"__data__": {"id_": "15d5497e-73c5-4354-aeee-72b97c7f914e", "embedding": null, "metadata": {"page_label": "2", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1229b78-9125-4cc5-b4e7-5dd9d13a6e19", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "bf69af0d33bd6f6672acc8b028892d3cc6813c669cfa41293b83d73779537537", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ffd272f7-7798-4852-b1b6-ae3b1b964370", "node_type": "1", "metadata": {"page_label": "1", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "65a62cdad24355903a7495641160f2f4d07086a429b70c406962ac773227eca4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2486c76a-fefa-4b3a-90ba-de82faf2a2d1", "node_type": "1", "metadata": {}, "hash": "ab4ade43e483adf461fde37f0ab1ee810773b91b4620bff91b31c1e395427dcf", "class_name": "RelatedNodeInfo"}}, "text": "original Paciente de 64 a \u02dcnosoperado de una hernia el 12/01/2016 por la Dra Lopez\nexample 1 Paciente de XXXXXXX operado de una hernia el XXXXXXXXXX por XXXXXXXXXXXX\nexample 2 Paciente de [-AGE-] operado de una hernia el [--DATE--] por [--DOCTOR--]\nexample 3 Paciente de 59 a \u02dcnosoperado de una hernia el 05/06/2019 por el Dr Sancho\nTable 1: Anonymization examples of \u201c64-year-old patient operated on a hernia on the 12/01/2016 by Dr Lopez\u201d; sensitive\ndata and their substitutions are highlighted in bold.\nevaluated in this paper, including the BERT-based system;\n\ufb01nally, it details the experimental design. Section 4. intro-\nduces the results for each set of experiments. Finally, Sec-\ntion 5. contains the conclusions and future lines of work.\n2. Related Work\nThe state of the art in the \ufb01eld of Natural Language Pro-\ncessing (NLP) has reached an important milestone in the\nlast couple of years thanks to deep-learning architectures,\nincreasing in several points the performance of new models\nfor almost any text processing task.\nThe major change started with the Transformers model pro-\nposed by Vaswani et al. (2017). It substituted the widely\nused recurrent and convolutional neural network architec-\ntures by another approach based solely on self-attention,\nobtaining an impressive performance gain. The original\nproposal was focused on an encoder-decoder architecture\nfor machine translation, but soon the use of Transformers\nwas made more general (Wolf et al., 2019). There are sev-\neral other popular models that use Transformers, such as\nOpen AI\u2019s GPT and GPT2 (Radford et al., 2019), RoBERTa\n(Liu et al., 2019) and the most recent XLNet (Yang et al.,\n2019); still, BERT (Devlin et al., 2019) is one of the most\nwidespread Transformer-based models.\nBERT trains its unsupervised language model using a\nMasked Language Model and Next Sentence Prediction.\nA common problem in NLP is the lack of enough training\ndata. BERT can be pre-trained to learn general or speci\ufb01c\nlanguage models using very large amounts of unlabelled\ntext (e.g. web content, Wikipedia, etc.), and this knowl-\nedge can be transferred to a different downstream task in a\nprocess that receives the name \ufb01ne-tuning .\nDevlin et al. (2019) have used \ufb01ne-tuning to achieve state-\nof-the-art results on a wide variety of challenging natural\nlanguage tasks, such as text classi\ufb01cation, Question An-\nswering (QA) and Named Entity Recognition and Classi-\n\ufb01cation (NERC). BERT has also been used successfully by\nother community practitioners for a wide range of NLP-\nrelated tasks (Liu and Lapata, 2019; Nogueira and Cho,\n2019, among others).\nRegarding the task of data anonymisation in particular,\nanonymisation systems may follow different approaches\nand pursue different objectives (Cormode and Srivastava,\n2009). The \ufb01rst objective of these systems is to detect\nand classify the sensitive information contained in the doc-\numents to be anonymised. In order to achieve that, they\nuse rule-based approaches, Machine Learning (ML) ap-\nproaches, or a combination of both.\nAlthough most of these efforts are for English texts \u2013\nsee, among others, the i2b2 de-identi\ufb01cation challenges(Uzuner et al., 2007; Stubbs et al., 2015), Dernoncourt et\nal. (2016), or Khin et al. (2018)\u2013, other languages are also\nattracting growing interest. Some examples are Mamede\net al. (2016) for Portuguese and Tveit et al. (2004) for\nNorwegian. With respect to the anonymisation of text writ-\nten in Spanish, recent studies include Medina and Turmo\n(2018), Hassan et al. (2018) and Garc \u00b4\u0131a-Sardi \u02dcna (2018).\nMost notably, in 2019 the \ufb01rst community challenge about\nanonymisation of medical documents in Spanish, MED-\nDOCAN1(Marimon et al., 2019), was held as part of\nthe IberLEF initiative.", "start_char_idx": 0, "end_char_idx": 3738, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2486c76a-fefa-4b3a-90ba-de82faf2a2d1": {"__data__": {"id_": "2486c76a-fefa-4b3a-90ba-de82faf2a2d1", "embedding": null, "metadata": {"page_label": "2", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a1229b78-9125-4cc5-b4e7-5dd9d13a6e19", "node_type": "4", "metadata": {"page_label": "2", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "bf69af0d33bd6f6672acc8b028892d3cc6813c669cfa41293b83d73779537537", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "15d5497e-73c5-4354-aeee-72b97c7f914e", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "6719650ff280912f6c4f6849f30419dc7ce0bea971339787a07525279c1ec5db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "98337baa-5a3b-47bb-80db-947a190218c3", "node_type": "1", "metadata": {}, "hash": "1b9c8eee4e5bf5f9448b7ecb4deaa8123251b281c762214f27a698436d060794", "class_name": "RelatedNodeInfo"}}, "text": "(2016), or Khin et al. (2018)\u2013, other languages are also\nattracting growing interest. Some examples are Mamede\net al. (2016) for Portuguese and Tveit et al. (2004) for\nNorwegian. With respect to the anonymisation of text writ-\nten in Spanish, recent studies include Medina and Turmo\n(2018), Hassan et al. (2018) and Garc \u00b4\u0131a-Sardi \u02dcna (2018).\nMost notably, in 2019 the \ufb01rst community challenge about\nanonymisation of medical documents in Spanish, MED-\nDOCAN1(Marimon et al., 2019), was held as part of\nthe IberLEF initiative. The winners of the challenge \u2013the\nNeither-Language-nor-Domain-Experts (NLNDE) (Lange\net al., 2019)\u2013 achieved F1-scores as high as 0.975 in the\ntask of sensitive information detection and categorisation\nby using recurrent neural networks with Conditional Ran-\ndom Field (CRF) output layers.\nAt the same challenge, Mao and Liu (2019) occupied the\n8thposition among 18 participants using BERT. According\nto the description of the system, the authors used BERT-\nBase Multilingual Cased and an output CRF layer. How-\never, their system is \u223c3 F1-score points below our imple-\nmentation without the CRF layer.\n3. Materials and Methods\nThe aim of this paper is to evaluate BERT\u2019s multilingual\nmodel and compare it to other established machine-learning\nalgorithms in a speci\ufb01c task: sensitive data detection and\nclassi\ufb01cation in Spanish clinical free text. This section de-\nscribes the data involved in the experiments and the systems\nevaluated. Finally, we introduce the experimental setup.\n3.1. Data\nTwo datasets are exploited in this article. Both datasets\nconsist of plain text containing clinical narrative written in\nSpanish, and their respective manual annotations of sensi-\ntive information in BRAT (Stenetorp et al., 2012) standoff\nformat2. In order to feed the data to the different algorithms\npresented in Section 3.2., these datasets were transformed\nto comply with the commonly used BIO sequence repre-\nsentation scheme (Ramshaw and Marcus, 1999).\n3.1.1. NUB ES-PHI\nNUB ES(Lima et al., 2019) is a corpus of around 7,000 real\nmedical reports written in Spanish and annotated with nega-\ntion and uncertainty information. Before being published,\nsensitive information had to be manually annotated and re-\nplaced for the corpus to be safely shared. In this article,\n1http://temu.bsc.es/meddocan/\n2https://brat.nlplab.org/standoff.html", "start_char_idx": 3213, "end_char_idx": 5573, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "98337baa-5a3b-47bb-80db-947a190218c3": {"__data__": {"id_": "98337baa-5a3b-47bb-80db-947a190218c3", "embedding": null, "metadata": {"page_label": "3", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7f4886d-fc40-4339-9763-26ef2704bbc6", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a14f5e8e43bb9de9818d287519fec06002174ce35edeff8660314aaf95f3ad73", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2486c76a-fefa-4b3a-90ba-de82faf2a2d1", "node_type": "1", "metadata": {"page_label": "2", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "346916da036986dfec1b823cb7549ffc16f0e014f7889376d961a6db3962efb9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "17e5fbd5-a8aa-4d50-afec-90cce86ac83b", "node_type": "1", "metadata": {}, "hash": "b2ea5221178c920269bc4c76e2b9aacb1d2fcb299b69f8172af903d746119400", "class_name": "RelatedNodeInfo"}}, "text": "we work with the NUB ESversion prior to its anonymisa-\ntion, that is, with the manual annotations of sensitive in-\nformation. It follows that the version we work with is not\npublicly available and, due to contractual restrictions, we\ncannot reveal the provenance of the data. In order to avoid\nconfusion between the two corpus versions, we henceforth\nrefer to the version relevant in this paper as NUB ES-PHI\n(from \u2018NUB ESwith Personal Health Information\u2019).\nNUB ES-PHI consists of 32,055 sentences annotated for 11\ndifferent sensitive information categories. Overall, it con-\ntains 7,818 annotations. The corpus has been randomly\nsplit into train (72%), development (8%) and test (20%) sets\nto conduct the experiments described in this paper. The size\nof each split and the distribution of the annotations can be\nconsulted in Tables 2 and 3, respectively.\ntrain dev test\n# sentences 23,079 2,565 6,411\n# tokens 379,401 41,936 107,024\nvocabulary 25,304 7,483 12,750\n# annotations 5,562 677 1,579\nTable 2: Size of the NUB ES-PHI corpus\nThe majority of sensitive information in NUB ES-PHI are\ntemporal expressions (\u2018Date\u2019 and \u2018Time\u2019), followed by\nhealthcare facility mentions (\u2018Hospital\u2019), and the age of the\npatient. Mentions of people are not that frequent, with\nphysician names (\u2018Doctor\u2019) occurring much more often\nthan patient names (\u2018Patient\u2019). The least frequent sensitive\ninformation types, which account for \u223c10% of the remain-\ning annotations, consist of the patient\u2019s sex, job, and kin-\nship, and locations other than healthcare facilities (\u2018Loca-\ntion\u2019). Finally, the tag \u2018Other\u2019 includes, for instance, men-\ntions to institutions unrelated to healthcare and whether the\npatient is right- or left-handed. It occurs just 36 times.\ntrain dev test\n# % # % # %\nDate 2,165 39 251 37 660 41\nHospital 1,012 18 105 16 275 17\nAge 701 13 133 20 200 13\nTime 608 11 63 9 155 10\nDoctor 486 9 44 6 134 8\nSex 270 5 35 5 71 4\nKinship 158 3 20 3 44 3\nLocation 71 1 10 1 19 1\nPatient 48 1 5 1 11 1\nJob 31 1 3 0 9 1\nOther 12 0 8 1 16 1\nTotal 5,562 100 677 100 1,579 100\nTable 3: Label distribution in the NUB ES-PHI corpus\n3.1.2. The MEDDOCAN corpus\nThe organisers of the MEDDOCAN shared task (Marimon\net al., 2019) curated a synthetic corpus of clinical cases en-\nriched with sensitive information by health documentalists.In this regard, the MEDDOCAN evaluation scenario could\nbe said to be somewhat far from the real use case the tech-\nnology developed for the shared task is supposed to be ap-\nplied in. However, at the moment it also provides the only\npublic means for a rigorous comparison between systems\nfor sensitive health information detection in Spanish texts.\nThe size of the MEDDOCAN corpus is shown in Table 4.\nCompared to NUB ES-PHI (Table 2), this corpus contains\nmore sensitive information annotations, both in absolute\nand relative terms.\ntrain dev test\n# documents 500 250 250\n# tokens 360,407 138,812 132,961\nvocabulary 26,355 15,985 15,397\n# annotations 11,333 5,801 5,661\nTable 4: Size of the MEDDOCAN corpus\nThe sensitive annotation categories considered in MED-\nDOCAN differ in part from those in NUB ES-PHI. Most\nnotably, it contains \ufb01ner-grained labels for location-related\nmentions \u2013namely, \u2018Address\u2019, \u2018Territory\u2019, and \u2018Country\u2019\u2013,\nand other sensitive information categories that we did not\nencounter in NUB ES-PHI (e.g., identi\ufb01ers, phone num-\nbers, e-mail addresses, etc.). In total, the MEDDOCAN\ncorpus has 21 sensitive information categories.", "start_char_idx": 0, "end_char_idx": 3462, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17e5fbd5-a8aa-4d50-afec-90cce86ac83b": {"__data__": {"id_": "17e5fbd5-a8aa-4d50-afec-90cce86ac83b", "embedding": null, "metadata": {"page_label": "3", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7f4886d-fc40-4339-9763-26ef2704bbc6", "node_type": "4", "metadata": {"page_label": "3", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "a14f5e8e43bb9de9818d287519fec06002174ce35edeff8660314aaf95f3ad73", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "98337baa-5a3b-47bb-80db-947a190218c3", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "355c77d6f8b7d7994a98783a8e0fe9348a389004fef160008a521f552decb103", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "12dbeb4b-9df0-4829-a88b-4154ec7c3e84", "node_type": "1", "metadata": {}, "hash": "e9956f09f31b21cc6e9f33939268e7f229d81727e5a01e5cfcb2f96d6aa6b795", "class_name": "RelatedNodeInfo"}}, "text": "train dev test\n# documents 500 250 250\n# tokens 360,407 138,812 132,961\nvocabulary 26,355 15,985 15,397\n# annotations 11,333 5,801 5,661\nTable 4: Size of the MEDDOCAN corpus\nThe sensitive annotation categories considered in MED-\nDOCAN differ in part from those in NUB ES-PHI. Most\nnotably, it contains \ufb01ner-grained labels for location-related\nmentions \u2013namely, \u2018Address\u2019, \u2018Territory\u2019, and \u2018Country\u2019\u2013,\nand other sensitive information categories that we did not\nencounter in NUB ES-PHI (e.g., identi\ufb01ers, phone num-\nbers, e-mail addresses, etc.). In total, the MEDDOCAN\ncorpus has 21 sensitive information categories. We refer\nthe reader to the organisers\u2019 article (Marimon et al., 2019)\nfor more detailed information about this corpus.\n3.2. Systems\nApart from experimenting with a pre-trained BERT model,\nwe have run experiments with other systems and base-\nlines, to compare them and obtain a better perspective about\nBERT\u2019s performance in these datasets.\n3.2.1. Baseline\nAs the simplest baseline, a sensitive data recogniser and\nclassi\ufb01er has been developed that consists of regular-\nexpressions and dictionary look-ups. For each category to\ndetect a speci\ufb01c method has been implemented. For in-\nstance, the Date, Age, Time and Doctor detectors are based\non regular-expressions; Hospital, Sex, Kinship, Location,\nPatient and Job are looked up in dictionaries. The dic-\ntionaries are hand-crafted from the training data available,\nexcept for the Patient\u2019s case, for which the possible can-\ndidates considered are the 100 most common female and\nmale names in Spain according to the Instituto Nacional de\nEstad \u00b4\u0131stica (INE; Spanish Statistical Of\ufb01ce ).\n3.2.2. CRF\nConditional Random Fields (CRF) (Lafferty et al., 2001)\nhave been extensively used for tasks of sequential nature. In\nthis paper, we propose as one of the competitive baselines\na CRF classi\ufb01er trained with sklearn-crfsuite3for Python\n3.5 and the following con\ufb01guration: algorithm = lbfgs ;\nmaximum iterations = 100; c1 = c2 = 0.1; all transitions\n=true ; optimise = false . The features extracted from\neach token are as follows:\n3https://sklearn-crfsuite.readthedocs.io", "start_char_idx": 2847, "end_char_idx": 4979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "12dbeb4b-9df0-4829-a88b-4154ec7c3e84": {"__data__": {"id_": "12dbeb4b-9df0-4829-a88b-4154ec7c3e84", "embedding": null, "metadata": {"page_label": "4", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9513c1ed-af72-4ae6-96ef-1ec019cdfb1e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "12cef3fbef748e19107b84f6753d53af66d039d9794c164da95df20dc9828a7a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "17e5fbd5-a8aa-4d50-afec-90cce86ac83b", "node_type": "1", "metadata": {"page_label": "3", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "82dfcfbf68f1a627604aa2f5dbe1521e2821db179d44e40ff9a360959f2f3787", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2308c4ca-156a-4602-bcfd-f5fde7e2ddef", "node_type": "1", "metadata": {}, "hash": "ddb188db56cdcfb0caf49efc7e5658260f7093ded914ac6684c85abdca552692", "class_name": "RelatedNodeInfo"}}, "text": "\u2013 pre\ufb01xes and suf\ufb01xes of 2 and 3 characters;\n\u2013 the length of the token in characters and the length of\nthe sentence in tokens;\n\u2013 whether the token is all-letters, a number, or a se-\nquence of punctuation marks;\n\u2013 whether the token contains the character \u2018@\u2019;\n\u2013 whether the token is the start or end of the sentence;\n\u2013 the token\u2019s casing and the ratio of uppercase charac-\nters, digits, and punctuation marks to its length;\n\u2013 and, the lemma, part-of-speech tag, and named-entity\ntag given by ixa-pipes4(Agerri et al., 2014) upon\nanalysing the sentence the token belongs to.\nNoticeably, none of the features used to train the CRF clas-\nsi\ufb01er is domain-dependent. However, the latter group of\nfeatures is language dependent.\n3.2.3. spaCy\nspaCy5is a widely used NLP library that implements state-\nof-the-art text processing pipelines, including a sequence-\nlabelling pipeline similar to the one described by Strubell\net al. (2017). spaCy offers several pre-trained models in\nSpanish, which perform basic NLP tasks such as Named\nEntity Recognition (NER). In this paper, we have trained a\nnew NER model to detect NUB ES-PHI labels. For this\npurpose, the new model uses all the labels of the train-\ning corpus coded with its context at sentence level. The\nnetwork optimisation parameters and dropout values are\nthe ones recommended in the documentation for small\ndatasets6. Finally, the model is trained using batches of\nsize 64. No more features are included, so the classi\ufb01er is\nlanguage-dependent but not domain-dependent.\n3.2.4. BERT\nAs introduced earlier, BERT has shown an outstanding\nperformance in NERC-like tasks, improving the start-of-\nthe-art results for almost every dataset and language. We\ntake the same approach here, by using the model BERT-\nBase Multilingual Cased7with a Fully Connected (FC)\nlayer on top to perform a \ufb01ne-tuning of the whole model\nfor an anonymisation task in Spanish clinical data. Our\nimplementation is built on PyTorch8and the PyTorch-\nTransformers library9(Wolf et al., 2019). The training\nphase consists in the following steps (roughly depicted in\nFigure 1):\n1.Pre-processing: since we are relying on a pre-trained\nBERT model, we must match the same con\ufb01guration\nby using a speci\ufb01c tokenisation and vocabulary. BERT\nalso needs that the inputs contains special tokens to\nsignal the beginning and the end of each sequence.\n2.Fine-tuning: the pre-processed sequence is fed into\nthe model. BERT outputs the contextual embeddings\nthat encode each of the inputted tokens. This embed-\nding representation for each token is fed into the FC\n4https://ixa2.si.ehu.es/ixa-pipes\n5https://spacy.io\n6https://spacy.io/usage/training\n7https://github.com/google-research/bert\n8https://pytorch.org\n9https://github.com/huggingface/transformers\nFigure 1: Pre-trained BERT with a Fully Connected layer\non top to perform the \ufb01ne-tuning\nlinear layer after a dropout layer (with a 0.1 dropout\nprobability), which in turn outputs the logits for each\npossible class. The cross-entropy loss function is cal-\nculated comparing the logits and the gold labels, and\nthe error is back-propagated to adjust the model pa-\nrameters.\nWe have trained the model using an AdamW optimiser\n(Loshchilov and Hutter, 2019) with the learning rate set to\n3e-5, as recommended by Devlin et al. (2019), and with\na gradient clipping of 1.0. We also applied a learning-rate\nscheduler that warms up the learning rate from zero to its\nmaximum value as the training progresses, which is also a\ncommon practice. For each experiment set proposed below,\nthe training was run with an early-stopping patience of 15\nepochs. Then, the model that performed best against the\ndevelopment set was used to produce the reported results.\nThe experiments were run on a 64-core server with operat-\ning system Ubuntu 16.04, 250GB of RAM memory, and 4\nGeForce RTX 2080 GPUs with 11GB of memory. The max-\nimum sequence length was set at 500 and the batch size at\n12.", "start_char_idx": 0, "end_char_idx": 3927, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2308c4ca-156a-4602-bcfd-f5fde7e2ddef": {"__data__": {"id_": "2308c4ca-156a-4602-bcfd-f5fde7e2ddef", "embedding": null, "metadata": {"page_label": "4", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9513c1ed-af72-4ae6-96ef-1ec019cdfb1e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "12cef3fbef748e19107b84f6753d53af66d039d9794c164da95df20dc9828a7a", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "12dbeb4b-9df0-4829-a88b-4154ec7c3e84", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "d4c597fc62c509907e5d48a2bad7c01b2ff5629425a1d8ddba81a0bad3c727b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3385d8f8-559e-4744-8876-94f649b6f829", "node_type": "1", "metadata": {}, "hash": "f021ec89c88a00c839adb65de9f6db15dd77a23eec2306140f10ed80fc465d00", "class_name": "RelatedNodeInfo"}}, "text": "(2019), and with\na gradient clipping of 1.0. We also applied a learning-rate\nscheduler that warms up the learning rate from zero to its\nmaximum value as the training progresses, which is also a\ncommon practice. For each experiment set proposed below,\nthe training was run with an early-stopping patience of 15\nepochs. Then, the model that performed best against the\ndevelopment set was used to produce the reported results.\nThe experiments were run on a 64-core server with operat-\ning system Ubuntu 16.04, 250GB of RAM memory, and 4\nGeForce RTX 2080 GPUs with 11GB of memory. The max-\nimum sequence length was set at 500 and the batch size at\n12. In this setting, each epoch \u2013a full pass through all the\ntraining data\u2013 required about 10 minutes to complete.\n3.3. Experimental design\nWe have conducted experiments with BERT in the two\ndatasets of Spanish clinical narrative presented in Section\n3.1. The \ufb01rst experiment set uses NUB ES-PHI, a corpus\nof real medical reports manually annotated with sensitive\ninformation. Because this corpus is not publicly available,\nand in order to compare the BERT-based model to other re-\nlated published systems, the second set of experiments uses\nthe MEDDOCAN 2019 shared task competition dataset.\nThe following sections provide greater detail about the two\nexperimental setups.\n3.3.1. Experiment A: NUB ES-PHI\nIn this experiment set, we evaluate all the systems presented\nin Section 3.2., namely, the rule-based baseline, the CRF", "start_char_idx": 3280, "end_char_idx": 4749, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3385d8f8-559e-4744-8876-94f649b6f829": {"__data__": {"id_": "3385d8f8-559e-4744-8876-94f649b6f829", "embedding": null, "metadata": {"page_label": "5", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "06a4455f-7516-4a87-a65f-5b202f6e1001", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "5d39b589aacaf4d6e3ad6ca3afdbae31d1f4e585832ffb07d540fbd0545eb274", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2308c4ca-156a-4602-bcfd-f5fde7e2ddef", "node_type": "1", "metadata": {"page_label": "4", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "6f1b240de13d59e8752525e3fd4b3397c4fea4e53028e9e0b41691ff23a40ea1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d819170f-c131-4bbc-894b-b8b99e3112dd", "node_type": "1", "metadata": {}, "hash": "ce9d071ca09ef145266801919b996d7634c27d66a349e60faeb155d30f509723", "class_name": "RelatedNodeInfo"}}, "text": "Detection Classi\ufb01cation (relaxed) Classi\ufb01cation (strict)\nPrec Rec F1 Prec Rec F1 Prec Rec F1\nbaseline 0.853 0.469 0.605 0.779 0.429 0.553 0.721 0.396 0.512\nCRF 0.968 0.921 0.944 0.952 0.907 0.929 0.941 0.896 0.918\nspaCy 0.964 0.938 0.951 0.942 0.852 0.895 0.942 0.852 0.895\nBERT 0.952 0.979 0.965 0.938 0.963 0.950 0.925 0.950 0.937\nTable 5: Results of Experiment A: NUB ES-PHI\nclassi\ufb01er, the spaCy entity tagger, and BERT. The evalua-\ntion comprises three scenarios of increasing dif\ufb01culty:\nDetection - Evaluates the performance of the systems at\npredicting whether each token is sensitive or non-\nsensitive; that is, the measurements only take into ac-\ncount whether a sensitive token has been recognised\nor not, regardless of the BIO label and the category as-\nsigned. This scenario shows how good a system would\nbe at obfuscating sensitive data (e.g., by replacing sen-\nsitive tokens with asterisks).\nClassi\ufb01cation (relaxed) - We measure the performance of\nthe systems at predicting the sensitive information\ntype of each token \u2013i.e., the 11 categories presented\nin Section 3.1.1. or \u2018out\u2019. Detecting entity types cor-\nrectly is important if a system is going to be used to\nreplace sensitive data by fake data of the same type\n(e.g., random people names).\nClassi\ufb01cation (strict) - This is the strictest evaluation, as\nit takes into account both the BIO label and the cate-\ngory assigned to each individual token. Being able to\ndiscern between two contiguous sensitive entities of\nthe same type is relevant not only because it is help-\nful when producing fake replacements, but because it\nalso yields more accurate statistics of the sensitive in-\nformation present in a given document collection.\nThe systems are evaluated in terms of micro-average preci-\nsion, recall and F1-score in all the scenarios.\nIn addition to the scenarios proposed, a subject worth be-\ning studied is the need of labelled data. Manually labelled\ndata is an scarce and expensive resource, which for some\napplication domains or languages is dif\ufb01cult to come by.\nIn order to obtain an estimation of the dependency of each\nsystem on the available amount of training data, we have re-\ntrained all the compared models using decreasing amounts\nof data \u2013from 100% of the available training instances to\njust 1%. The same data subsets have been used to train\nall the systems. Due to the knowledge transferred from\nthe pre-trained BERT model, the BERT-based model is ex-\npected to be more robust to data scarcity than those that\nstart their training from scratch.\n3.3.2. Experiment B: MEDDOCAN\nIn this experiment set, our BERT implementation is com-\npared to several systems that participated in the MEDDO-\nCAN challenge: a CRF classi\ufb01er (Perez et al., 2019), a\nspaCy entity recogniser (Perez et al., 2019), and NLNDE\n(Lange et al., 2019), the winner of the shared task and cur-\nrent state of the art for sensitive information detection andclassi\ufb01cation in Spanish clinical text. Speci\ufb01cally, we in-\nclude the results of a domain-independent NLNDE model\n(S2), and the results of a model enriched with domain-\nspeci\ufb01c embeddings (S3). Finally, we include the results\nobtained by Mao and Liu (2019) with a CRF output layer\non top of BERT embeddings. MEDDOCAN consists of\ntwo scenarios:\nDetection - This evaluation measures how good a system\nis at detecting sensitive text spans, regardless of the\ncategory assigned to them.\nClassi\ufb01cation - In this scenario, systems are required to\nmatch exactly not only the boundaries of each sensi-\ntive span, but also the category assigned.\nThe systems are evaluated in terms of micro-averaged pre-\ncision, recall and F-1 score.", "start_char_idx": 0, "end_char_idx": 3633, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d819170f-c131-4bbc-894b-b8b99e3112dd": {"__data__": {"id_": "d819170f-c131-4bbc-894b-b8b99e3112dd", "embedding": null, "metadata": {"page_label": "5", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "06a4455f-7516-4a87-a65f-5b202f6e1001", "node_type": "4", "metadata": {"page_label": "5", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "5d39b589aacaf4d6e3ad6ca3afdbae31d1f4e585832ffb07d540fbd0545eb274", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3385d8f8-559e-4744-8876-94f649b6f829", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "238f98ad357a76832a64ec2bfc92bbc8fdf8c9d72ca797998f8addb32705beca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1bd21ce6-da1e-4d6b-a4bd-7fe899733266", "node_type": "1", "metadata": {}, "hash": "186741169b0cc3e77fb6042e8c617716ce7c5f54962e46be1256211089c58ac1", "class_name": "RelatedNodeInfo"}}, "text": "Speci\ufb01cally, we in-\nclude the results of a domain-independent NLNDE model\n(S2), and the results of a model enriched with domain-\nspeci\ufb01c embeddings (S3). Finally, we include the results\nobtained by Mao and Liu (2019) with a CRF output layer\non top of BERT embeddings. MEDDOCAN consists of\ntwo scenarios:\nDetection - This evaluation measures how good a system\nis at detecting sensitive text spans, regardless of the\ncategory assigned to them.\nClassi\ufb01cation - In this scenario, systems are required to\nmatch exactly not only the boundaries of each sensi-\ntive span, but also the category assigned.\nThe systems are evaluated in terms of micro-averaged pre-\ncision, recall and F-1 score. Note that, in contrast to the\nevaluation in Experiment A, MEDDOCAN measurements\nare entity-based instead of tokenwise. An exhaustive expla-\nnation of the MEDDOCAN evaluation procedure is avail-\nable online10, as well as the of\ufb01cial evaluation script11,\nwhich we used to obtain the reported results.\n4. Results\nThis section describes the results obtained in the two sets\nof experiments: NUB ES-PHI and MEDDOCAN.\n4.1. Experiment A: NUB ES-PHI\nTable 5 shows the results of the conducted experiments in\nNUB ES-PHI for all the compared systems. The included\nbaseline serves to give a quick insight about how chal-\nlenging the data is. With simple regular expressions and\ngazetteers a precision of 0.853 is obtained. On the other\nhand, the recall, which directly depends on the coverage\nprovided by the rules and resources, drops to 0.469. Hence,\nthis task is unlikely to be solved without the generalisa-\ntion capabilities provided by machine-learning and deep-\nlearning models.\nRegarding the detection scenario \u2013that is, the scenario con-\ncerned with a binary classi\ufb01cation to determine whether\neach individual token conveys sensitive information or not\u2013\n, it can be observed that BERT outperforms its competitors.\nA fact worth highlighting is that, according to these results,\nBERT achieves a precision lower than the rest of the sys-\ntems (i.e., it makes more false positive predictions); in ex-\nchange, it obtains a remarkably higher recall. Noticeably, it\n10http://temu.bsc.es/meddocan/index.php/evaluation/\n11https://github.com/PlanTL-SANIDAD/MEDDOCAN-\nEvaluation-Script", "start_char_idx": 2950, "end_char_idx": 5205, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1bd21ce6-da1e-4d6b-a4bd-7fe899733266": {"__data__": {"id_": "1bd21ce6-da1e-4d6b-a4bd-7fe899733266", "embedding": null, "metadata": {"page_label": "6", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "446ea4bd-d744-4aee-8fe8-8631c6a6007e", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "7c8339d0efe414104d8aacde8f721718ae4376a425fc49a01f4205553d44454b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d819170f-c131-4bbc-894b-b8b99e3112dd", "node_type": "1", "metadata": {"page_label": "5", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "51117c275cb70f89779513951c5e3af6e3f070befe154dc1451b28b46a24e1b1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3c5c9e79-5f94-4ef2-b3ff-46f2540cf49b", "node_type": "1", "metadata": {}, "hash": "ae44548c18faf2133d9db67b6a7073427ce366a73ba7857bae8ff8a152540e4f", "class_name": "RelatedNodeInfo"}}, "text": "reaches a recall of 0.979, improving by more than 4 points\nthe second-best system, spaCy.\nThe table also shows the results for the relaxed metric that\nonly takes into account the entity type detected, regardless\nof the BIO label (i.e., ignoring whether the token is at the\nbeginning or in the middle of a sensitive sequence of to-\nkens). The conclusions are very similar to those extracted\npreviously, with BERT gaining 2.1 points of F1-score over\nthe CRF based approach. The confusion matrices of the\npredictions made by CRF, spaCy, and BERT in this scenario\npredicted\nDat Hos Age Tim Doc Sex Kin Loc Pat Job Oth OtrueDat 0.90 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.08\nHos 0.00 0.89 0.00 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.00 0.11\nAge 0.00 0.00 0.96 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.03\nTim 0.01 0.00 0.00 0.95 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04\nDoc 0.00 0.01 0.00 0.00 0.93 0.00 0.00 0.00 0.00 0.00 0.00 0.05\nSex 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00\nKin 0.00 0.00 0.00 0.00 0.00 0.00 0.84 0.00 0.00 0.00 0.00 0.16\nLoc 0.00 0.08 0.00 0.00 0.00 0.00 0.00 0.35 0.00 0.08 0.00 0.58\nPat 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.21 0.00 0.00 0.79\nJob 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.12 0.00 0.88\nOth 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00\nO0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00\n(a) CRF\npredicted\nDat Hos Age Tim Doc Sex Kin Loc Pat Job Oth OtrueDat 0.93 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.05\nHos 0.00 0.88 0.00 0.00 0.01 0.00 0.00 0.01 0.00 0.00 0.00 0.10\nAge 0.00 0.00 0.98 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02\nTim 0.01 0.00 0.00 0.95 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04\nDoc 0.00 0.00 0.00 0.00 0.95 0.00 0.", "start_char_idx": 0, "end_char_idx": 1743, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c5c9e79-5f94-4ef2-b3ff-46f2540cf49b": {"__data__": {"id_": "3c5c9e79-5f94-4ef2-b3ff-46f2540cf49b", "embedding": null, "metadata": {"page_label": "6", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "446ea4bd-d744-4aee-8fe8-8631c6a6007e", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "7c8339d0efe414104d8aacde8f721718ae4376a425fc49a01f4205553d44454b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1bd21ce6-da1e-4d6b-a4bd-7fe899733266", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "bb0123394b6cdc264f99e9ad22c3ff660d8fc98301e177752cb4778f1e5eaaa0", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "eba08c57-1d0b-47b4-a315-9d109dd1502a", "node_type": "1", "metadata": {}, "hash": "75473ae3f884064b9598c8df1bc1185abe7dab065dbc31ba3489026376ea3fb5", "class_name": "RelatedNodeInfo"}}, "text": "00 0.00 0.00 0.00 0.05\nHos 0.00 0.88 0.00 0.00 0.01 0.00 0.00 0.01 0.00 0.00 0.00 0.10\nAge 0.00 0.00 0.98 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02\nTim 0.01 0.00 0.00 0.95 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.04\nDoc 0.00 0.00 0.00 0.00 0.95 0.00 0.00 0.00 0.00 0.00 0.00 0.05\nSex 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00\nKin 0.00 0.00 0.00 0.00 0.00 0.00 0.95 0.00 0.00 0.00 0.00 0.05\nLoc 0.00 0.15 0.00 0.00 0.00 0.00 0.00 0.27 0.00 0.08 0.00 0.58\nPat 0.00 0.00 0.00 0.00 0.07 0.00 0.00 0.07 0.21 0.00 0.00 0.64\nJob 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.12 0.00 0.88\nOth 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00\nO0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00\n(b) spaCy\npredicted\nDat Hos Age Tim Doc Sex Kin Loc Pat Job Oth OtrueDat 0.96 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02\nHos 0.00 0.96 0.00 0.00 0.01 0.00 0.00 0.01 0.00 0.00 0.00 0.03\nAge 0.00 0.00 0.99 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nTim 0.01 0.00 0.00 0.99 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nDoc 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nSex 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00\nKin 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00\nLoc 0.00 0.23 0.00 0.00 0.00 0.00 0.00 0.50 0.00 0.08 0.00 0.19\nPat 0.00 0.00 0.00 0.", "start_char_idx": 1492, "end_char_idx": 2818, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eba08c57-1d0b-47b4-a315-9d109dd1502a": {"__data__": {"id_": "eba08c57-1d0b-47b4-a315-9d109dd1502a", "embedding": null, "metadata": {"page_label": "6", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "446ea4bd-d744-4aee-8fe8-8631c6a6007e", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "7c8339d0efe414104d8aacde8f721718ae4376a425fc49a01f4205553d44454b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3c5c9e79-5f94-4ef2-b3ff-46f2540cf49b", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "cf39adb14f18ffa6f59d16fd624d564ca3a3d5273b709164c01501164a04c7e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2f5b7e0e-f0fd-4a65-a628-367d430d61f7", "node_type": "1", "metadata": {}, "hash": "2b65d5a91ac11864e9d2c62ec1daa910c44656424e8b3b709ce011ed99fe21ce", "class_name": "RelatedNodeInfo"}}, "text": "00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nSex 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00\nKin 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00\nLoc 0.00 0.23 0.00 0.00 0.00 0.00 0.00 0.50 0.00 0.08 0.00 0.19\nPat 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00\nJob 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.41 0.00 0.59\nOth 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00\nO0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00\n(c) BERT\nTable 6: Confusion matrices for the sensitive information\nclassi\ufb01cation task on the NUB ES-PHI corpusare shown in Table 6. As can bee seen, BERT has less dif-\n\ufb01culty in predicting correctly less frequent categories, such\nas \u2018Location\u2019, \u2018Job\u2019, and \u2018Patient\u2019. One of the most com-\nmon mistakes according to the confusion matrices is classi-\nfying hospital names as \u2018Location\u2019 instead of the more ac-\ncurate \u2018Hospital\u2019; this is hardly a harmful error, given that a\nhospital is actually a location. Last, the category \u2018Other\u2019 is\ncompletely leaked by all the compared systems, most likely\ndue to its almost total lack of support in both training and\nevaluation datasets.\nTo \ufb01nish with this experiment set, Table 5 also shows the\nstrict classi\ufb01cation precision, recall and F1-score for the\ncompared systems. Despite the fact that, in general, the\nsystems obtain high values, BERT outperforms them again.\nBERT\u2019s F1-score is 1.9 points higher than the next most\ncompetitive result in the comparison. More remarkably, the\nrecall obtained by BERT is about 5 points above.\nUpon manual inspection of the errors committed by the\nBERT-based model, we discovered that it has a slight ten-\ndency towards producing ill-formed BIO sequences (e.g,\nstarting a sensitive span with \u2018Inside\u2019 instead of \u2018Begin\u2019;\nsee Table 7). We could expect that complementing the\nBERT-based model with a CRF layer on top would help en-\nforce the emission of valid sequences, alleviating this kind\nof errors and further improving its results.\nAcudir\u00b4a a la Cl \u00b4\u0131nica Marseille\ntrue O O B I I\npredicted O O B B I\n(a) Example 1: \u201c[The patient] will attend the Marseille Clinic\u201d\ncontrol ( 15 y 22 de junio )\ntrue O O B O BI I O\npredicted O O B O II I O\n(b) Example 2: \u201cinspection (15 and 22 of june)\u201d\nNi\u02dcno de 4 a \u02dcnos y medio\ntrue BO B I I I\npredicted OO B I O O\n(c) Example 3: \u201c4 and a half years-old boy\u201d\nTable 7: BERT error examples (only BIO-tags are shown;\ndifferences between gold annotations and predictions are\nhighlighted in bold)\nFinally, Figure 2 shows the impact of decreasing the\namount of training data in the detection scenario. It shows\nthe difference in precision, recall, and F1-score with respect\nto that obtained using 100% of the training data. A general\ndownward trend can be observed, as one would expect: less\ntraining data leads to less accurate predictions.", "start_char_idx": 2562, "end_char_idx": 5401, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2f5b7e0e-f0fd-4a65-a628-367d430d61f7": {"__data__": {"id_": "2f5b7e0e-f0fd-4a65-a628-367d430d61f7", "embedding": null, "metadata": {"page_label": "6", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "446ea4bd-d744-4aee-8fe8-8631c6a6007e", "node_type": "4", "metadata": {"page_label": "6", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "7c8339d0efe414104d8aacde8f721718ae4376a425fc49a01f4205553d44454b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "eba08c57-1d0b-47b4-a315-9d109dd1502a", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "6d4f37d541035c838c5b794d359be5558295897ec68ad0539b15f1b386036bb3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af89c182-3a76-468c-b35c-44d8b14b168d", "node_type": "1", "metadata": {}, "hash": "7429a71878d1c7b23560e9cd1f7d2572139be2da0ef1b4c221be7fe9d60cfbf7", "class_name": "RelatedNodeInfo"}}, "text": "It shows\nthe difference in precision, recall, and F1-score with respect\nto that obtained using 100% of the training data. A general\ndownward trend can be observed, as one would expect: less\ntraining data leads to less accurate predictions. However,\nthe BERT-based model is the most robust to training-data\nreduction, showing an steadily low performance loss. With\n1% of the dataset (230 training instances), the BERT-based\nmodel only suffers a striking 7-point F1-score loss, in con-\ntrast to the 32 and 39 points lost by the CRF and spaCy\nmodels, respectively. This steep performance drop stems to", "start_char_idx": 5162, "end_char_idx": 5760, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af89c182-3a76-468c-b35c-44d8b14b168d": {"__data__": {"id_": "af89c182-3a76-468c-b35c-44d8b14b168d", "embedding": null, "metadata": {"page_label": "7", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca575dbe-9a0c-427d-9cba-690f1a65a0ee", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "e53b41c91ee17e23d25bbb07f15fcd2c01b1ce8388105ca386a6bd0c58c921fe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "2f5b7e0e-f0fd-4a65-a628-367d430d61f7", "node_type": "1", "metadata": {"page_label": "6", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "23af748f95d7eb4d64f834e8f467c6cc8e79d5dbf8cef080c0fc17d97287bf62", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6ea21953-f114-4145-b496-97203eedf2e5", "node_type": "1", "metadata": {}, "hash": "44697fd552b08becb372e9c3c3bd2aaf380c9fd9b457a1900f3987ce63eeb1f3", "class_name": "RelatedNodeInfo"}}, "text": "0 50 1000.40.60.81\ntrain data (%)precision\n0 50 1000.40.60.81\ntrain data (%)recall\n0 50 1000.40.60.81\ntrain data (%)F1-score\nCRF spaCy BERT\nFigure 2: Performance decay with decreasing amounts of training data on the sensitive information detection task in the\nNUB ES-PHI corpus\nDetection Classi\ufb01cation\nPrec Rec F1 Prec Rec F1\nCRF (Perez et al., 2019) 0.977 0.943 0.960 0.971 0.937 0.954\nspaCy (Perez et al., 2019) 0.967 0.953 0.965 0.965 0.947 0.956\nNLND S2 (Lange et al., 2019) 0.976 0.973 0.974 0.971 0.968 0.970\nNLND S3 (Lange et al., 2019) 0.975 0.975 0.975 0.970 0.969 0.970\nBERT + CRF (Mao and Liu, 2019) 0.968 0.919 0.943 0.965 0.912 0.937\nBERT 0.973 0.972 0.972 0.968 0.967 0.967\nTable 8: Results of Experiment B: MEDDOCAN\na larger extent from recall decline, which is not that marked\nin the case of BERT. Overall, these results indicate that the\ntransfer-learning achieved through the BERT multilingual\npre-trained model not only helps obtain better results, but\nalso lowers the need of manually labelled data for this ap-\nplication domain.\n4.2. Experiment B: MEDDOCAN\nThe results of the two MEDDOCAN scenarios \u2013detection\nand classi\ufb01cation\u2013 are shown in Table 8. These results fol-\nlow the same pattern as in the previous experiments, with\nthe CRF classi\ufb01er being the most precise of all, and BERT\noutperforming both the CRF and spaCy classi\ufb01ers thanks to\nits greater recall. We also show the results of Mao and Liu\n(2019) who, despite of having used a BERT-based system,\nachieve lower scores than our models. The reason why it\nshould be so remain unclear.\nWith regard to the winner of the MEDDOCAN shared\ntask, the BERT-based model has not improved the scores\nobtained by neither the domain-dependent (S3) nor the\ndomain-independent (S2) NLNDE model. However, at-\ntending to the obtained results, BERT remains only 0.3 F1-\nscore points behind, and would have achieved the second\nposition among all the MEDDOCAN shared task competi-\ntors. Taking into account that only 3% of the gold labels\nremain incorrectly annotated, the task can be considered al-\nmost solved, and it is not clear if the differences among the\nsystems are actually signi\ufb01cant, or whether they stem from\nminor variations in initialisation or a long-tail of minor la-belling inconsistencies.\n5. Conclusions and Future Work\nIn this work we have brie\ufb02y introduced the problems re-\nlated to data privacy protection in clinical domain. We have\nalso described some of the groundbreaking advances on the\nNatural Language Processing \ufb01eld due to the appearance of\nTransformers-based deep-learning architectures and trans-\nfer learning from very large general-domain multilingual\ncorpora, focusing our attention in one of its most represen-\ntative examples, Google\u2019s BERT model.\nIn order to assess the performance of BERT for Span-\nish clinical data anonymisation, we have conducted sev-\neral experiments with a BERT-based sequence labelling\napproach using the pre-trained multilingual BERT model\nshared by Google as the starting point for the model train-\ning. We have compared this BERT-based sequence la-\nbelling against other methods and systems. One of the ex-\nperiments uses the MEDDOCAN 2019 shared task dataset,\nwhile the other uses a novel Spanish clinical reports dataset\ncalled NUB ES-PHI.\nThe results of the experiments show that, in NUB ES-PHI,\nthe BERT-based model outperforms the other systems with-\nout requiring any adaptation or domain-speci\ufb01c feature en-\ngineering, just by being trained on the provided labelled\ndata.", "start_char_idx": 0, "end_char_idx": 3505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6ea21953-f114-4145-b496-97203eedf2e5": {"__data__": {"id_": "6ea21953-f114-4145-b496-97203eedf2e5", "embedding": null, "metadata": {"page_label": "7", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ca575dbe-9a0c-427d-9cba-690f1a65a0ee", "node_type": "4", "metadata": {"page_label": "7", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "e53b41c91ee17e23d25bbb07f15fcd2c01b1ce8388105ca386a6bd0c58c921fe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af89c182-3a76-468c-b35c-44d8b14b168d", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "619af2aaec873f9df6979a793cf4c5f335249ad3062030c533f4f2e9c898a5de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "646fb8a8-b3e2-49c6-8588-00e9634e8329", "node_type": "1", "metadata": {}, "hash": "b277c83aa04dd0f3f8b5d7611038751f691857e54391c47b71a9b13b8f986e6f", "class_name": "RelatedNodeInfo"}}, "text": "In order to assess the performance of BERT for Span-\nish clinical data anonymisation, we have conducted sev-\neral experiments with a BERT-based sequence labelling\napproach using the pre-trained multilingual BERT model\nshared by Google as the starting point for the model train-\ning. We have compared this BERT-based sequence la-\nbelling against other methods and systems. One of the ex-\nperiments uses the MEDDOCAN 2019 shared task dataset,\nwhile the other uses a novel Spanish clinical reports dataset\ncalled NUB ES-PHI.\nThe results of the experiments show that, in NUB ES-PHI,\nthe BERT-based model outperforms the other systems with-\nout requiring any adaptation or domain-speci\ufb01c feature en-\ngineering, just by being trained on the provided labelled\ndata. Interestingly, the BERT-based model obtains a re-\nmarkably higher recall than the other systems. High recall\nis a desirable outcome because, when anonymising sensible\ndocuments, the accidental leak of sensible data is likely to", "start_char_idx": 2747, "end_char_idx": 3733, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "646fb8a8-b3e2-49c6-8588-00e9634e8329": {"__data__": {"id_": "646fb8a8-b3e2-49c6-8588-00e9634e8329", "embedding": null, "metadata": {"page_label": "8", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4a1502c8-8b8d-43fe-9459-97798f3c3283", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "f41ca852fffc7a28f93b9be7782b042d7895ed0ac5c10778907115d129131279", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6ea21953-f114-4145-b496-97203eedf2e5", "node_type": "1", "metadata": {"page_label": "7", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "dd03a1f92af3e88db3c8614ddf8737280837acc2a1ecc0a5991a912853e601ab", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "914d4f34-98c3-4f33-90f7-c73efadf56eb", "node_type": "1", "metadata": {}, "hash": "5a68b590e478a9312c4fa038893edea406a40d40c1f5248b9a3e601c484623ad", "class_name": "RelatedNodeInfo"}}, "text": "be more dangerous than the unintended over-obfuscation of\nnon-sensitive text.\nFurther, we have conducted an additional experiment on\nthis dataset by progressively reducing the training data for\nall the compared systems. The BERT-based model shows\nthe highest robustness to training-data scarcity, loosing only\n7 points of F1-score when trained on 230 instances instead\nof 21,371. These observation are in line with the results ob-\ntained by the NLP community using BERT for other tasks.\nThe experiments with the MEDDOCAN 2019 shared task\ndataset follow the same pattern. In this case, the BERT-\nbased model falls 0.3 F1-score points behind the shared task\nwinning system, but it would have achieved the second po-\nsition in the competition with no further re\ufb01nement.\nSince we have used a pre-trained multilingual BERT model,\nthe same approach is likely to work for other languages just\nby providing some labelled training data. Further, this is the\nsimplest \ufb01ne-tuning that can be performed based on BERT.\nMore sophisticated \ufb01ne-tuning layers could help improve\nthe results. For example, it could be expected that a CRF\nlayer helped enforce better BIO tagging sequence predic-\ntions. Precisely, Mao and Liu (2019) participated in the\nMEDDOCAN competition using a BERT+CRF architec-\nture, but their reported scores are about 3 points lower than\nour implementation. From the description of their work, it\nis unclear what the source of this score difference could be.\nFurther, at the time of writing this paper, new multilingual\npre-trained models and Transformer architectures have be-\ncome available. It would not come as a surprise that these\nnew resources and systems \u2013e.g., XLM-RoBERTa (Con-\nneau et al., 2019) or BETO (Wu and Dredze, 2019), a BERT\nmodel fully pre-trained on Spanish texts\u2013 further advanced\nthe state of the art in this task.\n6. Acknowledgements\nThis work has been supported by Vicomtech and partially\nfunded by the project DeepReading (RTI2018-096846-B-\nC21, MCIU/AEI/FEDER,UE).\n7. Bibliographical References\nAbouelmehdi, K., Beni-Hessane, A., and Khalou\ufb01, H.\n(2018). Big healthcare data: preserving security and pri-\nvacy. Journal of Big Data , 5(1):1\u201318.\nAgerri, R., Bermudez, J., and Rigau, G. (2014). IXA\npipeline: Ef\ufb01cient and Ready to Use Multilingual NLP\ntools. In Proceedings of the 9th Language Resources and\nEvaluation Conference (LREC 2014) , pages 3823\u20133828.\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary,\nV ., Wenzek, G., Guzm \u00b4an, F., Grave, E., Ott, M.,\nZettlemoyer, L., and Stoyanov, V . (2019). Unsuper-\nvised Cross-lingual Representation Learning at Scale.\narXiv:1911.02116 .\nDernoncourt, F., Lee, J. Y ., Uzuner, \u00a8O., and Szolovits, P.\n(2016). De-identi\ufb01cation of Patient Notes with Recur-\nrent Neural Networks. Journal of the American Medical\nInformatics Association , 24(3):596\u2013606.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K.\n(2019). BERT: Pre-training of Deep Bidirectional Trans-\nformers for Language Understanding. In Proceedings of\nthe 2019 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Pa-\npers) , pages 4171\u20134186.\nGarc \u00b4\u0131a-Sardi \u02dcna, L. (2018). Automating the anonymisa-\ntion of textual corpora. Master\u2019s thesis, University of the\nBasque Country (UPV/EHU).\nHassan, F., Domingo-Ferrer, J., and Soria-Comas, J.\n(2018). Anonimizaci \u00b4on de datos no estructurados a\ntrav\u00b4es del reconocimiento de entidades nominadas.", "start_char_idx": 0, "end_char_idx": 3470, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "914d4f34-98c3-4f33-90f7-c73efadf56eb": {"__data__": {"id_": "914d4f34-98c3-4f33-90f7-c73efadf56eb", "embedding": null, "metadata": {"page_label": "8", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4a1502c8-8b8d-43fe-9459-97798f3c3283", "node_type": "4", "metadata": {"page_label": "8", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "f41ca852fffc7a28f93b9be7782b042d7895ed0ac5c10778907115d129131279", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "646fb8a8-b3e2-49c6-8588-00e9634e8329", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "2678412ba2179fc029541ac0f3b40bddde179850198204ee58f741489aee4969", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0942c7f2-4bed-4610-8ba9-4a1003e5206f", "node_type": "1", "metadata": {}, "hash": "379b320bbfbdc5ac906f5f9e9b1805f3661914b21b9ef76c0844a05fdcb95c55", "class_name": "RelatedNodeInfo"}}, "text": "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.\n(2019). BERT: Pre-training of Deep Bidirectional Trans-\nformers for Language Understanding. In Proceedings of\nthe 2019 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Pa-\npers) , pages 4171\u20134186.\nGarc \u00b4\u0131a-Sardi \u02dcna, L. (2018). Automating the anonymisa-\ntion of textual corpora. Master\u2019s thesis, University of the\nBasque Country (UPV/EHU).\nHassan, F., Domingo-Ferrer, J., and Soria-Comas, J.\n(2018). Anonimizaci \u00b4on de datos no estructurados a\ntrav\u00b4es del reconocimiento de entidades nominadas. In\nActas de la XV Reuni \u00b4on Espa \u02dcnola sobre Criptolog \u00b4\u0131a y\nSeguridad de la Informaci \u00b4on (RECSI 2018) .\nKhin, K., Burckhardt, P., and Padman, R. (2018). A Deep\nLearning Architecture for De-identi\ufb01cation of Patient\nNotes: Implementation and Evaluation. In 28th Work-\nshop on Information Technologies and Systems (WITS\n2018) .\nLafferty, J. D., McCallum, A., and Pereira, F. C. N. (2001).\nConditional Random Fields: Probabilistic Models for\nSegmenting and Labeling Sequence Data. In Proceed-\nings of the Eighteenth International Conference on Ma-\nchine Learning (ICML 2001) , pages 282\u2013289.\nLange, L., Adel, H., and Str \u00a8otgen, J. (2019). NLNDE: The\nNeither-Language-Nor-Domain-Experts\u2019 Way of Span-\nish Medical Document De-Identi\ufb01cation. In Proceed-\nings of the Iberian Languages Evaluation Forum (Iber-\nLEF 2019) , pages 671\u2013678.\nLima, S., Perez, N., Cuadros, M., and Rigau, G. (2019).\nNUBes: a Corpus of Negation and Uncertainty in Span-\nish Clinical Texts. In Proceedings of the 12th Language\nResources and Evaluation Conference (LREC 2020) .\nLiu, Y . and Lapata, M. (2019). Text Summarization with\nPretrained Encoders. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP) , pages\n3721\u20133731.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .\n(2019). RoBERTa: A Robustly Optimized BERT Pre-\ntraining Approach. arXiv:1907.11692 .\nLoshchilov, I. and Hutter, F. (2019). Decoupled Weight\nDecay Regularization. In Proceedings of the Seventh\nInternational Conference on Learning Representations\n(ICLR 2019) .\nMamede, N., Baptista, J., and Dias, F. (2016). Auto-\nmated anonymization of text documents. In 2016 IEEE\nCongress on Evolutionary Computation (CEC) , pages\n1287\u20131294.\nMao, J. and Liu, W. (2019). Hadoken: a BERT-CRF Model\nfor Medical Document Anonymization. In Proceedings\nof the Iberian Languages Evaluation Forum (IberLEF\n2019) , pages 720\u2013726.\nMarimon, M., Gonzalez-Agirre, A., Intxaurrondo, A.,\nRodr \u00b4\u0131guez, H., Lopez Martin, J. A., Villegas, M., and\nKrallinger, M. (2019). Automatic de-identi\ufb01cation of\nmedical texts in Spanish: the MEDDOCAN track, cor-\npus, guidelines, methods and evaluation of results. In\nProceedings of the Iberian Languages Evaluation Forum\n(IberLEF 2019) , pages 618\u2013638.", "start_char_idx": 2828, "end_char_idx": 5869, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0942c7f2-4bed-4610-8ba9-4a1003e5206f": {"__data__": {"id_": "0942c7f2-4bed-4610-8ba9-4a1003e5206f", "embedding": null, "metadata": {"page_label": "9", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8d1f3851-add1-4e50-91dc-c33dbf3c7a93", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "2e0eb5899d2dbf5ad4c269c5fba1202709996eab0f80d6854e30a6d045c46ba2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "914d4f34-98c3-4f33-90f7-c73efadf56eb", "node_type": "1", "metadata": {"page_label": "8", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "348c87535a4874ebbdc2d2eb4b26753712c5467195fa63f0bda85a52303dc13b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1e06fbfd-1dd1-4d70-8e08-6ad929bf0b77", "node_type": "1", "metadata": {}, "hash": "b1251cdf6e28c2115e04dc462cac15309fbb3c0145f2397c1de551d6e7ac0901", "class_name": "RelatedNodeInfo"}}, "text": "Medina, S. and Turmo, J. (2018). Building a Span-\nish/Catalan Health Records Corpus with Very Sparse\nProtected Information Labelled. In Proceedings of the\nEleventh International Conference on Language Re-\nsources and Evaluation (LREC 2018) .\nNogueira, R. and Cho, K. (2019). Passage Re-ranking with\nBERT. arXiv:1901.04085 .\nPerez, N., Garc \u00b4\u0131a-Sardi \u02dcna, L., Serras, M., and Del Pozo,\nA. (2019). Vicomtech at MEDDOCAN: Medical Doc-\nument Anonymization. In Proceedings of the Iberian\nLanguages Evaluation Forum (IberLEF 2019) , pages\n696\u2013703.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. (2019). Language Models are Unsuper-\nvised Multitask Learners.\nRamshaw, L. A. and Marcus, M. P., (1999). Natural Lan-\nguage Processing Using Very Large Corpora , chapter 9,\npages 157\u2013176. Springer Netherlands.\nStenetorp, P., Pyysalo, S., Topi \u00b4c, G., Ohta, T., Ananiadou,\nS., and Tsujii, J. (2012). BRAT: A Web-based Tool\nfor NLP-assisted Text Annotation. In Proceedings of the\nDemonstrations at the 13th Conference of the European\nChapter of the Association for Computational Linguis-\ntics (EACL \u201912) , pages 102\u2013107.\nStrubell, E., Verga, P., Belanger, D., and McCallum, A.\n(2017). Fast and Accurate Entity Recognition with It-\nerated Dilated Convolutions. In Proceedings of the 2017\nConference on Empirical Methods in Natural Language\nProcessing (EMNLP 2017) , pages 2670\u20132680.\nStubbs, A., Kot\ufb01la, C., and Uzuner, O. (2015). Automated\nsystems for the de-identi\ufb01cation of longitudinal clinical\nnarratives: Overview of 2014 i2b2/UTHealth shared task\nTrack 1. Journal of Biomedical Informatics , 58(Supple-\nment):S11 \u2013 S19.\nTveit, A., Edsberg, O., R\u00f8st, T., Faxvaag, A., Nytr\u00f8, \u00d8.,\nNordg \u02daard, T., Ranang, M., and Grimsmo, A. (2004).\nAnonymization of General Practioner Medical Records.\nInProceedings of the Second HelsIT Conference .\nUzuner, O., Luo, Y ., and Szolovits, P. (2007). Evaluat-\ning the State-of-the-Art in Automatic De-identi\ufb01cation.\nJournal of the American Medical Informatics Associa-\ntion, 14(5):550\u201363.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. (2017).\nAttention Is All You Need. In Proceedings of the 31st In-\nternational Conference on Advances in Neural Informa-\ntion Processing Systems (NIPS 2017) , pages 5998\u20136008.\nWolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue,\nC., Moi, A., Cistac, P., Rault, T., Louf, R., Funtow-\nicz, M., and Brew, J. (2019). HuggingFace\u2019s Trans-\nformers: State-of-the-art Natural Language Processing.\narXiv:1910.03771 .\nWu, S. and Dredze, M. (2019). Beto, Bentz, Becas:\nThe Surprising Cross-Lingual Effectiveness of BERT. In\nProceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP) , pages 833\u2013844.\nYang, Z., Dai, Z., Yang, Y ., Carbonell, J., Salakhutdinov,R.", "start_char_idx": 0, "end_char_idx": 2916, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1e06fbfd-1dd1-4d70-8e08-6ad929bf0b77": {"__data__": {"id_": "1e06fbfd-1dd1-4d70-8e08-6ad929bf0b77", "embedding": null, "metadata": {"page_label": "9", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8d1f3851-add1-4e50-91dc-c33dbf3c7a93", "node_type": "4", "metadata": {"page_label": "9", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "2e0eb5899d2dbf5ad4c269c5fba1202709996eab0f80d6854e30a6d045c46ba2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0942c7f2-4bed-4610-8ba9-4a1003e5206f", "node_type": "1", "metadata": {"page_label": "9", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}, "hash": "7e34483c368acce46627cd3863eda38ab5ced5955cbe82aff68fc49f2519d5fe", "class_name": "RelatedNodeInfo"}}, "text": "(2019). HuggingFace\u2019s Trans-\nformers: State-of-the-art Natural Language Processing.\narXiv:1910.03771 .\nWu, S. and Dredze, M. (2019). Beto, Bentz, Becas:\nThe Surprising Cross-Lingual Effectiveness of BERT. In\nProceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP) , pages 833\u2013844.\nYang, Z., Dai, Z., Yang, Y ., Carbonell, J., Salakhutdinov,R. R., and Le, Q. V . (2019). XLNet: Generalized Au-\ntoregressive Pretraining for Language Understanding. In\nAdvances in neural information processing systems 32 ,\npages 5753\u20135763.", "start_char_idx": 2450, "end_char_idx": 3094, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"aef2a166-0712-45c2-bb36-09e7045e08c1": {"node_ids": ["680c7591-3db1-481b-bf89-cffa94d6aaac"], "metadata": {"page_label": "1", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "64411d7e-6a32-4c01-b88e-2aed643b2657": {"node_ids": ["35d70a01-7141-44c2-9619-192647e015f7"], "metadata": {"page_label": "2", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "4fc45785-6dbf-4612-850e-f05194cafe45": {"node_ids": ["1234c90e-1b55-4931-a123-c9f0523057d1", "950ac749-b405-4e42-948c-7524f3250b28"], "metadata": {"page_label": "3", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "de718598-3ff9-4424-9331-9cf36d6af3e0": {"node_ids": ["cf796dc6-21a2-429b-9e20-25d42565c7f0"], "metadata": {"page_label": "4", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "035e0516-a0e1-4bd8-a828-e7f4bcb9b02f": {"node_ids": ["6249c657-f3fc-4c4a-9b65-e0fc63a91c1e", "2b9d3ef7-1eca-48d2-bb74-fd0d81ed5f95"], "metadata": {"page_label": "5", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "d3ea2d0b-aa99-4af7-b94d-e1c6f526133a": {"node_ids": ["84d9d203-41b3-4c5d-bb30-301013d3f233", "f40aa89d-8ce6-4e8f-9ca8-2a861d4a9134"], "metadata": {"page_label": "6", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "d8662ecb-8c77-4f22-96e8-b223e930ee0c": {"node_ids": ["cbd6b41f-98c7-448b-9fb1-9db0c71ed4c6", "4d3d6771-fa9f-4f38-8c0a-68a39c13baaa"], "metadata": {"page_label": "7", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "fca6b329-8ebf-4504-9598-7a25e9b677bc": {"node_ids": ["1a5d552b-e299-4dcc-8350-4b90710e33b3", "959781a9-7849-4b2a-b213-89f25189a72a"], "metadata": {"page_label": "8", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "5e7bc5b4-0b67-4c5a-a893-268c550a3abb": {"node_ids": ["8162bf79-863e-49f9-8d40-b0329bd68b6a", "f2c48808-43db-4927-a36a-d1a013f1479c"], "metadata": {"page_label": "9", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "5e98a887-73f9-4e55-a4ca-96e985805783": {"node_ids": ["2ef6d38f-9c00-4c55-993b-fa5bef23c4a2"], "metadata": {"page_label": "10", "file_name": "1609.00425.pdf", "file_path": "data/1609.00425.pdf", "file_type": "application/pdf", "file_size": 229651, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "31165a1b-a78a-4f9e-bc74-e87582a28c61": {"node_ids": ["3b99d716-ecdf-4e0f-968b-85a3f7535f64"], "metadata": {"page_label": "1", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "cdb55b56-a520-4e68-a7f3-a410dabf27d4": {"node_ids": ["7688cb58-efb8-4253-b488-d6a6acb150f4", "2dbf0002-0e09-4c42-b9a9-133a614a3409"], "metadata": {"page_label": "2", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "2797e83c-4e17-4161-b0e8-4670879fd46d": {"node_ids": ["4e912775-8bfe-438d-a92a-a411df0aef65", "007664a8-83f5-4530-b6d1-d4fd70c0488e"], "metadata": {"page_label": "3", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "32caede3-2d9c-4c3f-a2b8-ef9608a8efda": {"node_ids": ["8dac8b99-7844-4832-8190-64046c95b28f", "311df028-ffba-48d2-87eb-7544267a4b83"], "metadata": {"page_label": "4", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "92db31fd-64b7-4603-9417-8743b5e0ce44": {"node_ids": ["e7a7412b-fd26-4785-88ae-5d4f1ce3d9d1"], "metadata": {"page_label": "5", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "30b8bf60-b0a3-4d25-ba1d-0665188f6124": {"node_ids": ["3b072c93-b875-4d91-8399-67e5d39ce8b7", "bd9f4dab-d6c8-4fca-a754-61b399002eef"], "metadata": {"page_label": "6", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "6d80b32d-19ee-46cc-9d84-38d1d98ff2e7": {"node_ids": ["c39c02d5-3afb-4bdc-b84f-ab5259c1b5da", "bbcd8385-2908-4832-a9b9-16e9bb32d580"], "metadata": {"page_label": "7", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "a322bbac-55d8-4024-aafb-3a18236b59e2": {"node_ids": ["0a8184b8-1064-421b-a59b-1101caff8be7", "1366e121-f984-43b4-a772-dbcb39632bda"], "metadata": {"page_label": "8", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "76e6460c-f850-4db3-84a3-c90d0d4d108c": {"node_ids": ["30ac4b62-3548-47e5-961c-8f2d4c86f360", "108db81e-6951-468c-a52c-8bace41261d4"], "metadata": {"page_label": "9", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "4bd4c3a1-61f8-4f02-86a6-017c0f023645": {"node_ids": ["5375fc8f-5497-4fb2-a4be-9c58efc07577", "e2102027-836c-407c-ba7c-0c5ee0492fb6"], "metadata": {"page_label": "10", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "da6b4d72-fd10-4b13-89f5-8c3604e4fefa": {"node_ids": ["b692bd4f-765e-4503-a82c-8a2fb897c0d5", "197d7bd5-e401-4506-8bd7-ac68b66276bf"], "metadata": {"page_label": "11", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "d94be76f-1a2c-46e7-979b-389c19fd3b82": {"node_ids": ["df5099fc-9b9f-4dcf-9e7a-13837bacc5a1"], "metadata": {"page_label": "12", "file_name": "1704.06194.pdf", "file_path": "data/1704.06194.pdf", "file_type": "application/pdf", "file_size": 497850, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "a73e442e-f027-42b5-b16a-3cf06321c6df": {"node_ids": ["9dd4669d-2859-44dc-8333-8c5efe456dbc"], "metadata": {"page_label": "1", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "cb90669e-9444-4916-b9a7-101e2ea7d96b": {"node_ids": ["0d3408db-ac49-43ce-8b3b-8724c62e525c", "13c17320-1a7b-4e6e-bf5f-172679e963f0"], "metadata": {"page_label": "2", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "6dcddd5f-dfe2-47f8-8106-bc3416d45f0f": {"node_ids": ["c037ee30-8133-4839-b08b-7c652fb85248"], "metadata": {"page_label": "3", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "c402d675-8b25-4e53-9b00-3ddde2cc03c4": {"node_ids": ["76b58edd-ef42-4c84-be01-57af7fda0024", "c8474b79-dfec-41da-8800-44a185accb65"], "metadata": {"page_label": "4", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "d2814921-d22c-45db-8c20-209e09adc6c2": {"node_ids": ["fd6b9d1b-3ce9-470f-8b1d-0ef4283b90fb", "49b06c9a-2624-4921-9962-bb6f5c36b2a5"], "metadata": {"page_label": "5", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "c3ead361-c540-4751-afbe-ccfe6f550959": {"node_ids": ["d527e945-a7e4-42dd-b463-2d3b427514da"], "metadata": {"page_label": "6", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "6c50c7bb-d1a3-470a-92f9-df245329242c": {"node_ids": ["77a13371-9836-4f53-9a3f-f252b9716849", "1de9c534-ed62-4759-9689-25566bf91a84"], "metadata": {"page_label": "7", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "c322d402-f3b8-4588-9192-a7a9d0f09457": {"node_ids": ["04cebd34-d844-4bde-9e44-55afa023d9e8"], "metadata": {"page_label": "8", "file_name": "1708.01464.pdf", "file_path": "data/1708.01464.pdf", "file_type": "application/pdf", "file_size": 209973, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "cd2b9134-d1eb-4afb-b4a2-13c7dfd106e1": {"node_ids": ["693cfe0b-d036-44a9-b7f2-868501c09665", "c99b79c7-0fb0-4f10-8dc9-330276e166bf"], "metadata": {"page_label": "1", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "289a0fc7-c14f-4200-906d-9991f20c2cab": {"node_ids": ["04d7545b-4492-4144-b8fb-820ce2afaa3c", "78335aaf-5fe7-4da6-8e9f-1d46ccc6bc3b"], "metadata": {"page_label": "2", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "3b815434-5509-48f3-a18a-47d3cf16db98": {"node_ids": ["f3860c03-49d7-4dd6-b527-a4760b19a7c3", "e7943d8d-04a3-4ec7-bc75-66dd8b15fefb"], "metadata": {"page_label": "3", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "336e4992-5ac6-4372-9099-e6c746f70250": {"node_ids": ["2da53847-a6f9-4db8-b7e5-9a3b832ca833", "c6195407-1203-4178-8fbc-990540c57f42"], "metadata": {"page_label": "4", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "d5bb378b-222b-4cef-a97b-09e5b54acfa7": {"node_ids": ["c8862aa2-a8b0-4bd3-9c2f-895f1e4ca56a", "d3894d84-648a-42c2-93cb-ebff7d3dbc0f"], "metadata": {"page_label": "5", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "0bf2ba39-83a9-45d4-9814-13d20291b2b0": {"node_ids": ["f5ac39b8-8089-44df-86fd-91fb77f031d1", "8a8ec2c2-9bdf-4806-843e-6bb42388ae00"], "metadata": {"page_label": "6", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "dfe84564-5c00-4edc-998d-fdb7b356cfe0": {"node_ids": ["065f5cc5-0336-4413-bf85-42bcadb1c76e", "a03da7be-4d95-4004-abff-356c0117a88d", "e42104b7-2c60-4246-9b39-4afd0d15a53d"], "metadata": {"page_label": "7", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "6b639655-4cf4-4fc4-b263-c719dfbefd41": {"node_ids": ["b7a83585-17c0-42cc-a453-d5b8ae67bbd5", "53a631c9-fa33-42ec-a7d7-61e2f240a66f", "18aa9c90-7a35-4ca2-8e8c-af96978dc1bc"], "metadata": {"page_label": "8", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "0f40fa86-ec61-4f3a-876b-7076516f73f1": {"node_ids": ["bd11d076-87d2-4b16-b62e-a5c94c86b520"], "metadata": {"page_label": "9", "file_name": "1801.05147.pdf", "file_path": "data/1801.05147.pdf", "file_type": "application/pdf", "file_size": 488237, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "019bbff9-f260-4c69-a1ff-ac34882dab33": {"node_ids": ["cd90c7a9-4104-4364-8241-984097fc4137", "302a8d5c-6348-44d5-935b-2a02f23c7f37"], "metadata": {"page_label": "1", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "718b525c-8452-4fb2-a017-735f3fc2725d": {"node_ids": ["c6f4b3ee-aed7-4923-9da2-3b35ac9d12b7", "487f951b-7582-44a0-85de-f92bb3b90338"], "metadata": {"page_label": "2", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "1002b06f-4a58-47f5-bada-303fcfdb722b": {"node_ids": ["8b5451c1-4abe-47a3-8da5-f75e644dce5d", "72f07dcb-e55f-4d27-b77b-2aa3f9d6cf0c"], "metadata": {"page_label": "3", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "d578ed6c-fe42-4513-bc2c-a4d2b8cc92c8": {"node_ids": ["68af7989-bde8-4ce5-a0db-6ff8fc4d94fa", "d34ac9c1-9944-42b2-a752-56325798331e"], "metadata": {"page_label": "4", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "d0c966e3-49f6-471d-ab0b-8c874f7e822b": {"node_ids": ["e4690e11-1b8d-47e9-a444-b68d330b092a", "4c90538c-b368-465a-a8d4-622d4a89eadf"], "metadata": {"page_label": "5", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "ef2ffb70-a742-4e82-8f25-5953600ab075": {"node_ids": ["5a8a4aae-e59a-45c7-a61f-0ff627d70a42", "3fd8f3b7-f1fb-413f-b074-f4f719395c99"], "metadata": {"page_label": "6", "file_name": "1810.08699.pdf", "file_path": "data/1810.08699.pdf", "file_type": "application/pdf", "file_size": 134753, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "1d1e02e8-6587-4553-acb5-c0f5437307a3": {"node_ids": ["a2730dc0-dcd0-4abf-8968-c1328d855cd7"], "metadata": {"page_label": "1", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "066aa014-8322-42ee-ab37-5b2d37726129": {"node_ids": ["e9a66b0d-7bd4-48df-8d91-ab0a2eb0297c", "71297b9c-7a19-42dd-a820-4fad38649288"], "metadata": {"page_label": "2", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "2735b0a1-d30e-45a0-99be-1e60cd9830f5": {"node_ids": ["e1a95dc3-c4dd-4997-a3c8-a4837df5b320", "55e09196-11fa-412e-91c1-26ad4d6b0dbf"], "metadata": {"page_label": "3", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "1f45cb23-bfea-4041-bdba-f6e9a790db2a": {"node_ids": ["e13c8d83-73dc-424b-9924-4b2105aee24d", "925effbc-39d1-463b-a41b-378982fe6119"], "metadata": {"page_label": "4", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "cbe35ac3-764d-4dc3-a82d-79683d795e2a": {"node_ids": ["5c4c75a7-d59e-46b7-b13e-aa538ac70c33", "797609bd-80f4-432a-9a6a-ef0f3be2b5d4"], "metadata": {"page_label": "5", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "e0fe866c-0595-42c6-932a-a1c0cca353ec": {"node_ids": ["ae8561aa-972f-4c77-a63a-66579ee07ed2", "5463b287-7526-47dd-8d98-de5361182a08"], "metadata": {"page_label": "6", "file_name": "1811.00383.pdf", "file_path": "data/1811.00383.pdf", "file_type": "application/pdf", "file_size": 244791, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "5ddd3602-df76-409c-9975-77e97fb1d52d": {"node_ids": ["9bf05942-d94d-4c6c-8ce8-ba879a98bef7"], "metadata": {"page_label": "1", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "2b2a1ebf-0aa3-4a53-beb3-bf146bd06540": {"node_ids": ["97aa10c0-d7a4-490f-af32-2aa692bbbfaf", "11b0eb41-1d7c-4781-9b34-b9de27768f03"], "metadata": {"page_label": "2", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "0e16a3fb-8fc1-4891-a583-21dd8afd8356": {"node_ids": ["e2dff16d-89a6-48b8-bd7b-5304655867a2", "a2185e42-ddef-4835-88ae-df806aa54c3e"], "metadata": {"page_label": "3", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "87d2e3c6-4452-4b9e-bbfa-65a394905e4e": {"node_ids": ["f1b3d76a-df51-4569-b5b4-e3857b11c872", "1b2fb2d3-a1ac-4536-97b7-3a5399acc488"], "metadata": {"page_label": "4", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "05017b20-5666-4cd9-9a86-29ca466e2e33": {"node_ids": ["7a52b1d1-909c-4ead-a4d9-73ebfbd581b9"], "metadata": {"page_label": "5", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "04167c03-1197-4caa-993f-1db41fed7f7d": {"node_ids": ["478eee57-a17d-4ed1-b12b-eec7ee47dcc0"], "metadata": {"page_label": "6", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "558aabdc-ac0a-4131-87fb-e9351c01caca": {"node_ids": ["25b2116a-4dde-4a3e-8633-7d486d102a01"], "metadata": {"page_label": "7", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "b0bb577b-65bf-4e62-a71c-0720fe801dcf": {"node_ids": ["49597c31-bc55-41dc-b18a-946066c88ae2", "da73fe1c-4c99-4a8c-ba60-a90e371e014c"], "metadata": {"page_label": "8", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "0521f782-b619-4268-92e9-cde7d75f6fc2": {"node_ids": ["d14f2c6f-635b-4e07-b44a-bfb346c93977", "fc09df64-ca00-4506-ac63-d172b4445dd8"], "metadata": {"page_label": "9", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "a2eaf068-12a3-4a47-b796-c9d13306cab9": {"node_ids": ["1d518517-d758-4ce6-bbe9-30fad3f5aa68", "79eeb578-880a-42c4-81c6-d5d0ffafbd0d"], "metadata": {"page_label": "10", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "19004571-ee66-46ad-99d0-eba9866758c3": {"node_ids": ["188c676f-50c0-45bc-81a1-48eed91d1866"], "metadata": {"page_label": "11", "file_name": "1909.00512.pdf", "file_path": "data/1909.00512.pdf", "file_type": "application/pdf", "file_size": 340087, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "e6b1fc99-8048-4656-bc23-b2a6a2bc326c": {"node_ids": ["ee6895fb-9704-4772-873a-3a3be7515948", "c9a4c489-b6e7-4fc2-9aea-7d84fa71703c"], "metadata": {"page_label": "1", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "4037ae60-e748-4652-b18e-67f34a2960dd": {"node_ids": ["54098528-d356-4665-919b-d5060a1f6503", "993377cd-0f5d-480b-8168-7b6634716d97"], "metadata": {"page_label": "2", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "4c66e2e6-0588-4327-973d-5776b26e67e6": {"node_ids": ["a215aae4-5dba-42df-b308-20cc0e752ff6"], "metadata": {"page_label": "3", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "2f9d15aa-5c08-4cde-99bb-a233f1b579f0": {"node_ids": ["157f391d-1774-4ec1-a4a0-63f259bb5ec6"], "metadata": {"page_label": "4", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "724bf392-6e79-4b38-8f3c-0985f0824a0c": {"node_ids": ["32ae6ca5-d98f-4555-80b0-ce5a11ae731e"], "metadata": {"page_label": "5", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "0f643fb8-b06e-490b-b3e9-909d6292b389": {"node_ids": ["b33375fc-dbd2-4930-8026-d5c2e5c33c3f", "3c154563-728a-4cfe-a93d-33ebee912126"], "metadata": {"page_label": "6", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "0fb92626-4265-408b-a0c6-ba0be90c2643": {"node_ids": ["714da7de-3625-4641-8b80-17676d0dd35b"], "metadata": {"page_label": "7", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "2cff0bdf-dd24-4e4f-b1e2-934ef0398901": {"node_ids": ["09f86ebd-84e2-465a-a362-8432855eceab"], "metadata": {"page_label": "8", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "208aea5f-885c-49b5-beaf-016acff75fa9": {"node_ids": ["5710873f-708e-4d54-a2d8-1b6977cf6a03", "705780c9-c843-42c7-ad41-94c5988c706e"], "metadata": {"page_label": "9", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "c7a4f182-d990-4c2c-9a2e-97548e03eb8b": {"node_ids": ["085aec09-83e4-4d6b-8e35-06577191b8d9"], "metadata": {"page_label": "10", "file_name": "1909.09067.pdf", "file_path": "data/1909.09067.pdf", "file_type": "application/pdf", "file_size": 90488, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "53b70b3b-1485-4a7c-9de5-4490c71dd120": {"node_ids": ["28edaeb2-8077-4ec4-b006-f8784f939fcd", "e11d07b4-7bef-485c-908b-1fcff15fa321"], "metadata": {"page_label": "1", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "606bf2d5-8b74-490e-82c5-4d7995453e0d": {"node_ids": ["3002dda1-f9ad-4b77-a875-5fd3cbfb9272", "366429d4-41bb-4e6e-9177-81ef84a6c571"], "metadata": {"page_label": "2", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "6137caa2-d008-463d-bbad-6c37b53d7a39": {"node_ids": ["78e626a2-3ae5-46ec-a91d-65af9b82ac33"], "metadata": {"page_label": "3", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "44d116a9-5dc5-4866-962c-377581814389": {"node_ids": ["518edc28-ea32-4802-8965-d3f86b569bbc", "540f99c3-e1c8-4e72-9857-86c8f568c94e"], "metadata": {"page_label": "4", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "4689cc6b-9f9b-46dd-93c1-559c5c1e7f08": {"node_ids": ["a63100a3-f57c-457d-ba80-fc482dfd5f56", "41e50465-360e-4d54-9b27-e9d2b91f4af0"], "metadata": {"page_label": "5", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "7095e65f-fb04-4bf9-897b-614c9d5524ca": {"node_ids": ["87b1fe1b-0470-4a3d-b9ab-64b7e05e8b5c", "17aa8243-b803-4f95-90da-a27d33ba8ebe"], "metadata": {"page_label": "6", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "4946db25-fac9-4260-bd32-94c4b9c5b838": {"node_ids": ["77e6e8bc-49a6-484f-a81a-b35b2521e6e2", "c81cb8d8-b0ce-46c4-8b19-7e451e36370c"], "metadata": {"page_label": "7", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "fc2578e5-f9ca-4125-931f-681cf3b1566a": {"node_ids": ["7c296753-c436-4f67-80c9-b316fdbd5e3a", "fc53e336-b57f-4a8d-b408-cd941cdd8ccc", "1f4b5d80-2026-4b65-b044-571b7c975c31"], "metadata": {"page_label": "8", "file_name": "1912.01214.pdf", "file_path": "data/1912.01214.pdf", "file_type": "application/pdf", "file_size": 737407, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "e45afb9f-6fed-477f-bcfa-d877f32c9658": {"node_ids": ["e380dea2-bfc0-4b13-9434-8a0d9b58cae7", "ffd272f7-7798-4852-b1b6-ae3b1b964370"], "metadata": {"page_label": "1", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "a1229b78-9125-4cc5-b4e7-5dd9d13a6e19": {"node_ids": ["15d5497e-73c5-4354-aeee-72b97c7f914e", "2486c76a-fefa-4b3a-90ba-de82faf2a2d1"], "metadata": {"page_label": "2", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "d7f4886d-fc40-4339-9763-26ef2704bbc6": {"node_ids": ["98337baa-5a3b-47bb-80db-947a190218c3", "17e5fbd5-a8aa-4d50-afec-90cce86ac83b"], "metadata": {"page_label": "3", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "9513c1ed-af72-4ae6-96ef-1ec019cdfb1e": {"node_ids": ["12dbeb4b-9df0-4829-a88b-4154ec7c3e84", "2308c4ca-156a-4602-bcfd-f5fde7e2ddef"], "metadata": {"page_label": "4", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "06a4455f-7516-4a87-a65f-5b202f6e1001": {"node_ids": ["3385d8f8-559e-4744-8876-94f649b6f829", "d819170f-c131-4bbc-894b-b8b99e3112dd"], "metadata": {"page_label": "5", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "446ea4bd-d744-4aee-8fe8-8631c6a6007e": {"node_ids": ["1bd21ce6-da1e-4d6b-a4bd-7fe899733266", "3c5c9e79-5f94-4ef2-b3ff-46f2540cf49b", "eba08c57-1d0b-47b4-a315-9d109dd1502a", "2f5b7e0e-f0fd-4a65-a628-367d430d61f7"], "metadata": {"page_label": "6", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "ca575dbe-9a0c-427d-9cba-690f1a65a0ee": {"node_ids": ["af89c182-3a76-468c-b35c-44d8b14b168d", "6ea21953-f114-4145-b496-97203eedf2e5"], "metadata": {"page_label": "7", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "4a1502c8-8b8d-43fe-9459-97798f3c3283": {"node_ids": ["646fb8a8-b3e2-49c6-8588-00e9634e8329", "914d4f34-98c3-4f33-90f7-c73efadf56eb"], "metadata": {"page_label": "8", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}, "8d1f3851-add1-4e50-91dc-c33dbf3c7a93": {"node_ids": ["0942c7f2-4bed-4610-8ba9-4a1003e5206f", "1e06fbfd-1dd1-4d70-8e08-6ad929bf0b77"], "metadata": {"page_label": "9", "file_name": "2003.03106.pdf", "file_path": "data/2003.03106.pdf", "file_type": "application/pdf", "file_size": 300931, "creation_date": "2024-02-19", "last_modified_date": "2024-02-19", "last_accessed_date": "2024-02-19"}}}}