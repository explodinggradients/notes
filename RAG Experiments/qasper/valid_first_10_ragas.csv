,question,contexts,ground_truth,evolution_type,episode_done
0,What are some linguistic signals of dogmatic language?,"['ers, often in a strongly opinionated way (“you are a\nmoron” or “they are keeping us down”). Other pro-\nnoun types do not show signiﬁcant relationships.\nLike pronouns, verb tense can reveal subtle sig-\nnals in language use, such as the tendency of medi-\ncal inpatients to focus on the past (Wolf et al., 2007).\nOn social media, comments written in the present\ntense are more likely to be oriented towards a user’s\ncurrent interaction (“this isall so stupid”), creating\nopportunities to signal dogmatism. Alternatively,\ncomments in the past tense are more likely to re-\nfer to outside experiences (“it wasan awful party”),\nspeaking less to a user’s stance towards an ongoing\ndiscussion. We ﬁnd present tense is a positive sig-\nnal for dogmatism (1.11 odds) and past tense is a\nnegative signal (0.69 odds).\nDogmatic language can be either positively or\nnegatively charged in sentiment: for example, con-\nsider the positive statement “ Trump is the SAVIOR\nof this country!!! ” or the negative statement “ Are\nyou REALLY that stupid?? Education is the only\nway out of this horrible mess. It’s hard to imagine\nhow anyone could be so deluded. ” In diverse com-\nmunities, where people hold many different kinds\nof opinions, dogmatic opinions will often tend to\ncome into conﬂict with one another (McCluskey and\nHmielowski, 2012), producing a greater likelihood\nof negative sentiment. Perhaps for this reason, neg-\native emotion (2.09 odds) and swearing (3.80 odds)\nare useful positive signals of dogmatism, while pos-\nitive emotion shows no signiﬁcant relationship.\nFinally, we ﬁnd that interrogative language (1.12\nodds) and negation (1.35 odds) are two additional\npositive signals of dogmatism. While interrogative\nwords like “how” or “what” have many benign uses,\nthey disproportionately appear in our data in the\nform of rhetorical or emotionally charged questions,\nsuch as “how can anyone be that dumb?”\nMany of these linguistic signals are correlated\nwith each other, suggesting that dogmatism is the\ncumulative effect of many component relationships.\nFor example, consider the relatively non-dogmatic\nstatement: “I think the reviewers are wrong in this\ninstance.” Removing signals of insight , we have:\n“the reviewers are wrong in this instance,” which\nis slightly more dogmatic. Then removing relativ-\nity, we have: “the reviewers are wrong.” And ﬁ-\nnally, adding certainty , we have a dogmatic state-Classiﬁer In-domain Cross-domain\nBOW 0.853 0.776\nSENT 0.677 0.646\nLING 0.801 0.728\nBOW + SENT 0.860 0.783\nBOW + LING 0.881 0.791\nTable 2: The AUC scores for dogmatism classiﬁers within and\nacross domains. BOW (bag-of-words) and SENT (sentiment\nsignals) are baselines, and LING uses the linguistic features\nfrom Table 1. We compute in-domain accuracy using 15-fold\ncross-validation on the Reddit dataset, and cross-domain accu-\nracy by training on Reddit and evaluating on comments on arti-\ncles from the New York Times. Chance AUC is 0.5.\nment: “the reviewers are always wrong.”\n4 Predicting dogmatism\nWe now show how we can use the linguistic feature\nsets we have described to build a classiﬁer that pre-\ndicts dogmatism in comments. A predictive model\nfurther validates our feature sets, and also allows us\nto analyze dogmatism in millions of other Reddit\ncomments in a scalable way, with multiple uses in\nongoing, downstream analyses.\nPrediction task. Our goal is (1) to understand\nhow well we can use the strategies in Section 3\nto predict dogmatism, and (2) to test the domain-\nindependence of these strategies. First, we test the\nperformance of our model under cross-validation\nwithin the Reddit comment dataset. We then eval-\nuate the Reddit-based model on a held out corpus\nof New York Times comments annotated using the', '\ntechnique in Section 2. We did not refer to this sec-\nond dataset during feature construction.\nFor classiﬁcation, we consider two classes of\ncomments: dogmatic andnon-dogmatic . As in the\nprior analysis, we draw these comments from the top\nand bottom quartiles of the dogmatism distribution.\nThis means the classes are balanced, with 2,500 total\ncomments in the Reddit training data and 500 total\ncomments in the New York Times testing data.\nWe compare the predictions of logistic regression\nmodels based on unigram bag-of-words features\n(BOW), sentiment signals2(SENT), the linguistic\n2For SENT, we use normalized word counts from LIWC’s\npositive and negative emotional categories.']","Present tense, past tense, negative emotion, swearing, interrogative language, and negation are some linguistic signals of dogmatic language.",simple,TRUE
1,How does the previous vowel affect the pronunciation of words in English?,"[' ein English that affects the pro-\nnunciation of the previous vowel, as seen in the\npair of words cape andcap.\nIt is possible for an orthographic system to have\nany or all of the above phenomena while remain-\ning unambiguous. However, some orthographic\n1The Chinese script, in which characters have both phono-\nlogical form and semantic meaning, is the best-known excep-\ntion.']",The previous vowel affects the pronunciation of words in English through the phenomenon of vowel reduction.,simple,TRUE
2,How does the proposed cross-lingual pretraining based transfer approach improve zero-shot translation?,"['SA with back translation also achieves better performance\nthan the original supervised Transformer.\nAnalysis\nSentence Representation. We ﬁrst evaluate the represen-\ntational invariance across languages for all cross-lingual pre-\ntraining methods. Following Arivazhagan et al. (2018), we\nadopt max-pooling operation to collect the sentence rep-\nresentation of each encoder layer for all source-pivot sen-\ntence pairs in the Europarl validation sets. Then we calcu-\nlate the cosine similarity for each sentence pair and aver-\nage all cosine scores. As shown in Figure 3, we can ob-\nserve that, MLM+BRLM-SA has the most stable and similar\ncross-lingual representations of sentence pairs on all layers,\nwhile it achieves the best performance in zero-shot transla-\ntion. This demonstrates that better cross-lingual representa-\ntions can beneﬁt for the process of transfer learning. Besides,\nMLM+BRLM-HA is not as superior as MLM+BRLM-\nSA and even worse than MLM+TLM on Fr-En, since\nMLM+BRLM-HA may suffer from the wrong alignment\nknowledge from an external aligner tool. We also ﬁnd an in-\nteresting phenomenon that as the number of layers increases,\nthe cosine similarity decreases.\nContextualized Word Representation. We further sam-\nple an English-Russian sentence pair from the MultiUN\nvalidation sets and visualize the cosine similarity between\nhidden states of the top encoder layer to further investi-\ngate the difference of all cross-lingual pre-training meth-\nods. As shown in Figure 4, the hidden states generated by\nMLM+BRLM-SA have higher similarity for two aligned\nwords. It indicates that MLM+BRLM-SA can gain bet-\nter word-level representation alignment between source and\npivot languages, which better relieves the burden of the do-\nmain shift problem .\nThe Effect of Freezing Parameters. To freeze parame-\nters is a common strategy to avoid catastrophic forgetting in\ntransfer learning (Howard and Ruder 2018). Table 4 shows\nthe performance of transfer learning with freezing different\nlayers on MultiUN test set, in which En →Ru denotes the\nparent model, Ar→Ru and Es→Ru are two child models,\nand all models are based on MLM+BRLM-SA. We can ﬁnd\nthat updating all parameters during training will cause a no-\ntable drop on the zero-shot direction due to the catastrophic\nforgetting. On the contrary, freezing all the parameters leads\nto the decline on supervised direction because the language\nfeatures extracted during pre-training is not sufﬁcient for\nMT task. Freezing the ﬁrst four layers of the transformer\nshows the best performance and keeps the balance between\npre-training and ﬁne-tuning.\nConclusion\nIn this paper, we propose a cross-lingual pretraining based\ntransfer approach for the challenging zero-shot translation\ntask, in which source and target languages have no parallel\ndata, while they both have parallel data with a high resource\n(a) MLM\n (b) MLM+TLM\n(c) MLM+BRLM-HA\n (d) MLM+BRLM-SA\nFigure 4: Cosine similarity visualization at word level given\nan English-Russian sentence pair from the MultiUN valida-\ntion sets. Brighter indicates higher similarity.\nFreezing Layers En→Ru Ar→Ru Es→Ru\nNone 37.80 16.09 19.80\n2 37.79 21.47 28.35\n4 37.55 25.49 30.47\n6 35.31 22.90 28.22\nTable 4: BLEU score of freezing different layers. The num-\nber in Freezing Layers column denotes that the number of\nencoder layers will not be updated.\npivot language. With the aim of building the language in-\nvariant representation between source and pivot languages\nfor smooth transfer of the parent model of pivot →target di-\nrection to the child model of source →target direction, we in-\ntroduce one monolingual pretraining method and two bilin-\ngual pretraining methods to construct an universal encoder\nfor the source and pivot languages. Experiments on public\ndatasets show that our approaches signiﬁcantly outperforms\nseveral strong baseline systems, and manifest the language\ninvariance characteristics in both sentence level and word\nlevel neural representations.\nAcknowledgments\nWe would like', 'MultiUN Ar,Es,Ru↔En\nDirection Ar→Es Es→Ar Ar→Ru Ru→Ar Es→Ru Ru→Es A-ZST A-ST\nBaselines\nCross-lingual Transfer 10.26 12.44 4.58 4.42 13.80 7.93 8.90 44.73\nMNMT(Johnson et al. 2016) 27.40 20.18 15.12 16.19 17.88 27.93 20.78 43.95\nPivoting m 42.29 30.15 27.23 26.16 29.57 40.08 32.58 43.95\nProposed Cross-lingual Pretraining Based Transfer\nMLM 16.50 23.41 9.61 14.23 22.80 23.66 18.36 44.25\nMLM+TLM 25.98 26.55 16.84 20.07 25.91 29.52 24.14 43.71\nMLM+BRLM-HA 29.05 27.58 18.10 20.42 25.39 30.96 25.25 44.67\nMLM+BRLM-SA 36.01 31.08 25.49 25.06 30.47 36.01 30.68 44.54\nAdding Back Translation\nMNMT* (Gu et al. 2019) 39.72 28.05 24.67 24.43 27.41 38.01 30.38 43.98\nMLM 40.98 31.53 26.06 26.69 31.28 40.02 32.76 44.28\nMLM+TLM 41.15 29.77 27.61 27.74 31.02 40.37 32.39 44.14\nMLM+BRLM-HA 41.74 31.89 27.24 27.54 31.29 40.34 33.35 44.52\nMLM+BRLM-SA 44.17 33.20 29.01 28.91 32.53 41.93 34.95 45.49\nTable 3: Results on MultiUN test sets. The six zero-shot translation directions are evaluated. The column “A-ZST"" reports av-\neraged BLEU of zero-shot translation, while the column “A-ST"" reports averaged BLEU of supervised pivot →target direction.\n(a) Fr-En\n (b) De-En\n (c) Ro-En\nFigure 3: Cosine similarity between sentence representation of each encoder layer across all source-pivot sentence pairs in the\nEuroparl validation set.\ncoder trained on both large-scale monolingual data and par-\nallel data between multiple languages.\nMLM alone that does not use source ↔pivot parallel data\nperforms much better than the cross-lingual transfer, and\nachieves comparable results to pivoting. When MLM is\ncombined with TLM or the proposed BRLM, the perfor-\nmance is further improved. MLM+BRLM-SA performs the\nbest, and is better than MLM+BRLM-HA indicating that\nsoft alignment is helpful than hard alignment for the cross-\nlingual pretraining.\nResults on MultiUN Dataset. Like experimental results\non Europarl, MLM+BRLM-SA performs the best among\nall proposed cross-lingual pretraining based transfer ap-\nproaches as shown in Table 3. When comparing systems\nconsisting of one encoder-decoder model for all zero-shot\ntranslation, our approaches performs signiﬁcantly better\nthan MNMT (Johnson et al. 2016).Although it is challenging for one model to translate all\nzero-shot directions between multiple distant language pairs\nof MultiUN, MLM+BRLM-SA still achieves better perfor-\nmances on Es→Ar and Es→Ru than strong pivoting m,\nwhich uses MNMT to translate source to pivot then to tar-\nget in two separate steps with each step receiving supervised\nsignal of parallel corpora. Our approaches surpass pivoting m\nin all zero-shot directions by adding back translation (Sen-\nnrich, Haddow, and Birch 2015) to generate pseudo parallel\nsentences for all zero-shot directions based on our pretrained\nmodels such as MLM+BRLM-SA, and further training our\nuniversal encoder-decoder model with these pseudo data.\nGu et al. (2019) introduces back translation into MNMT,\nwhile we adopt it in our transfer approaches. Finally, our\nbest MLM+BRLM-SA with back translation outperforms\npivoting mby 2.4 BLEU points averagely, and outperforms\nMNMT (Gu et al. 2019) by 4.6']",nan,simple,TRUE
3,What are the two natural language processing tasks that deal with the concept of simplified language?,"['arXiv:1909.09067v1  [cs.CL]  19 Sep 2019A Corpus for Automatic Readability Assessment and Text Simp liﬁcation\nof German\nAlessia Battisti\nInstitute of Computational Linguistics\nUniversity of Zurich\nAndreasstrasse 15, 8050 Zurich\nalessia.battisti@uzh.chSarah Ebling\nInstitute of Computational Linguistics\nUniversity of Zurich\nAndreasstrasse 15, 8050 Zurich\nebling@cl.uzh.ch\nAbstract\nIn this paper, we present a corpus for use in\nautomatic readability assessment and auto-\nmatic text simpliﬁcation of German. The\ncorpus is compiled from web sources and\nconsists of approximately 211,000 sen-\ntences. As a novel contribution, it con-\ntains information on text structure, typog-\nraphy, and images, which can be exploited\nas part of machine learning approaches to\nreadability assessment and text simpliﬁca-\ntion. The focus of this publication is on\nrepresenting such information as an exten-\nsion to an existing corpus standard.\n1 Introduction\nSimpliﬁed language is a variety of standard lan-\nguage characterized by reduced lexical and syn-\ntactic complexity, the addition of explanations\nfor difﬁcult concepts, and clearly structured lay-\nout.1Among the target groups of simpliﬁed lan-\nguage commonly mentioned are persons with cog-\nnitive impairment or learning disabilities, prelin-\ngually deaf persons, functionally illiterate persons,\nand foreign language learners (Bredel and Maaß,\n2016).\nTwo natural language processing tasks deal with\nthe concept of simpliﬁed language: automatic\nreadability assessment and automatic text simpli-\nﬁcation. Readability assessment refers to the pro-\ncess of determining the level of difﬁculty of a text,\ne.g., along readability measures, school grades, or\nlevels of the Common European Framework of\nReference for Languages (CEFR) (Council of Eu-\nrope, 2009). Readability measures, in their tra-\nditional form, take into account only surface fea-\ntures. For example, the Flesch Reading Ease Score\n1The term plain language is avoided, as it refers to a spe-\nciﬁc level of simpliﬁcation. Simpliﬁed language subsumes all\nefforts of reducing the complexity of a piece of text.(Flesch, 1948) measures the length of words (in\nsyllables) and sentences (in words). While read-\nability has been shown to correlate with such fea-\ntures to some extent (Just and Carpenter, 1980), a\nconsensus has emerged according to which they\nare not sufﬁcient to account for all of the com-\nplexity inherent in a text. As Kauchak et al.\n(2014, p. 2618) state, “the usability of readabil-\nity formulas is limited and there is little evidence\nthat the output of these tools directly results in\nimproved understanding by readers”. Recently,\nmore sophisticated models employing (deeper) lin-\nguistic features such as lexical, semantic, mor-\nphological, morphosyntactic, syntactic, pragmatic,\ndiscourse, psycholinguistic, and language model\nfeatures have been proposed (Collins-Thompson,\n2014; Heimann M¨ uhlenbock, 2013; Pitler and\nNenkova, 2008; Schwarm and Ostendorf, 2005;\nTanaka et al., 2013).\nAutomatic text simpliﬁcation was initiated in\nthe late 1990s (Carroll et al., 1998; Chandrasekar\net al., 1996) and since then has been approached\nby means of rule-based and statistical methods. As\npart of a rule-based approach, the operations car-\nried out typically include replacing complex lex-\nical and syntactic units by simpler ones. A sta-\ntistical approach generally conceptualizes the sim-\npliﬁcation task as one of converting a standard-\nlanguage into a simpliﬁed-language text using ma-\nchine translation. Nisioi et al. (2017) introduced\nneural machine translation to automatic text sim-\npliﬁcation. Research on automatic text simpliﬁ-\ncation is comparatively widespread for languages\nsuch as English, Swedish, Spanish, and Brazilian\nPortuguese. To the authors’ knowledge,', ' no pro-\nductive system exists for German. Suter (2015),\nSuter et al. (2016) presented a prototype of a rule-\nbased system for German.\nMachine learning approaches to both readabil-\nity assessment and text simpliﬁcation rely on\ndata systematically prepared in the form of cor-', 'pora. Speciﬁcally, for automatic text simpliﬁca-\ntion via machine translation, pairs of standard-\nlanguage/simpliﬁed-language texts aligned at the\nsentence level (i.e., parallel corpora) are needed.\nThe paper at hand introduces a corpus devel-\noped for use in automatic readability assessment\nand automatic text simpliﬁcation of German. The\nfocus of this publication is on representing infor-\nmation that is valuable for these tasks but that hith-\nerto has largely been ignored in machine learning\napproaches centering around simpliﬁed language,\nspeciﬁcally, text structure (e.g., paragraphs, lines),\ntypography (e.g., font type, font style), and im-\nage (content, position, and dimensions) informa-\ntion. The importance of considering such infor-\nmation has repeatedly been asserted theoretically\n(Arf´ e et al., 2018; Bock, 2018; Bredel and Maaß,\n2016).\nThe remainder of this paper is structured as fol-\nlows: Section 2 presents previous corpora used for\nautomatic readability assessment and text simpliﬁ-\ncation. Section 3 describes our corpus, introduc-\ning its novel aspects and presenting the primary\ndata (Section 3.1), the metadata (Section 3.2), the\nsecondary data (Section 3.3), the proﬁle (Section\n3.4), and the results of machine learning experi-\nments carried out on the corpus (Section 3.5).\n2 Previous Corpora for Automatic\nReadability Assessment and Automatic\nText Simpliﬁcation\nA number of corpora for use in automatic read-\nability assessment and automatic text simpliﬁca-\ntion exist. The most well-known example is the\nParallel Wikipedia Simpliﬁcation Corpus (PWKP)\ncompiled from parallel articles of the English\nWikipedia and Simple English Wikipedia (Zhu et\nal., 2010) and consisting of around 108,000 sen-\ntence pairs. The corpus proﬁle is shown in Table 1.\nWhile the corpus represents the largest dataset in-\nvolving simpliﬁed language to date, its applica-\ntion has been criticized for various reasons (Aman-\ncio and Specia, 2014; Xu et al., 2015; ˇStajner et\nal., 2018); among these, the fact that Simple En-\nglish Wikipedia articles are not necessarily direct\ntranslations of articles from the English Wikipedia\nstands out. Hwang et al. (2015) provided an up-\ndated version of the corpus that includes a total\nof 280,000 full and partial matches between the\ntwo Wikipedia versions. Another frequently used\ndata collection for English is the Newsela Corpus(Xu et al., 2015) consisting of 1,130 news articles,\neach simpliﬁed into four school grade levels by\nprofessional editors. Table 2 shows the proﬁle of\nthe Newsela Corpus. The table obviates that the\ndifference in vocabulary size between the English\nand the simpliﬁed English side of the PWKP Cor-\npus amounts to only 18%, while the corresponding\nnumber for the English side and the level repre-\nsenting the highest amount of simpliﬁcation in the\nNewsela Corpus (Simple-4) is 50.8%. V ocabulary\nsize as an indicator of lexical richness is generally\ntaken to correlate positively with complexity (Vaj-\njala and Meurers, 2012).\nGasperin et al. (2010) compiled the PorSimples\nCorpus consisting of Brazilian Portuguese texts\n(2,116 sentences), each with a natural and a strong\nsimpliﬁcation, resulting in around 4,500 aligned\nsentences. Drndarevi´ c and Saggion (2012), Bott\net al. (2012), Bott and Saggion (2012) produced\nthe Simplext Corpus consisting of 200 Span-\nish/simpliﬁed Spanish document pairs, amount-\ning to a total of 1,149 (Spanish)/1,808 (simpliﬁed\nSpanish) sentences (approximately 1,000 aligned\nsentences).\nKlaper et al. (2013) created the ﬁrst parallel cor-\npus for German/simpliﬁed German, consisting of\n256 parallel texts downloaded from the web (ap-\nproximately 70,000 tokens).\n3 Building a Corpus for Automatic\nRead']",The two natural language processing tasks that deal with the concept of simplified language are automatic readability assessment and automatic text simplification.,simple,TRUE
4,How do our approaches using NMT (MNMT) compare to other approaches in terms of performance?,"[' pivoting, multilingual\nNMT (MNMT) (Johnson et al. 2016), and cross-lingual\ntransfer without pretraining (Kim, Gao, and Ney 2019). The\nresults show that our approaches consistently outperform\nother approaches across languages and datasets, especially\nsurpass pivoting, which is a strong baseline in the zero-\nshot scenario that multilingual NMT systems often fail to\nbeat (Johnson et al. 2016; Al-Shedivat and Parikh 2019;\nArivazhagan et al. 2018). Pivoting translates source to\npivot then to target in two steps, causing inefﬁcient trans-\nlation process. Our approaches use one encoder-decoder\nmodel to translate between any zero-shot directions, which\nis more efﬁcient than pivoting. Regarding the comparison\nbetween transfer approaches, our cross-lingual pretraining\nbased transfer outperforms transfer method that does not use\npretraining by a large margin.\nResults on Europarl Dataset. Regarding comparison be-\ntween the baselines in table 2, we ﬁnd that pivoting is the\nstrongest baseline that has signiﬁcant advantage over other\ntwo baselines. Cross-lingual transfer for languages without\nshared vocabularies (Kim, Gao, and Ney 2019) manifests the\nworst performance because of not using source ↔pivot par-\nallel data, which is utilized as beneﬁcial supervised signal\nfor the other two baselines.\nOur best approach of MLM+BRLM-SA achieves the sig-\nniﬁcant superior performance to all baselines in the zero-\nshot directions, improving by 0.9-4.8 BLEU points over the\nstrong pivoting. Meanwhile, in the supervised direction of\npivot→target, our approaches performs even better than the\noriginal supervised Transformer thanks to the shared en-']",nan,simple,TRUE
5,What are the results obtained by the NLP community using BERT for other tasks?,"['be more dangerous than the unintended over-obfuscation of\nnon-sensitive text.\nFurther, we have conducted an additional experiment on\nthis dataset by progressively reducing the training data for\nall the compared systems. The BERT-based model shows\nthe highest robustness to training-data scarcity, loosing only\n7 points of F1-score when trained on 230 instances instead\nof 21,371. These observation are in line with the results ob-\ntained by the NLP community using BERT for other tasks.\nThe experiments with the MEDDOCAN 2019 shared task\ndataset follow the same pattern. In this case, the BERT-\nbased model falls 0.3 F1-score points behind the shared task\nwinning system, but it would have achieved the second po-\nsition in the competition with no further reﬁnement.\nSince we have used a pre-trained multilingual BERT model,\nthe same approach is likely to work for other languages just\nby providing some labelled training data. Further, this is the\nsimplest ﬁne-tuning that can be performed based on BERT.\nMore sophisticated ﬁne-tuning layers could help improve\nthe results. For example, it could be expected that a CRF\nlayer helped enforce better BIO tagging sequence predic-\ntions. Precisely, Mao and Liu (2019) participated in the\nMEDDOCAN competition using a BERT+CRF architec-\nture, but their reported scores are about 3 points lower than\nour implementation. From the description of their work, it\nis unclear what the source of this score difference could be.\nFurther, at the time of writing this paper, new multilingual\npre-trained models and Transformer architectures have be-\ncome available. It would not come as a surprise that these\nnew resources and systems –e.g., XLM-RoBERTa (Con-\nneau et al., 2019) or BETO (Wu and Dredze, 2019), a BERT\nmodel fully pre-trained on Spanish texts– further advanced\nthe state of the art in this task.\n6. Acknowledgements\nThis work has been supported by Vicomtech and partially\nfunded by the project DeepReading (RTI2018-096846-B-\nC21, MCIU/AEI/FEDER,UE).\n7. Bibliographical References\nAbouelmehdi, K., Beni-Hessane, A., and Khalouﬁ, H.\n(2018). Big healthcare data: preserving security and pri-\nvacy. Journal of Big Data , 5(1):1–18.\nAgerri, R., Bermudez, J., and Rigau, G. (2014). IXA\npipeline: Efﬁcient and Ready to Use Multilingual NLP\ntools. In Proceedings of the 9th Language Resources and\nEvaluation Conference (LREC 2014) , pages 3823–3828.\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary,\nV ., Wenzek, G., Guzm ´an, F., Grave, E., Ott, M.,\nZettlemoyer, L., and Stoyanov, V . (2019). Unsuper-\nvised Cross-lingual Representation Learning at Scale.\narXiv:1911.02116 .\nDernoncourt, F., Lee, J. Y ., Uzuner, ¨O., and Szolovits, P.\n(2016). De-identiﬁcation of Patient Notes with Recur-\nrent Neural Networks. Journal of the American Medical\nInformatics Association , 24(3):596–606.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K.\n(2019). BERT: Pre-training of Deep Bidirectional Trans-\nformers for Language Understanding. In Proceedings of\nthe 2019 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Pa-\npers) , pages 4171–4186.\nGarc ´ıa-Sardi ˜na, L. (2018). Automating the anonymisa-\ntion of textual corpora. Master’s thesis, University of the\nBasque Country (UPV/EHU).\nHassan, F., Domingo-Ferrer, J., and Soria-Comas, J.\n(2018). Anonimizaci ´on de datos no estructurados a\ntrav´es del reconocimiento de entidades nominadas. In', '0 50 1000.40.60.81\ntrain data (%)precision\n0 50 1000.40.60.81\ntrain data (%)recall\n0 50 1000.40.60.81\ntrain data (%)F1-score\nCRF spaCy BERT\nFigure 2: Performance decay with decreasing amounts of training data on the sensitive information detection task in the\nNUB ES-PHI corpus\nDetection Classiﬁcation\nPrec Rec F1 Prec Rec F1\nCRF (Perez et al., 2019) 0.977 0.943 0.960 0.971 0.937 0.954\nspaCy (Perez et al., 2019) 0.967 0.953 0.965 0.965 0.947 0.956\nNLND S2 (Lange et al., 2019) 0.976 0.973 0.974 0.971 0.968 0.970\nNLND S3 (Lange et al., 2019) 0.975 0.975 0.975 0.970 0.969 0.970\nBERT + CRF (Mao and Liu, 2019) 0.968 0.919 0.943 0.965 0.912 0.937\nBERT 0.973 0.972 0.972 0.968 0.967 0.967\nTable 8: Results of Experiment B: MEDDOCAN\na larger extent from recall decline, which is not that marked\nin the case of BERT. Overall, these results indicate that the\ntransfer-learning achieved through the BERT multilingual\npre-trained model not only helps obtain better results, but\nalso lowers the need of manually labelled data for this ap-\nplication domain.\n4.2. Experiment B: MEDDOCAN\nThe results of the two MEDDOCAN scenarios –detection\nand classiﬁcation– are shown in Table 8. These results fol-\nlow the same pattern as in the previous experiments, with\nthe CRF classiﬁer being the most precise of all, and BERT\noutperforming both the CRF and spaCy classiﬁers thanks to\nits greater recall. We also show the results of Mao and Liu\n(2019) who, despite of having used a BERT-based system,\nachieve lower scores than our models. The reason why it\nshould be so remain unclear.\nWith regard to the winner of the MEDDOCAN shared\ntask, the BERT-based model has not improved the scores\nobtained by neither the domain-dependent (S3) nor the\ndomain-independent (S2) NLNDE model. However, at-\ntending to the obtained results, BERT remains only 0.3 F1-\nscore points behind, and would have achieved the second\nposition among all the MEDDOCAN shared task competi-\ntors. Taking into account that only 3% of the gold labels\nremain incorrectly annotated, the task can be considered al-\nmost solved, and it is not clear if the differences among the\nsystems are actually signiﬁcant, or whether they stem from\nminor variations in initialisation or a long-tail of minor la-belling inconsistencies.\n5. Conclusions and Future Work\nIn this work we have brieﬂy introduced the problems re-\nlated to data privacy protection in clinical domain. We have\nalso described some of the groundbreaking advances on the\nNatural Language Processing ﬁeld due to the appearance of\nTransformers-based deep-learning architectures and trans-\nfer learning from very large general-domain multilingual\ncorpora, focusing our attention in one of its most represen-\ntative examples, Google’s BERT model.\nIn order to assess the performance of BERT for Span-\nish clinical data anonymisation, we have conducted sev-\neral experiments with a BERT-based sequence labelling\napproach using the pre-trained multilingual BERT model\nshared by Google as the starting point for the model train-\ning. We have compared this BERT-based sequence la-\nbelling against other methods and systems. One of the ex-\nperiments uses the MEDDOCAN 2019 shared task dataset,\nwhile the other uses a novel Spanish clinical reports dataset\ncalled NUB ES-PHI.\nThe results of the experiments show that, in NUB ES-PHI,\nthe BERT-based model outperforms the other systems with-\nout requiring any adaptation or domain-speciﬁc feature en-\ngineering, just by being trained on the provided labelled\ndata. Interestingly, the BERT-based model obtains a re-\n']",nan,simple,TRUE
6,What is the Cloze objective of MLM in cross-lingual pre-training?,"['Figure 2: The overview of BRidge Language Modeling (BRLM). The BRLM extends MLM (Lample and Conneau 2019) to\npairs of parallel sentences and leverages explicit alignment information obtained by external aligner tool or additional attention\nlayer to encourage word representation alignment across different languages.\n•Directly translate source sentences into target sentences\nwith the parent model, which beneﬁts from the availabil-\nity of the universal encoder.\nThe key difﬁculty of this method is to ensure the intermedi-\nate representations of the universal encoder are language in-\nvariant. In the rest of this section, we ﬁrst present two exist-\ning methods yet to be explored in zero-shot translation, and\nthen propose a straightforward but effective cross-lingual\npre-training method. In the end, we present the whole train-\ning and inference protocol for transfer.\nMasked and Translation Language Model\nPretraining\nTwo existing cross-lingual pre-training methods, Masked\nLanguage Modeling (MLM) and Translation Language\nModeling (TLM), have shown their effectiveness on XNLI\ncross-lingual classiﬁcation task (Lample and Conneau 2019;\nHuang et al. 2019), but these methods have not been well\nstudied on cross-lingual generation tasks in zero-shot condi-\ntion. We attempt to take advantage of the cross-lingual abil-\nity of the two methods for zero-shot translation.\nSpeciﬁcally, MLM adopts the Cloze objective of\nBERT (Devlin et al. 2018) and predicts the masked words\nthat are randomly selected and replaced with [MASK] to-\nken on monolingual corpus. In practice, MLM takes dif-\nferent language monolingual corpora as input to ﬁnd fea-\ntures shared across different languages. With this method,\nword pieces shared in all languages have been mapped into\na shared space, which makes the sentence representationsacross different languages close (Pires, Schlinger, and Gar-\nrette 2019).\nSince MLM objective is unsupervised and only requires\nmonolingual data, TLM is designed to leverage parallel data\nwhen it is available. Actually, TLM is a simple extension of\nMLM, with the difference that TLM concatenates sentence\npair into a whole sentence, and then randomly masks words\nin both the source and target sentences. In this way, the\nmodel can either attend to surrounding words or to the trans-\nlation sentence, implicitly encouraging the model to align\nthe source and target language representations. Note that al-\nthough each sentence pair is formed into one sentence, the\npositions of the target sentence are reset to count form zero.\nBridge Language Model Pretraining\nAside from MLM and TLM, we propose BRidge Language\nModeling (BRLM) to further obtain word-level representa-\ntion alignment between different languages. This method is\ninspired by the assumption that if the feature spaces of dif-\nferent languages are aligned very well, the masked words\nin the corrupted sentence can also be guessed by the con-\ntext of the correspondingly aligned words on the other side.\nTo achieve this goal, BRLM is designed to strengthen the\nability to infer words across languages based on alignment\ninformation, instead of inferring words within monolingual\nsentence as in MLM or within the pseudo sentence formed\nby concatenating sentence pair as in TLM.\nAs illustrated in Figure 2, BRLM stacks shared encoder\nover both side sentences separately. In particular, we design\ntwo network structures for BRLM, which are divided into']",The Cloze objective of MLM in cross-lingual pre-training is to predict the masked words that are randomly selected and replaced with [MASK] token on monolingual corpus.,simple,TRUE
7,What is the main challenge that leads to the failure of zero-shot translation in transfer learning?,"['Cross-lingual Pre-training Based Transfer for Zero-shot Neural\nMachine Translation\nBaijun Ji‡, Zhirui Zhang§, Xiangyu Duan†‡∗, Min Zhang†‡, Boxing Chen§and Weihua Luo§\n†Institute of Artiﬁcial Intelligence, Soochow University, Suzhou, China\n‡School of Computer Science and Technology, Soochow University, Suzhou, China\n§Alibaba DAMO Academy, Hangzhou, China\n‡bjji@stu.suda.edu.cn†{xiangyuduan, minzhang}@suda.edu.cn\n§{zhirui.zzr,boxing.cbx,weihua.luowh}@alibaba-inc.com\nAbstract\nTransfer learning between different language pairs has shown\nits effectiveness for Neural Machine Translation (NMT) in\nlow-resource scenario. However, existing transfer methods\ninvolving a common target language are far from success in\nthe extreme scenario of zero-shot translation, due to the lan-\nguage space mismatch problem between transferor (the par-\nent model) and transferee (the child model) on the source\nside. To address this challenge, we propose an effective trans-\nfer learning approach based on cross-lingual pre-training. Our\nkey idea is to make all source languages share the same fea-\nture space and thus enable a smooth transition for zero-shot\ntranslation. To this end, we introduce one monolingual pre-\ntraining method and two bilingual pre-training methods to\nobtain a universal encoder for different languages. Once the\nuniversal encoder is constructed, the parent model built on\nsuch encoder is trained with large-scale annotated data and\nthen directly applied in zero-shot translation scenario. Exper-\niments on two public datasets show that our approach signif-\nicantly outperforms strong pivot-based baseline and various\nmultilingual NMT approaches.\nIntroduction\nAlthough Neural Machine Translation (NMT) has domi-\nnated recent research on translation tasks (Wu et al. 2016;\nVaswani et al. 2017; Hassan et al. 2018), NMT heavily relies\non large-scale parallel data, resulting in poor performance\non low-resource or zero-resource language pairs (Koehn\nand Knowles 2017). Translation between these low-resource\nlanguages (e.g., Arabic →Spanish) is usually accomplished\nwith pivoting through a rich-resource language (such as En-\nglish), i.e., Arabic (source) sentence is translated to En-\nglish (pivot) ﬁrst which is later translated to Spanish (tar-\nget) (Kauers et al. 2002; de Gispert and Mariño 2006).\nHowever, the pivot-based method requires doubled decoding\ntime and suffers from the propagation of translation errors.\nOne common alternative to avoid pivoting in NMT is\ntransfer learning (Zoph et al. 2016; Nguyen and Chiang\n2017; Kocmi and Bojar 2018; Kim et al. 2019) which lever-\nages a high-resource pivot →target model ( parent ) to ini-\n∗Corresponding Author.\nCopyright c⃝2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The circle and triangle dots represent source sen-\ntences in different language l1andl2, and the square dots\nmeans target sentences in language l3. A sample of transla-\ntion pairs is connected by the dashed line. We would like to\nforce each of the translation pairs has the same latent rep-\nresentation as the right part of the ﬁgure so as to transfer\nl1→l3model directly to l2→l3model.\ntialize a low-resource source →target model ( child ) that is\nfurther optimized with a small amount of available paral-\nlel data. Although this approach has achieved success in\nsome low-resource language pairs, it still performs very\npoorly in extremely low-resource or zero-resource transla-\ntion scenario. Speciﬁcally, Kocmi and Bojar (2018) reports\nthat without any child model training data, the performance\nof the parent model on the child test set is miserable.\nIn this work, we argue that the language space mis-\nmatch problem, also named domain shift problem (Fu et al.\n2015), brings about the zero-shot translation failure in trans-\nfer learning. It is because transfer learning has no explicit\ntraining process to guarantee that the source and pivot', 'as shown in the right of Figure 1. One way to achieve this\ngoal is the ﬁne-tuning technique, which forces the model to\nforget the speciﬁc knowledge from parent data and learn new\nfeatures from child data. However, the domain shift problem\nstill exists, and the demand of parallel child data for ﬁne-\ntuning heavily hinders transfer learning for NMT towards\nthe zero-resource setting.\nIn this paper, we explore the transfer learning in\na common zero-shot scenario where there are a lot\nof source↔pivot and pivot↔target parallel data but no\nsource↔target parallel data. In this scenario, we propose\na simple but effective transfer approach, the key idea\nof which is to relieve the burden of the domain shift\nproblem by means of cross-lingual pre-training. To this\nend, we ﬁrstly investigate the performance of two exist-\ning cross-lingual pre-training methods proposed by Lam-\nple and Conneau (2019) in zero-shot translation scenario.\nBesides, a novel pre-training method called BRidge Lan-\nguage Modeling (BRLM) is designed to make full use of the\nsource↔pivot bilingual data to obtain a universal encoder\nfor different languages. Once the universal encoder is con-\nstructed, we only need to train the pivot →target model and\nthen test this model in source →target direction directly. The\nmain contributions of this paper are as follows:\n•We propose a new transfer learning approach for NMT\nwhich uses the cross-lingual language model pre-training\nto enable a high performance on zero-shot translation.\n•We propose a novel pre-training method called BRLM,\nwhich can effectively alleviates the distance between dif-\nferent source language spaces.\n•Our proposed approach signiﬁcantly improves zero-shot\ntranslation performance, consistently surpassing pivot-\ning and multilingual approaches. Meanwhile, the perfor-\nmance on supervised translation direction remains the\nsame level or even better when using our method.\nRelated Work\nIn recent years, zero-shot translation in NMT has attracted\nwidespread attention in academic research. Existing meth-\nods are mainly divided into four categories: pivot-based\nmethod, transfer learning, multilingual NMT, and unsuper-\nvised NMT.\n•Pivot-based Method is a common strategy to obtain a\nsource→target model by introducing a pivot language.\nThis approach is further divided into pivoting and pivot-\nsynthetic. While the former ﬁrstly translates a source lan-\nguage into the pivot language which is later translated\nto the target language (Kauers et al. 2002; de Gispert\nand Mariño 2006; Utiyama and Isahara 2007), the lat-\nter trains a source→target model with pseudo data gener-\nated from source-pivot or pivot-target parallel data (Chen\net al. 2017; Zheng, Cheng, and Liu 2017). Although the\npivot-based methods can achieve not bad performance, it\nalways falls into a computation-expensive and parameter-\nvast dilemma of quadratic growth in the number of source\nlanguages, and suffers from the error propagation prob-\nlem (Zhu et al. 2013).•Transfer Learning is ﬁrstly introduced for NMT by\nZoph et al. (2016), which leverages a high-resource par-\nent model to initialize the low-resource child model. On\nthis basis, Nguyen and Chiang (2017) and Kocmi and\nBojar (2018) use shared vocabularies for source/target\nlanguage to improve transfer learning, while Kim, Gao,\nand Ney (2019) relieve the vocabulary mismatch by\nmainly using cross-lingual word embedding. Although\nthese methods are successful in the low-resource scene,\nthey have limited effects in zero-shot translation.\n•Multilingual NMT (MNMT) enables training a single\nmodel that supports translation from multiple source lan-\nguages into multiple target languages, even those unseen\nlanguage pairs (Firat, Cho, and Bengio 2016; Firat et al.\n2016; Johnson et al. 2016; Al-Shedivat and Parikh 2019;\nAharoni, Johnson, and Firat 2019). Aside from sim-\npler deployment, MNMT beneﬁts from transfer learning\nwhere low-resource language pairs are trained together\nwith high-resource ones. However, Gu et al. (2019) point\nout that MNMT for zero-shot translation easily fails, and\nis sensitive to the hyper-parameter setting. Also']",The main challenge that leads to the failure of zero-shot translation in transfer learning is the language space mismatch problem between the transferor (the parent model) and transferee (the child model) on the source side.,simple,TRUE
8,What is the significance of zero-shot learning capability in relation extraction?,"['cation .arXiv:1704.06194v2  [cs.CL]  27 May 2017', 'Question: what episode was mike kelleythe writer of\nKnowledgeBaseMike\tKelley(American\ttelevision\twriter/producer)Mike\tKelley(American\tbaseball\tplayer)…Entity Linking \nLove\tWill\tFind\ta\tWayUSA…First\tbaseman…episodes_written\nposition_playedRelation Detection(a)(b)Question: what tvshow did grantshowplay onin2008Mike\tKelley?episodes_writtenEntity Linking RelationDetectionGrantShow?starring_rolesseries(date)from2008ConstraintDetectionGrantShow\t(American\tactor)SwingTown\nBigLoveepisodesScoundrelsseries2011from20102008Figure 1: KBQA examples and its three key components. (a) A single relation example. We ﬁrst identify the topic entity with\nentity linking and then detect the relation asked by the question with relation detection (from all relations connecting the topic\nentity). Based on the detected entity and relation, we form a query to search the KB for the correct answer “ Love Will Find a\nWay”. (b) A more complex question containing two entities. By using “ Grant Show ” as the topic entity, we could detect a chain\nof relations “ starring roles-series ” pointing to the answer. An additional constraint detection takes the other entity “ 2008 ” as\na constraint, to ﬁlter the correct answer “ SwingTown ” from all candidates found by the topic entity and relation.\nthat original relation names can sometimes help\nto match longer question contexts, we propose\nto build both relation-level and word-level rela-\ntion representations. Third, we use deep bidirec-\ntional LSTMs ( BiLSTM s) to learn different levels\nof question representations in order to match the\ndifferent levels of relation information. Finally, we\npropose a residual learning method for sequence\nmatching, which makes the model training easier\nand results in more abstract (deeper) question rep-\nresentations, thus improves hierarchical matching.\nIn order to assess how the proposed improved\nrelation detection could beneﬁt the KBQA end\ntask, we also propose a simple KBQA implemen-\ntation composed of two-step relation detection .\nGiven an input question and a set of candidate enti-\nties retrieved by an entity linker based on the ques-\ntion, our proposed relation detection model plays a\nkey role in the KBQA process: (1) Re-ranking the\nentity candidates according to whether they con-\nnect to high conﬁdent relations detected from the\nraw question text by the relation detection model.\nThis step is important to deal with the ambigui-\nties normally present in entity linking results. (2)\nFinding the core relation (chains) for each topic\nentity2selection from a much smaller candidate\nentity set after re-ranking. The above steps are\nfollowed by an optional constraint detection step,\nwhen the question cannot be answered by single\nrelations (e.g., multiple entities in the question).\nFinally the highest scored query from the above\n2Following Yih et al. (2015), here topic entity refers to\nthe root of the (directed) query tree; and core-chain is the\ndirected path of relation from root to the answer node.steps is used to query the KB for answers.\nOur main contributions include: (i) An im-\nproved relation detection model by hierarchical\nmatching between questions and relations with\nresidual learning; (ii) We demonstrate that the im-\nproved relation detector enables our simple KBQA\nsystem to achieve state-of-the-art results on both\nsingle-relation and multi-relation KBQA tasks.\n2 Related Work\nRelation Extraction Relation extraction (RE) is\nan important sub-ﬁeld of information extraction.\nGeneral research in this ﬁeld usually works on a\n(small) pre-deﬁned relation set, where given a text\nparagraph and two target entities, the goal is to\ndetermine whether the text indicates any types of\nrelations between the entities or not. As a result\nRE is usually formulated as a classiﬁcation task .\nTraditional RE methods rely on large amount of\nhand-crafted features (Zhou et al., 2005; Rink and\nHarabagiu, 2010; Sun et al., 2011). Recent re-\nsearch beneﬁts a lot from the advancement of deep\nlearning: from word embeddings (Nguyen and Gr-\nishman, 2014; Gormley et al., 2015) to deep net-\nworks', ' like CNNs and LSTMs (Zeng et al., 2014;\ndos Santos et al., 2015; Vu et al., 2016) and atten-\ntion models (Zhou et al., 2016; Wang et al., 2016).\nThe above research assumes there is a ﬁxed\n(closed) set of relation types, thus no zero-shot\nlearning capability is required. The number\nof relations is usually not large: The widely\nused ACE2005 has 11/32 coarse/ﬁne-grained rela-\ntions; SemEval2010 Task8 has 19 relations; TAC-', 'Question: what episode was mike kelleythe writer of\nKnowledgeBaseMike\tKelley(American\ttelevision\twriter/producer)Mike\tKelley(American\tbaseball\tplayer)…Entity Linking \nLove\tWill\tFind\ta\tWayUSA…First\tbaseman…episodes_written\nposition_playedRelation Detection(a)(b)Question: what tvshow did grantshowplay onin2008Mike\tKelley?episodes_writtenEntity Linking RelationDetectionGrantShow?starring_rolesseries(date)from2008ConstraintDetectionGrantShow\t(American\tactor)SwingTown\nBigLoveepisodesScoundrelsseries2011from20102008Figure 1: KBQA examples and its three key components. (a) A single relation example. We ﬁrst identify the topic entity with\nentity linking and then detect the relation asked by the question with relation detection (from all relations connecting the topic\nentity). Based on the detected entity and relation, we form a query to search the KB for the correct answer “ Love Will Find a\nWay”. (b) A more complex question containing two entities. By using “ Grant Show ” as the topic entity, we could detect a chain\nof relations “ starring roles-series ” pointing to the answer. An additional constraint detection takes the other entity “ 2008 ” as\na constraint, to ﬁlter the correct answer “ SwingTown ” from all candidates found by the topic entity and relation.\nthat original relation names can sometimes help\nto match longer question contexts, we propose\nto build both relation-level and word-level rela-\ntion representations. Third, we use deep bidirec-\ntional LSTMs ( BiLSTM s) to learn different levels\nof question representations in order to match the\ndifferent levels of relation information. Finally, we\npropose a residual learning method for sequence\nmatching, which makes the model training easier\nand results in more abstract (deeper) question rep-\nresentations, thus improves hierarchical matching.\nIn order to assess how the proposed improved\nrelation detection could beneﬁt the KBQA end\ntask, we also propose a simple KBQA implemen-\ntation composed of two-step relation detection .\nGiven an input question and a set of candidate enti-\nties retrieved by an entity linker based on the ques-\ntion, our proposed relation detection model plays a\nkey role in the KBQA process: (1) Re-ranking the\nentity candidates according to whether they con-\nnect to high conﬁdent relations detected from the\nraw question text by the relation detection model.\nThis step is important to deal with the ambigui-\nties normally present in entity linking results. (2)\nFinding the core relation (chains) for each topic\nentity2selection from a much smaller candidate\nentity set after re-ranking. The above steps are\nfollowed by an optional constraint detection step,\nwhen the question cannot be answered by single\nrelations (e.g., multiple entities in the question).\nFinally the highest scored query from the above\n2Following Yih et al. (2015), here topic entity refers to\nthe root of the (directed) query tree; and core-chain is the\ndirected path of relation from root to the answer node.steps is used to query the KB for answers.\nOur main contributions include: (i) An im-\nproved relation detection model by hierarchical\nmatching between questions and relations with\nresidual learning; (ii) We demonstrate that the im-\nproved relation detector enables our simple KBQA\nsystem to achieve state-of-the-art results on both\nsingle-relation and multi-relation KBQA tasks.\n2 Related Work\nRelation Extraction Relation extraction (RE) is\nan important sub-ﬁeld of information extraction.\nGeneral research in this ﬁeld usually works on a\n(small) pre-deﬁned relation set, where given a text\nparagraph and two target entities, the goal is to\ndetermine whether the text indicates any types of\nrelations between the entities or not. As a result\nRE is usually formulated as a classiﬁcation task .\nTraditional RE methods rely on large amount of\nhand-crafted features (Zhou et al., 2005; Rink and\nHarabagiu, 2010; Sun et al., 2011). Recent re-\nsearch beneﬁts a lot from the advancement of deep\nlearning: from word embeddings (Nguyen and Gr-\nishman, 2014; Gormley et al., 2015) to deep net-\nworks']",nan,simple,TRUE
9,What is the NUB ES-PHI dataset used for in the experiments?,"[' The winners of the challenge –the\nNeither-Language-nor-Domain-Experts (NLNDE) (Lange\net al., 2019)– achieved F1-scores as high as 0.975 in the\ntask of sensitive information detection and categorisation\nby using recurrent neural networks with Conditional Ran-\ndom Field (CRF) output layers.\nAt the same challenge, Mao and Liu (2019) occupied the\n8thposition among 18 participants using BERT. According\nto the description of the system, the authors used BERT-\nBase Multilingual Cased and an output CRF layer. How-\never, their system is ∼3 F1-score points below our imple-\nmentation without the CRF layer.\n3. Materials and Methods\nThe aim of this paper is to evaluate BERT’s multilingual\nmodel and compare it to other established machine-learning\nalgorithms in a speciﬁc task: sensitive data detection and\nclassiﬁcation in Spanish clinical free text. This section de-\nscribes the data involved in the experiments and the systems\nevaluated. Finally, we introduce the experimental setup.\n3.1. Data\nTwo datasets are exploited in this article. Both datasets\nconsist of plain text containing clinical narrative written in\nSpanish, and their respective manual annotations of sensi-\ntive information in BRAT (Stenetorp et al., 2012) standoff\nformat2. In order to feed the data to the different algorithms\npresented in Section 3.2., these datasets were transformed\nto comply with the commonly used BIO sequence repre-\nsentation scheme (Ramshaw and Marcus, 1999).\n3.1.1. NUB ES-PHI\nNUB ES(Lima et al., 2019) is a corpus of around 7,000 real\nmedical reports written in Spanish and annotated with nega-\ntion and uncertainty information. Before being published,\nsensitive information had to be manually annotated and re-\nplaced for the corpus to be safely shared. In this article,\n1http://temu.bsc.es/meddocan/\n2https://brat.nlplab.org/standoff.html']",nan,simple,TRUE
10,How does adversarial learning resolve the problem of large divergences between training and test examples in the context of neural networks?,"['-\nlikelihood objective as the loss function. We apply softmax\nover all candidate output label sequences, thus the probabil-\nity of the crowd-annotated label sequence is computed by:\np(¯ y|X) =exp(\nscore (X,¯ y))\n∑\ny∈YXexp(\nscore (X,y)), (3)\nwhere ¯ yis the crowd-annotated label sequences and YXis\nall candidate label sequence of input X.\nBased on the above formula, the loss function of our base-\nline model is:\nloss(Θ,X,¯ y) =−logp(¯ y|X), (4)\nwhere Θis the set of all model parameters. We use standard\nback-propagation method to minimize the loss function of\nthe baseline CRF model.', 'Worker Adversarial\nAdversarial learning has been an effective mechanism to re-\nsolve the problem of the input features between the training\nand test examples having large divergences (Goodfellow et\nal. 2014; Ganin et al. 2016). It has been successfully applied\non domain adaption (Gui et al. 2017), cross-lingual learn-\ning (Chen et al. 2016) and multi-task learning (Liu, Qiu, and\nHuang 2017). All settings involve feature shifting between\nthe training and testing.\nIn this paper, our setting is different. We are using the\nannotations from non-experts, which are noise and can in-\nﬂuence the ﬁnal performances if they are not properly pro-\ncessed. Directly learning based on the resulting corpus may\nadapt the neural feature extraction into the biased annota-\ntions. In this work, we assume that individual workers have\ntheir own guidelines in mind after short training. For exam-\nple, a perfect worker can annotate highly consistently with\nan expert, while common crowdsourcing workers may be\nconfused and have different understandings on certain con-\ntexts. Based on the assumption, we make an adaption for the\noriginal adversarial neural network to our setting.\nOur adaption is very simple. Brieﬂy speaking, the original\nadversarial learning adds an additional discriminator to clas-\nsify the type of source inputs, for example, the domain cate-\ngory in the domain adaption setting, while we add a discrim-\ninator to classify the annotation workers. Solely the features\nfrom the input sentence is not enough for worker classiﬁ-\ncation. The annotation result of the worker is also required.\nThus the inputs of our discriminator are different. Here we\nexploit both the source sentences and the crowd-annotated\nNE labels as basic inputs for the worker discrimination.\nIn the following, we describe the proposed adversarial\nlearning module, including both the submodels and the train-\ning method. As shown by the left part of Figure 1, the\nsubmodel consists of four parts: (1) a common Bi-LSTM\nover input characters; (2) an additional Bi-LSTM to en-\ncode crowd-annotated NE label sequence; (3) a convolu-\ntional neural network (CNN) to extract features for worker\ndiscriminator; (4) output and prediction.\nCommon Bi-LSTM over Characters\nTo build the adversarial part, ﬁrst we create a new bi-\ndirectional LSTM, named by the common Bi-LSTM:\nhcommon\n1hcommon\n2···hcommon\nn =Bi-LSTM (x1x2···xn).(5)\nAs shown in Figure 1, this Bi-LSTM is constructed over\nthe same input character representations of the private Bi-\nLSTM, in order to extract worker independent features.\nThe resulting features of the common Bi-LSTM are used\nfor both NER and the worker discriminator, different with\nthe features of private Bi-LSTM which are used for NER\nonly. As shown in Figure 1, we concatenate the outputs of\nthe common and private Bi-LSTMs together, and then feed\nthe results into the feed-forward combination layer of the\nNER part. Thus Formula 1 can be rewritten as:\nhner\nt=W(hcommon\nt⊕hprivate\nt) +b, (6)\nwhere Wis wider than the original combination because the\nnewly-added hcommon\nt .Noticeably, although the resulting common features are\nused for the worker discriminator, they actually have no ca-\npability to distinguish the workers. Because this part is ex-\nploited to maximize the loss of the worker discriminator, it\nwill be interpreted in the later training subsection. These fea-\ntures are invariant among different workers, thus they can\nhave less noises for NER. This is the goal of adversarial\nlearning, and we hope the NER being able to ﬁnd useful\nfeatures from these worker independent features.\nAdditional Bi-LSTM over Annotated NER Labels\nIn order to incorporate the annotated NE labels to predict the\nexact worker, we build another bi-directional LSTM (named\nby label Bi-LSTM) based on the crowd-annotated NE label\nsequence. This Bi-LSTM is used for worker discriminator\nonly. During the decoding of the testing phase, we will never\nhave this Bi-', '-\nlikelihood objective as the loss function. We apply softmax\nover all candidate output label sequences, thus the probabil-\nity of the crowd-annotated label sequence is computed by:\np(¯ y|X) =exp(\nscore (X,¯ y))\n∑\ny∈YXexp(\nscore (X,y)), (3)\nwhere ¯ yis the crowd-annotated label sequences and YXis\nall candidate label sequence of input X.\nBased on the above formula, the loss function of our base-\nline model is:\nloss(Θ,X,¯ y) =−logp(¯ y|X), (4)\nwhere Θis the set of all model parameters. We use standard\nback-propagation method to minimize the loss function of\nthe baseline CRF model.']",Adversarial learning resolves the problem of large divergences between training and test examples in the context of neural networks by adding an additional discriminator to classify the type of source inputs.,simple,TRUE
11,What operating system is being used in the core server?,"[' compare them and obtain a better perspective about\nBERT’s performance in these datasets.\n3.2.1. Baseline\nAs the simplest baseline, a sensitive data recogniser and\nclassiﬁer has been developed that consists of regular-\nexpressions and dictionary look-ups. For each category to\ndetect a speciﬁc method has been implemented. For in-\nstance, the Date, Age, Time and Doctor detectors are based\non regular-expressions; Hospital, Sex, Kinship, Location,\nPatient and Job are looked up in dictionaries. The dic-\ntionaries are hand-crafted from the training data available,\nexcept for the Patient’s case, for which the possible can-\ndidates considered are the 100 most common female and\nmale names in Spain according to the Instituto Nacional de\nEstad ´ıstica (INE; Spanish Statistical Ofﬁce ).\n3.2.2. CRF\nConditional Random Fields (CRF) (Lafferty et al., 2001)\nhave been extensively used for tasks of sequential nature. In\nthis paper, we propose as one of the competitive baselines\na CRF classiﬁer trained with sklearn-crfsuite3for Python\n3.5 and the following conﬁguration: algorithm = lbfgs ;\nmaximum iterations = 100; c1 = c2 = 0.1; all transitions\n=true ; optimise = false . The features extracted from\neach token are as follows:\n3https://sklearn-crfsuite.readthedocs.io', '– preﬁxes and sufﬁxes of 2 and 3 characters;\n– the length of the token in characters and the length of\nthe sentence in tokens;\n– whether the token is all-letters, a number, or a se-\nquence of punctuation marks;\n– whether the token contains the character ‘@’;\n– whether the token is the start or end of the sentence;\n– the token’s casing and the ratio of uppercase charac-\nters, digits, and punctuation marks to its length;\n– and, the lemma, part-of-speech tag, and named-entity\ntag given by ixa-pipes4(Agerri et al., 2014) upon\nanalysing the sentence the token belongs to.\nNoticeably, none of the features used to train the CRF clas-\nsiﬁer is domain-dependent. However, the latter group of\nfeatures is language dependent.\n3.2.3. spaCy\nspaCy5is a widely used NLP library that implements state-\nof-the-art text processing pipelines, including a sequence-\nlabelling pipeline similar to the one described by Strubell\net al. (2017). spaCy offers several pre-trained models in\nSpanish, which perform basic NLP tasks such as Named\nEntity Recognition (NER). In this paper, we have trained a\nnew NER model to detect NUB ES-PHI labels. For this\npurpose, the new model uses all the labels of the train-\ning corpus coded with its context at sentence level. The\nnetwork optimisation parameters and dropout values are\nthe ones recommended in the documentation for small\ndatasets6. Finally, the model is trained using batches of\nsize 64. No more features are included, so the classiﬁer is\nlanguage-dependent but not domain-dependent.\n3.2.4. BERT\nAs introduced earlier, BERT has shown an outstanding\nperformance in NERC-like tasks, improving the start-of-\nthe-art results for almost every dataset and language. We\ntake the same approach here, by using the model BERT-\nBase Multilingual Cased7with a Fully Connected (FC)\nlayer on top to perform a ﬁne-tuning of the whole model\nfor an anonymisation task in Spanish clinical data. Our\nimplementation is built on PyTorch8and the PyTorch-\nTransformers library9(Wolf et al., 2019). The training\nphase consists in the following steps (roughly depicted in\nFigure 1):\n1.Pre-processing: since we are relying on a pre-trained\nBERT model, we must match the same conﬁguration\nby using a speciﬁc tokenisation and vocabulary. BERT\nalso needs that the inputs contains special tokens to\nsignal the beginning and the end of each sequence.\n2.Fine-tuning: the pre-processed sequence is fed into\nthe model. BERT outputs the contextual embeddings\nthat encode each of the inputted tokens. This embed-\nding representation for each token is fed into the FC\n4https://ixa2.si.ehu.es/ixa-pipes\n5https://spacy.io\n6https://spacy.io/usage/training\n7https://github.com/google-research/bert\n8https://pytorch.org\n9https://github.com/huggingface/transformers\nFigure 1: Pre-trained BERT with a Fully Connected layer\non top to perform the ﬁne-tuning\nlinear layer after a dropout layer (with a 0.1 dropout\nprobability), which in turn outputs the logits for each\npossible class. The cross-entropy loss function is cal-\nculated comparing the logits and the gold labels, and\nthe error is back-propagated to adjust the model pa-\nrameters.\nWe have trained the model using an AdamW optimiser\n(Loshchilov and Hutter, 2019) with the learning rate set to\n3e-5, as recommended by Devlin et al. (2019), and with\na gradient clipping of 1.0. We also applied a learning-rate\nscheduler that warms up the learning rate from zero to its\nmaximum value as the training progresses, which is also a\ncommon practice. For each experiment set proposed below,\nthe training was run with an early-stopping patience of 15\nepochs. Then, the model that performed best against the\ndevelopment set was used to produce the reported results.\nThe experiments were run on a 64-', 'core server with operat-\ning system Ubuntu 16.04, 250GB of RAM memory, and 4\nGeForce RTX 2080 GPUs with 11GB of memory. The max-\nimum sequence length was set at 500 and the batch size at\n12. In this setting, each epoch –a full pass through all the\ntraining data– required about 10 minutes to complete.\n3.3. Experimental design\nWe have conducted experiments with BERT in the two\ndatasets of Spanish clinical narrative presented in Section\n3.1. The ﬁrst experiment set uses NUB ES-PHI, a corpus\nof real medical reports manually annotated with sensitive\ninformation. Because this corpus is not publicly available,\nand in order to compare the BERT-based model to other re-\nlated published systems, the second set of experiments uses\nthe MEDDOCAN 2019 shared task competition dataset.\nThe following sections provide greater detail about the two\nexperimental setups.\n3.3.1. Experiment A: NUB ES-PHI\nIn this experiment set, we evaluate all the systems presented\nin Section 3.2., namely, the rule-based baseline, the CRF']",nan,simple,TRUE
12,How much of the variance in a word's contextualized representations can be explained by a static embedding?,"['Figure 3: The intra-sentence similarity is the average cosine similarity between each word representation in a\nsentence and their mean (see Deﬁnition 2). Above, we plot the average intra-sentence similarity of uniformly\nrandomly sampled sentences, adjusted for anisotropy. This statistic reﬂects how context-speciﬁcity manifests in\nthe representation space, and as seen above, it manifests very differently for ELMo, BERT, and GPT-2.\naverage intra-sentence similarity is above 0.20 for\nall but one layer.\nAs noted earlier when discussing BERT, this be-\nhavior still makes intuitive sense: two words in the\nsame sentence do not necessarily have a similar\nmeaning simply because they share the same con-\ntext. The success of GPT-2 suggests that unlike\nanisotropy, which accompanies context-speciﬁcity\nin all three models, a high intra-sentence similar-\nity is not inherent to contextualization. Words in\nthe same sentence can have highly contextualized\nrepresentations without those representations be-\ning any more similar to each other than two ran-\ndom word representations. It is unclear, however,\nwhether these differences in intra-sentence simi-\nlarity can be traced back to differences in model\narchitecture; we leave this question as future work.\n4.3 Static vs. Contextualized\nOn average, less than 5% of the variance in\na word’s contextualized representations can be\nexplained by a static embedding. Recall from\nDeﬁnition 3 that the maximum explainable vari-\nance (MEV) of a word, for a given layer of a given\nmodel, is the proportion of variance in its con-\ntextualized representations that can be explained\nby their ﬁrst principal component. This gives us\nan upper bound on how well a static embedding\ncould replace a word’s contextualized representa-\ntions. Because contextualized representations are\nanisotropic (see section 4.1), much of the varia-\ntion across all words can be explained by a sin-gle vector. We adjust for anisotropy by calculating\nthe proportion of variance explained by the ﬁrst\nprincipal component of uniformly randomly sam-\npled word representations and subtracting this pro-\nportion from the raw MEV . In Figure 4, we plot\nthe average anisotropy-adjusted MEV across uni-\nformly randomly sampled words.\nIn no layer of ELMo, BERT, or GPT-2 can more\nthan 5% of the variance in a word’s contextual-\nized representations be explained by a static em-\nbedding, on average. Though not visible in Figure\n4, the raw MEV of many words is actually below\nthe anisotropy baseline: i.e., a greater proportion\nof the variance across all words can be explained\nby a single vector than can the variance across\nall representations of a single word. Note that\nthe 5% threshold represents the best-case scenario,\nand there is no theoretical guarantee that a word\nvector obtained using GloVe, for example, would\nbe similar to the static embedding that maximizes\nMEV . This suggests that contextualizing models\nare not simply assigning one of a ﬁnite number of\nword-sense representations to each word – other-\nwise, the proportion of variance explained would\nbe much higher. Even the average raw MEV is be-\nlow 5% for all layers of ELMo and BERT; only\nfor GPT-2 is the raw MEV non-negligible, being\naround 30% on average for layers 2 to 11 due to\nextremely high anisotropy.\nPrincipal components of contextualized repre-\nsentations in lower layers outperform GloVe\nand FastText on many benchmarks. As noted', 'Figure 2: The average cosine similarity between representations of the same word in different contexts is called\nthe word’s self-similarity (see Deﬁnition 1). Above, we plot the average self-similarity of uniformly randomly\nsampled words after adjusting for anisotropy (see section 3.4). In all three models, the higher the layer, the lower\nthe self-similarity, suggesting that contextualized word representations are more context-speciﬁc in higher layers.\nof contexts a word appears in, rather than its inher-\nent polysemy, is what drives variation in its con-\ntextualized representations. This answers one of\nthe questions we posed in the introduction: ELMo,\nBERT, and GPT-2 are not simply assigning one of\na ﬁnite number of word-sense representations to\neach word; otherwise, there would not be so much\nvariation in the representations of words with so\nfew word senses.\nContext-speciﬁcity manifests very differently in\nELMo, BERT, and GPT-2. As noted earlier,\ncontextualized representations are more context-\nspeciﬁc in upper layers of ELMo, BERT, and GPT-\n2. However, how does this increased context-\nspeciﬁcity manifest in the vector space? Do word\nrepresentations in the same sentence converge to a\nsingle point, or do they remain distinct from one\nanother while still being distinct from their repre-\nsentations in other contexts? To answer this ques-\ntion, we can measure a sentence’s intra-sentence\nsimilarity. Recall from Deﬁnition 2 that the intra-\nsentence similarity of a sentence, in a given layer\nof a given model, is the average cosine similarity\nbetween each of its word representations and their\nmean, adjusted for anisotropy. In Figure 3, we plot\nthe average intra-sentence similarity of 500 uni-\nformly randomly sampled sentences.\nIn ELMo, words in the same sentence are more\nsimilar to one another in upper layers. As\nword representations in a sentence become more\ncontext-speciﬁc in upper layers, the intra-sentencesimilarity also rises. This suggests that, in prac-\ntice, ELMo ends up extending the intuition behind\nFirth’s (1957) distributional hypothesis to the sen-\ntence level: that because words in the same sen-\ntence share the same context, their contextualized\nrepresentations should also be similar.\nIn BERT, words in the same sentence are more\ndissimilar to one another in upper layers. As\nword representations in a sentence become more\ncontext-speciﬁc in upper layers, they drift away\nfrom one another, although there are exceptions\n(see layer 12 in Figure 3). However, in all lay-\ners, the average similarity between words in the\nsame sentence is still greater than the average sim-\nilarity between randomly chosen words (i.e., the\nanisotropy baseline). This suggests a more nu-\nanced contextualization than in ELMo, with BERT\nrecognizing that although the surrounding sen-\ntence informs a word’s meaning, two words in the\nsame sentence do not necessarily have a similar\nmeaning because they share the same context.\nIn GPT-2, word representations in the same\nsentence are no more similar to each other than\nrandomly sampled words. On average, the un-\nadjusted intra-sentence similarity is roughly the\nsame as the anisotropic baseline, so as seen in Fig-\nure 3, the anisotropy-adjusted intra-sentence simi-\nlarity is close to 0 in most layers of GPT-2. In fact,\nthe intra-sentence similarity is highest in the input\nlayer, which does not contextualize words at all.\nThis is in contrast to ELMo and BERT, where the']","In no layer of ELMo, BERT, or GPT-2 can more than 5% of the variance in a word's contextualized representations be explained by a static embedding, on average.",simple,TRUE
13,What is the purpose of the transfer phase in the proposed cross-lingual pretraining based transfer approach?,"['Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-\nSA) according to the way of generating the alignment infor-\nmation. These two structures actually extend MLM into a\nbilingual scenario, with the difference that BRLM leverages\nexternal aligner tool or additional attention layer to explic-\nitly introduce alignment information during model training.\n•Hard Alignment (BRLM-HA). We ﬁrst use exter-\nnal aligner tool on source ↔pivot parallel data to ex-\ntract the alignment information of sentence pair. Dur-\ning model training, given source ↔pivot sentence pair,\nBRLM-HA randomly masks some words in source sen-\ntence and leverages alignment information to obtain the\naligned words in pivot sentence for masked words. Based\non the processed input, BRLM-HA adopts the Trans-\nformer (Vaswani et al. 2017) encoder to gain the hid-\nden states for source and pivot sentences respectively.\nThen the training objective of BRLM-HA is to predict\nthe masked words by not only the surrounding words\nin source sentence but also the encoder outputs of the\naligned words. Note that this training process is also car-\nried out in a symmetric situation, in which we mask some\nwords in pivot sentence and obtain the aligned words in\nthe source sentence.\n•Soft Alignment (BRLM-SA). Instead of using external\naligner tool, BRLM-SA introduces an additional atten-\ntion layer to learn the alignment information together with\nmodel training. In this way, BRLM-SA avoids the effect\ncaused by external wrong alignment information and en-\nables many-to-one soft alignment during model training.\nSimilar with BRLM-HA, the training objective of BRLM-\nSA is to predict the masked words by not only the sur-\nrounding words in source sentence but also the outputs of\nattention layer. In our implementation, the attention layer\nis a multi-head attention layer adopted in Transformer,\nwhere the queries come from the masked source sentence,\nthe keys and values come from the pivot sentence.\nIn principle, MLM and TLM can learn some implicit align-\nment information during model training. However, the align-\nment process in MLM is inefﬁcient since the shared word\npieces only account for a small proportion of the whole cor-\npus, resulting in the difﬁculty of expanding the shared in-\nformation to align the whole corpus. TLM also lacks effort\nin alignment between the source and target sentences since\nTLM concatenates the sentence pair into one sequence, mak-\ning the explicit alignment between the source and target in-\nfeasible. BRLM fully utilizes the alignment information to\nobtain better word-level representation alignment between\ndifferent languages, which better relieves the burden of the\ndomain shift problem .\nTransfer Protocol\nWe consider the typical zero-shot translation scenario in\nwhich a high resource pivot language has parallel data with\nboth source and target languages, while source and target\nlanguages has no parallel data between themselves. Our pro-\nposed cross-lingual pretraining based transfer approach for\nsource→target zero-shot translation is mainly divided into\ntwo phrases: the pretraining phase and the transfer phase.Corpus Language Train Dev Test\nEuroparlDe-En,En-Fr 1M,1M 2,000 2,000\nFr-En,En-Es 1M,1M 2,000 2,000\nRo-En,En-De 0.6M,1.5M 2,000 1,000\nMultiUNAr-En,En-Es\nEn-Ru9.7M,11.3M\n11.6M4,000 4,000\nTable 1: Data Statistics.\nIn the pretraining phase, we ﬁrst pretrain MLM on mono-\nlingual corpora of both source and pivot languages, and con-\ntinue to pretrain TLM or the proposed BRLM on the avail-\nable parallel data between source and pivot languages, in\norder to build a cross-lingual encoder shared by the source\nand pivot languages.\nIn the transfer phase, we train pivot →target NMT model\ninitialized by the cross-lingually pre-trained encoder, and\nﬁnally transfer the trained NMT model to source →target\ntranslation thanks to the shared encoder. Note that during\ntraining pivot→target NMT model, we freeze several layers\nof the cross-lingually pre-trained encoder to avoid the de-\ngeneracy issue.\nFor the', 'Figure 2: The overview of BRidge Language Modeling (BRLM). The BRLM extends MLM (Lample and Conneau 2019) to\npairs of parallel sentences and leverages explicit alignment information obtained by external aligner tool or additional attention\nlayer to encourage word representation alignment across different languages.\n•Directly translate source sentences into target sentences\nwith the parent model, which beneﬁts from the availabil-\nity of the universal encoder.\nThe key difﬁculty of this method is to ensure the intermedi-\nate representations of the universal encoder are language in-\nvariant. In the rest of this section, we ﬁrst present two exist-\ning methods yet to be explored in zero-shot translation, and\nthen propose a straightforward but effective cross-lingual\npre-training method. In the end, we present the whole train-\ning and inference protocol for transfer.\nMasked and Translation Language Model\nPretraining\nTwo existing cross-lingual pre-training methods, Masked\nLanguage Modeling (MLM) and Translation Language\nModeling (TLM), have shown their effectiveness on XNLI\ncross-lingual classiﬁcation task (Lample and Conneau 2019;\nHuang et al. 2019), but these methods have not been well\nstudied on cross-lingual generation tasks in zero-shot condi-\ntion. We attempt to take advantage of the cross-lingual abil-\nity of the two methods for zero-shot translation.\nSpeciﬁcally, MLM adopts the Cloze objective of\nBERT (Devlin et al. 2018) and predicts the masked words\nthat are randomly selected and replaced with [MASK] to-\nken on monolingual corpus. In practice, MLM takes dif-\nferent language monolingual corpora as input to ﬁnd fea-\ntures shared across different languages. With this method,\nword pieces shared in all languages have been mapped into\na shared space, which makes the sentence representationsacross different languages close (Pires, Schlinger, and Gar-\nrette 2019).\nSince MLM objective is unsupervised and only requires\nmonolingual data, TLM is designed to leverage parallel data\nwhen it is available. Actually, TLM is a simple extension of\nMLM, with the difference that TLM concatenates sentence\npair into a whole sentence, and then randomly masks words\nin both the source and target sentences. In this way, the\nmodel can either attend to surrounding words or to the trans-\nlation sentence, implicitly encouraging the model to align\nthe source and target language representations. Note that al-\nthough each sentence pair is formed into one sentence, the\npositions of the target sentence are reset to count form zero.\nBridge Language Model Pretraining\nAside from MLM and TLM, we propose BRidge Language\nModeling (BRLM) to further obtain word-level representa-\ntion alignment between different languages. This method is\ninspired by the assumption that if the feature spaces of dif-\nferent languages are aligned very well, the masked words\nin the corrupted sentence can also be guessed by the con-\ntext of the correspondingly aligned words on the other side.\nTo achieve this goal, BRLM is designed to strengthen the\nability to infer words across languages based on alignment\ninformation, instead of inferring words within monolingual\nsentence as in MLM or within the pseudo sentence formed\nby concatenating sentence pair as in TLM.\nAs illustrated in Figure 2, BRLM stacks shared encoder\nover both side sentences separately. In particular, we design\ntwo network structures for BRLM, which are divided into']",nan,simple,TRUE
14,What are GloVe word embeddings used for in the experiments?,"['arXiv:1810.08699v1  [cs.CL]  19 Oct 2018pioNER: Datasets and Baselines for Armenian\nNamed Entity Recognition\nTsolak Ghukasyan1, Garnik Davtyan2, Karen Avetisyan3, Ivan Andrianov4\nIvannikov Laboratory for System Programming at Russian-Ar menian University1,2,3, Yerevan, Armenia\nIvannikov Institute for System Programming of the Russian A cademy of Sciences4, Moscow, Russia\nEmail: {1tsggukasyan,2garnik.davtyan,3karavet,4ivan.andrianov}@ispras.ru\nAbstract —In this work, we tackle the problem of Armenian\nnamed entity recognition, providing silver- and gold-stan dard\ndatasets as well as establishing baseline results on popula r mod-\nels. We present a 163000-token named entity corpus automati cally\ngenerated and annotated from Wikipedia, and another 53400-\ntoken corpus of news sentences with manual annotation of peo ple,\norganization and location named entities. The corpora were used\nto train and evaluate several popular named entity recognit ion\nmodels. Alongside the datasets, we release 50-, 100-, 200-, 300-\ndimensional GloVe word embeddings trained on a collection o f\nArmenian texts from Wikipedia, news, blogs, and encycloped ia.\nIndex Terms —machine learning, deep learning, natural lan-\nguage processing, named entity recognition, word embeddin gs\nI. I NTRODUCTION\nNamed entity recognition is an important task of natural\nlanguage processing, featuring in many popular text proces sing\ntoolkits. This area of natural language processing has been\nactively studied in the latest decades and the advent of deep\nlearning reinvigorated the research on more effective and\naccurate models. However, most of existing approaches requ ire\nlarge annotated corpora. To the best of our knowledge, no suc h\nwork has been done for the Armenian language, and in this\nwork we address several problems, including the creation of a\ncorpus for training machine learning models, the developme nt\nof gold-standard test corpus and evaluation of the effectiv eness\nof established approaches for the Armenian language.\nConsidering the cost of creating manually annotated named\nentity corpus, we focused on alternative approaches. Lack\nof named entity corpora is a common problem for many\nlanguages, thus bringing the attention of many researchers\naround the globe. Projection based transfer schemes have be en\nshown to be very effective (e.g. [1], [2], [3]), using resour ce-\nrich language’s corpora to generate annotated data for the l ow-\nresource language. In this approach, the annotations of hig h-\nresource language are projected over the corresponding tok ens\nof the parallel low-resource language’s texts. This strate gy\ncan be applied for language pairs that have parallel corpora .\nHowever, that approach would not work for Armenian as we\ndid not have access to sufﬁciently large parallel corpus wit h a\nresource-rich language.\nAnother popular approach is using Wikipedia. Klesti\nHoxha and Artur Baxhaku employ gazetteers extracted from\nWikipedia to generate an annotated corpus for Albanian [4],\nand Weber and Pötzl propose a rule-based system for Germanthat leverages the information from Wikipedia [5]. However ,\nthe latter relies on external tools such as part-of-speech t ag-\ngers, making it nonviable for the Armenian language.\nNothman et al. generated a silver-standard corpus for 9\nlanguages by extracting Wikipedia article texts with outgo ing\nlinks and turning those links into named entity annotations\nbased on the target article’s type [6]. Sysoev and Andrianov\nused a similar approach for the Russian language [7] [8].\nBased on its success for a wide range of languages, our choice\nfell on this model to tackle automated data generation and\nannotation for the Armenian language.\nAside from the lack of training data, we also address the\nabsence of a benchmark dataset of Armenian texts for named\nentity recognition. We propose a gold-standard corpus with\nmanual annotation of CoNLL named entity categories: person ,\nlocation, and organization [9] [10], hoping it will be used t o\nevaluate future named entity recognition models.\nFurthermore, popular entity recognition models were ap-\nplied to the mentioned data in order to obtain baseline resul ts\nfor future research in the area. Along with the datasets, we\n', 'developed GloVe [11] word embeddings to train and evaluate\nthe deep learning models in our experiments.\nThe contributions of this work are (i) the silver-standard\ntraining corpus, (ii) the gold-standard test corpus, (iii) GloVe\nword embeddings, (iv) baseline results for 3 different mode ls\non the proposed benchmark data set. All aforementioned\nresources are available on GitHub1.\nII. A UTOMATED TRAINING CORPUS GENERATION\nWe used Sysoev and Andrianov’s modiﬁcation of the\nNothman et al. approach to automatically generate data for\ntraining a named entity recognizer. This approach uses link s\nbetween Wikipedia articles to generate sequences of named-\nentity annotated tokens.\nA. Dataset extraction\nThe main steps of the dataset extraction system are de-\nscribed in Figure 1.\nFirst, each Wikipedia article is assigned a named entity\nclass (e.g. the article /Armke/armini/armmen /Armke/armayb/armsha/armke/armayb/armsha/armhi/armayb/armnu (Kim Kashkashian) is\nclassiﬁed as PER (person), /Armayb/armza/armgim/armyech/armre/armini /armlyun/armini/armgim/armayb (League of Nations)\nasORG (organization), /Armse/armini/armre/armini/armayb (Syria) as LOC etc). One of the\n1https://github.com/ispras-texterra/pioner\n© 2018 IEEE. Personal use of this material is permitted. Perm ission from IEEE must be obtained for all other uses, in any cu rrent or future media, including\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for r esale or redistribution to servers or lists, or\nreuse of any copyrighted component of this work in other work s.', 'Fig. 1: Steps of automatic dataset extraction from Wikipedi a\nClassiﬁcation of Wikipedia articles into NE types\nLabelling common article aliases to increase coverage\nExtraction of text fragments with outgoing links\nLabelling links according to their target article’s type\nAdjustment of labeled entities’ boundaries\ncore differences between our approach and Nothman’s system\nis that we do not rely on manual classiﬁcation of articles and\ndo not use inter-language links to project article classiﬁc ations\nacross languages. Instead, our classiﬁcation algorithm us es\nonly an article’s Wikidata entry’s ﬁrst instance of label’s\nparent subclass of labels, which are, incidentally, language\nindependent and thus can be used for any language.\nThen, outgoing links in articles are assigned the article’s\ntype they are leading to. Sentences are included in the train ing\ncorpus only if they contain at least one named entity and\nall contained capitalized words have an outgoing link to an\narticle of known type. Since in Wikipedia articles only the ﬁ rst\nmention of each entity is linked, this approach becomes very\nrestrictive and in order to include more sentences, additio nal\nlinks are inferred. This is accomplished by compiling a list of\ncommon aliases for articles corresponding to named entitie s,\nand then ﬁnding text fragments matching those aliases to\nassign a named entity label. An article’s aliases include it s\ntitle, titles of disambiguation pages with the article, and texts\nof links leading to the article (e.g. /Armlyun/armyech/armnu/armini/armnu/armgim/armre/armayb/armda (Leningrad),\n/Armpe/armyech/armtyun/armre/armvo/armgim/armre/armayb/armda (Petrograd), /Armpe/armyech/armtyun/armyech/armre/armben/armvo/armvyun/armre/armgim (Peterburg) are aliases\nfor/Armse/armayb/armnu/armken/armtyun /Armpe/armyech/armtyun/armyech/armre/armben/armvo/armvyun/armre/armgim (Saint Petersburg)). The list of\naliases is compiled for all PER ,ORG ,LOC articles.\nAfter that, link boundaries are adjusted by removing the\nlabels for expressions in parentheses, the text after a comm a,\nand in some cases breaking into separate named entities if th e\nlinked text contains a comma. For example, [LOC/Armayb/armben/armvo/armvev/armhi/armayb/armnu\n/armparenleft/armke/armayb/armghat/armayb/armke/armparenright ](Abovyan (town)) is reworked into [LOC/Armayb/armben/armvo/armvev/armhi/armayb/armnu ]\n/armparenleft/armke/armayb/armghat/armayb/armke/armparenright .\nB. Using Wikidata to classify Wikipedia\nInstead of manually classifying Wikipedia articles as it wa s\ndone in Nothman et al., we developed a rule-based classiﬁer\nthat used an article’s Wikidata instance of andsubclass of\nattributes to ﬁnd the corresponding named entity type.The classiﬁcation could be done using solely instance\noflabels, but these labels are unnecessarily speciﬁc for the\ntask and building a mapping on it would require a more\ntime-consuming and meticulous work. Therefore, we classiﬁ ed\narticles based on their ﬁrst instance of attribute’s subclass of\nvalues. Table I displays the mapping between these values an d\nnamed entity types. Using higher-level subclass of values was\nnot an option as their values often were too general, making\nit impossible to derive the correct named entity category.\nC. Generated data\nUsing the algorithm described above, we generated 7455\nannotated sentences with 163247 tokens based on 20 February\n2018 dump of Armenian Wikipedia.\nThe generated data is still signiﬁcantly smaller than the\nmanually annotated corpora from CoNLL 2002 and 2003.\nFor comparison, the train set of English CoNLL 2003 corpus\ncontains 203621 tokens and the German one 2069', '31, while\nthe Spanish and Dutch corpora from CoNLL 2002 respectively\n273037 and 218737 lines. The smaller size of our generated\ndata can be attributed to the strict selection of candidate\nsentences as well as simply to the relatively small size of\nArmenian Wikipedia.\nThe accuracy of annotation in the generated corpus heavily\nrelies on the quality of links in Wikipedia articles. During gen-\neration, we assumed that ﬁrst mentions of all named entities\nhave an outgoing link to their article, however this was not a l-\nways the case in actual source data and as a result the train se t\ncontained sentences where not all named entities are labele d.\nAnnotation inaccuracies also stemmed from wrongly assigne d\nlink boundaries (for example, in Wikipedia article /Armayb/armre/armto/armvo/armvyun/armre\n/Armvo/armvyun/armyech/armlyun/armse/armlyun/armini /Armvev/armyech/armlyun/armini/armnu/armgim/armto/armvo/armnu (Arthur Wellesley) there is a link to the\nNapoleon article with the text "" /arme/Armnu/armayb/armpe/armvo/armlyun/armyech/armvo/armnu/armat "" (""Napoleon is""),\nwhen it should be "" /Armnu/armayb/armpe/armvo/armlyun/armyech/armvo/armnu/armat "" (""Napoleon"")). Another kind\nof common annotation errors occurred when a named entity\nappeared inside a link not targeting a LOC ,ORG , orPER\narticle (e.g. "" /Armayb/Armmen/Armnu /armnu/armayb/armkhe/armayb/armgim/armayb/armho/armayb/armken/armayb/armnu /armat/armnu/armtyun/armre/armvo/armvyun/armto/armhi/armvo/armvyun/armnu/armnu/armyech/armre/armvo/armvyun/armmen ""\n(""USA presidential elections"") is linked to the article /Armayb/Armmen/Armnu\n/armnu/armayb/armkhe/armayb/armgim/armayb/armho/armayb/armken/armayb/armnu /armat/armnu/armtyun/armre/armvo/armvyun/armto/armhi/armvo/armvyun/armnu/armnu/armyech/armre 2016 (United States presi-\ndential election, 2016) and as a result [ LOC/Armayb/Armmen/Armnu ] (USA) is\nlost).\nIII. T EST DATASET\nIn order to evaluate the models trained on generated data,\nwe manually annotated a named entities dataset comprising\n53453 tokens and 2566 sentences selected from over 250 news\ntexts from ilur.am2. This dataset is comparable in size with\nthe test sets of other languages (Table II). Included senten ces\nare from political, sports, local and world news (Figures 2,\n3), covering the period between August 2012 and July 2018.\nThe dataset provides annotations for 3 popular named entity\nclasses: people ( PER ), organizations ( ORG ), and locations\n(LOC ), and is released in CoNLL03 format with IOB tagging\nscheme. Tokens and sentences were segmented according to\nthe UD standards for the Armenian language [12].\n2http://ilur.am/news/newsline.html']",nan,simple,TRUE
15,How does the method exploit entity-relation collocation for entity linking in the context of TV writers?,"['\nWe use the same example in Fig 1(a) to illustrate\nthe idea. Given the input question in the exam-\nple, a relation detector is very likely to assign high\nscores to relations such as “ episodes written ”,\n“author of” and “ profession ”. Then, according\nto the connections of entity candidates in KB,\nwe ﬁnd that the TV writer “ Mike Kelley ” will\nbe scored higher than the baseball player “ Mike\nKelley ”, because the former has the relations\n“episodes written ” and “ profession ”. This method\ncan be viewed as exploiting entity-relation collo-\ncation for entity linking.\n5.2 Relation Detection\nIn this step, for each candidate entity e∈\nEL′\nK(q), we use the question text as the input to a\nrelation detector to score all the relations r∈Re\nthat are associated to the entity ein the KB.4Be-\ncause we have a single topic entity input in this\nstep, we do the following question reformatting:\nwe replace the the candidate e’s entity mention in\n4Note that the number of entities and the number of rela-\ntion candidates will be much smaller than those in the previ-\nous step.']",nan,simple,TRUE
16,How does pre-ordering the assisting language improve translation quality in a low-resource setting?,"[' 1999) which enables the model to learn\nthe word-order of the source language when sufﬁ-\ncient child task parallel corpus is available.\nWe also compare the performance of the ﬁne-\ntuned model with the model trained only on the\navailable source-target parallel corpus with ran-\ndomly initialized weights (No Transfer Learning).\nTransfer learning, with and without pre-ordering,\nis better compared to training only on the small\nsource-target parallel corpus.\n6 Conclusion\nIn this paper, we show that handling word-order\ndivergence between the source and assisting lan-\nguages is crucial for the success of multilingual\nNMT in an extremely low-resource setting. We\nshow that pre-ordering the assisting language to\nmatch the word order of the source language sig-\nniﬁcantly improves translation quality in an ex-\ntremely low-resource setting. If pre-ordering is\nnot possible, ﬁne-tuning on a small source-target', '2 Related Work\nTo the best of our knowledge, no work has ad-\ndressed word order divergence in transfer learn-\ning for multilingual NMT. However, some work\nexists for other NLP tasks in a multilingual set-\nting. For Named Entity Recognition (NER), Xie\net al. (2018) use a self-attention layer after the\nBi-LSTM layer to address word-order divergence\nfor Named Entity Recognition (NER) task. The\napproach does not show any signiﬁcant improve-\nments, possibly because the divergence has to be\naddressed before/during construction of the con-\ntextual embeddings in the Bi-LSTM layer. Joty\net al. (2017) use adversarial training for cross-\nlingual question-question similarity ranking. The\nadversarial training tries to force the sentence rep-\nresentation generated by the encoder of similar\nsentences from different input languages to have\nsimilar representations.\nPre-ordering the source language sentences to\nmatch the target language word order has been\nfound useful in addressing word-order divergence\nfor Phrase-Based SMT (Collins et al., 2005; Ra-\nmanathan et al., 2008; Navratil et al., 2012; Chat-\nterjee et al., 2014). For NMT, Ponti et al. (2018)\nand Kawara et al. (2018) have explored pre-\nordering. Ponti et al. (2018) demonstrated that\nby reducing the syntactic divergence between the\nsource and the target languages, consistent im-\nprovements in NMT performance can be obtained.\nOn the contrary, Kawara et al. (2018) reported\ndrop in NMT performance due to pre-ordering.\nNote that these works address source-target diver-\ngence, not divergence between source languages\nin multilingual NMT scenario.\n3 Proposed Solution\nConsider the task of translating for an extremely\nlow-resource language pair. The parallel corpus\nbetween the two languages, if available may be\ntoo small to train an NMT model. Similar to Zoph\net al. (2016), we use transfer learning to over-\ncome data sparsity between the source and the\ntarget languages. We choose English as the as-\nsisting language in all our experiments. In our\nresource-scarce scenario, we have no parallel cor-\npus for training the child model. Hence, at test\ntime, the source language sentence is translated\nusing the parent model after performing a word-\nby-word translation from source to the assisting\nlanguage using a bilingual dictionary.Before Reordering After Reordering\nS\nNP0 VP\nV NP 1S\nNP0 VP\nNP1V\nS\nNP\nNNP\nAnuragVP\nMD\nwillVP\nVB\nmeetNP\nNNP\nThakurS\nNP\nNNP\nAnuragVP\nNP\nNNP\nThakurVP\nMD\nwillVP\nVB\nmeet\nTable 1: Example showing transitive verb before and\nafter reordering (Adapted from Chatterjee et al. (2014))\nSince the source language and the assisting lan-\nguage (English) have different word order, we hy-\npothesize that it leads to inconsistencies in the\ncontextual representations generated by the en-\ncoder for the two languages. Speciﬁcally, given an\nEnglish sentence (SVO word order) and its transla-\ntion in the source language (SOV word order), the\nencoder representations for words in the two sen-\ntences will be different due to different contexts\nof synonymous words. This could lead to the at-\ntention and the decoder layers generating different\ntranslations from the same (parallel) sentence in\nthe source or assisting language. This is undesir-\nable as we want the knowledge to be transferred\nfrom the parent model (assisting source →target)\nto the child model (source →target).\nIn this paper, we propose to pre-order English\nsentences (assisting language sentences) to match\nthe source language word-order and train the par-\nent model on the pre-ordered corpus. Table 1\nshows one of the pre-ordering rules (Ramanathan\net al., 2008) used along with an example sentence\nillustrating the effect of pre-ordering. This will en-\nsure that context of words in the parallel source\nand assisting language sentences are similar, lead-\ning to consistent contextual representations across\nthe source languages. Pre-ordering may also be\nbeneﬁcial for other word order divergence scenar-\nios (e.g., SOV to SVO), but we leave']","Pre-ordering the assisting language improves translation quality in a low-resource setting by ensuring that the contextual representations generated by the encoder for the source and assisting languages are consistent, leading to consistent translations.",simple,TRUE
17,What is a domain-specific attitude?,"[')\nDogmatism is widely considered to be a domain-\nspeciﬁc attitude (for example, oriented towards re-\nligion or politics) as opposed to a deeper personality\ntrait (Rokeach, 1954). Here we use Reddit as a lens\nto examine this idea more closely. Are users who\nare dogmatic about one topic likely to be dogmatic\nabout others? Do clusters of dogmatism exist around\nparticular topics? To ﬁnd out, we examine the re-', 'Libertarianism business conspiracy science Christianity lgbt\nAnarcho Capitalism Bitcoin Republican Christianity DebateAChristian feminisms\nBitcoin economy conspiritard relationship advice DebateReligion Equality\nronpaul entertainment ronpaul worldpolitics science SubredditDrama\nConservative TrueReddit collapse MensRights videos TwoXChromosomes\nAndroid socialism guns IAmA news MensRights\nukpolitics bestof worldpolitics TwoXChromosomes Libertarianism offbeat\nEquality philosophy occupywallstreet WTF atheism fffffffuuuuuuuuuuuu\nTable 4: Clusters of subreddits that share dogmatic users. For example, users who are dogmatic on the conspiracy subreddit (a\nplace to discuss conspiracy theories) are also likely to be dogmatic on guns oroccupywallstreet .\nlationships between subreddits over which individ-\nual users are dogmatic. For example, if many users\noften post dogmatic comments on both the politics\nandChristianity subreddits, but less often on world-\nnews , that would suggest politics andChristianity\nare linked per a boost in likelihood of individuals\nbeing dogmatic in both.\nWe sample 1000 Reddit users who posted at least\nonce a year between 2007 and 2015 to construct a\ncorpus of 10 million posts that constitute their entire\npost history. We then annotate these posts using the\nclassiﬁer and compute the average dogmatism score\nper subreddit per user. For example, one user might\nhave an average dogmatism level of 0.55 for the pol-\nitics subreddit and 0.45 for the economics subred-\ndit. Most users do not post in all subreddits, so we\ntrack only subreddits for which a user had posted at\nleast 10 times. Any subreddits with an average dog-\nmatism score higher than 0.50 we consider to be a\nuser’s dogmatic subreddits. We then count all pairs\nof these dogmatic subreddits. For example, 45 users\nhave politics andtechnology among their dogmatic\nsubreddits, so we consider politics andtechnology\nas linked 45 times. We compute the mutual informa-\ntion (Church and Hanks, 1990) between these links,\nwhich gives us a measure of the subreddits that are\nmost related through dogmatism.\nWe present the results of this analysis in Table 4,\nchoosing clusters that represent a diverse set of top-\nics. For example, Libertarianism is linked through\ndogmatism to other political communities like An-\narcho Capitalism ,ronpaul , orukpolitics , as well as\nother topical subreddits like guns oreconomy . Sim-\nilarly, people who are dogmatic in the business sub-\nreddit also tend to be dogmatic in subreddits for Bit-\ncoin,socialism , and technology . Notably, when we\napply the same mutual information analysis to links\ndeﬁned by subreddits posted in by the same user, weFeature Direction\ntotal user posts ↑\nproportion of posts in most active subreddit ↑\nnumber of subreddits posted in ↓\naverage number of posts in active articles ↓\nTable 5: User behavioral features that are positively and nega-\ntively associated with dogmatism. ↑means the feature is pos-\nitively predictive with dogmatism, and ↓means the feature is\nnegatively predictive. For example, the more subreddits a user\nposts in, the less likely they are to be dogmatic. All features are\nstatistically signiﬁcant ( p < 0.001).\nsee dramatically different results. For example, the\nsubreddits most linked to science through user posts\nareUpliftingNews ,photoshopbattles , and ﬁrstworl-\ndanarchist , and millionairemakers .\nFinally, we see less obvious connections between\nsubreddits that suggest some people may be dog-\nmatic by nature. For example, among the users who\nare dogmatic on politics , they are also disproportion-\nately dogmatic on unrelated subreddits such as sci-\nence (p < 0.001),technology (p < 0.001),IAmA\n(p < 0.001), and AskReddit (p < 0.05), with p-\nvalues computed under a binomial test.\n5.3 What user behaviors are predictive of\ndogmatism? (R3)\nWe have shown dogmatism is captured by many lin-\nguistic features, but can we discover other high-level\nuser behaviors that are similarly predictive?\nTo ﬁnd out, we compute metrics of user behavior\nusing the data sample of 1000 users and 10 million\nposts described in Section 5.2. Speciﬁcally, we cal-\nculate (1) activity : a user’s total number of posts,', 'Libertarianism business conspiracy science Christianity lgbt\nAnarcho Capitalism Bitcoin Republican Christianity DebateAChristian feminisms\nBitcoin economy conspiritard relationship advice DebateReligion Equality\nronpaul entertainment ronpaul worldpolitics science SubredditDrama\nConservative TrueReddit collapse MensRights videos TwoXChromosomes\nAndroid socialism guns IAmA news MensRights\nukpolitics bestof worldpolitics TwoXChromosomes Libertarianism offbeat\nEquality philosophy occupywallstreet WTF atheism fffffffuuuuuuuuuuuu\nTable 4: Clusters of subreddits that share dogmatic users. For example, users who are dogmatic on the conspiracy subreddit (a\nplace to discuss conspiracy theories) are also likely to be dogmatic on guns oroccupywallstreet .\nlationships between subreddits over which individ-\nual users are dogmatic. For example, if many users\noften post dogmatic comments on both the politics\nandChristianity subreddits, but less often on world-\nnews , that would suggest politics andChristianity\nare linked per a boost in likelihood of individuals\nbeing dogmatic in both.\nWe sample 1000 Reddit users who posted at least\nonce a year between 2007 and 2015 to construct a\ncorpus of 10 million posts that constitute their entire\npost history. We then annotate these posts using the\nclassiﬁer and compute the average dogmatism score\nper subreddit per user. For example, one user might\nhave an average dogmatism level of 0.55 for the pol-\nitics subreddit and 0.45 for the economics subred-\ndit. Most users do not post in all subreddits, so we\ntrack only subreddits for which a user had posted at\nleast 10 times. Any subreddits with an average dog-\nmatism score higher than 0.50 we consider to be a\nuser’s dogmatic subreddits. We then count all pairs\nof these dogmatic subreddits. For example, 45 users\nhave politics andtechnology among their dogmatic\nsubreddits, so we consider politics andtechnology\nas linked 45 times. We compute the mutual informa-\ntion (Church and Hanks, 1990) between these links,\nwhich gives us a measure of the subreddits that are\nmost related through dogmatism.\nWe present the results of this analysis in Table 4,\nchoosing clusters that represent a diverse set of top-\nics. For example, Libertarianism is linked through\ndogmatism to other political communities like An-\narcho Capitalism ,ronpaul , orukpolitics , as well as\nother topical subreddits like guns oreconomy . Sim-\nilarly, people who are dogmatic in the business sub-\nreddit also tend to be dogmatic in subreddits for Bit-\ncoin,socialism , and technology . Notably, when we\napply the same mutual information analysis to links\ndeﬁned by subreddits posted in by the same user, weFeature Direction\ntotal user posts ↑\nproportion of posts in most active subreddit ↑\nnumber of subreddits posted in ↓\naverage number of posts in active articles ↓\nTable 5: User behavioral features that are positively and nega-\ntively associated with dogmatism. ↑means the feature is pos-\nitively predictive with dogmatism, and ↓means the feature is\nnegatively predictive. For example, the more subreddits a user\nposts in, the less likely they are to be dogmatic. All features are\nstatistically signiﬁcant ( p < 0.001).\nsee dramatically different results. For example, the\nsubreddits most linked to science through user posts\nareUpliftingNews ,photoshopbattles , and ﬁrstworl-\ndanarchist , and millionairemakers .\nFinally, we see less obvious connections between\nsubreddits that suggest some people may be dog-\nmatic by nature. For example, among the users who\nare dogmatic on politics , they are also disproportion-\nately dogmatic on unrelated subreddits such as sci-\nence (p < 0.001),technology (p < 0.001),IAmA\n(p < 0.001), and AskReddit (p < 0.05), with p-\nvalues computed under a binomial test.\n5.3 What user behaviors are predictive of\ndogmatism? (R3)\nWe have shown dogmatism is captured by many lin-\nguistic features, but can we discover other high-level\nuser behaviors that are similarly predictive?\nTo ﬁnd out, we compute metrics of user behavior\nusing the data sample of 1000 users and 10 million\nposts described in Section 5.2. Speciﬁcally, we cal-\nculate (1) activity : a user’s total number of posts,']","A domain-specific attitude is an attitude that is oriented towards a specific domain, such as religion or politics.",simple,TRUE
18,What is the process of Hard Alignment in BRLM-HA?,"[', MNMT\nusually performs worse than the pivot-based method in\nzero-shot translation setting (Arivazhagan et al. 2018).\n•Unsupervised NMT (UNMT) considers a harder setting,\nin which only large-scale monolingual corpora are avail-\nable for training. Recently, many methods have been pro-\nposed to improve the performance of UNMT, including\nusing denoising auto-encoder, statistic machine transla-\ntion (SMT) and unsupervised pre-training (Artetxe et\nal. 2017; Lample et al. 2018; Ren et al. 2019; Lample\nand Conneau 2019). Since UNMT performs well between\nsimilar languages (e.g., English-German translation), its\nperformance between distant languages is still far from\nexpectation.\nOur proposed method belongs to the transfer learning,\nbut it is different from traditional transfer methods which\ntrain a parent model as starting point. Before training a par-\nent model, our approach fully leverages cross-lingual pre-\ntraining methods to make all source languages share the\nsame feature space and thus enables a smooth transition for\nzero-shot translation.\nApproach\nIn this section, we will present a cross-lingual pre-\ntraining based transfer approach. This method is designed\nfor a common zero-shot scenario where there are a lot\nof source↔pivot and pivot↔target bilingual data but no\nsource↔target parallel data, and the whole training process\ncan be summarized as follows step by step:\n•Pre-train a universal encoder with source/pivot monolin-\ngual or source↔pivot bilingual data.\n•Train a pivot→target parent model built on the pre-trained\nuniversal encoder with the available parallel data. Dur-\ning the training process, we freeze several layers of the\npre-trained universal encoder to avoid the degeneracy is-\nsue (Howard and Ruder 2018).', 'Figure 2: The overview of BRidge Language Modeling (BRLM). The BRLM extends MLM (Lample and Conneau 2019) to\npairs of parallel sentences and leverages explicit alignment information obtained by external aligner tool or additional attention\nlayer to encourage word representation alignment across different languages.\n•Directly translate source sentences into target sentences\nwith the parent model, which beneﬁts from the availabil-\nity of the universal encoder.\nThe key difﬁculty of this method is to ensure the intermedi-\nate representations of the universal encoder are language in-\nvariant. In the rest of this section, we ﬁrst present two exist-\ning methods yet to be explored in zero-shot translation, and\nthen propose a straightforward but effective cross-lingual\npre-training method. In the end, we present the whole train-\ning and inference protocol for transfer.\nMasked and Translation Language Model\nPretraining\nTwo existing cross-lingual pre-training methods, Masked\nLanguage Modeling (MLM) and Translation Language\nModeling (TLM), have shown their effectiveness on XNLI\ncross-lingual classiﬁcation task (Lample and Conneau 2019;\nHuang et al. 2019), but these methods have not been well\nstudied on cross-lingual generation tasks in zero-shot condi-\ntion. We attempt to take advantage of the cross-lingual abil-\nity of the two methods for zero-shot translation.\nSpeciﬁcally, MLM adopts the Cloze objective of\nBERT (Devlin et al. 2018) and predicts the masked words\nthat are randomly selected and replaced with [MASK] to-\nken on monolingual corpus. In practice, MLM takes dif-\nferent language monolingual corpora as input to ﬁnd fea-\ntures shared across different languages. With this method,\nword pieces shared in all languages have been mapped into\na shared space, which makes the sentence representationsacross different languages close (Pires, Schlinger, and Gar-\nrette 2019).\nSince MLM objective is unsupervised and only requires\nmonolingual data, TLM is designed to leverage parallel data\nwhen it is available. Actually, TLM is a simple extension of\nMLM, with the difference that TLM concatenates sentence\npair into a whole sentence, and then randomly masks words\nin both the source and target sentences. In this way, the\nmodel can either attend to surrounding words or to the trans-\nlation sentence, implicitly encouraging the model to align\nthe source and target language representations. Note that al-\nthough each sentence pair is formed into one sentence, the\npositions of the target sentence are reset to count form zero.\nBridge Language Model Pretraining\nAside from MLM and TLM, we propose BRidge Language\nModeling (BRLM) to further obtain word-level representa-\ntion alignment between different languages. This method is\ninspired by the assumption that if the feature spaces of dif-\nferent languages are aligned very well, the masked words\nin the corrupted sentence can also be guessed by the con-\ntext of the correspondingly aligned words on the other side.\nTo achieve this goal, BRLM is designed to strengthen the\nability to infer words across languages based on alignment\ninformation, instead of inferring words within monolingual\nsentence as in MLM or within the pseudo sentence formed\nby concatenating sentence pair as in TLM.\nAs illustrated in Figure 2, BRLM stacks shared encoder\nover both side sentences separately. In particular, we design\ntwo network structures for BRLM, which are divided into', 'Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-\nSA) according to the way of generating the alignment infor-\nmation. These two structures actually extend MLM into a\nbilingual scenario, with the difference that BRLM leverages\nexternal aligner tool or additional attention layer to explic-\nitly introduce alignment information during model training.\n•Hard Alignment (BRLM-HA). We ﬁrst use exter-\nnal aligner tool on source ↔pivot parallel data to ex-\ntract the alignment information of sentence pair. Dur-\ning model training, given source ↔pivot sentence pair,\nBRLM-HA randomly masks some words in source sen-\ntence and leverages alignment information to obtain the\naligned words in pivot sentence for masked words. Based\non the processed input, BRLM-HA adopts the Trans-\nformer (Vaswani et al. 2017) encoder to gain the hid-\nden states for source and pivot sentences respectively.\nThen the training objective of BRLM-HA is to predict\nthe masked words by not only the surrounding words\nin source sentence but also the encoder outputs of the\naligned words. Note that this training process is also car-\nried out in a symmetric situation, in which we mask some\nwords in pivot sentence and obtain the aligned words in\nthe source sentence.\n•Soft Alignment (BRLM-SA). Instead of using external\naligner tool, BRLM-SA introduces an additional atten-\ntion layer to learn the alignment information together with\nmodel training. In this way, BRLM-SA avoids the effect\ncaused by external wrong alignment information and en-\nables many-to-one soft alignment during model training.\nSimilar with BRLM-HA, the training objective of BRLM-\nSA is to predict the masked words by not only the sur-\nrounding words in source sentence but also the outputs of\nattention layer. In our implementation, the attention layer\nis a multi-head attention layer adopted in Transformer,\nwhere the queries come from the masked source sentence,\nthe keys and values come from the pivot sentence.\nIn principle, MLM and TLM can learn some implicit align-\nment information during model training. However, the align-\nment process in MLM is inefﬁcient since the shared word\npieces only account for a small proportion of the whole cor-\npus, resulting in the difﬁculty of expanding the shared in-\nformation to align the whole corpus. TLM also lacks effort\nin alignment between the source and target sentences since\nTLM concatenates the sentence pair into one sequence, mak-\ning the explicit alignment between the source and target in-\nfeasible. BRLM fully utilizes the alignment information to\nobtain better word-level representation alignment between\ndifferent languages, which better relieves the burden of the\ndomain shift problem .\nTransfer Protocol\nWe consider the typical zero-shot translation scenario in\nwhich a high resource pivot language has parallel data with\nboth source and target languages, while source and target\nlanguages has no parallel data between themselves. Our pro-\nposed cross-lingual pretraining based transfer approach for\nsource→target zero-shot translation is mainly divided into\ntwo phrases: the pretraining phase and the transfer phase.Corpus Language Train Dev Test\nEuroparlDe-En,En-Fr 1M,1M 2,000 2,000\nFr-En,En-Es 1M,1M 2,000 2,000\nRo-En,En-De 0.6M,1.5M 2,000 1,000\nMultiUNAr-En,En-Es\nEn-Ru9.7M,11.3M\n11.6M4,000 4,000\nTable 1: Data Statistics.\nIn the pretraining phase, we ﬁrst pretrain MLM on mono-\nlingual corpora of both source and pivot languages, and con-\ntinue to pretrain TLM or the proposed BRLM on the avail-\nable parallel data between source and pivot languages, in\norder to build a cross-lingual encoder shared by the source\nand pivot languages.\nIn the transfer phase, we train pivot →target NMT model\ninitialized by the cross-lingually pre-trained encoder, and\nﬁnally transfer the trained NMT model to source →target\ntranslation thanks to the shared encoder. Note that during\ntraining pivot→target NMT model, we freeze several layers\nof the cross-lingually pre-trained encoder to avoid the de-\ngeneracy issue.\nFor the', ' more complicated scenario that either the source\nside or the target side has multiple languages, the encoder\nand the decoder are also shared across each side languages\nfor efﬁcient deployment of translation between multiple lan-\nguages.\nExperiments\nSetup\nWe evaluate our cross-lingual pre-training based transfer ap-\nproach against several strong baselines on two public datat-\nsets, Europarl (Koehn 2005) and MultiUN (Eisele and Chen\n2010), which contain multi-parallel evaluation data to assess\nthe zero-shot performance. In all experiments, we use BLEU\nas the automatic metric for translation evaluation.1\nDatasets. The statistics of Europarl and MultiUN cor-\npora are summarized in Table 1. For Europarl corpus, we\nevaluate on French-English-Spanish (Fr-En-Es), German-\nEnglish-French (De-En-Fr) and Romanian-English-German\n(Ro-En-De), where English acts as the pivot language, its\nleft side is the source language, and its right side is the target\nlanguage. We remove the multi-parallel sentences between\ndifferent training corpora to ensure zero-shot settings. We\nuse the devtest2006 as the validation set and the test2006 as\nthe test set for Fr→Es and De→Fr. For distant language pair\nRo→De, we extract 1,000 overlapping sentences from new-\nstest2016 as the test set and the 2,000 overlapping sentences\nsplit from the training set as the validation set since there is\nno ofﬁcial validation and test sets. For vocabulary, we use\n60K sub-word tokens based on Byte Pair Encoding (BPE)\n(Sennrich, Haddow, and Birch 2015).\nFor MultiUN corpus, we use four languages: English\n(En) is set as the pivot language, which has parallel data\n1We calculate BLEU scores with the multi-bleu.perl script.', 'Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-\nSA) according to the way of generating the alignment infor-\nmation. These two structures actually extend MLM into a\nbilingual scenario, with the difference that BRLM leverages\nexternal aligner tool or additional attention layer to explic-\nitly introduce alignment information during model training.\n•Hard Alignment (BRLM-HA). We ﬁrst use exter-\nnal aligner tool on source ↔pivot parallel data to ex-\ntract the alignment information of sentence pair. Dur-\ning model training, given source ↔pivot sentence pair,\nBRLM-HA randomly masks some words in source sen-\ntence and leverages alignment information to obtain the\naligned words in pivot sentence for masked words. Based\non the processed input, BRLM-HA adopts the Trans-\nformer (Vaswani et al. 2017) encoder to gain the hid-\nden states for source and pivot sentences respectively.\nThen the training objective of BRLM-HA is to predict\nthe masked words by not only the surrounding words\nin source sentence but also the encoder outputs of the\naligned words. Note that this training process is also car-\nried out in a symmetric situation, in which we mask some\nwords in pivot sentence and obtain the aligned words in\nthe source sentence.\n•Soft Alignment (BRLM-SA). Instead of using external\naligner tool, BRLM-SA introduces an additional atten-\ntion layer to learn the alignment information together with\nmodel training. In this way, BRLM-SA avoids the effect\ncaused by external wrong alignment information and en-\nables many-to-one soft alignment during model training.\nSimilar with BRLM-HA, the training objective of BRLM-\nSA is to predict the masked words by not only the sur-\nrounding words in source sentence but also the outputs of\nattention layer. In our implementation, the attention layer\nis a multi-head attention layer adopted in Transformer,\nwhere the queries come from the masked source sentence,\nthe keys and values come from the pivot sentence.\nIn principle, MLM and TLM can learn some implicit align-\nment information during model training. However, the align-\nment process in MLM is inefﬁcient since the shared word\npieces only account for a small proportion of the whole cor-\npus, resulting in the difﬁculty of expanding the shared in-\nformation to align the whole corpus. TLM also lacks effort\nin alignment between the source and target sentences since\nTLM concatenates the sentence pair into one sequence, mak-\ning the explicit alignment between the source and target in-\nfeasible. BRLM fully utilizes the alignment information to\nobtain better word-level representation alignment between\ndifferent languages, which better relieves the burden of the\ndomain shift problem .\nTransfer Protocol\nWe consider the typical zero-shot translation scenario in\nwhich a high resource pivot language has parallel data with\nboth source and target languages, while source and target\nlanguages has no parallel data between themselves. Our pro-\nposed cross-lingual pretraining based transfer approach for\nsource→target zero-shot translation is mainly divided into\ntwo phrases: the pretraining phase and the transfer phase.Corpus Language Train Dev Test\nEuroparlDe-En,En-Fr 1M,1M 2,000 2,000\nFr-En,En-Es 1M,1M 2,000 2,000\nRo-En,En-De 0.6M,1.5M 2,000 1,000\nMultiUNAr-En,En-Es\nEn-Ru9.7M,11.3M\n11.6M4,000 4,000\nTable 1: Data Statistics.\nIn the pretraining phase, we ﬁrst pretrain MLM on mono-\nlingual corpora of both source and pivot languages, and con-\ntinue to pretrain TLM or the proposed BRLM on the avail-\nable parallel data between source and pivot languages, in\norder to build a cross-lingual encoder shared by the source\nand pivot languages.\nIn the transfer phase, we train pivot →target NMT model\ninitialized by the cross-lingually pre-trained encoder, and\nﬁnally transfer the trained NMT model to source →target\ntranslation thanks to the shared encoder. Note that during\ntraining pivot→target NMT model, we freeze several layers\nof the cross-lingually pre-trained encoder to avoid the de-\ngeneracy issue.\nFor the']","BRLM-HA uses an external aligner tool on source ↔pivot parallel data to extract the alignment information of sentence pair. During model training, BRLM-HA randomly masks some words in the source sentence and leverages alignment information to obtain the aligned words in the pivot sentence for masked words. Based on the processed input, BRLM-HA adopts the Transformer encoder to gain the hidden states for source and pivot sentences respectively. The training objective of BRLM-HA is to predict the masked words by not only the surrounding words in the source sentence but also the encoder outputs of the aligned words.",simple,TRUE
19,What is the relationship between layer depth and context-specificity in contextualized word representations?,"['Figure 1: In almost all layers of BERT, ELMo, and GPT-2, the word representations are anisotropic (i.e., not\ndirectionally uniform): the average cosine similarity between uniformly randomly sampled words is non-zero.\nThe one exception is ELMo’s input layer; this is not surprising given that it generates character-level embeddings\nwithout using context. Representations in higher layers are generally more anisotropic than those in lower ones.\nELMo as well, though there are exceptions: for ex-\nample, the anisotropy in BERT’s penultimate layer\nis much higher than in its ﬁnal layer.\nIsotropy has both theoretical and empirical ben-\neﬁts for static word embeddings. In theory, it\nallows for stronger “self-normalization” during\ntraining (Arora et al., 2017), and in practice, sub-\ntracting the mean vector from static embeddings\nleads to improvements on several downstream\nNLP tasks (Mu et al., 2018). Thus the extreme\ndegree of anisotropy seen in contextualized word\nrepresentations – particularly in higher layers –\nis surprising. As seen in Figure 1, for all three\nmodels, the contextualized hidden layer represen-\ntations are almost all more anisotropic than the in-\nput layer representations, which do not incorpo-\nrate context. This suggests that high anisotropy is\ninherent to, or least a by-product of, the process of\ncontextualization.\n4.2 Context-Speciﬁcity\nContextualized word representations are more\ncontext-speciﬁc in higher layers. Recall from\nDeﬁnition 1 that the self-similarity of a word, in\na given layer of a given model, is the average co-\nsine similarity between its representations in dif-\nferent contexts, adjusted for anisotropy. If the\nself-similarity is 1, then the representations are\nnot context-speciﬁc at all; if the self-similarity is\n0, that the representations are maximally context-\nspeciﬁc. In Figure 2, we plot the average self-\nsimilarity of uniformly randomly sampled wordsin each layer of BERT, ELMo, and GPT-2. For\nexample, the self-similarity is 1.0 in ELMo’s in-\nput layer because representations in that layer are\nstatic character-level embeddings.\nIn all three models, the higher the layer, the\nlower the self-similarity is on average. In other\nwords, the higher the layer, the more context-\nspeciﬁc the contextualized representations. This\nﬁnding makes intuitive sense. In image classiﬁca-\ntion models, lower layers recognize more generic\nfeatures such as edges while upper layers recog-\nnize more class-speciﬁc features (Yosinski et al.,\n2014). Similarly, upper layers of LSTMs trained\non NLP tasks learn more task-speciﬁc represen-\ntations (Liu et al., 2019a). Therefore, it fol-\nlows that upper layers of neural language mod-\nels learn more context-speciﬁc representations, so\nas to predict the next word for a given context\nmore accurately. Of all three models, representa-\ntions in GPT-2 are the most context-speciﬁc, with\nthose in GPT-2’s last layer being almost maxi-\nmally context-speciﬁc.\nStopwords (e.g., ‘the’, ‘of’, ‘to’ ) have among the\nmost context-speciﬁc representations. Across\nall layers, stopwords have among the lowest self-\nsimilarity of all words, implying that their con-\ntextualized representations are among the most\ncontext-speciﬁc. For example, the words with the\nlowest average self-similarity across ELMo’s lay-\ners are ‘and’, ‘of’, ‘’s’, ‘the’ , and ‘to’. This is rel-\natively surprising, given that these words are not\npolysemous. This ﬁnding suggests that the variety', 'Figure 2: The average cosine similarity between representations of the same word in different contexts is called\nthe word’s self-similarity (see Deﬁnition 1). Above, we plot the average self-similarity of uniformly randomly\nsampled words after adjusting for anisotropy (see section 3.4). In all three models, the higher the layer, the lower\nthe self-similarity, suggesting that contextualized word representations are more context-speciﬁc in higher layers.\nof contexts a word appears in, rather than its inher-\nent polysemy, is what drives variation in its con-\ntextualized representations. This answers one of\nthe questions we posed in the introduction: ELMo,\nBERT, and GPT-2 are not simply assigning one of\na ﬁnite number of word-sense representations to\neach word; otherwise, there would not be so much\nvariation in the representations of words with so\nfew word senses.\nContext-speciﬁcity manifests very differently in\nELMo, BERT, and GPT-2. As noted earlier,\ncontextualized representations are more context-\nspeciﬁc in upper layers of ELMo, BERT, and GPT-\n2. However, how does this increased context-\nspeciﬁcity manifest in the vector space? Do word\nrepresentations in the same sentence converge to a\nsingle point, or do they remain distinct from one\nanother while still being distinct from their repre-\nsentations in other contexts? To answer this ques-\ntion, we can measure a sentence’s intra-sentence\nsimilarity. Recall from Deﬁnition 2 that the intra-\nsentence similarity of a sentence, in a given layer\nof a given model, is the average cosine similarity\nbetween each of its word representations and their\nmean, adjusted for anisotropy. In Figure 3, we plot\nthe average intra-sentence similarity of 500 uni-\nformly randomly sampled sentences.\nIn ELMo, words in the same sentence are more\nsimilar to one another in upper layers. As\nword representations in a sentence become more\ncontext-speciﬁc in upper layers, the intra-sentencesimilarity also rises. This suggests that, in prac-\ntice, ELMo ends up extending the intuition behind\nFirth’s (1957) distributional hypothesis to the sen-\ntence level: that because words in the same sen-\ntence share the same context, their contextualized\nrepresentations should also be similar.\nIn BERT, words in the same sentence are more\ndissimilar to one another in upper layers. As\nword representations in a sentence become more\ncontext-speciﬁc in upper layers, they drift away\nfrom one another, although there are exceptions\n(see layer 12 in Figure 3). However, in all lay-\ners, the average similarity between words in the\nsame sentence is still greater than the average sim-\nilarity between randomly chosen words (i.e., the\nanisotropy baseline). This suggests a more nu-\nanced contextualization than in ELMo, with BERT\nrecognizing that although the surrounding sen-\ntence informs a word’s meaning, two words in the\nsame sentence do not necessarily have a similar\nmeaning because they share the same context.\nIn GPT-2, word representations in the same\nsentence are no more similar to each other than\nrandomly sampled words. On average, the un-\nadjusted intra-sentence similarity is roughly the\nsame as the anisotropic baseline, so as seen in Fig-\nure 3, the anisotropy-adjusted intra-sentence simi-\nlarity is close to 0 in most layers of GPT-2. In fact,\nthe intra-sentence similarity is highest in the input\nlayer, which does not contextualize words at all.\nThis is in contrast to ELMo and BERT, where the']","The higher the layer, the more context-specific the contextualized representations.",simple,TRUE
20,How does the pre-ordering of assisting language sentences help bridge the word order gap in low resource language translation?,"['Addressing Word-order Divergence in Multilingual Neural Machine\nTranslation for extremely Low Resource Languages\nRudra Murthy V†, Anoop Kunchukuttan‡, Pushpak Bhattacharyya†\n†Center for Indian Language Technology (CFILT)\nDepartment of Computer Science and Engineering\nIIT Bombay, India.\n‡Microsoft AI & Research, Hyderabad, India.\n{rudra,pb }@cse.iitb.ac.in, ankunchu@microsoft.com\nAbstract\nTransfer learning approaches for Neural Ma-\nchine Translation (NMT) trains a NMT model\non an assisting language-target language pair\n(parent model) which is later ﬁne-tuned for\nthe source language-target language pair of in-\nterest (child model), with the target language\nbeing the same. In many cases, the assist-\ning language has a different word order from\nthe source language. We show that divergent\nword order adversely limits the beneﬁts from\ntransfer learning when little to no parallel cor-\npus between the source and target language is\navailable. To bridge this divergence, we pro-\npose to pre-order the assisting language sen-\ntences to match the word order of the source\nlanguage and train the parent model. Our ex-\nperiments on many language pairs show that\nbridging the word order gap leads to major\nimprovements in the translation quality in ex-\ntremely low-resource scenarios.\n1 Introduction\nTransfer learning for multilingual Neural Machine\nTranslation (NMT) (Zoph et al., 2016; Dabre et al.,\n2017; Nguyen and Chiang, 2017) attempts to im-\nprove the NMT performance on the source to\ntarget language pair (child task) using an assist-\ning source language (assisting to target language\ntranslation is the parent task). Here, the parent\nmodel is trained on the assisting and target lan-\nguage parallel corpus and the trained weights are\nused to initialize the child model. If source-target\nlanguage pair parallel corpus is available, the child\nmodel can further be ﬁne-tuned. The weight ini-\ntialization reduces the requirement on the training\ndata for the source-target language pair by trans-\nferring knowledge from the parent task, thereby\nimproving the performance on the child task.\nHowever, the divergence between the source\nand the assisting language can adversely impactthe beneﬁts obtained from transfer learning. Mul-\ntiple studies have shown that transfer learning\nworks best when the languages are related (Zoph\net al., 2016; Nguyen and Chiang, 2017; Dabre\net al., 2017). Zoph et al. (2016) studied the in-\nﬂuence of language divergence between languages\nchosen for training the parent and the child model,\nand showed that choosing similar languages for\ntraining the parent and the child model leads to\nbetter improvements from transfer learning.\nSeveral studies have tried to address the lex-\nical divergence between the source and the tar-\nget languages either by using Byte Pair Encoding\n(BPE) as basic input representation units (Nguyen\nand Chiang, 2017) or character-level NMT sys-\ntem (Lee et al., 2017) or bilingual embeddings\n(Gu et al., 2018). However, the effect of word\norder divergence and its mitigation has not been\nexplored. In a practical setting, it is not uncom-\nmon to have source and assisting languages with\ndifferent word order. For instance, it is possible to\nﬁnd parallel corpora between English (SVO word\norder) and some Indian (SOV word order) lan-\nguages, but very little parallel corpora between In-\ndian languages. Hence, it is natural to use English\nas an assisting language for inter-Indian language\ntranslation.\nTo address the word order divergence, we pro-\npose to pre-order the assisting language sentences\n(SVO) to match the word order of the source lan-\nguage (SOV). We consider an extremely resource-\nconstrained scenario, where there is no parallel\ncorpus for the child task. From our experiments,\nwe show that there is a signiﬁcant increase in the\ntranslation accuracy for the unseen source-target\nlanguage pair.arXiv:1811.00383v2  [cs.CL]  10 Apr 2019']",The pre-ordering of assisting language sentences helps bridge the word order gap in low resource language translation by matching the word order of the source language.,simple,TRUE
21,What is the BIEO schema used for in the baseline model?,"[' the LSTM-CRF\nmodel at train time, but ignored them at test time. In this\npaper, we apply adversarial training on crowd annotations\non Chinese NER in new domains, and achieve better perfor-\nmances than previous studies on crowdsourcing learning.\nBaseline: LSTM-CRF\nWe use a neural CRF model as the baseline system (Ratinov\nand Roth 2009), treating NER as a sequence labeling prob-\nlem over Chinese characters, which has achieved state-of-\nthe-art performances (Peng and Dredze 2015). To this end,\nwe explore the BIEO schema to convert NER into sequence\nlabeling, following Lample et al. (2016), where sentential\ncharacter is assigned with one unique tag. Concretely, we tag\nthe non-entity character by label “O”, the beginning charac-\nter of an entity by “B-XX”, the ending character of an entity\nby “E-XX” and the other character of an entity by “I-XX”,\nwhere “XX” denotes the entity type.\nWe build high-level neural features from the input char-\nacter sequence by a bi-directional LSTM (Lample et al.\n2016). The resulting features are combined and then are\nfed into an output CRF layer for decoding. In summary, the\nbaseline model has three main components. First, we make\nvector representations for sentential characters x1x2···xn,\ntransforming the discrete inputs into low-dimensional neu-\nral inputs. Second, feature extraction is performed to obtain\nhigh-level features hner\n1hner\n2···hner\nn, by using a bi-directional\nLSTM (Bi-LSTM) structure together with a linear trans-\nformation over x1x2···xn. Third, we apply a CRF tag-\nging module over hner\n1hner\n2···hner\nn, obtaining the ﬁnal output\nNE labels. The overall framework of the baseline model is\nshown by the right part of Figure 1.\nVector Representation of Characters\nTo represent Chinese characters, we simply exploit a neu-\nral embedding layer to map discrete characters into the low-\ndimensional vector representations. The goal is achieved\nby a looking-up table EW, which is a model parameter\nand will be ﬁne-tuned during training. The looking-up ta-\nble can be initialized either by random or by using a pre-\ntrained embeddings from large scale raw corpus. For a given\nChinese character sequence c1c2···cn, we obtain the vec-']",nan,simple,TRUE
22,What approach was used for performing crowd annotation learning in Chinese Named Entity Recognition?,"['��\uf0f4\uf022\uf022\uf084\uf0d7\n\uf060\uf0f4\uf022\uf022\uf084\uf0d7 \uf0d7 \uf0d7\uf0e5 \uf0e5 \uf0e5\uf0e5 \uf0e5 \uf0e5\n\uf060\uf0f4\uf022 \uf022 \uf022\uf022 \uf022 \uf022\uf084\uf0d7 \uf0d7 \uf0d7\uf0e5 \uf0e5 \uf0e5\uf0e5 \uf0e5 \uf0e5Figure 3: Case studies of different systems, where named\nentities are illustrated by square brackets.\nthe random embeddings, demonstrating that the pretrained\nembeddings successfully provide useful information.\nCase Studies. Second, we present several case studies in or-\nder to study the differences between our baseline and the\nworker adversarial models. We conduct a closed test on the\ntraining set, the results of which can be regarded as modiﬁ-\ncations of the training corpus, since there exist inconsistent\nannotations for each training sentence among the different\nworkers. Figure 3 shows the two examples from the DL-PS\ndataset, which compares the outputs of the baseline and our\nﬁnal models, as well as the majority-voting strategy.\nIn the ﬁrst case, none of the annotations get the cor-\nrect NER result, but our proposed model can capture it.\nThe result of LSTM-CRF is the same as majority-voting.\nIn the second example, the output of majority-voting is the\nworst, which can account for the reason why the same model\ntrained on the voted corpus performs so badly, as shown in\nTable 2. The model of LSTM-CRF fails to recognize the\nnamed entity “Xiexie” because of not trusting the second\nannotation, treating it as one noise annotation. Our proposed\nmodel is able to recognize it, because of its ability of extract-\ning worker independent features.\nConclusions\nIn this paper, we presented an approach to performing crowd\nannotation learning based on the idea of adversarial training\nfor Chinese Named Entity Recognition (NER). In our ap-\nproach, we use a common and private Bi-LSTMs for rep-\nresenting annotator-generic and -speciﬁc information, and\nlearn a label Bi-LSTM from the crowd-annotated NE label\nsequences. Finally, the proposed approach adopts a LSTM-\nCRF model to perform tagging. In our experiments, we cre-\nate two data sets for Chinese NER tasks in the dialog and e-\ncommerce domains. The experimental results show that the\nproposed approach outperforms strong baseline systems.']",The approach used for performing crowd annotation learning in Chinese Named Entity Recognition is adversarial training.,simple,TRUE
23,How does the performance of BERT compare to other machine-learning-based systems in sensitive information detection and classification on Spanish clinical text?,"[' appearance of the so-called Transformers neural net-\nwork architectures (Wolf et al., 2019). In this paper, we\nconduct several experiments in sensitive information de-\ntection and classiﬁcation on Spanish clinical text using\nBERT (from ‘Bidirectional Encoder Representations from\nTransformers’) (Devlin et al., 2019) as the base for a se-\nquence labelling approach. The experiments are carried\nout on two datasets: the MEDDOCAN: Medical Document\nAnonymization shared task dataset (Marimon et al., 2019),\nand NUB ES(Lima et al., 2019), a corpus of real medical\nreports in Spanish. In these experiments, we compare the\nperformance of BERT with other machine-learning-based\nsystems, some of which use language-speciﬁc features. Our\naim is to evaluate how good a BERT-based model performs\nwithout language nor domain specialisation apart from the\ntraining data labelled for the task at hand.\nThe rest of the paper is structured as follows: the next sec-\ntion describes related work about data anonymisation in\ngeneral and clinical data anonymisation in particular; it also\nprovides a more detailed explanation and background about\nthe Transformers architecture and BERT. Section 3. de-\nscribes the data involved in the experiments and the systemsarXiv:2003.03106v2  [cs.CL]  17 Mar 2020']",nan,simple,TRUE
24,How many annotated sentences were generated based on the 20 February 2018 dump of Armenian Wikipedia?,"['Fig. 1: Steps of automatic dataset extraction from Wikipedi a\nClassiﬁcation of Wikipedia articles into NE types\nLabelling common article aliases to increase coverage\nExtraction of text fragments with outgoing links\nLabelling links according to their target article’s type\nAdjustment of labeled entities’ boundaries\ncore differences between our approach and Nothman’s system\nis that we do not rely on manual classiﬁcation of articles and\ndo not use inter-language links to project article classiﬁc ations\nacross languages. Instead, our classiﬁcation algorithm us es\nonly an article’s Wikidata entry’s ﬁrst instance of label’s\nparent subclass of labels, which are, incidentally, language\nindependent and thus can be used for any language.\nThen, outgoing links in articles are assigned the article’s\ntype they are leading to. Sentences are included in the train ing\ncorpus only if they contain at least one named entity and\nall contained capitalized words have an outgoing link to an\narticle of known type. Since in Wikipedia articles only the ﬁ rst\nmention of each entity is linked, this approach becomes very\nrestrictive and in order to include more sentences, additio nal\nlinks are inferred. This is accomplished by compiling a list of\ncommon aliases for articles corresponding to named entitie s,\nand then ﬁnding text fragments matching those aliases to\nassign a named entity label. An article’s aliases include it s\ntitle, titles of disambiguation pages with the article, and texts\nof links leading to the article (e.g. /Armlyun/armyech/armnu/armini/armnu/armgim/armre/armayb/armda (Leningrad),\n/Armpe/armyech/armtyun/armre/armvo/armgim/armre/armayb/armda (Petrograd), /Armpe/armyech/armtyun/armyech/armre/armben/armvo/armvyun/armre/armgim (Peterburg) are aliases\nfor/Armse/armayb/armnu/armken/armtyun /Armpe/armyech/armtyun/armyech/armre/armben/armvo/armvyun/armre/armgim (Saint Petersburg)). The list of\naliases is compiled for all PER ,ORG ,LOC articles.\nAfter that, link boundaries are adjusted by removing the\nlabels for expressions in parentheses, the text after a comm a,\nand in some cases breaking into separate named entities if th e\nlinked text contains a comma. For example, [LOC/Armayb/armben/armvo/armvev/armhi/armayb/armnu\n/armparenleft/armke/armayb/armghat/armayb/armke/armparenright ](Abovyan (town)) is reworked into [LOC/Armayb/armben/armvo/armvev/armhi/armayb/armnu ]\n/armparenleft/armke/armayb/armghat/armayb/armke/armparenright .\nB. Using Wikidata to classify Wikipedia\nInstead of manually classifying Wikipedia articles as it wa s\ndone in Nothman et al., we developed a rule-based classiﬁer\nthat used an article’s Wikidata instance of andsubclass of\nattributes to ﬁnd the corresponding named entity type.The classiﬁcation could be done using solely instance\noflabels, but these labels are unnecessarily speciﬁc for the\ntask and building a mapping on it would require a more\ntime-consuming and meticulous work. Therefore, we classiﬁ ed\narticles based on their ﬁrst instance of attribute’s subclass of\nvalues. Table I displays the mapping between these values an d\nnamed entity types. Using higher-level subclass of values was\nnot an option as their values often were too general, making\nit impossible to derive the correct named entity category.\nC. Generated data\nUsing the algorithm described above, we generated 7455\nannotated sentences with 163247 tokens based on 20 February\n2018 dump of Armenian Wikipedia.\nThe generated data is still signiﬁcantly smaller than the\nmanually annotated corpora from CoNLL 2002 and 2003.\nFor comparison, the train set of English CoNLL 2003 corpus\ncontains 203621 tokens and the German one 2069']",7455 annotated sentences were generated based on the 20 February 2018 dump of Armenian Wikipedia.,simple,TRUE
25,What are word vectors used for in the experiments?,"['qwith a token “ <e>”. This helps the model bet-\nter distinguish the relative position of each word\ncompared to the entity. We use the HR-BiLSTM\nmodel to predict the score of each relation r∈Re:\nsrel(r;e,q).\n5.3 Query Generation\nFinally, the system outputs the <entity, relation (or\ncore-chain)>pair(ˆe,ˆr)according to:\ns(ˆe,ˆr;q) = max\ne∈EL′\nK′(q),r∈Re(β·srerank (e;q)\n+(1−β)·srel(r;e,q)),\nwhereβis a hyperparameter to be tuned.\n5.4 Constraint Detection\nSimilar to (Yih et al., 2015), we adopt an ad-\nditional constraint detection step based on text\nmatching. Our method can be viewed as entity-\nlinking on a KB sub-graph. It contains two steps:\n(1)Sub-graph generation : given the top scored\nquery generated by the previous 3 steps5, for each\nnodev(answer node or the CVT node like in Fig-\nure 1(b)), we collect all the nodes cconnecting to\nv(with relation rc) with any relation, and generate\na sub-graph associated to the original query. (2)\nEntity-linking on sub-graph nodes : we compute\na matching score between each n-gram in the input\nquestion (without overlapping the topic entity) and\nentity name of c(except for the node in the orig-\ninal query) by taking into account the maximum\noverlapping sequence of characters between them\n(see Appendix A for details and B for special rules\ndealing with date/answer type constraints). If the\nmatching score is larger than a threshold θ(tuned\non training set), we will add the constraint entity c\n(andrc) to the query by attaching it to the corre-\nsponding node von the core-chain.\n6 Experiments\n6.1 Task Introduction & Settings\nWe use the SimpleQuestions (Bordes et al., 2015)\nand WebQSP (Yih et al., 2016) datasets. Each\nquestion in these datasets is labeled with the gold\nsemantic parse. Hence we can directly evaluate\nrelation detection performance independently as\nwell as evaluate on the KBQA end task.\n5Starting with the top-1 query suffers more from error\npropagation. However we still achieve state-of-the-art on We-\nbQSP in Sec.6, showing the advantage of our relation detec-\ntion model. We leave in future work beam-search and feature\nextraction on beam for ﬁnal answer re-ranking like in previ-\nous research.SimpleQuestions (SQ): It is a single-relation\nKBQA task. The KB we use consists of a Freebase\nsubset with 2M entities (FB2M) (Bordes et al.,\n2015), in order to compare with previous research.\nYin et al. (2016) also evaluated their relation ex-\ntractor on this data set and released their proposed\nquestion-relation pairs, so we run our relation de-\ntection model on their data set. For the KBQA\nevaluation, we also start with their entity linking\nresults6. Therefore, our results can be compared\nwith their reported results on both tasks.\nWebQSP (WQ): A multi-relation KBQA task.\nWe use the entire Freebase KB for evaluation\npurposes. Following Yih et al. (2016), we use\nS-MART (Yang and Chang, 2015) entity-linking\noutputs.7In order to evaluate the relation detec-\ntion models, we create a new relation detection\ntask from the WebQSP data set.8For each ques-\ntion and its labeled semantic parse: (1) we ﬁrst\nselect the topic entity from the parse; and then (2)\nselect all the relations and relation chains (length\n≤2) connected to the topic entity, and set the core-\nchain labeled in the parse as the positive label and\nall the others as the negative examples.\nWe tune the following hyper-parameters on de-\nvelopment sets: (1) the size of hidden states for\nLSTMs ({50, 100, 200, 400})9; (2) learning rate\n({0.1, 0.5, 1.0, 2.0}); (3) whether the shortcut\nconnections are between hidden states or between\n', 'max-pooling results (see Section 4.3); and (4) the\nnumber of training epochs.\nFor both the relation detection experiments and\nthe second-step relation detection in KBQA, we\nhave entity replacement ﬁrst (see Section 5.2\nand Figure 1). All word vectors are initialized\nwith 300-dpretrained word embeddings (Mikolov\net al., 2013). The embeddings of relation names\nare randomly initialized, since existing pre-trained\nrelation embeddings (e.g. TransE) usually support\nlimited sets of relation names. We leave the usage\nof pre-trained relation embeddings to future work.\n6.2 Relation Detection Results\nTable 2 shows the results on two relation detec-\ntion tasks. The AMPCNN result is from (Yin\net al., 2016), which yielded state-of-the-art scores\nby outperforming several attention-based meth-\n6The two resources have been downloaded from https:\n//github.com/Gorov/SimpleQuestions-EntityLinking\n7https://github.com/scottyih/STAGG\n8The dataset is available at https://github.com/Gorov/\nKBQA_RE_data .\n9For CNNs we double the size for fair comparison.']",nan,simple,TRUE
26,What is the loss function used in the baseline CRF model?,"['-\nlikelihood objective as the loss function. We apply softmax\nover all candidate output label sequences, thus the probabil-\nity of the crowd-annotated label sequence is computed by:\np(¯ y|X) =exp(\nscore (X,¯ y))\n∑\ny∈YXexp(\nscore (X,y)), (3)\nwhere ¯ yis the crowd-annotated label sequences and YXis\nall candidate label sequence of input X.\nBased on the above formula, the loss function of our base-\nline model is:\nloss(Θ,X,¯ y) =−logp(¯ y|X), (4)\nwhere Θis the set of all model parameters. We use standard\nback-propagation method to minimize the loss function of\nthe baseline CRF model.', 'Worker Adversarial\nAdversarial learning has been an effective mechanism to re-\nsolve the problem of the input features between the training\nand test examples having large divergences (Goodfellow et\nal. 2014; Ganin et al. 2016). It has been successfully applied\non domain adaption (Gui et al. 2017), cross-lingual learn-\ning (Chen et al. 2016) and multi-task learning (Liu, Qiu, and\nHuang 2017). All settings involve feature shifting between\nthe training and testing.\nIn this paper, our setting is different. We are using the\nannotations from non-experts, which are noise and can in-\nﬂuence the ﬁnal performances if they are not properly pro-\ncessed. Directly learning based on the resulting corpus may\nadapt the neural feature extraction into the biased annota-\ntions. In this work, we assume that individual workers have\ntheir own guidelines in mind after short training. For exam-\nple, a perfect worker can annotate highly consistently with\nan expert, while common crowdsourcing workers may be\nconfused and have different understandings on certain con-\ntexts. Based on the assumption, we make an adaption for the\noriginal adversarial neural network to our setting.\nOur adaption is very simple. Brieﬂy speaking, the original\nadversarial learning adds an additional discriminator to clas-\nsify the type of source inputs, for example, the domain cate-\ngory in the domain adaption setting, while we add a discrim-\ninator to classify the annotation workers. Solely the features\nfrom the input sentence is not enough for worker classiﬁ-\ncation. The annotation result of the worker is also required.\nThus the inputs of our discriminator are different. Here we\nexploit both the source sentences and the crowd-annotated\nNE labels as basic inputs for the worker discrimination.\nIn the following, we describe the proposed adversarial\nlearning module, including both the submodels and the train-\ning method. As shown by the left part of Figure 1, the\nsubmodel consists of four parts: (1) a common Bi-LSTM\nover input characters; (2) an additional Bi-LSTM to en-\ncode crowd-annotated NE label sequence; (3) a convolu-\ntional neural network (CNN) to extract features for worker\ndiscriminator; (4) output and prediction.\nCommon Bi-LSTM over Characters\nTo build the adversarial part, ﬁrst we create a new bi-\ndirectional LSTM, named by the common Bi-LSTM:\nhcommon\n1hcommon\n2···hcommon\nn =Bi-LSTM (x1x2···xn).(5)\nAs shown in Figure 1, this Bi-LSTM is constructed over\nthe same input character representations of the private Bi-\nLSTM, in order to extract worker independent features.\nThe resulting features of the common Bi-LSTM are used\nfor both NER and the worker discriminator, different with\nthe features of private Bi-LSTM which are used for NER\nonly. As shown in Figure 1, we concatenate the outputs of\nthe common and private Bi-LSTMs together, and then feed\nthe results into the feed-forward combination layer of the\nNER part. Thus Formula 1 can be rewritten as:\nhner\nt=W(hcommon\nt⊕hprivate\nt) +b, (6)\nwhere Wis wider than the original combination because the\nnewly-added hcommon\nt .Noticeably, although the resulting common features are\nused for the worker discriminator, they actually have no ca-\npability to distinguish the workers. Because this part is ex-\nploited to maximize the loss of the worker discriminator, it\nwill be interpreted in the later training subsection. These fea-\ntures are invariant among different workers, thus they can\nhave less noises for NER. This is the goal of adversarial\nlearning, and we hope the NER being able to ﬁnd useful\nfeatures from these worker independent features.\nAdditional Bi-LSTM over Annotated NER Labels\nIn order to incorporate the annotated NE labels to predict the\nexact worker, we build another bi-directional LSTM (named\nby label Bi-LSTM) based on the crowd-annotated NE label\nsequence. This Bi-LSTM is used for worker discriminator\nonly. During the decoding of the testing phase, we will never\nhave this Bi-']",The loss function used in the baseline CRF model is -logp(¯ y|X).,simple,TRUE
27,What are some positive signals of dogmatism in language use?,"['ers, often in a strongly opinionated way (“you are a\nmoron” or “they are keeping us down”). Other pro-\nnoun types do not show signiﬁcant relationships.\nLike pronouns, verb tense can reveal subtle sig-\nnals in language use, such as the tendency of medi-\ncal inpatients to focus on the past (Wolf et al., 2007).\nOn social media, comments written in the present\ntense are more likely to be oriented towards a user’s\ncurrent interaction (“this isall so stupid”), creating\nopportunities to signal dogmatism. Alternatively,\ncomments in the past tense are more likely to re-\nfer to outside experiences (“it wasan awful party”),\nspeaking less to a user’s stance towards an ongoing\ndiscussion. We ﬁnd present tense is a positive sig-\nnal for dogmatism (1.11 odds) and past tense is a\nnegative signal (0.69 odds).\nDogmatic language can be either positively or\nnegatively charged in sentiment: for example, con-\nsider the positive statement “ Trump is the SAVIOR\nof this country!!! ” or the negative statement “ Are\nyou REALLY that stupid?? Education is the only\nway out of this horrible mess. It’s hard to imagine\nhow anyone could be so deluded. ” In diverse com-\nmunities, where people hold many different kinds\nof opinions, dogmatic opinions will often tend to\ncome into conﬂict with one another (McCluskey and\nHmielowski, 2012), producing a greater likelihood\nof negative sentiment. Perhaps for this reason, neg-\native emotion (2.09 odds) and swearing (3.80 odds)\nare useful positive signals of dogmatism, while pos-\nitive emotion shows no signiﬁcant relationship.\nFinally, we ﬁnd that interrogative language (1.12\nodds) and negation (1.35 odds) are two additional\npositive signals of dogmatism. While interrogative\nwords like “how” or “what” have many benign uses,\nthey disproportionately appear in our data in the\nform of rhetorical or emotionally charged questions,\nsuch as “how can anyone be that dumb?”\nMany of these linguistic signals are correlated\nwith each other, suggesting that dogmatism is the\ncumulative effect of many component relationships.\nFor example, consider the relatively non-dogmatic\nstatement: “I think the reviewers are wrong in this\ninstance.” Removing signals of insight , we have:\n“the reviewers are wrong in this instance,” which\nis slightly more dogmatic. Then removing relativ-\nity, we have: “the reviewers are wrong.” And ﬁ-\nnally, adding certainty , we have a dogmatic state-Classiﬁer In-domain Cross-domain\nBOW 0.853 0.776\nSENT 0.677 0.646\nLING 0.801 0.728\nBOW + SENT 0.860 0.783\nBOW + LING 0.881 0.791\nTable 2: The AUC scores for dogmatism classiﬁers within and\nacross domains. BOW (bag-of-words) and SENT (sentiment\nsignals) are baselines, and LING uses the linguistic features\nfrom Table 1. We compute in-domain accuracy using 15-fold\ncross-validation on the Reddit dataset, and cross-domain accu-\nracy by training on Reddit and evaluating on comments on arti-\ncles from the New York Times. Chance AUC is 0.5.\nment: “the reviewers are always wrong.”\n4 Predicting dogmatism\nWe now show how we can use the linguistic feature\nsets we have described to build a classiﬁer that pre-\ndicts dogmatism in comments. A predictive model\nfurther validates our feature sets, and also allows us\nto analyze dogmatism in millions of other Reddit\ncomments in a scalable way, with multiple uses in\nongoing, downstream analyses.\nPrediction task. Our goal is (1) to understand\nhow well we can use the strategies in Section 3\nto predict dogmatism, and (2) to test the domain-\nindependence of these strategies. First, we test the\nperformance of our model under cross-validation\nwithin the Reddit comment dataset. We then eval-\nuate the Reddit-based model on a held out corpus\nof New York Times comments annotated using the']","Positive signals of dogmatism in language use include pronouns used in a strongly opinionated way, verb tense focused on the past, present tense indicating dogmatism, negative sentiment, swearing, interrogative language, and negation.",simple,TRUE
28,How does the matching between different levels of relation/question representations pose a difficulty?,"['��rst\nlayer, intuitively it could learn more general and\nabstract information compared to the ﬁrst layer.\nNote that the ﬁrst(second)-layer of question rep-\nresentations does not necessarily correspond to the\nword(relation)-level relation representations, in-\nstead either layer of question representations could\npotentially match to either level of relation repre-\nsentations. This raises the difﬁculty of matching\nbetween different levels of relation/question rep-\nresentations; the following section gives our pro-\nposal to deal with such problem.']",The matching between different levels of relation/question representations poses a difficulty because the first (second) layer of question representations does not necessarily correspond to the word (relation)-level relation representations.,simple,TRUE
29,What is the region of spacetime?,"['each other. However, these statistics have not been\nwidely reported for other g2p systems, so we omit\nthem here.\n6.2 Baseline\nResults on LangID and NoLangID are compared\nto the system presented by Deri and Knight\n(2016), which is identiﬁed in our results as wFST.\nTheir results can be divided into two parts:\n•High resource results, computed with wFSTs\ntrained on a combination of Wiktionary pro-\nnunciation data and g2p rules extracted from\nWikipedia IPA Help pages. They report high\nresource results for 85 languages.\n•Adapted results, where they apply various\nmapping strategies in order to adapt high re-\nsource models to other languages. The ﬁnal\nadapted results they reported include most of\nthe 85 languages with high resource results,\nas well as the various languages they were\nable to adapt them for, for a total of 229 lan-\nguages. This test set omits 23 of the high re-\nsource languages that are written in unique\nscripts or for which language distance met-\nrics could not be computed.\n6.3 Training\nWe train the LangID and NoLangID versions of\nour model each on three subsets of the Wiktionary\ndata:\n•LangID-High and NoLangID-High: Trained\non data from the 85 languages for which Deri\nand Knight (2016) used non-adapted wFST\nmodels.\n•LangID-Adapted and NoLangID-Adapted:\nTrained on data from any of the 229 lan-\nguages for which they built adapted mod-\nels. Because many of these languages had\nno training data at all, the model is actually\nonly trained on data in 157 languages. As is\nnoted above, the Adapted set omits 23 lan-\nguages which are in the High test set.\n•LangID-All and NoLangID-All: Trained on\ndata in all 311 languages in the Wiktionary\ntraining corpus.\nIn order to ease comparison to Deri and\nKnight’s system, we limited our use of the training\ncorpus to 10,000 words per language. We set aside10 percent of the data in each language for valida-\ntion, so the maximum number of training words\nfor any language is 9000 for our systems.\n6.4 Adapted Results\nOn the 229 languages for which Deri and Knight\n(2016) presented their ﬁnal results, the LangID\nversion of our system outperforms the baseline by\na wide margin. The best performance came with\nthe version of our model that was trained on data\nin all available languages, not just the languages\nit was tested on. Using a language ID token im-\nproves results considerably, but even NoLangID\nbeats the baseline in WER and WER 100. Full\nresults are presented in Table 4.\nModel WER WER 100 PER\nwFST 88.04 69.80 48.01\nLangID-High 74.99 46.18 42.64\nLangID-Adapted 75.06 46.39 41.77\nLangID-All 74.10 43.23 37.85\nNoLangID-High 82.14 50.17 54.05\nNoLangID-Adapted 85.11 48.24 55.93\nNoLangID-All 83.65 47.13 51.87\nTable 4: Adapted Results\n6.5 High Resource Results\nHaving shown that our model exceeds the perfor-\nmance of the wFST-adaptation approach, we next\ncompare it to the baseline models for just high\nresource languages. The wFST models here are\npurely monolingual – they do not use data adap-\ntation because there is sufﬁcient training data for\neach of them. Full results are presented in Table 5.\nWe omit models trained on the Adapted languages\nbecause they were not trained on high resource\nlanguages with unique writing systems, such as\nGeorgian and Greek, and consequently performed\nvery poorly on them.\nIn contrast to the larger-scale Adapted results,\nin the High Resource experiments none of the\nsequence-to-sequence approaches equal the per-\nformance of the wFST model in WER and PER,\nalthough LangID-High does come close. The\nLangID models do beat wFST in WER 100. A\npossible explanation is that a monolingual wFST\nmodel will never generate phonemes that are not\npart of the language’s inventory. A mult', 'ilingual\nmodel, on the other hand, could potentially gener-', 'ate phonemes from the inventories of any language\nit has been trained on.\nEven if LangID-High does not present a more\naccurate result, it does present a more compact\none: LangID-High is 15.4 MB, while the com-\nbined wFST high resource models are 197.5 MB.\nModel WER WER 100 PER\nwFST 44.17 21.97 14.70\nLangID-High 47.88 15.50 16.89\nLangID-All 48.76 15.78 17.35\nNoLangID-High 69.72 29.24 35.16\nNoLangID-All 69.82 29.27 35.47\nTable 5: High Resource Results\n6.6 Results on Unseen Languages\nFinally, we report our models’ results on unseen\nlanguages in Table 6. The unseen languages are\nany that are present in the test corpus but absent\nfrom the training data. Deri and Knight did not\nreport results speciﬁcally on these languages. Al-\nthough the NoLangID models sometimes do better\non WER 100, even here the LangID models have a\nslight advantage in WER and PER. This is some-\nwhat surprising because the LangID models have\nnot learned embeddings for the language ID to-\nkens of unseen languages. Perhaps negative asso-\nciations are also being learned, driving the model\ntowards predicting more common pronunciations\nfor unseen languages.\nModel WER WER 100 PER\nLangID-High 85.94 58.10 53.06\nLangID-Adapted 87.78 68.40 65.62\nLangID-All 86.27 62.31 54.33\nNoLangID-High 88.52 58.21 62.02\nNoLangID-Adapted 91.27 57.61 74.07\nNoLangID-All 89.96 56.29 62.79\nTable 6: Results on languages not in the training\ncorpus\n7 Discussion\n7.1 Language ID Tokens\nAdding a language ID token always improves\nresults in cases where an embedding has been\nlearned for that token. The power of these em-\nbeddings is demonstrated by what happens whenone feeds the same input word to the model with\ndifferent language tokens, as is seen in Table 7.\nImpressively, this even works when the source se-\nquence is in the wrong script for the language, as\nis seen in the entry for Arabic.\nLanguage Pronunciation\nEnglish d Z u: æI s\nGerman j U t s @\nSpanish x w i T eﬂ\nItalian d Z u i t S e\nPortuguese Z w i s ˜i\nTurkish Z U I d ” Z E\nArabic j u: i s\nTable 7: The word ‘juice’ translated by the\nLangID-All model with various language ID to-\nkens. The incorrect English pronunciation rhymes\nwith the system’s result for ‘ice’\n7.2 Language Embeddings\nBecause these language ID tokens are so useful,\nit would be good if they could be effectively es-\ntimated for unseen languages. ¨Ostling and Tiede-\nmann (2017) found that the language vectors their\nmodels learned correlated well to genetic relation-\nships, so it would be interesting to see if the em-\nbeddings our source encoder learned for the lan-\nguage ID tokens showed anything similar. In a few\ncases they do (the languages closest to German\nin the vector space are Luxembourgish, Bavarian,\nand Yiddish, all close relatives). However, for the\nmost part the structure of these vectors is not in-\nterpretable. Therefore, it would be difﬁcult to esti-\nmate the embedding for an unseen language, or to\n“borrow” the language ID token of a similar lan-\nguage. A more promising way forward is to ﬁnd a\nmodel that uses an externally constructed typolog-\nical representation of the language.\n7.3 Phoneme Embeddings\nIn contrast to the language embeddings, the\nphoneme embeddings appear to show many reg-\nularities (see Table 8). This is a sign that our\nmultilingual model learns similar embeddings for\nphonemes that are written with the same grapheme\nin different languages. These phonemes tend to be\nphonetically similar to each other.\nPerhaps the structure of the phoneme embed-\nding space is what leads to our']",nan,simple,TRUE
30,How is information on font type and font style used in the corpus?,"['language speciﬁed as the value of an\nxml:lang attribute and alternatives to the\noriginal title (e.g., translations) stored as\ndcterms:alternative (cf. Figure 1 for\nan example)\n•contributor : all person entities linked\nto the creation of a document, with an\nolac:code attribute with values from the\nOLAC role vocabulary used to further spec-\nify the role of the contributor, e.g., author ,\neditor ,publisher , ortranslator\n•date : date mentioned in the metadata of the\nHTML or PDF source or, for news and blog\narticles, date mentioned in the body of the\ntext, in W3C date and time format\n•description : value of the description in\nthe metadata of an HTML document or list\nof sections of a PDF document, using the\nDublin Core qualiﬁer TableOfContents\n•format : distinction between the Internet\nMedia Types (MIME types) text/html\n(for webpages) and application/pdf\n(for PDFs)\n•identifier : URL of the document or In-\nternational Standard Book Number (ISBN)\nfor books or brochures\n•language : language of the document as\nvalue of the attribute olac:code (i.e.,de,\nas conforming to ISO 639), with the CEFR\nlevel as optional element content\n•publisher : organization or person that\nmade the document available\n•relation : used to establish a link between\ndocuments in German and simpliﬁed German\nfor the parallel part of the corpus, using the\nDublin Core qualiﬁers hasVersion (for\nthe German text) and isVersionOf (for\nthe simpliﬁed German text)\n•rights : any piece of information about the\nrights of a document, as far as available in the\nsource\n•source : source document, i.e., HTML for\nweb documents and TETML for PDFs•type : nature or genre of the content of\nthe document, which, in accordance with\nthe DCMI Type V ocabulary, is Text in\nall cases and additionally StillImage in\ncases where a document also contains im-\nages. Additionally, the linguistic type is spec-\niﬁed according to the OLAC Linguistic Data\nType V ocabulary, as either primary text\n(applies to most documents) or lexicon in\ncases where a document represents an entry\nof a simpliﬁed language vocabulary\nThe elements coverage (to denote the spatial\nor temporal scope of the content of a resource),\ncreator (to denote the author of a text, see\ncontributor above), and subject (to denote\nthe topic of the document content) were not used.\nFigure 1 shows an example of OLAC meta-\ndata. The source document described with this\nmetadata record is a PDF structured into chap-\nters, with text corresponding to the CEFR level\nA2 and images. Metadata in OLAC can be con-\nverted into the metadata standard of CLARIN (a\nEuropean research infrastructure for language re-\nsources and technology),6the Component Meta-\nData Infrastructure (CMDI).7The CMDI standard\nwas chosen since it is the supported metadata ver-\nsion of CLARIN, which is speciﬁcally popular in\nGerman-speaking countries.\nInformation on the language level of a simpli-\nﬁed German text (typically A1, A2, or B1) is par-\nticularly valuable, as it allows for conducting au-\ntomatic readability assessment and graded auto-\nmatic text simpliﬁcation experiments on the data.\n52 websites and 233 PDFs (amounting to approx-\nimately 26,000 sentences) have an explicit lan-\nguage level label.\n3.3 Secondary Data\nAnnotations were added in the Text Corpus\nFormat by WebLicht (TCF)8developed as part\nof CLARIN. TCF supports standoff annotation,\nwhich allows for representation of annotations\nwith conﬂicting hierarchies. TCF does not assign\na separate ﬁle for each annotation layer; instead,\n6https://www.clarin.eu/ (last accessed: Febru-\nary 27, 2019)\n7https://www.clarin.eu/faq/\nhow-can-i-convert-my-dc-or-olac-records-cmdi\n(last accessed: February 28, 2019)\n8https://weblicht.sfs.uni-tuebingen.\nde/weblichtwiki/index.', 'php/The_TCF_Format\n(last accessed: April 11, 2019)', 'the source text and all annotation layers are stored\njointly in a single ﬁle. A token layer acts as the\nkey element to which all other annotation layers\nare linked.\nThe following types of annotations were added:\ntext structure, fonts, images, tokens, parts of\nspeech, morphological units, lemmas, sentences,\nand dependency parses. TCF does not readily ac-\ncommodate the incorporation of all of these types\nof information. We therefore extended the format\nin the following ways:\n•Information on the font type and font style\n(e.g., italics, bold print) of a token and its po-\nsition on the physical page (for PDFs only)\nwas speciﬁed as attributes to the token ele-\nments of the tokens layer (cf. Figure 2 for\nan example)\n•Information on physical page segmenta-\ntion (for PDFs only), paragraph segmen-\ntation, and line segmentation was added\nas part of a textspan element in the\ntextstructure layer\n•A separate images layer was introduced to\nholdimage elements that take as attributes\nthe x and y coordinates of the images, their\ndimensions (width and height), and the num-\nber of the page on which they occur\n•A separate fonts layer was introduced to\npreserve detailed information on the font con-\nﬁgurations referenced in the tokens layer\nLinguistic annotation was added automatically\nusing the ParZu dependency parser for German\n(Sennrich et al., 2009) (for tokens and depen-\ndency parses), the NLTK toolkit (Bird et al., 2009)\n(for sentences), the TreeTagger (Schmid, 1995)\n(for part-of-speech tags and lemmas), and Zmorge\n(Sennrich and Kunz, 2014) (for morphological\nunits). Figure 2 shows a sample corpus annotation.\nTogether, the metadata shown in Figure 1 and the\nannotations presented in Figure 2 constitute a com-\nplete TCF ﬁle.\n3.4 Corpus Proﬁle\nThe resulting corpus contains 6,217 documents\n(5,461 monolingual documents plus 378 docu-\nments for each side of the parallel data). Table\n3 shows the corpus proﬁle. The monolingual-\nonly documents on average contain fewer sen-\ntences than the simpliﬁed German side of the par-allel data (average document length in sentences\n31.64 vs. 55.75). The average sentence length\nis almost equal (approx. 11 tokens). Hence, the\nmonolingual-only texts are shorter than the simpli-\nﬁed German texts in the parallel data. Compared\nto their German counterparts, the simpliﬁed Ger-\nman texts in the parallel data have clearly under-\ngone a process of lexical simpliﬁcation: The vo-\ncabulary is smaller by 51% (33,384 vs. 16,352\ntypes), which is comparable to the rate of reduc-\ntion reported in Section 2 for the Newsela Corpus\n(50.8%).\n3.5 Empirical validation of the corpus\nBattisti (2019) applied unsupervised machine\nlearning techniques to the simpliﬁed German texts\nof the corpus presented in this paper with the\naim of investigating evidence of multiple complex-\nity levels. While the detailed results are beyond\nthe scope of this paper, the author found features\nbased on the structural information that is a unique\nproperty of this corpus (e.g., number of images,\nnumber of paragraphs, number of lines, number of\nwords of a speciﬁc font type, and adherence to a\none-sentence-per-line rule) to be predictive of the\nlevel of difﬁculty of a simpliﬁed German text. To\nour knowledge, this is the ﬁrst study to deliver em-\npirical proof of the relevance of such features.\n4 Conclusion and Outlook\nWe have introduced a corpus compiled for use\nin automatic readability assessment and automatic\ntext simpliﬁcation of German. While such tasks\nhave been addressed for other languages, research\non German is still scarce. The features exploited\nas part of machine learning approaches to read-\nability assessment so far typically include surface\nand/or (deeper) linguistic features. The corpus\npresented in this paper additionally contains infor-\nmation on text structure, typography, and images.\nThese features have been shown to be indicative\nof simple vs. complex texts both theoretically and,\nusing the']",nan,simple,TRUE
31,How does photosynthesis involve the conversion of light and electromagnetic waves into chemical energy?,"['0 50 1000.40.60.81\ntrain data (%)precision\n0 50 1000.40.60.81\ntrain data (%)recall\n0 50 1000.40.60.81\ntrain data (%)F1-score\nCRF spaCy BERT\nFigure 2: Performance decay with decreasing amounts of training data on the sensitive information detection task in the\nNUB ES-PHI corpus\nDetection Classiﬁcation\nPrec Rec F1 Prec Rec F1\nCRF (Perez et al., 2019) 0.977 0.943 0.960 0.971 0.937 0.954\nspaCy (Perez et al., 2019) 0.967 0.953 0.965 0.965 0.947 0.956\nNLND S2 (Lange et al., 2019) 0.976 0.973 0.974 0.971 0.968 0.970\nNLND S3 (Lange et al., 2019) 0.975 0.975 0.975 0.970 0.969 0.970\nBERT + CRF (Mao and Liu, 2019) 0.968 0.919 0.943 0.965 0.912 0.937\nBERT 0.973 0.972 0.972 0.968 0.967 0.967\nTable 8: Results of Experiment B: MEDDOCAN\na larger extent from recall decline, which is not that marked\nin the case of BERT. Overall, these results indicate that the\ntransfer-learning achieved through the BERT multilingual\npre-trained model not only helps obtain better results, but\nalso lowers the need of manually labelled data for this ap-\nplication domain.\n4.2. Experiment B: MEDDOCAN\nThe results of the two MEDDOCAN scenarios –detection\nand classiﬁcation– are shown in Table 8. These results fol-\nlow the same pattern as in the previous experiments, with\nthe CRF classiﬁer being the most precise of all, and BERT\noutperforming both the CRF and spaCy classiﬁers thanks to\nits greater recall. We also show the results of Mao and Liu\n(2019) who, despite of having used a BERT-based system,\nachieve lower scores than our models. The reason why it\nshould be so remain unclear.\nWith regard to the winner of the MEDDOCAN shared\ntask, the BERT-based model has not improved the scores\nobtained by neither the domain-dependent (S3) nor the\ndomain-independent (S2) NLNDE model. However, at-\ntending to the obtained results, BERT remains only 0.3 F1-\nscore points behind, and would have achieved the second\nposition among all the MEDDOCAN shared task competi-\ntors. Taking into account that only 3% of the gold labels\nremain incorrectly annotated, the task can be considered al-\nmost solved, and it is not clear if the differences among the\nsystems are actually signiﬁcant, or whether they stem from\nminor variations in initialisation or a long-tail of minor la-belling inconsistencies.\n5. Conclusions and Future Work\nIn this work we have brieﬂy introduced the problems re-\nlated to data privacy protection in clinical domain. We have\nalso described some of the groundbreaking advances on the\nNatural Language Processing ﬁeld due to the appearance of\nTransformers-based deep-learning architectures and trans-\nfer learning from very large general-domain multilingual\ncorpora, focusing our attention in one of its most represen-\ntative examples, Google’s BERT model.\nIn order to assess the performance of BERT for Span-\nish clinical data anonymisation, we have conducted sev-\neral experiments with a BERT-based sequence labelling\napproach using the pre-trained multilingual BERT model\nshared by Google as the starting point for the model train-\ning. We have compared this BERT-based sequence la-\nbelling against other methods and systems. One of the ex-\nperiments uses the MEDDOCAN 2019 shared task dataset,\nwhile the other uses a novel Spanish clinical reports dataset\ncalled NUB ES-PHI.\nThe results of the experiments show that, in NUB ES-PHI,\nthe BERT-based model outperforms the other systems with-\nout requiring any adaptation or domain-speciﬁc feature en-\ngineering, just by being trained on the provided labelled\ndata. Interestingly, the BERT-based model obtains a re-\n']",nan,simple,TRUE
32,What is the dataset used for training a neural g2p system?,"['systems contain ambiguities. English is well-\nknown for its spelling ambiguities. Abjads, used\nfor Arabic and Hebrew, do not give full represen-\ntation to vowels.\nConsequently, g2p is harder than simply replac-\ning each grapheme symbol with a corresponding\nphoneme symbol. It is the problem of replacing a\ngrapheme sequence\nG=g1,g2,...,g m\nwith a phoneme sequence\nΦ =φ1,φ2,...,φ n\nwhere the sequences are not necessarily of the\nsame length. Data-driven g2p is therefore the\nproblem of ﬁnding the phoneme sequence that\nmaximizes the likelihood of the grapheme se-\nquence:\nˆΦ = arg max\nΦ′Pr(Φ′|G)\nData-driven approaches are especially useful\nfor problems in which the rules that govern them\nare complex and difﬁcult to engineer by hand.\ng2p for languages with ambiguous orthographies\nis such a problem. Multilingual g2p, in which the\nvarious languages have similar but different and\npossibly contradictory spelling rules, can be seen\nas an extreme case of that. Therefore, a data-\ndriven sequence-to-sequence model is a natural\nchoice.\n4 Methods\n4.1 Encoder–Decoder Models\nIn order to ﬁnd the best phoneme sequence, we\nuse a neural encoder–decoder model with atten-\ntion (Bahdanau et al., 2014). The model consists\nof two main parts: the encoder compresses each\nsource grapheme sequence Ginto a ﬁxed-length\nvector. The decoder , conditioned on this ﬁxed-\nlength vector, generates the output phoneme se-\nquence Φ.\nThe encoder and decoder are both implemented\nas recurrent neural networks, which have the ad-\nvantage of being able to process sequences of ar-\nbitrary length and use long histories efﬁciently.\nThey are trained jointly to minimize cross-entropy\non the training data. We had our best results\nwhen using a bidirectional encoder, which consists\nof two separate encoders which process the inputEnc. & dec. model type LSTM\nAttention General\nEnc. & dec. layers 2\nHidden layer size 150\nSource embedding size 150\nTarget embedding size 150\nBatch size 64\nOptimizer SGD\nLearning rate 1.0\nTraining epochs 13\nTable 1: Hyperparameters for multilingual g2p\nmodels\nin forward and reverse directions. We used long\nshort-term memory units (Hochreiter and Schmid-\nhuber, 1997) for both the encoder and decoder.\nFor the attention mechanism, we used the general\nglobal attention architecture described by Luong\net al. (2015).\nWe implemented2all models with OpenNMT\n(Klein et al., 2017). Our hyperparameters, which\nwe determined by experimentation, are listed in\nTable 1.\n4.2 Training Multilingual Models\nPresenting pronunciation data in several languages\nto the network might create problems because dif-\nferent languages have different pronunciation pat-\nterns. For example, the string ‘real’ is pronounced\ndifferently in English, German, Spanish, and Por-\ntuguese. We solve this problem by prepending\neach grapheme sequence with an artiﬁcial token\nconsisting of the language’s ISO 639-3 code en-\nclosed in angle brackets. The English word ‘real’,\nfor example, would be presented to the system as\n<eng>r e a l\nThe artiﬁcial token is treated simply as an element\nof the grapheme sequence. This is similar to the\napproach taken by Johnson et al. (2016) in their\nzero-shot NMT system. However, their source-\nside artiﬁcial tokens identify the target language,\nwhereas ours identify the source language. An\nalternative approach, used by ¨Ostling and Tiede-\nmann (2017), would be to concatenate a language\nembedding to the input at each time step. They\ndo not evaluate their approach on grapheme-to-\nphoneme conversion.\n5 Data\nIn order to train a neural g2p system, one needs a\nlarge quantity of pronunciation data. A standard\n2https://github.com/bpopeters/mg2p', 'Split Train Test\nLanguages 311 507\nWords 631,828 25,894\nScripts 42 45\nTable 2: Corpus Statistics\ndataset for g2p is the Carnegie Mellon Pronounc-\ning Dictionary (Lenzo, 2007). However, that is a\nmonolingual English resource, so it is unsuitable\nfor our multilingual task. Instead, we use the mul-\ntilingual pronunciation corpus collected by Deri\nand Knight (2016) for all experiments. This cor-\npus consists of spelling–pronunciation pairs ex-\ntracted from Wiktionary. It is already partitioned\ninto training and test sets. Corpus statistics are\npresented in Table 2.\nIn addition to the raw IPA transcriptions ex-\ntracted from Wiktionary, the corpus provides an\nautomatically cleaned version of transcriptions.\nCleaning is a necessary step because web-scraped\ndata is often noisy and may be transcribed at\nan inconsistent level of detail. The data clean-\ning used here attempts to make the transcriptions\nconsistent with the phonemic inventories used in\nPhoible (Moran et al., 2014). When a transcrip-\ntion contains a phoneme that is not in its lan-\nguage’s inventory in Phoible, that phoneme is re-\nplaced by the phoneme with the most similar ar-\nticulatory features that is in the language’s inven-\ntory. Sometimes this cleaning algorithm works\nwell: in the German examples in Table 3, the raw\nGerman symbols /X/and/ç/are both converted\nto/x/. This is useful because the /X/inAns-\nbach and the /ç/inKaninchen are instances of the\nsame phoneme, so their phonemic representations\nshould use the same symbol. However, the clean-\ning algorithm can also have negative effects on the\ndata quality. For example, the phoneme /ô/is not\npresent in the Phoible inventory for German, but it\nisused in several German transcriptions in the cor-\npus. The cleaning algorithm converts /ô/to/l/in\nall German transcriptions, whereas /r/would be\na more reasonable guess. The cleaning algorithm\nalso removes most suprasegmentals, even though\nthese are often an important part of a language’s\nphonology. Developing a more sophisticated pro-\ncedure for cleaning pronunciation data is a direc-\ntion for future work, but in this paper we use the\ncorpus’s provided cleaned transcriptions in order\nto ease comparison to previous results.Lang. Script Spelling Cleaned IPA Raw IPA\ndeu Latin Ansbach a: n s b a: x ""ansbaX\ndeu Latin Kaninchen k a: n I n x @ n ka""ni:nç@n\neus Latin untxi u n ” t ” S I ""un.>tSi\nTable 3: Example entries from the Wiktionary\ntraining corpus\n6 Experiments\nWe present experiments with two versions of our\nsequence-to-sequence model. LangID prepends\neach training, validation, and test sample with\nan artiﬁcial token identifying the language of the\nsample. NoLangID omits this token. LangID and\nNoLangID have identical structure otherwise. To\ntranslate the test corpus, we used a beam width of\n100. Although this is an unusually wide beam and\nhad negligible performance effects, it was neces-\nsary to compute our error metrics.\n6.1 Evaluation\nWe use the following three evaluation metrics:\n•Phoneme Error Rate (PER) is the Lev-\nenshtein distance between the predicted\nphoneme sequences and the gold standard\nphoneme sequences, divided by the length of\nthe gold standard phoneme sequences.\n•Word Error Rate (WER) is the percentage of\nwords in which the predicted phoneme se-\nquence does not exactly match the gold stan-\ndard phoneme sequence.\n•Word Error Rate 100 (WER 100) is the per-\ncentage of words in the test set for which the\ncorrect guess is not in the ﬁrst 100 guesses of\nthe system.\nIn system evaluations, WER, WER 100, and\nPER numbers presented for multiple languages are\naveraged, weighting each language equally (fol-\nlowing Deri and Knight, 2016).\nIt would be interesting to compute error metrics\nthat incorporate phoneme similarity, such as those\nproposed by Hixon et al. (2011). PER weights all\nphoneme errors the same, even though some errors\nare more harmful', ' than others: /d/and/k/are usu-\nally contrastive, whereas /d/and/d ”/almost never\nare. Such statistics would be especially interest-\ning for evaluating a multilingual system, because\ndifferent languages often map the same grapheme\nto phonemes that are only subtly different from', 'each other. However, these statistics have not been\nwidely reported for other g2p systems, so we omit\nthem here.\n6.2 Baseline\nResults on LangID and NoLangID are compared\nto the system presented by Deri and Knight\n(2016), which is identiﬁed in our results as wFST.\nTheir results can be divided into two parts:\n•High resource results, computed with wFSTs\ntrained on a combination of Wiktionary pro-\nnunciation data and g2p rules extracted from\nWikipedia IPA Help pages. They report high\nresource results for 85 languages.\n•Adapted results, where they apply various\nmapping strategies in order to adapt high re-\nsource models to other languages. The ﬁnal\nadapted results they reported include most of\nthe 85 languages with high resource results,\nas well as the various languages they were\nable to adapt them for, for a total of 229 lan-\nguages. This test set omits 23 of the high re-\nsource languages that are written in unique\nscripts or for which language distance met-\nrics could not be computed.\n6.3 Training\nWe train the LangID and NoLangID versions of\nour model each on three subsets of the Wiktionary\ndata:\n•LangID-High and NoLangID-High: Trained\non data from the 85 languages for which Deri\nand Knight (2016) used non-adapted wFST\nmodels.\n•LangID-Adapted and NoLangID-Adapted:\nTrained on data from any of the 229 lan-\nguages for which they built adapted mod-\nels. Because many of these languages had\nno training data at all, the model is actually\nonly trained on data in 157 languages. As is\nnoted above, the Adapted set omits 23 lan-\nguages which are in the High test set.\n•LangID-All and NoLangID-All: Trained on\ndata in all 311 languages in the Wiktionary\ntraining corpus.\nIn order to ease comparison to Deri and\nKnight’s system, we limited our use of the training\ncorpus to 10,000 words per language. We set aside10 percent of the data in each language for valida-\ntion, so the maximum number of training words\nfor any language is 9000 for our systems.\n6.4 Adapted Results\nOn the 229 languages for which Deri and Knight\n(2016) presented their ﬁnal results, the LangID\nversion of our system outperforms the baseline by\na wide margin. The best performance came with\nthe version of our model that was trained on data\nin all available languages, not just the languages\nit was tested on. Using a language ID token im-\nproves results considerably, but even NoLangID\nbeats the baseline in WER and WER 100. Full\nresults are presented in Table 4.\nModel WER WER 100 PER\nwFST 88.04 69.80 48.01\nLangID-High 74.99 46.18 42.64\nLangID-Adapted 75.06 46.39 41.77\nLangID-All 74.10 43.23 37.85\nNoLangID-High 82.14 50.17 54.05\nNoLangID-Adapted 85.11 48.24 55.93\nNoLangID-All 83.65 47.13 51.87\nTable 4: Adapted Results\n6.5 High Resource Results\nHaving shown that our model exceeds the perfor-\nmance of the wFST-adaptation approach, we next\ncompare it to the baseline models for just high\nresource languages. The wFST models here are\npurely monolingual – they do not use data adap-\ntation because there is sufﬁcient training data for\neach of them. Full results are presented in Table 5.\nWe omit models trained on the Adapted languages\nbecause they were not trained on high resource\nlanguages with unique writing systems, such as\nGeorgian and Greek, and consequently performed\nvery poorly on them.\nIn contrast to the larger-scale Adapted results,\nin the High Resource experiments none of the\nsequence-to-sequence approaches equal the per-\nformance of the wFST model in WER and PER,\nalthough LangID-High does come close. The\nLangID models do beat wFST in WER 100. A\npossible explanation is that a monolingual wFST\nmodel will never generate phonemes that are not\npart of the language’s inventory. A mult']",The multilingual pronunciation corpus collected by Deri and Knight (2016) is used for training the neural g2p system.,simple,TRUE
33,What is the formula used to compute the probability of the crowd-annotated label sequence in the baseline model?,"['-\nlikelihood objective as the loss function. We apply softmax\nover all candidate output label sequences, thus the probabil-\nity of the crowd-annotated label sequence is computed by:\np(¯ y|X) =exp(\nscore (X,¯ y))\n∑\ny∈YXexp(\nscore (X,y)), (3)\nwhere ¯ yis the crowd-annotated label sequences and YXis\nall candidate label sequence of input X.\nBased on the above formula, the loss function of our base-\nline model is:\nloss(Θ,X,¯ y) =−logp(¯ y|X), (4)\nwhere Θis the set of all model parameters. We use standard\nback-propagation method to minimize the loss function of\nthe baseline CRF model.']","p(¯ y|X) =exp(
score (X,¯ y))
∑
y∈YXexp(
score (X,y)), (3)",simple,TRUE
34,What linguistic features can be used to predict dogmatism in comments?,"['ers, often in a strongly opinionated way (“you are a\nmoron” or “they are keeping us down”). Other pro-\nnoun types do not show signiﬁcant relationships.\nLike pronouns, verb tense can reveal subtle sig-\nnals in language use, such as the tendency of medi-\ncal inpatients to focus on the past (Wolf et al., 2007).\nOn social media, comments written in the present\ntense are more likely to be oriented towards a user’s\ncurrent interaction (“this isall so stupid”), creating\nopportunities to signal dogmatism. Alternatively,\ncomments in the past tense are more likely to re-\nfer to outside experiences (“it wasan awful party”),\nspeaking less to a user’s stance towards an ongoing\ndiscussion. We ﬁnd present tense is a positive sig-\nnal for dogmatism (1.11 odds) and past tense is a\nnegative signal (0.69 odds).\nDogmatic language can be either positively or\nnegatively charged in sentiment: for example, con-\nsider the positive statement “ Trump is the SAVIOR\nof this country!!! ” or the negative statement “ Are\nyou REALLY that stupid?? Education is the only\nway out of this horrible mess. It’s hard to imagine\nhow anyone could be so deluded. ” In diverse com-\nmunities, where people hold many different kinds\nof opinions, dogmatic opinions will often tend to\ncome into conﬂict with one another (McCluskey and\nHmielowski, 2012), producing a greater likelihood\nof negative sentiment. Perhaps for this reason, neg-\native emotion (2.09 odds) and swearing (3.80 odds)\nare useful positive signals of dogmatism, while pos-\nitive emotion shows no signiﬁcant relationship.\nFinally, we ﬁnd that interrogative language (1.12\nodds) and negation (1.35 odds) are two additional\npositive signals of dogmatism. While interrogative\nwords like “how” or “what” have many benign uses,\nthey disproportionately appear in our data in the\nform of rhetorical or emotionally charged questions,\nsuch as “how can anyone be that dumb?”\nMany of these linguistic signals are correlated\nwith each other, suggesting that dogmatism is the\ncumulative effect of many component relationships.\nFor example, consider the relatively non-dogmatic\nstatement: “I think the reviewers are wrong in this\ninstance.” Removing signals of insight , we have:\n“the reviewers are wrong in this instance,” which\nis slightly more dogmatic. Then removing relativ-\nity, we have: “the reviewers are wrong.” And ﬁ-\nnally, adding certainty , we have a dogmatic state-Classiﬁer In-domain Cross-domain\nBOW 0.853 0.776\nSENT 0.677 0.646\nLING 0.801 0.728\nBOW + SENT 0.860 0.783\nBOW + LING 0.881 0.791\nTable 2: The AUC scores for dogmatism classiﬁers within and\nacross domains. BOW (bag-of-words) and SENT (sentiment\nsignals) are baselines, and LING uses the linguistic features\nfrom Table 1. We compute in-domain accuracy using 15-fold\ncross-validation on the Reddit dataset, and cross-domain accu-\nracy by training on Reddit and evaluating on comments on arti-\ncles from the New York Times. Chance AUC is 0.5.\nment: “the reviewers are always wrong.”\n4 Predicting dogmatism\nWe now show how we can use the linguistic feature\nsets we have described to build a classiﬁer that pre-\ndicts dogmatism in comments. A predictive model\nfurther validates our feature sets, and also allows us\nto analyze dogmatism in millions of other Reddit\ncomments in a scalable way, with multiple uses in\nongoing, downstream analyses.\nPrediction task. Our goal is (1) to understand\nhow well we can use the strategies in Section 3\nto predict dogmatism, and (2) to test the domain-\nindependence of these strategies. First, we test the\nperformance of our model under cross-validation\nwithin the Reddit comment dataset. We then eval-\nuate the Reddit-based model on a held out corpus\nof New York Times comments annotated using the']","The linguistic features that can be used to predict dogmatism in comments include pronoun types, verb tense, sentiment (positive or negative), swearing, interrogative language, and negation.",simple,TRUE
35,What is the process of query generation?,"['qwith a token “ <e>”. This helps the model bet-\nter distinguish the relative position of each word\ncompared to the entity. We use the HR-BiLSTM\nmodel to predict the score of each relation r∈Re:\nsrel(r;e,q).\n5.3 Query Generation\nFinally, the system outputs the <entity, relation (or\ncore-chain)>pair(ˆe,ˆr)according to:\ns(ˆe,ˆr;q) = max\ne∈EL′\nK′(q),r∈Re(β·srerank (e;q)\n+(1−β)·srel(r;e,q)),\nwhereβis a hyperparameter to be tuned.\n5.4 Constraint Detection\nSimilar to (Yih et al., 2015), we adopt an ad-\nditional constraint detection step based on text\nmatching. Our method can be viewed as entity-\nlinking on a KB sub-graph. It contains two steps:\n(1)Sub-graph generation : given the top scored\nquery generated by the previous 3 steps5, for each\nnodev(answer node or the CVT node like in Fig-\nure 1(b)), we collect all the nodes cconnecting to\nv(with relation rc) with any relation, and generate\na sub-graph associated to the original query. (2)\nEntity-linking on sub-graph nodes : we compute\na matching score between each n-gram in the input\nquestion (without overlapping the topic entity) and\nentity name of c(except for the node in the orig-\ninal query) by taking into account the maximum\noverlapping sequence of characters between them\n(see Appendix A for details and B for special rules\ndealing with date/answer type constraints). If the\nmatching score is larger than a threshold θ(tuned\non training set), we will add the constraint entity c\n(andrc) to the query by attaching it to the corre-\nsponding node von the core-chain.\n6 Experiments\n6.1 Task Introduction & Settings\nWe use the SimpleQuestions (Bordes et al., 2015)\nand WebQSP (Yih et al., 2016) datasets. Each\nquestion in these datasets is labeled with the gold\nsemantic parse. Hence we can directly evaluate\nrelation detection performance independently as\nwell as evaluate on the KBQA end task.\n5Starting with the top-1 query suffers more from error\npropagation. However we still achieve state-of-the-art on We-\nbQSP in Sec.6, showing the advantage of our relation detec-\ntion model. We leave in future work beam-search and feature\nextraction on beam for ﬁnal answer re-ranking like in previ-\nous research.SimpleQuestions (SQ): It is a single-relation\nKBQA task. The KB we use consists of a Freebase\nsubset with 2M entities (FB2M) (Bordes et al.,\n2015), in order to compare with previous research.\nYin et al. (2016) also evaluated their relation ex-\ntractor on this data set and released their proposed\nquestion-relation pairs, so we run our relation de-\ntection model on their data set. For the KBQA\nevaluation, we also start with their entity linking\nresults6. Therefore, our results can be compared\nwith their reported results on both tasks.\nWebQSP (WQ): A multi-relation KBQA task.\nWe use the entire Freebase KB for evaluation\npurposes. Following Yih et al. (2016), we use\nS-MART (Yang and Chang, 2015) entity-linking\noutputs.7In order to evaluate the relation detec-\ntion models, we create a new relation detection\ntask from the WebQSP data set.8For each ques-\ntion and its labeled semantic parse: (1) we ﬁrst\nselect the topic entity from the parse; and then (2)\nselect all the relations and relation chains (length\n≤2) connected to the topic entity, and set the core-\nchain labeled in the parse as the positive label and\nall the others as the negative examples.\nWe tune the following hyper-parameters on de-\nvelopment sets: (1) the size of hidden states for\nLSTMs ({50, 100, 200, 400})9; (2) learning rate\n({0.1, 0.5, 1.0, 2.0}); (3) whether the shortcut\nconnections are between hidden states or between\n', 'max-pooling results (see Section 4.3); and (4) the\nnumber of training epochs.\nFor both the relation detection experiments and\nthe second-step relation detection in KBQA, we\nhave entity replacement ﬁrst (see Section 5.2\nand Figure 1). All word vectors are initialized\nwith 300-dpretrained word embeddings (Mikolov\net al., 2013). The embeddings of relation names\nare randomly initialized, since existing pre-trained\nrelation embeddings (e.g. TransE) usually support\nlimited sets of relation names. We leave the usage\nof pre-trained relation embeddings to future work.\n6.2 Relation Detection Results\nTable 2 shows the results on two relation detec-\ntion tasks. The AMPCNN result is from (Yin\net al., 2016), which yielded state-of-the-art scores\nby outperforming several attention-based meth-\n6The two resources have been downloaded from https:\n//github.com/Gorov/SimpleQuestions-EntityLinking\n7https://github.com/scottyih/STAGG\n8The dataset is available at https://github.com/Gorov/\nKBQA_RE_data .\n9For CNNs we double the size for fair comparison.']",nan,simple,TRUE
36,What is relation extraction and how does it relate to information extraction?,"['Question: what episode was mike kelleythe writer of\nKnowledgeBaseMike\tKelley(American\ttelevision\twriter/producer)Mike\tKelley(American\tbaseball\tplayer)…Entity Linking \nLove\tWill\tFind\ta\tWayUSA…First\tbaseman…episodes_written\nposition_playedRelation Detection(a)(b)Question: what tvshow did grantshowplay onin2008Mike\tKelley?episodes_writtenEntity Linking RelationDetectionGrantShow?starring_rolesseries(date)from2008ConstraintDetectionGrantShow\t(American\tactor)SwingTown\nBigLoveepisodesScoundrelsseries2011from20102008Figure 1: KBQA examples and its three key components. (a) A single relation example. We ﬁrst identify the topic entity with\nentity linking and then detect the relation asked by the question with relation detection (from all relations connecting the topic\nentity). Based on the detected entity and relation, we form a query to search the KB for the correct answer “ Love Will Find a\nWay”. (b) A more complex question containing two entities. By using “ Grant Show ” as the topic entity, we could detect a chain\nof relations “ starring roles-series ” pointing to the answer. An additional constraint detection takes the other entity “ 2008 ” as\na constraint, to ﬁlter the correct answer “ SwingTown ” from all candidates found by the topic entity and relation.\nthat original relation names can sometimes help\nto match longer question contexts, we propose\nto build both relation-level and word-level rela-\ntion representations. Third, we use deep bidirec-\ntional LSTMs ( BiLSTM s) to learn different levels\nof question representations in order to match the\ndifferent levels of relation information. Finally, we\npropose a residual learning method for sequence\nmatching, which makes the model training easier\nand results in more abstract (deeper) question rep-\nresentations, thus improves hierarchical matching.\nIn order to assess how the proposed improved\nrelation detection could beneﬁt the KBQA end\ntask, we also propose a simple KBQA implemen-\ntation composed of two-step relation detection .\nGiven an input question and a set of candidate enti-\nties retrieved by an entity linker based on the ques-\ntion, our proposed relation detection model plays a\nkey role in the KBQA process: (1) Re-ranking the\nentity candidates according to whether they con-\nnect to high conﬁdent relations detected from the\nraw question text by the relation detection model.\nThis step is important to deal with the ambigui-\nties normally present in entity linking results. (2)\nFinding the core relation (chains) for each topic\nentity2selection from a much smaller candidate\nentity set after re-ranking. The above steps are\nfollowed by an optional constraint detection step,\nwhen the question cannot be answered by single\nrelations (e.g., multiple entities in the question).\nFinally the highest scored query from the above\n2Following Yih et al. (2015), here topic entity refers to\nthe root of the (directed) query tree; and core-chain is the\ndirected path of relation from root to the answer node.steps is used to query the KB for answers.\nOur main contributions include: (i) An im-\nproved relation detection model by hierarchical\nmatching between questions and relations with\nresidual learning; (ii) We demonstrate that the im-\nproved relation detector enables our simple KBQA\nsystem to achieve state-of-the-art results on both\nsingle-relation and multi-relation KBQA tasks.\n2 Related Work\nRelation Extraction Relation extraction (RE) is\nan important sub-ﬁeld of information extraction.\nGeneral research in this ﬁeld usually works on a\n(small) pre-deﬁned relation set, where given a text\nparagraph and two target entities, the goal is to\ndetermine whether the text indicates any types of\nrelations between the entities or not. As a result\nRE is usually formulated as a classiﬁcation task .\nTraditional RE methods rely on large amount of\nhand-crafted features (Zhou et al., 2005; Rink and\nHarabagiu, 2010; Sun et al., 2011). Recent re-\nsearch beneﬁts a lot from the advancement of deep\nlearning: from word embeddings (Nguyen and Gr-\nishman, 2014; Gormley et al., 2015) to deep net-\nworks', ' like CNNs and LSTMs (Zeng et al., 2014;\ndos Santos et al., 2015; Vu et al., 2016) and atten-\ntion models (Zhou et al., 2016; Wang et al., 2016).\nThe above research assumes there is a ﬁxed\n(closed) set of relation types, thus no zero-shot\nlearning capability is required. The number\nof relations is usually not large: The widely\nused ACE2005 has 11/32 coarse/ﬁne-grained rela-\ntions; SemEval2010 Task8 has 19 relations; TAC-']",Relation extraction is a sub-field of information extraction that involves determining whether a text indicates any types of relations between entities. It is related to information extraction as it focuses on extracting specific relationships between entities mentioned in a text.,simple,TRUE
37,How does the Linguistic Inquiry and Word Count (LIWC) system categorize words in relation to dogmatism?,"['(Doroudi et al., 2016). For instance, here is an ex-\nample of a highly dogmatic (5)comment:\nI won’t be happy until I see the executive\nsuite of BofA, Wells, and all the others, frog-\nmarched into waiting squad cars. It’s AL-\nREADY BEEN ESTABLISHED that...\nAnd a minimally dogmatic (1)comment:\nI agree. I would like to compile a playlist for\nus trance yogi’s, even if you just would like to\nexperiment with it. Is there any preference on\nwhich platform to use?\nEach comment has been annotated by three indepen-\ndent workers on AMT, which is enough to produce\nreliable results in most labeling tasks (Sheng et al.,\n2008). To compute an aggregate measure of dogma-\ntism for each comment, we summed the scores of all\nthree workers. We show the resulting distribution of\nannotations in Figure 1.\nInter-annotator agreement. To evaluate the reli-\nability of annotations we compute Krippendorff’s α,\na measure of agreement designed for variable levels\nof measurement such as a Likert scale (Hayes and\nKrippendorff, 2007). An αof0indicates agreement\nindistinguishable from chance, while an αof 1 indi-\ncates perfect agreement. Across all annotations we\nﬁndα= 0.44. While workers agree much more\nthan chance, clearly dogmatism is also subjective.\nIn fact, when we examine only the middle two quar-\ntiles of the dogmatism annotations, we ﬁnd agree-\nment is no better than chance. Alternatively, when\nwe measure agreement only among the top and bot-\ntom quartiles of annotations, we ﬁnd agreement of\nα= 0.69. This suggests comments with scores that\nare only slightly dogmatic are unreliable and often\nsubject to human disagreement. For this reason, we\nuse only the top and bottom quartiles of comments\nwhen training our model.\n3 Approaches to Identifying Dogmatism\nWe now consider strategies for identifying dog-\nmatism based on prior work in psychology. We\nstart with the Linguistic Inquiry and Word Count\n(LIWC), a lexicon popular in the social sciences\n(Pennebaker et al., 2001). LIWC provides human\nvalidated lists of words that correspond to high-\nlevel psychological categories such as certainty or\nperception . In other studies, LIWC has uncoveredlinguistic signals relating to politeness (Danescu-\nNiculescu-Mizil et al., 2013), deception (Yoo and\nGretzel, 2009), or authority in texts (Gilbert, 2012).\nHere, we examine how dogmatism relates to 17 of\nLIWC’s categories (Table 1).\nTo compute the relationships between LIWC cat-\negories and dogmatism, we ﬁrst count the relevant\ncategory terms that appear in each annotated Reddit\ncomment, normalized by its word count. We then\ncalculate odds ratios on the aggregate counts of each\nLIWC category over the top and bottom quartiles of\ndogmatic comments. As we have discussed, using\nthe top and bottom quartiles of comments provides\na more reliable signal of dogmatism. We check for\nsigniﬁcant differences in categories between dog-\nmatic and non-dogmatic comments using the Mann-\nWhitney U test and apply Holmes method for cor-\nrection. All odds we report in this section are signif-\nicant after correction.\nDogmatic statements tend to express a high de-\ngree of certainty (Rokeach, 1954). Here we consider\nLIWC categories that express certainty both posi-\ntively ( certainty ) and negatively ( tentativeness ). For\nexample, the word “always” is certain, while “possi-\nbly” is tentative. Conforming to existing theory, cer-\ntainty is more associated with dogmatic comments\n(1.52 odds), while tentativeness is more associated\nwith the absence of dogmatism (0.88 odds).\nTerms used to verbalize cognition can act as a\nhedge that often characterizes non-dogmatic lan-\nguage. LIWC’s insight category captures this effect\nthrough words such as “think,” “know,” or “believe.”\nThese words add nuance to a statement (Pennebaker\nand Francis, 1996), signaling it is the product of\nsomeone’s mind (“ I think you should give']",The LIWC system categorizes words in relation to dogmatism based on high-level psychological categories such as certainty and tentativeness.,simple,TRUE
38,What type of embeddings does ELMo's input layer produce?,"['. The only exception is ELMo’s\ninput layer, which produces static character-level\nembeddings without using contextual or even po-\nsitional information (Peters et al., 2018). It should\nbe noted that not all static embeddings are neces-\nsarily isotropic, however; Mimno and Thompson\n(2017) found that skipgram embeddings, which\nare also static, are not isotropic.\nContextualized representations are generally\nmore anisotropic in higher layers. As seen in\nFigure 1, for GPT-2, the average cosine similarity\nbetween uniformly randomly words is roughly 0.6\nin layers 2 through 8 but increases exponentially\nfrom layers 8 through 12. In fact, word represen-\ntations in GPT-2’s last layer are so anisotropic that\nany two words have on average an almost perfect\ncosine similarity! This pattern holds for BERT and']",ELMo's input layer produces static character-level embeddings without using contextual or even positional information.,simple,TRUE
39,What is the concept of maximum explainable variance in the context of word embeddings?,"['Figure 4: The maximum explainable variance (MEV) of a word is the proportion of variance in its contextualized\nrepresentations that can be explained by their ﬁrst principal component (see Deﬁnition 3). Above, we plot the\naverage MEV of uniformly randomly sampled words after adjusting for anisotropy. In no layer of any model can\nmore than 5% of the variance in a word’s contextualized representations be explained by a static embedding.\nStatic Embedding SimLex999 MEN WS353 RW Google MSR SemEval2012(2) BLESS AP\nGloVe 0.194 0.216 0.339 0.127 0.189 0.312 0.097 0.390 0.308\nFastText 0.239 0.239 0.432 0.176 0.203 0.289 0.104 0.375 0.291\nELMo, Layer 1 0.276 0.167 0.317 0.148 0.170 0.326 0.114 0.410 0.308\nELMo, Layer 2 0.215 0.151 0.272 0.133 0.130 0.268 0.132 0.395 0.318\nBERT, Layer 1 0.315 0.200 0.394 0.208 0.236 0.389 0.166 0.365 0.321\nBERT, Layer 2 0.320 0.166 0.383 0.188 0.230 0.385 0.149 0.365 0.321\nBERT, Layer 11 0.221 0.076 0.319 0.135 0.175 0.290 0.149 0.370 0.289\nBERT, Layer 12 0.233 0.082 0.325 0.144 0.184 0.307 0.144 0.360 0.294\nGPT-2, Layer 1 0.174 0.012 0.176 0.183 0.052 0.081 0.033 0.220 0.184\nGPT-2, Layer 2 0.135 0.036 0.171 0.180 0.045 0.062 0.021 0.245 0.184\nGPT-2, Layer 11 0.126 0.034 0.165 0.182 0.031 0.038 0.045 0.270 0.189\nGPT-2, Layer 12 0.140 -0.009 0.113 0.163 0.020 0.021 0.014 0.225 0.172\nTable 1: The performance of various static embeddings on word embedding benchmark tasks. The best result for\neach task is in bold. For the contextualizing models (ELMo, BERT, GPT-2), we use the ﬁrst principal component\nof a word’s contextualized representations in a given layer as its static embedding. The static embeddings created\nusing ELMo and BERT’s contextualized representations often outperform GloVe and FastText vectors.\nearlier, we can create static embeddings for each\nword by taking the ﬁrst principal component (PC)\nof its contextualized representations in a given\nlayer. In Table 1, we plot the performance of\nthese PC static embeddings on several benchmark\ntasks2. These tasks cover semantic similarity,\nanalogy solving, and concept categorization: Sim-\nLex999 (Hill et al., 2015), MEN (Bruni et al.,\n2014), WS353 (Finkelstein et al., 2002), RW (Lu-\nong et al., 2013), SemEval-2012 (Jurgens et al.,\n2012), Google analogy solving (Mikolov et al.,\n2013a) MSR analogy solving (Mikolov et al.,\n2013b), BLESS (Baroni and Lenci, 2011) and AP\n(Almuhareb and Poesio, 2004). We leave out lay-\ners 3 - 10 in Table 1 because their performance is\n2The Word Embeddings Benchmarks package was used\nfor evaluation.between those of Layers 2 and 11.\nThe best-performing PC static embeddings be-\nlong to the ﬁrst layer of BERT, although those\nfrom the other layers of BERT and ELMo also out-\nperform GloVe and FastText on most benchmarks.\nFor all three contextualizing models, PC static em-\nbeddings created from lower layers are more effec-\ntive those created from upper layers. Those cre-\nated using GPT-2 also perform markedly worse\nthan their counterparts from ELMo and BERT.\nGiven that upper layers are much more context-\nspeciﬁc than lower layers, and given that GPT-']",The concept of maximum explainable variance in the context of word embeddings refers to the proportion of variance in a word's contextualized representations that can be explained by their first principal component.,simple,TRUE
40,What is the role of the HR-BiLSTM model in the entity linking process in the KBQA pipeline system?,"['Remark: Another way of hierarchical matching\nconsists in relying on attention mechanism , e.g.\n(Parikh et al., 2016), to ﬁnd the correspondence\nbetween different levels of representations. This\nperforms below the HR-BiLSTM (see Table 2).\n5 KBQA Enhanced by Relation\nDetection\nThis section describes our KBQA pipeline system.\nWe make minimal efforts beyond the training of\nthe relation detection model, making the whole\nsystem easy to build.\nFollowing previous work (Yih et al., 2015; Xu\net al., 2016), our KBQA system takes an existing\nentity linker to produce the top- Klinked entities,\nELK(q), for a question q(“initial entity linking ”).\nThen we generate the KB queries for qfollowing\nthe four steps illustrated in Algorithm 1.\nAlgorithm 1: KBQA with two-step relation detection\nInput : Question q, Knowledge Base KB, the initial\ntop-Kentity candidates ELK(q)\nOutput: Top query tuple (ˆe,ˆr,{(c, rc)})\n1Entity Re-Ranking (ﬁrst-step relation detection ): Use\ntheraw question text as input for a relation detector to\nscore all relations in the KB that are associated to the\nentities in ELK(q); use the relation scores to re-rank\nELK(q)and generate a shorter list EL′\nK′(q)\ncontaining the top- K′entity candidates (Section 5.1)\n2Relation Detection : Detect relation(s) using the\nreformatted question text in which the topic entity is\nreplaced by a special token <e>(Section 5.2)\n3Query Generation : Combine the scores from step 1\nand 2, and select the top pair (ˆe,ˆr)(Section 5.3)\n4Constraint Detection (optional): Compute similarity\nbetween qand any neighbor entity cof the entities\nalong ˆr(connecting by a relation rc) , add the high\nscoring candrcto the query (Section 5.4).\nCompared to previous approaches, the main dif-\nference is that we have an additional entity re-\nranking step after the initial entity linking . We\nhave this step because we have observed that entity\nlinking sometimes becomes a bottleneck in KBQA\nsystems. For example, on SimpleQuestions the\nbest reported linker could only get 72.7% top-1\naccuracy on identifying topic entities. This is usu-\nally due to the ambiguities of entity names, e.g. in\nFig 1(a), there are TV writer andbaseball player\n“Mike Kelley ”, which is impossible to distinguish\nwith only entity name matching.\nHaving observed that different entity candidates\nusually connect to different relations, here we pro-\npose to help entity disambiguation in the initial en-\ntity linking with relations detected in questions.Sections 5.1 and 5.2 elaborate how our relation\ndetection help to re-rank entities in the initial en-\ntity linking, and then those re-ranked entities en-\nable more accurate relation detection. The KBQA\nend task, as a result, beneﬁts from this process.\n5.1 Entity Re-Ranking\nIn this step, we use the raw question text as input\nfor a relation detector to score all relations in the\nKB with connections to at least one of the entity\ncandidates in ELK(q). We call this step relation\ndetection on entity set since it does not work on\na single topic entity as the usual settings. We use\nthe HR-BiLSTM as described in Sec. 4. For each\nquestionq, after generating a score srel(r;q)for\neach relation using HR-BiLSTM, we use the top\nlbest scoring relations ( Rl\nq) to re-rank the origi-\nnal entity candidates. Concretely, for each entity\neand its associated relations Re, given the origi-\nnal entity linker score slinker , and the score of the\nmost conﬁdent relation r∈Rl\nq∩Re, we sum these\ntwo scores to re-rank the entities:\nsrerank (e;q) =α·slinker(e;q)\n+(1−α)·max\nr∈Rlq∩Resrel(r;q).\nFinally, we select top K′<K entities according to\nscoresrerank to form the re-ranked list EL′\nK′(q).', '-art on WebQSP.\n7 Conclusion\nKB relation detection is a key step in KBQA and\nis signiﬁcantly different from general relation ex-\ntraction tasks. We propose a novel KB relation\ndetection model, HR-BiLSTM, that performs hier-\narchical matching between questions and KB rela-\ntions. Our model outperforms the previous meth-\nods on KB relation detection tasks and allows our\nKBQA system to achieve state-of-the-arts. For fu-\nture work, we will investigate the integration of\nour HR-BiLSTM into end-to-end systems. For ex-\nample, our model could be integrated into the de-\ncoder in (Liang et al., 2016), to provide better se-\nquence prediction. We will also investigate new\nemerging datasets like GraphQuestions (Su et al.,\n2016) and ComplexQuestions (Bao et al., 2016) to\nhandle more characteristics of general QA.\n12Note that another reason is that we are evaluating on ac-\ncuracy here. When evaluating on F1 the gap will be smaller.']",The HR-BiLSTM model is used in the entity re-ranking step of the entity linking process in the KBQA pipeline system.,multi_context,TRUE
41,"What metrics measure word representations in ELMo, BERT, and GPT-2 models?","['How Contextual are Contextualized Word Representations?\nComparing the Geometry of BERT, ELMo, and GPT-2 Embeddings\nKawin Ethayarajh∗\nStanford University\nkawin@stanford.edu\nAbstract\nReplacing static word embeddings with con-\ntextualized word representations has yielded\nsigniﬁcant improvements on many NLP tasks.\nHowever, just how contextual are the contex-\ntualized representations produced by models\nsuch as ELMo and BERT? Are there inﬁnitely\nmany context-speciﬁc representations for each\nword, or are words essentially assigned one of\na ﬁnite number of word-sense representations?\nFor one, we ﬁnd that the contextualized rep-\nresentations of all words are not isotropic in\nany layer of the contextualizing model. While\nrepresentations of the same word in differ-\nent contexts still have a greater cosine simi-\nlarity than those of two different words, this\nself-similarity is much lower in upper layers.\nThis suggests that upper layers of contextu-\nalizing models produce more context-speciﬁc\nrepresentations, much like how upper layers\nof LSTMs produce more task-speciﬁc repre-\nsentations. In all layers of ELMo, BERT, and\nGPT-2, on average, less than 5% of the vari-\nance in a word’s contextualized representa-\ntions can be explained by a static embedding\nfor that word, providing some justiﬁcation for\nthe success of contextualized representations.\n1 Introduction\nThe application of deep learning methods to NLP\nis made possible by representing words as vec-\ntors in a low-dimensional continuous space. Tradi-\ntionally, these word embeddings were static : each\nword had a single vector, regardless of context\n(Mikolov et al., 2013a; Pennington et al., 2014).\nThis posed several problems, most notably that\nall senses of a polysemous word had to share the\nsame representation. More recent work, namely\ndeep neural language models such as ELMo (Pe-\nters et al., 2018) and BERT (Devlin et al., 2018),\n∗Work partly done at the University of Toronto.have successfully created contextualized word rep-\nresentations , word vectors that are sensitive to\nthe context in which they appear. Replacing\nstatic embeddings with contextualized representa-\ntions has yielded signiﬁcant improvements on a di-\nverse array of NLP tasks, ranging from question-\nanswering to coreference resolution.\nThe success of contextualized word represen-\ntations suggests that despite being trained with\nonly a language modelling task, they learn highly\ntransferable and task-agnostic properties of lan-\nguage. In fact, linear probing models trained on\nfrozen contextualized representations can predict\nlinguistic properties of words (e.g., part-of-speech\ntags) almost as well as state-of-the-art models (Liu\net al., 2019a; Hewitt and Manning, 2019). Still,\nthese representations remain poorly understood.\nFor one, just how contextual are these contextu-\nalized word representations? Are there inﬁnitely\nmany context-speciﬁc representations that BERT\nand ELMo can assign to each word, or are words\nessentially assigned one of a ﬁnite number of\nword-sense representations?\nWe answer this question by studying the geom-\netry of the representation space for each layer of\nELMo, BERT, and GPT-2. Our analysis yields\nsome surprising ﬁndings:\n1. In all layers of all three models, the con-\ntextualized word representations of all words\nare not isotropic: they are not uniformly dis-\ntributed with respect to direction. Instead,\nthey are anisotropic , occupying a narrow\ncone in the vector space. The anisotropy in\nGPT-2’s last layer is so extreme that two ran-\ndom words will on average have almost per-\nfect cosine similarity! Given that isotropy\nhas both theoretical and empirical beneﬁts for\nstatic embeddings (Mu et al., 2018), the ex-\ntent of anisotropy in contextualized represen-arXiv:1909.00512v1  [cs.CL]  2 Sep 2019', 'what extent they can be replaced with static word\nembeddings, if at all. Our work in this paper is\nthus markedly different from most dissections of\ncontextualized representations. It is more similar\nto Mimno and Thompson (2017), which studied\nthe geometry of static word embedding spaces.\n3 Approach\n3.1 Contextualizing Models\nThe contextualizing models we study in this pa-\nper are ELMo, BERT, and GPT-21. We choose\nthe base cased version of BERT because it is most\ncomparable to GPT-2 with respect to number of\nlayers and dimensionality. The models we work\nwith are all pre-trained on their respective lan-\nguage modelling tasks. Although ELMo, BERT,\nand GPT-2 have 2, 12, and 12 hidden layers re-\nspectively, we also include the input layer of each\ncontextualizing model as its 0thlayer. This is be-\ncause the 0thlayer is not contextualized, making\nit a useful baseline against which to compare the\ncontextualization done by subsequent layers.\n3.2 Data\nTo analyze contextualized word representations,\nwe need input sentences to feed into our pre-\ntrained models. Our input data come from the\nSemEval Semantic Textual Similarity tasks from\nyears 2012 - 2016 (Agirre et al., 2012, 2013, 2014,\n2015). We use these datasets because they contain\nsentences in which the same words appear in dif-\nferent contexts. For example, the word ‘dog’ ap-\npears in “A panda dog is running on the road. ”\nand“A dog is trying to get bacon off his back. ”\nIf a model generated the same representation for\n‘dog’ in both these sentences, we could infer that\nthere was no contextualization; conversely, if the\ntwo representations were different, we could infer\nthat they were contextualized to some extent. Us-\ning these datasets, we map words to the list of sen-\ntences they appear in and their index within these\nsentences. We do not consider words that appear\nin less than 5 unique contexts in our analysis.\n3.3 Measures of Contextuality\nWe measure how contextual a word representation\nis using three different metrics: self-similarity ,\nintra-sentence similarity , and maximum explain-\nable variance .\n1We use the pretrained models provided in an earlier ver-\nsion of the PyTorch-Transformers library.Deﬁnition 1 Letwbe a word that appears in\nsentences{s1,...,sn}at indices{i1,...,in}respec-\ntively, such that w=s1[i1] =...=sn[in]. Let fℓ(s,i)\nbe a function that maps s[i]to its representation in\nlayerℓof model f. The self similarity ofwin layer\nℓis\nSelfSimℓ(w) =1\nn2−n∑\nj∑\nk̸=jcos(fℓ(sj,ij),fℓ(sk,ik))\n(1)\nwhere cos denotes the cosine similarity. In other\nwords, the self-similarity of a word win layer ℓis\nthe average cosine similarity between its contextu-\nalized representations across its nunique contexts.\nIf layer ℓdoes not contextualize the representa-\ntions at all, then SelfSimℓ(w) =1 (i.e., the repre-\nsentations are identical across all contexts). The\nmore contextualized the representations are for w,\nthe lower we would expect its self-similarity to be.\nDeﬁnition 2 Letsbe a sentence that is a se-\nquence⟨w1,...,wn⟩ofnwords. Let fℓ(s,i)be a\nfunction that maps s[i]to its representation in layer\nℓof model f. The intra-sentence similarity ofsin\nlayer ℓis\nIntraSim ℓ(s) =1\nn∑\nicos(⃗sℓ,fℓ(s,i))\nwhere ⃗sℓ=1\nn∑\nifℓ(s,i)(2)\nPut more simply, the intra-sentence similarity of a\nsentence is the average cosine similarity between\nits word representations and the sentence vector,\nwhich is just the mean of those word vectors. This\nmeasure']","We measure how contextual a word representation is using three different metrics: self-similarity, intra-sentence similarity, and maximum explainable variance.",multi_context,TRUE
42,How does MLM and BRLM-SA impact cross-lingual sentence representation and zero-shot translation?,"['SA with back translation also achieves better performance\nthan the original supervised Transformer.\nAnalysis\nSentence Representation. We ﬁrst evaluate the represen-\ntational invariance across languages for all cross-lingual pre-\ntraining methods. Following Arivazhagan et al. (2018), we\nadopt max-pooling operation to collect the sentence rep-\nresentation of each encoder layer for all source-pivot sen-\ntence pairs in the Europarl validation sets. Then we calcu-\nlate the cosine similarity for each sentence pair and aver-\nage all cosine scores. As shown in Figure 3, we can ob-\nserve that, MLM+BRLM-SA has the most stable and similar\ncross-lingual representations of sentence pairs on all layers,\nwhile it achieves the best performance in zero-shot transla-\ntion. This demonstrates that better cross-lingual representa-\ntions can beneﬁt for the process of transfer learning. Besides,\nMLM+BRLM-HA is not as superior as MLM+BRLM-\nSA and even worse than MLM+TLM on Fr-En, since\nMLM+BRLM-HA may suffer from the wrong alignment\nknowledge from an external aligner tool. We also ﬁnd an in-\nteresting phenomenon that as the number of layers increases,\nthe cosine similarity decreases.\nContextualized Word Representation. We further sam-\nple an English-Russian sentence pair from the MultiUN\nvalidation sets and visualize the cosine similarity between\nhidden states of the top encoder layer to further investi-\ngate the difference of all cross-lingual pre-training meth-\nods. As shown in Figure 4, the hidden states generated by\nMLM+BRLM-SA have higher similarity for two aligned\nwords. It indicates that MLM+BRLM-SA can gain bet-\nter word-level representation alignment between source and\npivot languages, which better relieves the burden of the do-\nmain shift problem .\nThe Effect of Freezing Parameters. To freeze parame-\nters is a common strategy to avoid catastrophic forgetting in\ntransfer learning (Howard and Ruder 2018). Table 4 shows\nthe performance of transfer learning with freezing different\nlayers on MultiUN test set, in which En →Ru denotes the\nparent model, Ar→Ru and Es→Ru are two child models,\nand all models are based on MLM+BRLM-SA. We can ﬁnd\nthat updating all parameters during training will cause a no-\ntable drop on the zero-shot direction due to the catastrophic\nforgetting. On the contrary, freezing all the parameters leads\nto the decline on supervised direction because the language\nfeatures extracted during pre-training is not sufﬁcient for\nMT task. Freezing the ﬁrst four layers of the transformer\nshows the best performance and keeps the balance between\npre-training and ﬁne-tuning.\nConclusion\nIn this paper, we propose a cross-lingual pretraining based\ntransfer approach for the challenging zero-shot translation\ntask, in which source and target languages have no parallel\ndata, while they both have parallel data with a high resource\n(a) MLM\n (b) MLM+TLM\n(c) MLM+BRLM-HA\n (d) MLM+BRLM-SA\nFigure 4: Cosine similarity visualization at word level given\nan English-Russian sentence pair from the MultiUN valida-\ntion sets. Brighter indicates higher similarity.\nFreezing Layers En→Ru Ar→Ru Es→Ru\nNone 37.80 16.09 19.80\n2 37.79 21.47 28.35\n4 37.55 25.49 30.47\n6 35.31 22.90 28.22\nTable 4: BLEU score of freezing different layers. The num-\nber in Freezing Layers column denotes that the number of\nencoder layers will not be updated.\npivot language. With the aim of building the language in-\nvariant representation between source and pivot languages\nfor smooth transfer of the parent model of pivot →target di-\nrection to the child model of source →target direction, we in-\ntroduce one monolingual pretraining method and two bilin-\ngual pretraining methods to construct an universal encoder\nfor the source and pivot languages. Experiments on public\ndatasets show that our approaches signiﬁcantly outperforms\nseveral strong baseline systems, and manifest the language\ninvariance characteristics in both sentence level and word\nlevel neural representations.\nAcknowledgments\nWe would like', 'MultiUN Ar,Es,Ru↔En\nDirection Ar→Es Es→Ar Ar→Ru Ru→Ar Es→Ru Ru→Es A-ZST A-ST\nBaselines\nCross-lingual Transfer 10.26 12.44 4.58 4.42 13.80 7.93 8.90 44.73\nMNMT(Johnson et al. 2016) 27.40 20.18 15.12 16.19 17.88 27.93 20.78 43.95\nPivoting m 42.29 30.15 27.23 26.16 29.57 40.08 32.58 43.95\nProposed Cross-lingual Pretraining Based Transfer\nMLM 16.50 23.41 9.61 14.23 22.80 23.66 18.36 44.25\nMLM+TLM 25.98 26.55 16.84 20.07 25.91 29.52 24.14 43.71\nMLM+BRLM-HA 29.05 27.58 18.10 20.42 25.39 30.96 25.25 44.67\nMLM+BRLM-SA 36.01 31.08 25.49 25.06 30.47 36.01 30.68 44.54\nAdding Back Translation\nMNMT* (Gu et al. 2019) 39.72 28.05 24.67 24.43 27.41 38.01 30.38 43.98\nMLM 40.98 31.53 26.06 26.69 31.28 40.02 32.76 44.28\nMLM+TLM 41.15 29.77 27.61 27.74 31.02 40.37 32.39 44.14\nMLM+BRLM-HA 41.74 31.89 27.24 27.54 31.29 40.34 33.35 44.52\nMLM+BRLM-SA 44.17 33.20 29.01 28.91 32.53 41.93 34.95 45.49\nTable 3: Results on MultiUN test sets. The six zero-shot translation directions are evaluated. The column “A-ZST"" reports av-\neraged BLEU of zero-shot translation, while the column “A-ST"" reports averaged BLEU of supervised pivot →target direction.\n(a) Fr-En\n (b) De-En\n (c) Ro-En\nFigure 3: Cosine similarity between sentence representation of each encoder layer across all source-pivot sentence pairs in the\nEuroparl validation set.\ncoder trained on both large-scale monolingual data and par-\nallel data between multiple languages.\nMLM alone that does not use source ↔pivot parallel data\nperforms much better than the cross-lingual transfer, and\nachieves comparable results to pivoting. When MLM is\ncombined with TLM or the proposed BRLM, the perfor-\nmance is further improved. MLM+BRLM-SA performs the\nbest, and is better than MLM+BRLM-HA indicating that\nsoft alignment is helpful than hard alignment for the cross-\nlingual pretraining.\nResults on MultiUN Dataset. Like experimental results\non Europarl, MLM+BRLM-SA performs the best among\nall proposed cross-lingual pretraining based transfer ap-\nproaches as shown in Table 3. When comparing systems\nconsisting of one encoder-decoder model for all zero-shot\ntranslation, our approaches performs signiﬁcantly better\nthan MNMT (Johnson et al. 2016).Although it is challenging for one model to translate all\nzero-shot directions between multiple distant language pairs\nof MultiUN, MLM+BRLM-SA still achieves better perfor-\nmances on Es→Ar and Es→Ru than strong pivoting m,\nwhich uses MNMT to translate source to pivot then to tar-\nget in two separate steps with each step receiving supervised\nsignal of parallel corpora. Our approaches surpass pivoting m\nin all zero-shot directions by adding back translation (Sen-\nnrich, Haddow, and Birch 2015) to generate pseudo parallel\nsentences for all zero-shot directions based on our pretrained\nmodels such as MLM+BRLM-SA, and further training our\nuniversal encoder-decoder model with these pseudo data.\nGu et al. (2019) introduces back translation into MNMT,\nwhile we adopt it in our transfer approaches. Finally, our\nbest MLM+BRLM-SA with back translation outperforms\npivoting mby 2.4 BLEU points averagely, and outperforms\nMNMT (Gu et al. 2019) by 4.6']","MLM and BRLM-SA have a positive impact on cross-lingual sentence representation and zero-shot translation. MLM alone performs better than cross-lingual transfer and achieves comparable results to pivoting. When MLM is combined with BRLM-SA, the performance is further improved. MLM+BRLM-SA performs the best among all proposed cross-lingual pretraining based transfer approaches.",multi_context,TRUE
43,"What aspects of a corpus are important for machine learning in readability assessment and text simplification, but have been overlooked?","['arXiv:1909.09067v1  [cs.CL]  19 Sep 2019A Corpus for Automatic Readability Assessment and Text Simp liﬁcation\nof German\nAlessia Battisti\nInstitute of Computational Linguistics\nUniversity of Zurich\nAndreasstrasse 15, 8050 Zurich\nalessia.battisti@uzh.chSarah Ebling\nInstitute of Computational Linguistics\nUniversity of Zurich\nAndreasstrasse 15, 8050 Zurich\nebling@cl.uzh.ch\nAbstract\nIn this paper, we present a corpus for use in\nautomatic readability assessment and auto-\nmatic text simpliﬁcation of German. The\ncorpus is compiled from web sources and\nconsists of approximately 211,000 sen-\ntences. As a novel contribution, it con-\ntains information on text structure, typog-\nraphy, and images, which can be exploited\nas part of machine learning approaches to\nreadability assessment and text simpliﬁca-\ntion. The focus of this publication is on\nrepresenting such information as an exten-\nsion to an existing corpus standard.\n1 Introduction\nSimpliﬁed language is a variety of standard lan-\nguage characterized by reduced lexical and syn-\ntactic complexity, the addition of explanations\nfor difﬁcult concepts, and clearly structured lay-\nout.1Among the target groups of simpliﬁed lan-\nguage commonly mentioned are persons with cog-\nnitive impairment or learning disabilities, prelin-\ngually deaf persons, functionally illiterate persons,\nand foreign language learners (Bredel and Maaß,\n2016).\nTwo natural language processing tasks deal with\nthe concept of simpliﬁed language: automatic\nreadability assessment and automatic text simpli-\nﬁcation. Readability assessment refers to the pro-\ncess of determining the level of difﬁculty of a text,\ne.g., along readability measures, school grades, or\nlevels of the Common European Framework of\nReference for Languages (CEFR) (Council of Eu-\nrope, 2009). Readability measures, in their tra-\nditional form, take into account only surface fea-\ntures. For example, the Flesch Reading Ease Score\n1The term plain language is avoided, as it refers to a spe-\nciﬁc level of simpliﬁcation. Simpliﬁed language subsumes all\nefforts of reducing the complexity of a piece of text.(Flesch, 1948) measures the length of words (in\nsyllables) and sentences (in words). While read-\nability has been shown to correlate with such fea-\ntures to some extent (Just and Carpenter, 1980), a\nconsensus has emerged according to which they\nare not sufﬁcient to account for all of the com-\nplexity inherent in a text. As Kauchak et al.\n(2014, p. 2618) state, “the usability of readabil-\nity formulas is limited and there is little evidence\nthat the output of these tools directly results in\nimproved understanding by readers”. Recently,\nmore sophisticated models employing (deeper) lin-\nguistic features such as lexical, semantic, mor-\nphological, morphosyntactic, syntactic, pragmatic,\ndiscourse, psycholinguistic, and language model\nfeatures have been proposed (Collins-Thompson,\n2014; Heimann M¨ uhlenbock, 2013; Pitler and\nNenkova, 2008; Schwarm and Ostendorf, 2005;\nTanaka et al., 2013).\nAutomatic text simpliﬁcation was initiated in\nthe late 1990s (Carroll et al., 1998; Chandrasekar\net al., 1996) and since then has been approached\nby means of rule-based and statistical methods. As\npart of a rule-based approach, the operations car-\nried out typically include replacing complex lex-\nical and syntactic units by simpler ones. A sta-\ntistical approach generally conceptualizes the sim-\npliﬁcation task as one of converting a standard-\nlanguage into a simpliﬁed-language text using ma-\nchine translation. Nisioi et al. (2017) introduced\nneural machine translation to automatic text sim-\npliﬁcation. Research on automatic text simpliﬁ-\ncation is comparatively widespread for languages\nsuch as English, Swedish, Spanish, and Brazilian\nPortuguese. To the authors’ knowledge,', ' no pro-\nductive system exists for German. Suter (2015),\nSuter et al. (2016) presented a prototype of a rule-\nbased system for German.\nMachine learning approaches to both readabil-\nity assessment and text simpliﬁcation rely on\ndata systematically prepared in the form of cor-', 'pora. Speciﬁcally, for automatic text simpliﬁca-\ntion via machine translation, pairs of standard-\nlanguage/simpliﬁed-language texts aligned at the\nsentence level (i.e., parallel corpora) are needed.\nThe paper at hand introduces a corpus devel-\noped for use in automatic readability assessment\nand automatic text simpliﬁcation of German. The\nfocus of this publication is on representing infor-\nmation that is valuable for these tasks but that hith-\nerto has largely been ignored in machine learning\napproaches centering around simpliﬁed language,\nspeciﬁcally, text structure (e.g., paragraphs, lines),\ntypography (e.g., font type, font style), and im-\nage (content, position, and dimensions) informa-\ntion. The importance of considering such infor-\nmation has repeatedly been asserted theoretically\n(Arf´ e et al., 2018; Bock, 2018; Bredel and Maaß,\n2016).\nThe remainder of this paper is structured as fol-\nlows: Section 2 presents previous corpora used for\nautomatic readability assessment and text simpliﬁ-\ncation. Section 3 describes our corpus, introduc-\ning its novel aspects and presenting the primary\ndata (Section 3.1), the metadata (Section 3.2), the\nsecondary data (Section 3.3), the proﬁle (Section\n3.4), and the results of machine learning experi-\nments carried out on the corpus (Section 3.5).\n2 Previous Corpora for Automatic\nReadability Assessment and Automatic\nText Simpliﬁcation\nA number of corpora for use in automatic read-\nability assessment and automatic text simpliﬁca-\ntion exist. The most well-known example is the\nParallel Wikipedia Simpliﬁcation Corpus (PWKP)\ncompiled from parallel articles of the English\nWikipedia and Simple English Wikipedia (Zhu et\nal., 2010) and consisting of around 108,000 sen-\ntence pairs. The corpus proﬁle is shown in Table 1.\nWhile the corpus represents the largest dataset in-\nvolving simpliﬁed language to date, its applica-\ntion has been criticized for various reasons (Aman-\ncio and Specia, 2014; Xu et al., 2015; ˇStajner et\nal., 2018); among these, the fact that Simple En-\nglish Wikipedia articles are not necessarily direct\ntranslations of articles from the English Wikipedia\nstands out. Hwang et al. (2015) provided an up-\ndated version of the corpus that includes a total\nof 280,000 full and partial matches between the\ntwo Wikipedia versions. Another frequently used\ndata collection for English is the Newsela Corpus(Xu et al., 2015) consisting of 1,130 news articles,\neach simpliﬁed into four school grade levels by\nprofessional editors. Table 2 shows the proﬁle of\nthe Newsela Corpus. The table obviates that the\ndifference in vocabulary size between the English\nand the simpliﬁed English side of the PWKP Cor-\npus amounts to only 18%, while the corresponding\nnumber for the English side and the level repre-\nsenting the highest amount of simpliﬁcation in the\nNewsela Corpus (Simple-4) is 50.8%. V ocabulary\nsize as an indicator of lexical richness is generally\ntaken to correlate positively with complexity (Vaj-\njala and Meurers, 2012).\nGasperin et al. (2010) compiled the PorSimples\nCorpus consisting of Brazilian Portuguese texts\n(2,116 sentences), each with a natural and a strong\nsimpliﬁcation, resulting in around 4,500 aligned\nsentences. Drndarevi´ c and Saggion (2012), Bott\net al. (2012), Bott and Saggion (2012) produced\nthe Simplext Corpus consisting of 200 Span-\nish/simpliﬁed Spanish document pairs, amount-\ning to a total of 1,149 (Spanish)/1,808 (simpliﬁed\nSpanish) sentences (approximately 1,000 aligned\nsentences).\nKlaper et al. (2013) created the ﬁrst parallel cor-\npus for German/simpliﬁed German, consisting of\n256 parallel texts downloaded from the web (ap-\nproximately 70,000 tokens).\n3 Building a Corpus for Automatic\nRead', 'pora. Speciﬁcally, for automatic text simpliﬁca-\ntion via machine translation, pairs of standard-\nlanguage/simpliﬁed-language texts aligned at the\nsentence level (i.e., parallel corpora) are needed.\nThe paper at hand introduces a corpus devel-\noped for use in automatic readability assessment\nand automatic text simpliﬁcation of German. The\nfocus of this publication is on representing infor-\nmation that is valuable for these tasks but that hith-\nerto has largely been ignored in machine learning\napproaches centering around simpliﬁed language,\nspeciﬁcally, text structure (e.g., paragraphs, lines),\ntypography (e.g., font type, font style), and im-\nage (content, position, and dimensions) informa-\ntion. The importance of considering such infor-\nmation has repeatedly been asserted theoretically\n(Arf´ e et al., 2018; Bock, 2018; Bredel and Maaß,\n2016).\nThe remainder of this paper is structured as fol-\nlows: Section 2 presents previous corpora used for\nautomatic readability assessment and text simpliﬁ-\ncation. Section 3 describes our corpus, introduc-\ning its novel aspects and presenting the primary\ndata (Section 3.1), the metadata (Section 3.2), the\nsecondary data (Section 3.3), the proﬁle (Section\n3.4), and the results of machine learning experi-\nments carried out on the corpus (Section 3.5).\n2 Previous Corpora for Automatic\nReadability Assessment and Automatic\nText Simpliﬁcation\nA number of corpora for use in automatic read-\nability assessment and automatic text simpliﬁca-\ntion exist. The most well-known example is the\nParallel Wikipedia Simpliﬁcation Corpus (PWKP)\ncompiled from parallel articles of the English\nWikipedia and Simple English Wikipedia (Zhu et\nal., 2010) and consisting of around 108,000 sen-\ntence pairs. The corpus proﬁle is shown in Table 1.\nWhile the corpus represents the largest dataset in-\nvolving simpliﬁed language to date, its applica-\ntion has been criticized for various reasons (Aman-\ncio and Specia, 2014; Xu et al., 2015; ˇStajner et\nal., 2018); among these, the fact that Simple En-\nglish Wikipedia articles are not necessarily direct\ntranslations of articles from the English Wikipedia\nstands out. Hwang et al. (2015) provided an up-\ndated version of the corpus that includes a total\nof 280,000 full and partial matches between the\ntwo Wikipedia versions. Another frequently used\ndata collection for English is the Newsela Corpus(Xu et al., 2015) consisting of 1,130 news articles,\neach simpliﬁed into four school grade levels by\nprofessional editors. Table 2 shows the proﬁle of\nthe Newsela Corpus. The table obviates that the\ndifference in vocabulary size between the English\nand the simpliﬁed English side of the PWKP Cor-\npus amounts to only 18%, while the corresponding\nnumber for the English side and the level repre-\nsenting the highest amount of simpliﬁcation in the\nNewsela Corpus (Simple-4) is 50.8%. V ocabulary\nsize as an indicator of lexical richness is generally\ntaken to correlate positively with complexity (Vaj-\njala and Meurers, 2012).\nGasperin et al. (2010) compiled the PorSimples\nCorpus consisting of Brazilian Portuguese texts\n(2,116 sentences), each with a natural and a strong\nsimpliﬁcation, resulting in around 4,500 aligned\nsentences. Drndarevi´ c and Saggion (2012), Bott\net al. (2012), Bott and Saggion (2012) produced\nthe Simplext Corpus consisting of 200 Span-\nish/simpliﬁed Spanish document pairs, amount-\ning to a total of 1,149 (Spanish)/1,808 (simpliﬁed\nSpanish) sentences (approximately 1,000 aligned\nsentences).\nKlaper et al. (2013) created the ﬁrst parallel cor-\npus for German/simpliﬁed German, consisting of\n256 parallel texts downloaded from the web (ap-\nproximately 70,000 tokens).\n3 Building a Corpus for Automatic\nRead']",nan,multi_context,TRUE
44,"What are the benefits of generating static word representations from contextualized ones in BERT, ELMo, and GPT-2?","['tations, such as those from Layer 1 of BERT, are\nmuch more effective.\n5 Future Work\nOur ﬁndings offer some new directions for future\nwork. For one, as noted earlier in the paper, Mu\net al. (2018) found that making static embeddings\nmore isotropic – by subtracting their mean from\neach embedding – leads to surprisingly large im-\nprovements in performance on downstream tasks.\nGiven that isotropy has beneﬁts for static embed-\ndings, it may also have beneﬁts for contextual-\nized word representations, although the latter have\nalready yielded signiﬁcant improvements despite\nbeing highly anisotropic. Therefore, adding an\nanisotropy penalty to the language modelling ob-\njective – to encourage the contextualized represen-\ntations to be more isotropic – may yield even better\nresults.\nAnother direction for future work is generat-\ning static word representations from contextual-\nized ones. While the latter offer superior per-\nformance, there are often challenges to deploying\nlarge models such as BERT in production, both\nwith respect to memory and run-time. In contrast,\nstatic representations are much easier to deploy.\nOur work in section 4.3 suggests that not only it is\npossible to extract static representations from con-\ntextualizing models, but that these extracted vec-\ntors often perform much better on a diverse array\nof tasks compared to traditional static embeddings\nsuch as GloVe and FastText. This may be a means\nof extracting some use from contextualizing mod-\nels without incurring the full cost of using them in\nproduction.\n6 Conclusion\nIn this paper, we investigated how contextual con-\ntextualized word representations truly are. For\none, we found that upper layers of ELMo, BERT,\nand GPT-2 produce more context-speciﬁc rep-\nresentations than lower layers. This increased\ncontext-speciﬁcity is always accompanied by in-\ncreased anisotropy. However, context-speciﬁcity\nalso manifests differently across the three models;\nthe anisotropy-adjusted similarity between words\nin the same sentence is highest in ELMo but al-\nmost non-existent in GPT-2. We ultimately found\nthat after adjusting for anisotropy, on average, less\nthan 5% of the variance in a word’s contextual-\nized representations could be explained by a staticembedding. This means that even in the best-case\nscenario, in all layers of all models, static word\nembeddings would be a poor replacement for con-\ntextualized ones. These insights help explain some\nof the remarkable success that contextualized rep-\nresentations have had on a diverse array of NLP\ntasks.\nAcknowledgments\nWe thank the anonymous reviewers for their in-\nsightful comments. We thank the Natural Sciences\nand Engineering Research Council of Canada\n(NSERC) for their ﬁnancial support.\nReferences\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel M\nCer, Mona T Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada\nMihalcea, et al. 2015. Semeval-2015 task 2: Seman-\ntic textual similarity, English, Spanish and pilot on\ninterpretability. In Proceedings SemEval@ NAACL-\nHLT . pages 252–263.\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel M\nCer, Mona T Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Rada Mihalcea, German Rigau, and Janyce\nWiebe. 2014. Semeval-2014 task 10: Multilin-\ngual semantic textual similarity. In Proceedings Se-\nmEval@ COLING . pages 81–91.\nEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-\nAgirre, and Weiwei Guo. 2013. Sem 2013 shared\ntask: Semantic textual similarity, including a pilot\non typed-similarity. In SEM 2013: The Second Joint\nConference on Lexical and Computational Seman-\ntics. Association for Computational Linguistics.\nEneko Agirre, Mona Diab, Daniel Cer, and Aitor\nGonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-\nlot on semantic textual similarity. In', 'Figure 1: In almost all layers of BERT, ELMo, and GPT-2, the word representations are anisotropic (i.e., not\ndirectionally uniform): the average cosine similarity between uniformly randomly sampled words is non-zero.\nThe one exception is ELMo’s input layer; this is not surprising given that it generates character-level embeddings\nwithout using context. Representations in higher layers are generally more anisotropic than those in lower ones.\nELMo as well, though there are exceptions: for ex-\nample, the anisotropy in BERT’s penultimate layer\nis much higher than in its ﬁnal layer.\nIsotropy has both theoretical and empirical ben-\neﬁts for static word embeddings. In theory, it\nallows for stronger “self-normalization” during\ntraining (Arora et al., 2017), and in practice, sub-\ntracting the mean vector from static embeddings\nleads to improvements on several downstream\nNLP tasks (Mu et al., 2018). Thus the extreme\ndegree of anisotropy seen in contextualized word\nrepresentations – particularly in higher layers –\nis surprising. As seen in Figure 1, for all three\nmodels, the contextualized hidden layer represen-\ntations are almost all more anisotropic than the in-\nput layer representations, which do not incorpo-\nrate context. This suggests that high anisotropy is\ninherent to, or least a by-product of, the process of\ncontextualization.\n4.2 Context-Speciﬁcity\nContextualized word representations are more\ncontext-speciﬁc in higher layers. Recall from\nDeﬁnition 1 that the self-similarity of a word, in\na given layer of a given model, is the average co-\nsine similarity between its representations in dif-\nferent contexts, adjusted for anisotropy. If the\nself-similarity is 1, then the representations are\nnot context-speciﬁc at all; if the self-similarity is\n0, that the representations are maximally context-\nspeciﬁc. In Figure 2, we plot the average self-\nsimilarity of uniformly randomly sampled wordsin each layer of BERT, ELMo, and GPT-2. For\nexample, the self-similarity is 1.0 in ELMo’s in-\nput layer because representations in that layer are\nstatic character-level embeddings.\nIn all three models, the higher the layer, the\nlower the self-similarity is on average. In other\nwords, the higher the layer, the more context-\nspeciﬁc the contextualized representations. This\nﬁnding makes intuitive sense. In image classiﬁca-\ntion models, lower layers recognize more generic\nfeatures such as edges while upper layers recog-\nnize more class-speciﬁc features (Yosinski et al.,\n2014). Similarly, upper layers of LSTMs trained\non NLP tasks learn more task-speciﬁc represen-\ntations (Liu et al., 2019a). Therefore, it fol-\nlows that upper layers of neural language mod-\nels learn more context-speciﬁc representations, so\nas to predict the next word for a given context\nmore accurately. Of all three models, representa-\ntions in GPT-2 are the most context-speciﬁc, with\nthose in GPT-2’s last layer being almost maxi-\nmally context-speciﬁc.\nStopwords (e.g., ‘the’, ‘of’, ‘to’ ) have among the\nmost context-speciﬁc representations. Across\nall layers, stopwords have among the lowest self-\nsimilarity of all words, implying that their con-\ntextualized representations are among the most\ncontext-speciﬁc. For example, the words with the\nlowest average self-similarity across ELMo’s lay-\ners are ‘and’, ‘of’, ‘’s’, ‘the’ , and ‘to’. This is rel-\natively surprising, given that these words are not\npolysemous. This ﬁnding suggests that the variety']",nan,multi_context,TRUE
45,How does BERT perform compared to other systems on the MEDDOCAN 2019 shared task dataset and the NUB ES-PHI dataset?,"['be more dangerous than the unintended over-obfuscation of\nnon-sensitive text.\nFurther, we have conducted an additional experiment on\nthis dataset by progressively reducing the training data for\nall the compared systems. The BERT-based model shows\nthe highest robustness to training-data scarcity, loosing only\n7 points of F1-score when trained on 230 instances instead\nof 21,371. These observation are in line with the results ob-\ntained by the NLP community using BERT for other tasks.\nThe experiments with the MEDDOCAN 2019 shared task\ndataset follow the same pattern. In this case, the BERT-\nbased model falls 0.3 F1-score points behind the shared task\nwinning system, but it would have achieved the second po-\nsition in the competition with no further reﬁnement.\nSince we have used a pre-trained multilingual BERT model,\nthe same approach is likely to work for other languages just\nby providing some labelled training data. Further, this is the\nsimplest ﬁne-tuning that can be performed based on BERT.\nMore sophisticated ﬁne-tuning layers could help improve\nthe results. For example, it could be expected that a CRF\nlayer helped enforce better BIO tagging sequence predic-\ntions. Precisely, Mao and Liu (2019) participated in the\nMEDDOCAN competition using a BERT+CRF architec-\nture, but their reported scores are about 3 points lower than\nour implementation. From the description of their work, it\nis unclear what the source of this score difference could be.\nFurther, at the time of writing this paper, new multilingual\npre-trained models and Transformer architectures have be-\ncome available. It would not come as a surprise that these\nnew resources and systems –e.g., XLM-RoBERTa (Con-\nneau et al., 2019) or BETO (Wu and Dredze, 2019), a BERT\nmodel fully pre-trained on Spanish texts– further advanced\nthe state of the art in this task.\n6. Acknowledgements\nThis work has been supported by Vicomtech and partially\nfunded by the project DeepReading (RTI2018-096846-B-\nC21, MCIU/AEI/FEDER,UE).\n7. Bibliographical References\nAbouelmehdi, K., Beni-Hessane, A., and Khalouﬁ, H.\n(2018). Big healthcare data: preserving security and pri-\nvacy. Journal of Big Data , 5(1):1–18.\nAgerri, R., Bermudez, J., and Rigau, G. (2014). IXA\npipeline: Efﬁcient and Ready to Use Multilingual NLP\ntools. In Proceedings of the 9th Language Resources and\nEvaluation Conference (LREC 2014) , pages 3823–3828.\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary,\nV ., Wenzek, G., Guzm ´an, F., Grave, E., Ott, M.,\nZettlemoyer, L., and Stoyanov, V . (2019). Unsuper-\nvised Cross-lingual Representation Learning at Scale.\narXiv:1911.02116 .\nDernoncourt, F., Lee, J. Y ., Uzuner, ¨O., and Szolovits, P.\n(2016). De-identiﬁcation of Patient Notes with Recur-\nrent Neural Networks. Journal of the American Medical\nInformatics Association , 24(3):596–606.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K.\n(2019). BERT: Pre-training of Deep Bidirectional Trans-\nformers for Language Understanding. In Proceedings of\nthe 2019 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Pa-\npers) , pages 4171–4186.\nGarc ´ıa-Sardi ˜na, L. (2018). Automating the anonymisa-\ntion of textual corpora. Master’s thesis, University of the\nBasque Country (UPV/EHU).\nHassan, F., Domingo-Ferrer, J., and Soria-Comas, J.\n(2018). Anonimizaci ´on de datos no estructurados a\ntrav´es del reconocimiento de entidades nominadas. In', '0 50 1000.40.60.81\ntrain data (%)precision\n0 50 1000.40.60.81\ntrain data (%)recall\n0 50 1000.40.60.81\ntrain data (%)F1-score\nCRF spaCy BERT\nFigure 2: Performance decay with decreasing amounts of training data on the sensitive information detection task in the\nNUB ES-PHI corpus\nDetection Classiﬁcation\nPrec Rec F1 Prec Rec F1\nCRF (Perez et al., 2019) 0.977 0.943 0.960 0.971 0.937 0.954\nspaCy (Perez et al., 2019) 0.967 0.953 0.965 0.965 0.947 0.956\nNLND S2 (Lange et al., 2019) 0.976 0.973 0.974 0.971 0.968 0.970\nNLND S3 (Lange et al., 2019) 0.975 0.975 0.975 0.970 0.969 0.970\nBERT + CRF (Mao and Liu, 2019) 0.968 0.919 0.943 0.965 0.912 0.937\nBERT 0.973 0.972 0.972 0.968 0.967 0.967\nTable 8: Results of Experiment B: MEDDOCAN\na larger extent from recall decline, which is not that marked\nin the case of BERT. Overall, these results indicate that the\ntransfer-learning achieved through the BERT multilingual\npre-trained model not only helps obtain better results, but\nalso lowers the need of manually labelled data for this ap-\nplication domain.\n4.2. Experiment B: MEDDOCAN\nThe results of the two MEDDOCAN scenarios –detection\nand classiﬁcation– are shown in Table 8. These results fol-\nlow the same pattern as in the previous experiments, with\nthe CRF classiﬁer being the most precise of all, and BERT\noutperforming both the CRF and spaCy classiﬁers thanks to\nits greater recall. We also show the results of Mao and Liu\n(2019) who, despite of having used a BERT-based system,\nachieve lower scores than our models. The reason why it\nshould be so remain unclear.\nWith regard to the winner of the MEDDOCAN shared\ntask, the BERT-based model has not improved the scores\nobtained by neither the domain-dependent (S3) nor the\ndomain-independent (S2) NLNDE model. However, at-\ntending to the obtained results, BERT remains only 0.3 F1-\nscore points behind, and would have achieved the second\nposition among all the MEDDOCAN shared task competi-\ntors. Taking into account that only 3% of the gold labels\nremain incorrectly annotated, the task can be considered al-\nmost solved, and it is not clear if the differences among the\nsystems are actually signiﬁcant, or whether they stem from\nminor variations in initialisation or a long-tail of minor la-belling inconsistencies.\n5. Conclusions and Future Work\nIn this work we have brieﬂy introduced the problems re-\nlated to data privacy protection in clinical domain. We have\nalso described some of the groundbreaking advances on the\nNatural Language Processing ﬁeld due to the appearance of\nTransformers-based deep-learning architectures and trans-\nfer learning from very large general-domain multilingual\ncorpora, focusing our attention in one of its most represen-\ntative examples, Google’s BERT model.\nIn order to assess the performance of BERT for Span-\nish clinical data anonymisation, we have conducted sev-\neral experiments with a BERT-based sequence labelling\napproach using the pre-trained multilingual BERT model\nshared by Google as the starting point for the model train-\ning. We have compared this BERT-based sequence la-\nbelling against other methods and systems. One of the ex-\nperiments uses the MEDDOCAN 2019 shared task dataset,\nwhile the other uses a novel Spanish clinical reports dataset\ncalled NUB ES-PHI.\nThe results of the experiments show that, in NUB ES-PHI,\nthe BERT-based model outperforms the other systems with-\nout requiring any adaptation or domain-speciﬁc feature en-\ngineering, just by being trained on the provided labelled\ndata. Interestingly, the BERT-based model obtains a re-\n']",The BERT-based model outperforms other systems on the MEDDOCAN 2019 shared task dataset and the NUB ES-PHI dataset.,multi_context,TRUE
46,What are the key components of KBQA examples and how do they contribute to the relation detection model?,"['Question: what episode was mike kelleythe writer of\nKnowledgeBaseMike\tKelley(American\ttelevision\twriter/producer)Mike\tKelley(American\tbaseball\tplayer)…Entity Linking \nLove\tWill\tFind\ta\tWayUSA…First\tbaseman…episodes_written\nposition_playedRelation Detection(a)(b)Question: what tvshow did grantshowplay onin2008Mike\tKelley?episodes_writtenEntity Linking RelationDetectionGrantShow?starring_rolesseries(date)from2008ConstraintDetectionGrantShow\t(American\tactor)SwingTown\nBigLoveepisodesScoundrelsseries2011from20102008Figure 1: KBQA examples and its three key components. (a) A single relation example. We ﬁrst identify the topic entity with\nentity linking and then detect the relation asked by the question with relation detection (from all relations connecting the topic\nentity). Based on the detected entity and relation, we form a query to search the KB for the correct answer “ Love Will Find a\nWay”. (b) A more complex question containing two entities. By using “ Grant Show ” as the topic entity, we could detect a chain\nof relations “ starring roles-series ” pointing to the answer. An additional constraint detection takes the other entity “ 2008 ” as\na constraint, to ﬁlter the correct answer “ SwingTown ” from all candidates found by the topic entity and relation.\nthat original relation names can sometimes help\nto match longer question contexts, we propose\nto build both relation-level and word-level rela-\ntion representations. Third, we use deep bidirec-\ntional LSTMs ( BiLSTM s) to learn different levels\nof question representations in order to match the\ndifferent levels of relation information. Finally, we\npropose a residual learning method for sequence\nmatching, which makes the model training easier\nand results in more abstract (deeper) question rep-\nresentations, thus improves hierarchical matching.\nIn order to assess how the proposed improved\nrelation detection could beneﬁt the KBQA end\ntask, we also propose a simple KBQA implemen-\ntation composed of two-step relation detection .\nGiven an input question and a set of candidate enti-\nties retrieved by an entity linker based on the ques-\ntion, our proposed relation detection model plays a\nkey role in the KBQA process: (1) Re-ranking the\nentity candidates according to whether they con-\nnect to high conﬁdent relations detected from the\nraw question text by the relation detection model.\nThis step is important to deal with the ambigui-\nties normally present in entity linking results. (2)\nFinding the core relation (chains) for each topic\nentity2selection from a much smaller candidate\nentity set after re-ranking. The above steps are\nfollowed by an optional constraint detection step,\nwhen the question cannot be answered by single\nrelations (e.g., multiple entities in the question).\nFinally the highest scored query from the above\n2Following Yih et al. (2015), here topic entity refers to\nthe root of the (directed) query tree; and core-chain is the\ndirected path of relation from root to the answer node.steps is used to query the KB for answers.\nOur main contributions include: (i) An im-\nproved relation detection model by hierarchical\nmatching between questions and relations with\nresidual learning; (ii) We demonstrate that the im-\nproved relation detector enables our simple KBQA\nsystem to achieve state-of-the-art results on both\nsingle-relation and multi-relation KBQA tasks.\n2 Related Work\nRelation Extraction Relation extraction (RE) is\nan important sub-ﬁeld of information extraction.\nGeneral research in this ﬁeld usually works on a\n(small) pre-deﬁned relation set, where given a text\nparagraph and two target entities, the goal is to\ndetermine whether the text indicates any types of\nrelations between the entities or not. As a result\nRE is usually formulated as a classiﬁcation task .\nTraditional RE methods rely on large amount of\nhand-crafted features (Zhou et al., 2005; Rink and\nHarabagiu, 2010; Sun et al., 2011). Recent re-\nsearch beneﬁts a lot from the advancement of deep\nlearning: from word embeddings (Nguyen and Gr-\nishman, 2014; Gormley et al., 2015) to deep net-\nworks', '\nWe use the same example in Fig 1(a) to illustrate\nthe idea. Given the input question in the exam-\nple, a relation detector is very likely to assign high\nscores to relations such as “ episodes written ”,\n“author of” and “ profession ”. Then, according\nto the connections of entity candidates in KB,\nwe ﬁnd that the TV writer “ Mike Kelley ” will\nbe scored higher than the baseball player “ Mike\nKelley ”, because the former has the relations\n“episodes written ” and “ profession ”. This method\ncan be viewed as exploiting entity-relation collo-\ncation for entity linking.\n5.2 Relation Detection\nIn this step, for each candidate entity e∈\nEL′\nK(q), we use the question text as the input to a\nrelation detector to score all the relations r∈Re\nthat are associated to the entity ein the KB.4Be-\ncause we have a single topic entity input in this\nstep, we do the following question reformatting:\nwe replace the the candidate e’s entity mention in\n4Note that the number of entities and the number of rela-\ntion candidates will be much smaller than those in the previ-\nous step.']","The key components of KBQA examples are entity linking, relation detection, and constraint detection. They contribute to the relation detection model by providing input for scoring relations and filtering candidate entities.",multi_context,TRUE
47,What approach is proposed in Cross-lingual Pre-training research to address language space mismatch and enable zero-shot translation?,"['Cross-lingual Pre-training Based Transfer for Zero-shot Neural\nMachine Translation\nBaijun Ji‡, Zhirui Zhang§, Xiangyu Duan†‡∗, Min Zhang†‡, Boxing Chen§and Weihua Luo§\n†Institute of Artiﬁcial Intelligence, Soochow University, Suzhou, China\n‡School of Computer Science and Technology, Soochow University, Suzhou, China\n§Alibaba DAMO Academy, Hangzhou, China\n‡bjji@stu.suda.edu.cn†{xiangyuduan, minzhang}@suda.edu.cn\n§{zhirui.zzr,boxing.cbx,weihua.luowh}@alibaba-inc.com\nAbstract\nTransfer learning between different language pairs has shown\nits effectiveness for Neural Machine Translation (NMT) in\nlow-resource scenario. However, existing transfer methods\ninvolving a common target language are far from success in\nthe extreme scenario of zero-shot translation, due to the lan-\nguage space mismatch problem between transferor (the par-\nent model) and transferee (the child model) on the source\nside. To address this challenge, we propose an effective trans-\nfer learning approach based on cross-lingual pre-training. Our\nkey idea is to make all source languages share the same fea-\nture space and thus enable a smooth transition for zero-shot\ntranslation. To this end, we introduce one monolingual pre-\ntraining method and two bilingual pre-training methods to\nobtain a universal encoder for different languages. Once the\nuniversal encoder is constructed, the parent model built on\nsuch encoder is trained with large-scale annotated data and\nthen directly applied in zero-shot translation scenario. Exper-\niments on two public datasets show that our approach signif-\nicantly outperforms strong pivot-based baseline and various\nmultilingual NMT approaches.\nIntroduction\nAlthough Neural Machine Translation (NMT) has domi-\nnated recent research on translation tasks (Wu et al. 2016;\nVaswani et al. 2017; Hassan et al. 2018), NMT heavily relies\non large-scale parallel data, resulting in poor performance\non low-resource or zero-resource language pairs (Koehn\nand Knowles 2017). Translation between these low-resource\nlanguages (e.g., Arabic →Spanish) is usually accomplished\nwith pivoting through a rich-resource language (such as En-\nglish), i.e., Arabic (source) sentence is translated to En-\nglish (pivot) ﬁrst which is later translated to Spanish (tar-\nget) (Kauers et al. 2002; de Gispert and Mariño 2006).\nHowever, the pivot-based method requires doubled decoding\ntime and suffers from the propagation of translation errors.\nOne common alternative to avoid pivoting in NMT is\ntransfer learning (Zoph et al. 2016; Nguyen and Chiang\n2017; Kocmi and Bojar 2018; Kim et al. 2019) which lever-\nages a high-resource pivot →target model ( parent ) to ini-\n∗Corresponding Author.\nCopyright c⃝2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The circle and triangle dots represent source sen-\ntences in different language l1andl2, and the square dots\nmeans target sentences in language l3. A sample of transla-\ntion pairs is connected by the dashed line. We would like to\nforce each of the translation pairs has the same latent rep-\nresentation as the right part of the ﬁgure so as to transfer\nl1→l3model directly to l2→l3model.\ntialize a low-resource source →target model ( child ) that is\nfurther optimized with a small amount of available paral-\nlel data. Although this approach has achieved success in\nsome low-resource language pairs, it still performs very\npoorly in extremely low-resource or zero-resource transla-\ntion scenario. Speciﬁcally, Kocmi and Bojar (2018) reports\nthat without any child model training data, the performance\nof the parent model on the child test set is miserable.\nIn this work, we argue that the language space mis-\nmatch problem, also named domain shift problem (Fu et al.\n2015), brings about the zero-shot translation failure in trans-\nfer learning. It is because transfer learning has no explicit\ntraining process to guarantee that the source and pivot', 'as shown in the right of Figure 1. One way to achieve this\ngoal is the ﬁne-tuning technique, which forces the model to\nforget the speciﬁc knowledge from parent data and learn new\nfeatures from child data. However, the domain shift problem\nstill exists, and the demand of parallel child data for ﬁne-\ntuning heavily hinders transfer learning for NMT towards\nthe zero-resource setting.\nIn this paper, we explore the transfer learning in\na common zero-shot scenario where there are a lot\nof source↔pivot and pivot↔target parallel data but no\nsource↔target parallel data. In this scenario, we propose\na simple but effective transfer approach, the key idea\nof which is to relieve the burden of the domain shift\nproblem by means of cross-lingual pre-training. To this\nend, we ﬁrstly investigate the performance of two exist-\ning cross-lingual pre-training methods proposed by Lam-\nple and Conneau (2019) in zero-shot translation scenario.\nBesides, a novel pre-training method called BRidge Lan-\nguage Modeling (BRLM) is designed to make full use of the\nsource↔pivot bilingual data to obtain a universal encoder\nfor different languages. Once the universal encoder is con-\nstructed, we only need to train the pivot →target model and\nthen test this model in source →target direction directly. The\nmain contributions of this paper are as follows:\n•We propose a new transfer learning approach for NMT\nwhich uses the cross-lingual language model pre-training\nto enable a high performance on zero-shot translation.\n•We propose a novel pre-training method called BRLM,\nwhich can effectively alleviates the distance between dif-\nferent source language spaces.\n•Our proposed approach signiﬁcantly improves zero-shot\ntranslation performance, consistently surpassing pivot-\ning and multilingual approaches. Meanwhile, the perfor-\nmance on supervised translation direction remains the\nsame level or even better when using our method.\nRelated Work\nIn recent years, zero-shot translation in NMT has attracted\nwidespread attention in academic research. Existing meth-\nods are mainly divided into four categories: pivot-based\nmethod, transfer learning, multilingual NMT, and unsuper-\nvised NMT.\n•Pivot-based Method is a common strategy to obtain a\nsource→target model by introducing a pivot language.\nThis approach is further divided into pivoting and pivot-\nsynthetic. While the former ﬁrstly translates a source lan-\nguage into the pivot language which is later translated\nto the target language (Kauers et al. 2002; de Gispert\nand Mariño 2006; Utiyama and Isahara 2007), the lat-\nter trains a source→target model with pseudo data gener-\nated from source-pivot or pivot-target parallel data (Chen\net al. 2017; Zheng, Cheng, and Liu 2017). Although the\npivot-based methods can achieve not bad performance, it\nalways falls into a computation-expensive and parameter-\nvast dilemma of quadratic growth in the number of source\nlanguages, and suffers from the error propagation prob-\nlem (Zhu et al. 2013).•Transfer Learning is ﬁrstly introduced for NMT by\nZoph et al. (2016), which leverages a high-resource par-\nent model to initialize the low-resource child model. On\nthis basis, Nguyen and Chiang (2017) and Kocmi and\nBojar (2018) use shared vocabularies for source/target\nlanguage to improve transfer learning, while Kim, Gao,\nand Ney (2019) relieve the vocabulary mismatch by\nmainly using cross-lingual word embedding. Although\nthese methods are successful in the low-resource scene,\nthey have limited effects in zero-shot translation.\n•Multilingual NMT (MNMT) enables training a single\nmodel that supports translation from multiple source lan-\nguages into multiple target languages, even those unseen\nlanguage pairs (Firat, Cho, and Bengio 2016; Firat et al.\n2016; Johnson et al. 2016; Al-Shedivat and Parikh 2019;\nAharoni, Johnson, and Firat 2019). Aside from sim-\npler deployment, MNMT beneﬁts from transfer learning\nwhere low-resource language pairs are trained together\nwith high-resource ones. However, Gu et al. (2019) point\nout that MNMT for zero-shot translation easily fails, and\nis sensitive to the hyper-parameter setting. Also']",The proposed approach in Cross-lingual Pre-training research to address language space mismatch and enable zero-shot translation is to use cross-lingual language model pre-training.,multi_context,TRUE
48,What is the role of improved relation detection in a KBQA system and how does it contribute to achieving state-of-the-art results in KBQA tasks?,"['cation .arXiv:1704.06194v2  [cs.CL]  27 May 2017', 'Question: what episode was mike kelleythe writer of\nKnowledgeBaseMike\tKelley(American\ttelevision\twriter/producer)Mike\tKelley(American\tbaseball\tplayer)…Entity Linking \nLove\tWill\tFind\ta\tWayUSA…First\tbaseman…episodes_written\nposition_playedRelation Detection(a)(b)Question: what tvshow did grantshowplay onin2008Mike\tKelley?episodes_writtenEntity Linking RelationDetectionGrantShow?starring_rolesseries(date)from2008ConstraintDetectionGrantShow\t(American\tactor)SwingTown\nBigLoveepisodesScoundrelsseries2011from20102008Figure 1: KBQA examples and its three key components. (a) A single relation example. We ﬁrst identify the topic entity with\nentity linking and then detect the relation asked by the question with relation detection (from all relations connecting the topic\nentity). Based on the detected entity and relation, we form a query to search the KB for the correct answer “ Love Will Find a\nWay”. (b) A more complex question containing two entities. By using “ Grant Show ” as the topic entity, we could detect a chain\nof relations “ starring roles-series ” pointing to the answer. An additional constraint detection takes the other entity “ 2008 ” as\na constraint, to ﬁlter the correct answer “ SwingTown ” from all candidates found by the topic entity and relation.\nthat original relation names can sometimes help\nto match longer question contexts, we propose\nto build both relation-level and word-level rela-\ntion representations. Third, we use deep bidirec-\ntional LSTMs ( BiLSTM s) to learn different levels\nof question representations in order to match the\ndifferent levels of relation information. Finally, we\npropose a residual learning method for sequence\nmatching, which makes the model training easier\nand results in more abstract (deeper) question rep-\nresentations, thus improves hierarchical matching.\nIn order to assess how the proposed improved\nrelation detection could beneﬁt the KBQA end\ntask, we also propose a simple KBQA implemen-\ntation composed of two-step relation detection .\nGiven an input question and a set of candidate enti-\nties retrieved by an entity linker based on the ques-\ntion, our proposed relation detection model plays a\nkey role in the KBQA process: (1) Re-ranking the\nentity candidates according to whether they con-\nnect to high conﬁdent relations detected from the\nraw question text by the relation detection model.\nThis step is important to deal with the ambigui-\nties normally present in entity linking results. (2)\nFinding the core relation (chains) for each topic\nentity2selection from a much smaller candidate\nentity set after re-ranking. The above steps are\nfollowed by an optional constraint detection step,\nwhen the question cannot be answered by single\nrelations (e.g., multiple entities in the question).\nFinally the highest scored query from the above\n2Following Yih et al. (2015), here topic entity refers to\nthe root of the (directed) query tree; and core-chain is the\ndirected path of relation from root to the answer node.steps is used to query the KB for answers.\nOur main contributions include: (i) An im-\nproved relation detection model by hierarchical\nmatching between questions and relations with\nresidual learning; (ii) We demonstrate that the im-\nproved relation detector enables our simple KBQA\nsystem to achieve state-of-the-art results on both\nsingle-relation and multi-relation KBQA tasks.\n2 Related Work\nRelation Extraction Relation extraction (RE) is\nan important sub-ﬁeld of information extraction.\nGeneral research in this ﬁeld usually works on a\n(small) pre-deﬁned relation set, where given a text\nparagraph and two target entities, the goal is to\ndetermine whether the text indicates any types of\nrelations between the entities or not. As a result\nRE is usually formulated as a classiﬁcation task .\nTraditional RE methods rely on large amount of\nhand-crafted features (Zhou et al., 2005; Rink and\nHarabagiu, 2010; Sun et al., 2011). Recent re-\nsearch beneﬁts a lot from the advancement of deep\nlearning: from word embeddings (Nguyen and Gr-\nishman, 2014; Gormley et al., 2015) to deep net-\nworks', ' like CNNs and LSTMs (Zeng et al., 2014;\ndos Santos et al., 2015; Vu et al., 2016) and atten-\ntion models (Zhou et al., 2016; Wang et al., 2016).\nThe above research assumes there is a ﬁxed\n(closed) set of relation types, thus no zero-shot\nlearning capability is required. The number\nof relations is usually not large: The widely\nused ACE2005 has 11/32 coarse/ﬁne-grained rela-\ntions; SemEval2010 Task8 has 19 relations; TAC-', 'Question: what episode was mike kelleythe writer of\nKnowledgeBaseMike\tKelley(American\ttelevision\twriter/producer)Mike\tKelley(American\tbaseball\tplayer)…Entity Linking \nLove\tWill\tFind\ta\tWayUSA…First\tbaseman…episodes_written\nposition_playedRelation Detection(a)(b)Question: what tvshow did grantshowplay onin2008Mike\tKelley?episodes_writtenEntity Linking RelationDetectionGrantShow?starring_rolesseries(date)from2008ConstraintDetectionGrantShow\t(American\tactor)SwingTown\nBigLoveepisodesScoundrelsseries2011from20102008Figure 1: KBQA examples and its three key components. (a) A single relation example. We ﬁrst identify the topic entity with\nentity linking and then detect the relation asked by the question with relation detection (from all relations connecting the topic\nentity). Based on the detected entity and relation, we form a query to search the KB for the correct answer “ Love Will Find a\nWay”. (b) A more complex question containing two entities. By using “ Grant Show ” as the topic entity, we could detect a chain\nof relations “ starring roles-series ” pointing to the answer. An additional constraint detection takes the other entity “ 2008 ” as\na constraint, to ﬁlter the correct answer “ SwingTown ” from all candidates found by the topic entity and relation.\nthat original relation names can sometimes help\nto match longer question contexts, we propose\nto build both relation-level and word-level rela-\ntion representations. Third, we use deep bidirec-\ntional LSTMs ( BiLSTM s) to learn different levels\nof question representations in order to match the\ndifferent levels of relation information. Finally, we\npropose a residual learning method for sequence\nmatching, which makes the model training easier\nand results in more abstract (deeper) question rep-\nresentations, thus improves hierarchical matching.\nIn order to assess how the proposed improved\nrelation detection could beneﬁt the KBQA end\ntask, we also propose a simple KBQA implemen-\ntation composed of two-step relation detection .\nGiven an input question and a set of candidate enti-\nties retrieved by an entity linker based on the ques-\ntion, our proposed relation detection model plays a\nkey role in the KBQA process: (1) Re-ranking the\nentity candidates according to whether they con-\nnect to high conﬁdent relations detected from the\nraw question text by the relation detection model.\nThis step is important to deal with the ambigui-\nties normally present in entity linking results. (2)\nFinding the core relation (chains) for each topic\nentity2selection from a much smaller candidate\nentity set after re-ranking. The above steps are\nfollowed by an optional constraint detection step,\nwhen the question cannot be answered by single\nrelations (e.g., multiple entities in the question).\nFinally the highest scored query from the above\n2Following Yih et al. (2015), here topic entity refers to\nthe root of the (directed) query tree; and core-chain is the\ndirected path of relation from root to the answer node.steps is used to query the KB for answers.\nOur main contributions include: (i) An im-\nproved relation detection model by hierarchical\nmatching between questions and relations with\nresidual learning; (ii) We demonstrate that the im-\nproved relation detector enables our simple KBQA\nsystem to achieve state-of-the-art results on both\nsingle-relation and multi-relation KBQA tasks.\n2 Related Work\nRelation Extraction Relation extraction (RE) is\nan important sub-ﬁeld of information extraction.\nGeneral research in this ﬁeld usually works on a\n(small) pre-deﬁned relation set, where given a text\nparagraph and two target entities, the goal is to\ndetermine whether the text indicates any types of\nrelations between the entities or not. As a result\nRE is usually formulated as a classiﬁcation task .\nTraditional RE methods rely on large amount of\nhand-crafted features (Zhou et al., 2005; Rink and\nHarabagiu, 2010; Sun et al., 2011). Recent re-\nsearch beneﬁts a lot from the advancement of deep\nlearning: from word embeddings (Nguyen and Gr-\nishman, 2014; Gormley et al., 2015) to deep net-\nworks']","The improved relation detection model in a KBQA system plays a key role in re-ranking entity candidates based on high confident relations detected from the raw question text. This helps to deal with ambiguities in entity linking results. Additionally, the improved relation detector enables the KBQA system to achieve state-of-the-art results in both single-relation and multi-relation KBQA tasks.",multi_context,TRUE
49,What were the evaluated systems in Experiment A using BERT in the NUB ES-PHI corpus and how did they compare to other systems?,"['core server with operat-\ning system Ubuntu 16.04, 250GB of RAM memory, and 4\nGeForce RTX 2080 GPUs with 11GB of memory. The max-\nimum sequence length was set at 500 and the batch size at\n12. In this setting, each epoch –a full pass through all the\ntraining data– required about 10 minutes to complete.\n3.3. Experimental design\nWe have conducted experiments with BERT in the two\ndatasets of Spanish clinical narrative presented in Section\n3.1. The ﬁrst experiment set uses NUB ES-PHI, a corpus\nof real medical reports manually annotated with sensitive\ninformation. Because this corpus is not publicly available,\nand in order to compare the BERT-based model to other re-\nlated published systems, the second set of experiments uses\nthe MEDDOCAN 2019 shared task competition dataset.\nThe following sections provide greater detail about the two\nexperimental setups.\n3.3.1. Experiment A: NUB ES-PHI\nIn this experiment set, we evaluate all the systems presented\nin Section 3.2., namely, the rule-based baseline, the CRF', '0 50 1000.40.60.81\ntrain data (%)precision\n0 50 1000.40.60.81\ntrain data (%)recall\n0 50 1000.40.60.81\ntrain data (%)F1-score\nCRF spaCy BERT\nFigure 2: Performance decay with decreasing amounts of training data on the sensitive information detection task in the\nNUB ES-PHI corpus\nDetection Classiﬁcation\nPrec Rec F1 Prec Rec F1\nCRF (Perez et al., 2019) 0.977 0.943 0.960 0.971 0.937 0.954\nspaCy (Perez et al., 2019) 0.967 0.953 0.965 0.965 0.947 0.956\nNLND S2 (Lange et al., 2019) 0.976 0.973 0.974 0.971 0.968 0.970\nNLND S3 (Lange et al., 2019) 0.975 0.975 0.975 0.970 0.969 0.970\nBERT + CRF (Mao and Liu, 2019) 0.968 0.919 0.943 0.965 0.912 0.937\nBERT 0.973 0.972 0.972 0.968 0.967 0.967\nTable 8: Results of Experiment B: MEDDOCAN\na larger extent from recall decline, which is not that marked\nin the case of BERT. Overall, these results indicate that the\ntransfer-learning achieved through the BERT multilingual\npre-trained model not only helps obtain better results, but\nalso lowers the need of manually labelled data for this ap-\nplication domain.\n4.2. Experiment B: MEDDOCAN\nThe results of the two MEDDOCAN scenarios –detection\nand classiﬁcation– are shown in Table 8. These results fol-\nlow the same pattern as in the previous experiments, with\nthe CRF classiﬁer being the most precise of all, and BERT\noutperforming both the CRF and spaCy classiﬁers thanks to\nits greater recall. We also show the results of Mao and Liu\n(2019) who, despite of having used a BERT-based system,\nachieve lower scores than our models. The reason why it\nshould be so remain unclear.\nWith regard to the winner of the MEDDOCAN shared\ntask, the BERT-based model has not improved the scores\nobtained by neither the domain-dependent (S3) nor the\ndomain-independent (S2) NLNDE model. However, at-\ntending to the obtained results, BERT remains only 0.3 F1-\nscore points behind, and would have achieved the second\nposition among all the MEDDOCAN shared task competi-\ntors. Taking into account that only 3% of the gold labels\nremain incorrectly annotated, the task can be considered al-\nmost solved, and it is not clear if the differences among the\nsystems are actually signiﬁcant, or whether they stem from\nminor variations in initialisation or a long-tail of minor la-belling inconsistencies.\n5. Conclusions and Future Work\nIn this work we have brieﬂy introduced the problems re-\nlated to data privacy protection in clinical domain. We have\nalso described some of the groundbreaking advances on the\nNatural Language Processing ﬁeld due to the appearance of\nTransformers-based deep-learning architectures and trans-\nfer learning from very large general-domain multilingual\ncorpora, focusing our attention in one of its most represen-\ntative examples, Google’s BERT model.\nIn order to assess the performance of BERT for Span-\nish clinical data anonymisation, we have conducted sev-\neral experiments with a BERT-based sequence labelling\napproach using the pre-trained multilingual BERT model\nshared by Google as the starting point for the model train-\ning. We have compared this BERT-based sequence la-\nbelling against other methods and systems. One of the ex-\nperiments uses the MEDDOCAN 2019 shared task dataset,\nwhile the other uses a novel Spanish clinical reports dataset\ncalled NUB ES-PHI.\nThe results of the experiments show that, in NUB ES-PHI,\nthe BERT-based model outperforms the other systems with-\nout requiring any adaptation or domain-speciﬁc feature en-\ngineering, just by being trained on the provided labelled\ndata. Interestingly, the BERT-based model obtains a re-\n']",nan,multi_context,TRUE
50,"""How does adversarial learning in worker adversarial settings address large divergences between training and test examples, and what modifications are made to the original adversarial neural network in this context?""","['-\nlikelihood objective as the loss function. We apply softmax\nover all candidate output label sequences, thus the probabil-\nity of the crowd-annotated label sequence is computed by:\np(¯ y|X) =exp(\nscore (X,¯ y))\n∑\ny∈YXexp(\nscore (X,y)), (3)\nwhere ¯ yis the crowd-annotated label sequences and YXis\nall candidate label sequence of input X.\nBased on the above formula, the loss function of our base-\nline model is:\nloss(Θ,X,¯ y) =−logp(¯ y|X), (4)\nwhere Θis the set of all model parameters. We use standard\nback-propagation method to minimize the loss function of\nthe baseline CRF model.', 'Worker Adversarial\nAdversarial learning has been an effective mechanism to re-\nsolve the problem of the input features between the training\nand test examples having large divergences (Goodfellow et\nal. 2014; Ganin et al. 2016). It has been successfully applied\non domain adaption (Gui et al. 2017), cross-lingual learn-\ning (Chen et al. 2016) and multi-task learning (Liu, Qiu, and\nHuang 2017). All settings involve feature shifting between\nthe training and testing.\nIn this paper, our setting is different. We are using the\nannotations from non-experts, which are noise and can in-\nﬂuence the ﬁnal performances if they are not properly pro-\ncessed. Directly learning based on the resulting corpus may\nadapt the neural feature extraction into the biased annota-\ntions. In this work, we assume that individual workers have\ntheir own guidelines in mind after short training. For exam-\nple, a perfect worker can annotate highly consistently with\nan expert, while common crowdsourcing workers may be\nconfused and have different understandings on certain con-\ntexts. Based on the assumption, we make an adaption for the\noriginal adversarial neural network to our setting.\nOur adaption is very simple. Brieﬂy speaking, the original\nadversarial learning adds an additional discriminator to clas-\nsify the type of source inputs, for example, the domain cate-\ngory in the domain adaption setting, while we add a discrim-\ninator to classify the annotation workers. Solely the features\nfrom the input sentence is not enough for worker classiﬁ-\ncation. The annotation result of the worker is also required.\nThus the inputs of our discriminator are different. Here we\nexploit both the source sentences and the crowd-annotated\nNE labels as basic inputs for the worker discrimination.\nIn the following, we describe the proposed adversarial\nlearning module, including both the submodels and the train-\ning method. As shown by the left part of Figure 1, the\nsubmodel consists of four parts: (1) a common Bi-LSTM\nover input characters; (2) an additional Bi-LSTM to en-\ncode crowd-annotated NE label sequence; (3) a convolu-\ntional neural network (CNN) to extract features for worker\ndiscriminator; (4) output and prediction.\nCommon Bi-LSTM over Characters\nTo build the adversarial part, ﬁrst we create a new bi-\ndirectional LSTM, named by the common Bi-LSTM:\nhcommon\n1hcommon\n2···hcommon\nn =Bi-LSTM (x1x2···xn).(5)\nAs shown in Figure 1, this Bi-LSTM is constructed over\nthe same input character representations of the private Bi-\nLSTM, in order to extract worker independent features.\nThe resulting features of the common Bi-LSTM are used\nfor both NER and the worker discriminator, different with\nthe features of private Bi-LSTM which are used for NER\nonly. As shown in Figure 1, we concatenate the outputs of\nthe common and private Bi-LSTMs together, and then feed\nthe results into the feed-forward combination layer of the\nNER part. Thus Formula 1 can be rewritten as:\nhner\nt=W(hcommon\nt⊕hprivate\nt) +b, (6)\nwhere Wis wider than the original combination because the\nnewly-added hcommon\nt .Noticeably, although the resulting common features are\nused for the worker discriminator, they actually have no ca-\npability to distinguish the workers. Because this part is ex-\nploited to maximize the loss of the worker discriminator, it\nwill be interpreted in the later training subsection. These fea-\ntures are invariant among different workers, thus they can\nhave less noises for NER. This is the goal of adversarial\nlearning, and we hope the NER being able to ﬁnd useful\nfeatures from these worker independent features.\nAdditional Bi-LSTM over Annotated NER Labels\nIn order to incorporate the annotated NE labels to predict the\nexact worker, we build another bi-directional LSTM (named\nby label Bi-LSTM) based on the crowd-annotated NE label\nsequence. This Bi-LSTM is used for worker discriminator\nonly. During the decoding of the testing phase, we will never\nhave this Bi-', '-\nlikelihood objective as the loss function. We apply softmax\nover all candidate output label sequences, thus the probabil-\nity of the crowd-annotated label sequence is computed by:\np(¯ y|X) =exp(\nscore (X,¯ y))\n∑\ny∈YXexp(\nscore (X,y)), (3)\nwhere ¯ yis the crowd-annotated label sequences and YXis\nall candidate label sequence of input X.\nBased on the above formula, the loss function of our base-\nline model is:\nloss(Θ,X,¯ y) =−logp(¯ y|X), (4)\nwhere Θis the set of all model parameters. We use standard\nback-propagation method to minimize the loss function of\nthe baseline CRF model.']",Adversarial learning in worker adversarial settings addresses large divergences between training and test examples by adding a discriminator to classify the annotation workers. Modifications made to the original adversarial neural network in this context include using both the source sentences and the crowd-annotated NE labels as inputs for the worker discriminator.,multi_context,TRUE
51,"""What were the results of the experiments comparing BERT-based models with other systems in the NUB ES-PHI and MEDDOCAN datasets?""","[' The winners of the challenge –the\nNeither-Language-nor-Domain-Experts (NLNDE) (Lange\net al., 2019)– achieved F1-scores as high as 0.975 in the\ntask of sensitive information detection and categorisation\nby using recurrent neural networks with Conditional Ran-\ndom Field (CRF) output layers.\nAt the same challenge, Mao and Liu (2019) occupied the\n8thposition among 18 participants using BERT. According\nto the description of the system, the authors used BERT-\nBase Multilingual Cased and an output CRF layer. How-\never, their system is ∼3 F1-score points below our imple-\nmentation without the CRF layer.\n3. Materials and Methods\nThe aim of this paper is to evaluate BERT’s multilingual\nmodel and compare it to other established machine-learning\nalgorithms in a speciﬁc task: sensitive data detection and\nclassiﬁcation in Spanish clinical free text. This section de-\nscribes the data involved in the experiments and the systems\nevaluated. Finally, we introduce the experimental setup.\n3.1. Data\nTwo datasets are exploited in this article. Both datasets\nconsist of plain text containing clinical narrative written in\nSpanish, and their respective manual annotations of sensi-\ntive information in BRAT (Stenetorp et al., 2012) standoff\nformat2. In order to feed the data to the different algorithms\npresented in Section 3.2., these datasets were transformed\nto comply with the commonly used BIO sequence repre-\nsentation scheme (Ramshaw and Marcus, 1999).\n3.1.1. NUB ES-PHI\nNUB ES(Lima et al., 2019) is a corpus of around 7,000 real\nmedical reports written in Spanish and annotated with nega-\ntion and uncertainty information. Before being published,\nsensitive information had to be manually annotated and re-\nplaced for the corpus to be safely shared. In this article,\n1http://temu.bsc.es/meddocan/\n2https://brat.nlplab.org/standoff.html', 'we work with the NUB ESversion prior to its anonymisa-\ntion, that is, with the manual annotations of sensitive in-\nformation. It follows that the version we work with is not\npublicly available and, due to contractual restrictions, we\ncannot reveal the provenance of the data. In order to avoid\nconfusion between the two corpus versions, we henceforth\nrefer to the version relevant in this paper as NUB ES-PHI\n(from ‘NUB ESwith Personal Health Information’).\nNUB ES-PHI consists of 32,055 sentences annotated for 11\ndifferent sensitive information categories. Overall, it con-\ntains 7,818 annotations. The corpus has been randomly\nsplit into train (72%), development (8%) and test (20%) sets\nto conduct the experiments described in this paper. The size\nof each split and the distribution of the annotations can be\nconsulted in Tables 2 and 3, respectively.\ntrain dev test\n# sentences 23,079 2,565 6,411\n# tokens 379,401 41,936 107,024\nvocabulary 25,304 7,483 12,750\n# annotations 5,562 677 1,579\nTable 2: Size of the NUB ES-PHI corpus\nThe majority of sensitive information in NUB ES-PHI are\ntemporal expressions (‘Date’ and ‘Time’), followed by\nhealthcare facility mentions (‘Hospital’), and the age of the\npatient. Mentions of people are not that frequent, with\nphysician names (‘Doctor’) occurring much more often\nthan patient names (‘Patient’). The least frequent sensitive\ninformation types, which account for ∼10% of the remain-\ning annotations, consist of the patient’s sex, job, and kin-\nship, and locations other than healthcare facilities (‘Loca-\ntion’). Finally, the tag ‘Other’ includes, for instance, men-\ntions to institutions unrelated to healthcare and whether the\npatient is right- or left-handed. It occurs just 36 times.\ntrain dev test\n# % # % # %\nDate 2,165 39 251 37 660 41\nHospital 1,012 18 105 16 275 17\nAge 701 13 133 20 200 13\nTime 608 11 63 9 155 10\nDoctor 486 9 44 6 134 8\nSex 270 5 35 5 71 4\nKinship 158 3 20 3 44 3\nLocation 71 1 10 1 19 1\nPatient 48 1 5 1 11 1\nJob 31 1 3 0 9 1\nOther 12 0 8 1 16 1\nTotal 5,562 100 677 100 1,579 100\nTable 3: Label distribution in the NUB ES-PHI corpus\n3.1.2. The MEDDOCAN corpus\nThe organisers of the MEDDOCAN shared task (Marimon\net al., 2019) curated a synthetic corpus of clinical cases en-\nriched with sensitive information by health documentalists.In this regard, the MEDDOCAN evaluation scenario could\nbe said to be somewhat far from the real use case the tech-\nnology developed for the shared task is supposed to be ap-\nplied in. However, at the moment it also provides the only\npublic means for a rigorous comparison between systems\nfor sensitive health information detection in Spanish texts.\nThe size of the MEDDOCAN corpus is shown in Table 4.\nCompared to NUB ES-PHI (Table 2), this corpus contains\nmore sensitive information annotations, both in absolute\nand relative terms.\ntrain dev test\n# documents 500 250 250\n# tokens 360,407 138,812 132,961\nvocabulary 26,355 15,985 15,397\n# annotations 11,333 5,801 5,661\nTable 4: Size of the MEDDOCAN corpus\nThe sensitive annotation categories considered in MED-\nDOCAN differ in part from those in NUB ES-PHI. Most\nnotably, it contains ﬁner-grained labels for location-related\nmentions –namely, ‘Address’, ‘Territory’, and ‘Country’–,\nand other sensitive information categories that we did not\nencounter in NUB ES-PHI (e.g., identiﬁers, phone num-\nbers, e-mail addresses, etc.). In total, the MEDDOCAN\ncorpus has 21 sensitive information categories. We refer\nthe reader to the organisers’ article (Marimon et al., 2019)\nfor more detailed information about this corpus.\n3.2. Systems\nApart from experimenting with a pre-trained BERT model,\nwe have run experiments with other systems and base-\nlines, to', ' compare them and obtain a better perspective about\nBERT’s performance in these datasets.\n3.2.1. Baseline\nAs the simplest baseline, a sensitive data recogniser and\nclassiﬁer has been developed that consists of regular-\nexpressions and dictionary look-ups. For each category to\ndetect a speciﬁc method has been implemented. For in-\nstance, the Date, Age, Time and Doctor detectors are based\non regular-expressions; Hospital, Sex, Kinship, Location,\nPatient and Job are looked up in dictionaries. The dic-\ntionaries are hand-crafted from the training data available,\nexcept for the Patient’s case, for which the possible can-\ndidates considered are the 100 most common female and\nmale names in Spain according to the Instituto Nacional de\nEstad ´ıstica (INE; Spanish Statistical Ofﬁce ).\n3.2.2. CRF\nConditional Random Fields (CRF) (Lafferty et al., 2001)\nhave been extensively used for tasks of sequential nature. In\nthis paper, we propose as one of the competitive baselines\na CRF classiﬁer trained with sklearn-crfsuite3for Python\n3.5 and the following conﬁguration: algorithm = lbfgs ;\nmaximum iterations = 100; c1 = c2 = 0.1; all transitions\n=true ; optimise = false . The features extracted from\neach token are as follows:\n3https://sklearn-crfsuite.readthedocs.io', '– preﬁxes and sufﬁxes of 2 and 3 characters;\n– the length of the token in characters and the length of\nthe sentence in tokens;\n– whether the token is all-letters, a number, or a se-\nquence of punctuation marks;\n– whether the token contains the character ‘@’;\n– whether the token is the start or end of the sentence;\n– the token’s casing and the ratio of uppercase charac-\nters, digits, and punctuation marks to its length;\n– and, the lemma, part-of-speech tag, and named-entity\ntag given by ixa-pipes4(Agerri et al., 2014) upon\nanalysing the sentence the token belongs to.\nNoticeably, none of the features used to train the CRF clas-\nsiﬁer is domain-dependent. However, the latter group of\nfeatures is language dependent.\n3.2.3. spaCy\nspaCy5is a widely used NLP library that implements state-\nof-the-art text processing pipelines, including a sequence-\nlabelling pipeline similar to the one described by Strubell\net al. (2017). spaCy offers several pre-trained models in\nSpanish, which perform basic NLP tasks such as Named\nEntity Recognition (NER). In this paper, we have trained a\nnew NER model to detect NUB ES-PHI labels. For this\npurpose, the new model uses all the labels of the train-\ning corpus coded with its context at sentence level. The\nnetwork optimisation parameters and dropout values are\nthe ones recommended in the documentation for small\ndatasets6. Finally, the model is trained using batches of\nsize 64. No more features are included, so the classiﬁer is\nlanguage-dependent but not domain-dependent.\n3.2.4. BERT\nAs introduced earlier, BERT has shown an outstanding\nperformance in NERC-like tasks, improving the start-of-\nthe-art results for almost every dataset and language. We\ntake the same approach here, by using the model BERT-\nBase Multilingual Cased7with a Fully Connected (FC)\nlayer on top to perform a ﬁne-tuning of the whole model\nfor an anonymisation task in Spanish clinical data. Our\nimplementation is built on PyTorch8and the PyTorch-\nTransformers library9(Wolf et al., 2019). The training\nphase consists in the following steps (roughly depicted in\nFigure 1):\n1.Pre-processing: since we are relying on a pre-trained\nBERT model, we must match the same conﬁguration\nby using a speciﬁc tokenisation and vocabulary. BERT\nalso needs that the inputs contains special tokens to\nsignal the beginning and the end of each sequence.\n2.Fine-tuning: the pre-processed sequence is fed into\nthe model. BERT outputs the contextual embeddings\nthat encode each of the inputted tokens. This embed-\nding representation for each token is fed into the FC\n4https://ixa2.si.ehu.es/ixa-pipes\n5https://spacy.io\n6https://spacy.io/usage/training\n7https://github.com/google-research/bert\n8https://pytorch.org\n9https://github.com/huggingface/transformers\nFigure 1: Pre-trained BERT with a Fully Connected layer\non top to perform the ﬁne-tuning\nlinear layer after a dropout layer (with a 0.1 dropout\nprobability), which in turn outputs the logits for each\npossible class. The cross-entropy loss function is cal-\nculated comparing the logits and the gold labels, and\nthe error is back-propagated to adjust the model pa-\nrameters.\nWe have trained the model using an AdamW optimiser\n(Loshchilov and Hutter, 2019) with the learning rate set to\n3e-5, as recommended by Devlin et al. (2019), and with\na gradient clipping of 1.0. We also applied a learning-rate\nscheduler that warms up the learning rate from zero to its\nmaximum value as the training progresses, which is also a\ncommon practice. For each experiment set proposed below,\nthe training was run with an early-stopping patience of 15\nepochs. Then, the model that performed best against the\ndevelopment set was used to produce the reported results.\nThe experiments were run on a 64-', 'core server with operat-\ning system Ubuntu 16.04, 250GB of RAM memory, and 4\nGeForce RTX 2080 GPUs with 11GB of memory. The max-\nimum sequence length was set at 500 and the batch size at\n12. In this setting, each epoch –a full pass through all the\ntraining data– required about 10 minutes to complete.\n3.3. Experimental design\nWe have conducted experiments with BERT in the two\ndatasets of Spanish clinical narrative presented in Section\n3.1. The ﬁrst experiment set uses NUB ES-PHI, a corpus\nof real medical reports manually annotated with sensitive\ninformation. Because this corpus is not publicly available,\nand in order to compare the BERT-based model to other re-\nlated published systems, the second set of experiments uses\nthe MEDDOCAN 2019 shared task competition dataset.\nThe following sections provide greater detail about the two\nexperimental setups.\n3.3.1. Experiment A: NUB ES-PHI\nIn this experiment set, we evaluate all the systems presented\nin Section 3.2., namely, the rule-based baseline, the CRF', '0 50 1000.40.60.81\ntrain data (%)precision\n0 50 1000.40.60.81\ntrain data (%)recall\n0 50 1000.40.60.81\ntrain data (%)F1-score\nCRF spaCy BERT\nFigure 2: Performance decay with decreasing amounts of training data on the sensitive information detection task in the\nNUB ES-PHI corpus\nDetection Classiﬁcation\nPrec Rec F1 Prec Rec F1\nCRF (Perez et al., 2019) 0.977 0.943 0.960 0.971 0.937 0.954\nspaCy (Perez et al., 2019) 0.967 0.953 0.965 0.965 0.947 0.956\nNLND S2 (Lange et al., 2019) 0.976 0.973 0.974 0.971 0.968 0.970\nNLND S3 (Lange et al., 2019) 0.975 0.975 0.975 0.970 0.969 0.970\nBERT + CRF (Mao and Liu, 2019) 0.968 0.919 0.943 0.965 0.912 0.937\nBERT 0.973 0.972 0.972 0.968 0.967 0.967\nTable 8: Results of Experiment B: MEDDOCAN\na larger extent from recall decline, which is not that marked\nin the case of BERT. Overall, these results indicate that the\ntransfer-learning achieved through the BERT multilingual\npre-trained model not only helps obtain better results, but\nalso lowers the need of manually labelled data for this ap-\nplication domain.\n4.2. Experiment B: MEDDOCAN\nThe results of the two MEDDOCAN scenarios –detection\nand classiﬁcation– are shown in Table 8. These results fol-\nlow the same pattern as in the previous experiments, with\nthe CRF classiﬁer being the most precise of all, and BERT\noutperforming both the CRF and spaCy classiﬁers thanks to\nits greater recall. We also show the results of Mao and Liu\n(2019) who, despite of having used a BERT-based system,\nachieve lower scores than our models. The reason why it\nshould be so remain unclear.\nWith regard to the winner of the MEDDOCAN shared\ntask, the BERT-based model has not improved the scores\nobtained by neither the domain-dependent (S3) nor the\ndomain-independent (S2) NLNDE model. However, at-\ntending to the obtained results, BERT remains only 0.3 F1-\nscore points behind, and would have achieved the second\nposition among all the MEDDOCAN shared task competi-\ntors. Taking into account that only 3% of the gold labels\nremain incorrectly annotated, the task can be considered al-\nmost solved, and it is not clear if the differences among the\nsystems are actually signiﬁcant, or whether they stem from\nminor variations in initialisation or a long-tail of minor la-belling inconsistencies.\n5. Conclusions and Future Work\nIn this work we have brieﬂy introduced the problems re-\nlated to data privacy protection in clinical domain. We have\nalso described some of the groundbreaking advances on the\nNatural Language Processing ﬁeld due to the appearance of\nTransformers-based deep-learning architectures and trans-\nfer learning from very large general-domain multilingual\ncorpora, focusing our attention in one of its most represen-\ntative examples, Google’s BERT model.\nIn order to assess the performance of BERT for Span-\nish clinical data anonymisation, we have conducted sev-\neral experiments with a BERT-based sequence labelling\napproach using the pre-trained multilingual BERT model\nshared by Google as the starting point for the model train-\ning. We have compared this BERT-based sequence la-\nbelling against other methods and systems. One of the ex-\nperiments uses the MEDDOCAN 2019 shared task dataset,\nwhile the other uses a novel Spanish clinical reports dataset\ncalled NUB ES-PHI.\nThe results of the experiments show that, in NUB ES-PHI,\nthe BERT-based model outperforms the other systems with-\nout requiring any adaptation or domain-speciﬁc feature en-\ngineering, just by being trained on the provided labelled\ndata. Interestingly, the BERT-based model obtains a re-\n']","The BERT-based model outperformed other systems in the NUB ES-PHI dataset and achieved comparable results in the MEDDOCAN dataset. In the NUB ES-PHI dataset, the BERT-based model achieved an F1-score of 0.972, while the CRF and spaCy models achieved F1-scores of 0.960 and 0.965, respectively. In the MEDDOCAN dataset, the BERT-based model achieved an F1-score of 0.967 for detection and 0.965 for classification, outperforming the CRF and spaCy models. However, the BERT-based model did not surpass the scores achieved by the NLNDE models in the MEDDOCAN dataset.",multi_context,TRUE
52,"How does context-specificity differ in ELMo, BERT, and GPT-2 and how does it relate to word similarity in the same sentence?","['Figure 3: The intra-sentence similarity is the average cosine similarity between each word representation in a\nsentence and their mean (see Deﬁnition 2). Above, we plot the average intra-sentence similarity of uniformly\nrandomly sampled sentences, adjusted for anisotropy. This statistic reﬂects how context-speciﬁcity manifests in\nthe representation space, and as seen above, it manifests very differently for ELMo, BERT, and GPT-2.\naverage intra-sentence similarity is above 0.20 for\nall but one layer.\nAs noted earlier when discussing BERT, this be-\nhavior still makes intuitive sense: two words in the\nsame sentence do not necessarily have a similar\nmeaning simply because they share the same con-\ntext. The success of GPT-2 suggests that unlike\nanisotropy, which accompanies context-speciﬁcity\nin all three models, a high intra-sentence similar-\nity is not inherent to contextualization. Words in\nthe same sentence can have highly contextualized\nrepresentations without those representations be-\ning any more similar to each other than two ran-\ndom word representations. It is unclear, however,\nwhether these differences in intra-sentence simi-\nlarity can be traced back to differences in model\narchitecture; we leave this question as future work.\n4.3 Static vs. Contextualized\nOn average, less than 5% of the variance in\na word’s contextualized representations can be\nexplained by a static embedding. Recall from\nDeﬁnition 3 that the maximum explainable vari-\nance (MEV) of a word, for a given layer of a given\nmodel, is the proportion of variance in its con-\ntextualized representations that can be explained\nby their ﬁrst principal component. This gives us\nan upper bound on how well a static embedding\ncould replace a word’s contextualized representa-\ntions. Because contextualized representations are\nanisotropic (see section 4.1), much of the varia-\ntion across all words can be explained by a sin-gle vector. We adjust for anisotropy by calculating\nthe proportion of variance explained by the ﬁrst\nprincipal component of uniformly randomly sam-\npled word representations and subtracting this pro-\nportion from the raw MEV . In Figure 4, we plot\nthe average anisotropy-adjusted MEV across uni-\nformly randomly sampled words.\nIn no layer of ELMo, BERT, or GPT-2 can more\nthan 5% of the variance in a word’s contextual-\nized representations be explained by a static em-\nbedding, on average. Though not visible in Figure\n4, the raw MEV of many words is actually below\nthe anisotropy baseline: i.e., a greater proportion\nof the variance across all words can be explained\nby a single vector than can the variance across\nall representations of a single word. Note that\nthe 5% threshold represents the best-case scenario,\nand there is no theoretical guarantee that a word\nvector obtained using GloVe, for example, would\nbe similar to the static embedding that maximizes\nMEV . This suggests that contextualizing models\nare not simply assigning one of a ﬁnite number of\nword-sense representations to each word – other-\nwise, the proportion of variance explained would\nbe much higher. Even the average raw MEV is be-\nlow 5% for all layers of ELMo and BERT; only\nfor GPT-2 is the raw MEV non-negligible, being\naround 30% on average for layers 2 to 11 due to\nextremely high anisotropy.\nPrincipal components of contextualized repre-\nsentations in lower layers outperform GloVe\nand FastText on many benchmarks. As noted', 'Figure 2: The average cosine similarity between representations of the same word in different contexts is called\nthe word’s self-similarity (see Deﬁnition 1). Above, we plot the average self-similarity of uniformly randomly\nsampled words after adjusting for anisotropy (see section 3.4). In all three models, the higher the layer, the lower\nthe self-similarity, suggesting that contextualized word representations are more context-speciﬁc in higher layers.\nof contexts a word appears in, rather than its inher-\nent polysemy, is what drives variation in its con-\ntextualized representations. This answers one of\nthe questions we posed in the introduction: ELMo,\nBERT, and GPT-2 are not simply assigning one of\na ﬁnite number of word-sense representations to\neach word; otherwise, there would not be so much\nvariation in the representations of words with so\nfew word senses.\nContext-speciﬁcity manifests very differently in\nELMo, BERT, and GPT-2. As noted earlier,\ncontextualized representations are more context-\nspeciﬁc in upper layers of ELMo, BERT, and GPT-\n2. However, how does this increased context-\nspeciﬁcity manifest in the vector space? Do word\nrepresentations in the same sentence converge to a\nsingle point, or do they remain distinct from one\nanother while still being distinct from their repre-\nsentations in other contexts? To answer this ques-\ntion, we can measure a sentence’s intra-sentence\nsimilarity. Recall from Deﬁnition 2 that the intra-\nsentence similarity of a sentence, in a given layer\nof a given model, is the average cosine similarity\nbetween each of its word representations and their\nmean, adjusted for anisotropy. In Figure 3, we plot\nthe average intra-sentence similarity of 500 uni-\nformly randomly sampled sentences.\nIn ELMo, words in the same sentence are more\nsimilar to one another in upper layers. As\nword representations in a sentence become more\ncontext-speciﬁc in upper layers, the intra-sentencesimilarity also rises. This suggests that, in prac-\ntice, ELMo ends up extending the intuition behind\nFirth’s (1957) distributional hypothesis to the sen-\ntence level: that because words in the same sen-\ntence share the same context, their contextualized\nrepresentations should also be similar.\nIn BERT, words in the same sentence are more\ndissimilar to one another in upper layers. As\nword representations in a sentence become more\ncontext-speciﬁc in upper layers, they drift away\nfrom one another, although there are exceptions\n(see layer 12 in Figure 3). However, in all lay-\ners, the average similarity between words in the\nsame sentence is still greater than the average sim-\nilarity between randomly chosen words (i.e., the\nanisotropy baseline). This suggests a more nu-\nanced contextualization than in ELMo, with BERT\nrecognizing that although the surrounding sen-\ntence informs a word’s meaning, two words in the\nsame sentence do not necessarily have a similar\nmeaning because they share the same context.\nIn GPT-2, word representations in the same\nsentence are no more similar to each other than\nrandomly sampled words. On average, the un-\nadjusted intra-sentence similarity is roughly the\nsame as the anisotropic baseline, so as seen in Fig-\nure 3, the anisotropy-adjusted intra-sentence simi-\nlarity is close to 0 in most layers of GPT-2. In fact,\nthe intra-sentence similarity is highest in the input\nlayer, which does not contextualize words at all.\nThis is in contrast to ELMo and BERT, where the']","Context-specificity differs in ELMo, BERT, and GPT-2. In ELMo, words in the same sentence are more similar to one another in upper layers. In BERT, words in the same sentence are more dissimilar to one another in upper layers. In GPT-2, word representations in the same sentence are no more similar to each other than randomly sampled words.",multi_context,TRUE
53,"""What are the phases and the role of BRLM in the proposed cross-lingual pretraining approach for source→target zero-shot translation?""","['Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-\nSA) according to the way of generating the alignment infor-\nmation. These two structures actually extend MLM into a\nbilingual scenario, with the difference that BRLM leverages\nexternal aligner tool or additional attention layer to explic-\nitly introduce alignment information during model training.\n•Hard Alignment (BRLM-HA). We ﬁrst use exter-\nnal aligner tool on source ↔pivot parallel data to ex-\ntract the alignment information of sentence pair. Dur-\ning model training, given source ↔pivot sentence pair,\nBRLM-HA randomly masks some words in source sen-\ntence and leverages alignment information to obtain the\naligned words in pivot sentence for masked words. Based\non the processed input, BRLM-HA adopts the Trans-\nformer (Vaswani et al. 2017) encoder to gain the hid-\nden states for source and pivot sentences respectively.\nThen the training objective of BRLM-HA is to predict\nthe masked words by not only the surrounding words\nin source sentence but also the encoder outputs of the\naligned words. Note that this training process is also car-\nried out in a symmetric situation, in which we mask some\nwords in pivot sentence and obtain the aligned words in\nthe source sentence.\n•Soft Alignment (BRLM-SA). Instead of using external\naligner tool, BRLM-SA introduces an additional atten-\ntion layer to learn the alignment information together with\nmodel training. In this way, BRLM-SA avoids the effect\ncaused by external wrong alignment information and en-\nables many-to-one soft alignment during model training.\nSimilar with BRLM-HA, the training objective of BRLM-\nSA is to predict the masked words by not only the sur-\nrounding words in source sentence but also the outputs of\nattention layer. In our implementation, the attention layer\nis a multi-head attention layer adopted in Transformer,\nwhere the queries come from the masked source sentence,\nthe keys and values come from the pivot sentence.\nIn principle, MLM and TLM can learn some implicit align-\nment information during model training. However, the align-\nment process in MLM is inefﬁcient since the shared word\npieces only account for a small proportion of the whole cor-\npus, resulting in the difﬁculty of expanding the shared in-\nformation to align the whole corpus. TLM also lacks effort\nin alignment between the source and target sentences since\nTLM concatenates the sentence pair into one sequence, mak-\ning the explicit alignment between the source and target in-\nfeasible. BRLM fully utilizes the alignment information to\nobtain better word-level representation alignment between\ndifferent languages, which better relieves the burden of the\ndomain shift problem .\nTransfer Protocol\nWe consider the typical zero-shot translation scenario in\nwhich a high resource pivot language has parallel data with\nboth source and target languages, while source and target\nlanguages has no parallel data between themselves. Our pro-\nposed cross-lingual pretraining based transfer approach for\nsource→target zero-shot translation is mainly divided into\ntwo phrases: the pretraining phase and the transfer phase.Corpus Language Train Dev Test\nEuroparlDe-En,En-Fr 1M,1M 2,000 2,000\nFr-En,En-Es 1M,1M 2,000 2,000\nRo-En,En-De 0.6M,1.5M 2,000 1,000\nMultiUNAr-En,En-Es\nEn-Ru9.7M,11.3M\n11.6M4,000 4,000\nTable 1: Data Statistics.\nIn the pretraining phase, we ﬁrst pretrain MLM on mono-\nlingual corpora of both source and pivot languages, and con-\ntinue to pretrain TLM or the proposed BRLM on the avail-\nable parallel data between source and pivot languages, in\norder to build a cross-lingual encoder shared by the source\nand pivot languages.\nIn the transfer phase, we train pivot →target NMT model\ninitialized by the cross-lingually pre-trained encoder, and\nﬁnally transfer the trained NMT model to source →target\ntranslation thanks to the shared encoder. Note that during\ntraining pivot→target NMT model, we freeze several layers\nof the cross-lingually pre-trained encoder to avoid the de-\ngeneracy issue.\nFor the', 'Figure 2: The overview of BRidge Language Modeling (BRLM). The BRLM extends MLM (Lample and Conneau 2019) to\npairs of parallel sentences and leverages explicit alignment information obtained by external aligner tool or additional attention\nlayer to encourage word representation alignment across different languages.\n•Directly translate source sentences into target sentences\nwith the parent model, which beneﬁts from the availabil-\nity of the universal encoder.\nThe key difﬁculty of this method is to ensure the intermedi-\nate representations of the universal encoder are language in-\nvariant. In the rest of this section, we ﬁrst present two exist-\ning methods yet to be explored in zero-shot translation, and\nthen propose a straightforward but effective cross-lingual\npre-training method. In the end, we present the whole train-\ning and inference protocol for transfer.\nMasked and Translation Language Model\nPretraining\nTwo existing cross-lingual pre-training methods, Masked\nLanguage Modeling (MLM) and Translation Language\nModeling (TLM), have shown their effectiveness on XNLI\ncross-lingual classiﬁcation task (Lample and Conneau 2019;\nHuang et al. 2019), but these methods have not been well\nstudied on cross-lingual generation tasks in zero-shot condi-\ntion. We attempt to take advantage of the cross-lingual abil-\nity of the two methods for zero-shot translation.\nSpeciﬁcally, MLM adopts the Cloze objective of\nBERT (Devlin et al. 2018) and predicts the masked words\nthat are randomly selected and replaced with [MASK] to-\nken on monolingual corpus. In practice, MLM takes dif-\nferent language monolingual corpora as input to ﬁnd fea-\ntures shared across different languages. With this method,\nword pieces shared in all languages have been mapped into\na shared space, which makes the sentence representationsacross different languages close (Pires, Schlinger, and Gar-\nrette 2019).\nSince MLM objective is unsupervised and only requires\nmonolingual data, TLM is designed to leverage parallel data\nwhen it is available. Actually, TLM is a simple extension of\nMLM, with the difference that TLM concatenates sentence\npair into a whole sentence, and then randomly masks words\nin both the source and target sentences. In this way, the\nmodel can either attend to surrounding words or to the trans-\nlation sentence, implicitly encouraging the model to align\nthe source and target language representations. Note that al-\nthough each sentence pair is formed into one sentence, the\npositions of the target sentence are reset to count form zero.\nBridge Language Model Pretraining\nAside from MLM and TLM, we propose BRidge Language\nModeling (BRLM) to further obtain word-level representa-\ntion alignment between different languages. This method is\ninspired by the assumption that if the feature spaces of dif-\nferent languages are aligned very well, the masked words\nin the corrupted sentence can also be guessed by the con-\ntext of the correspondingly aligned words on the other side.\nTo achieve this goal, BRLM is designed to strengthen the\nability to infer words across languages based on alignment\ninformation, instead of inferring words within monolingual\nsentence as in MLM or within the pseudo sentence formed\nby concatenating sentence pair as in TLM.\nAs illustrated in Figure 2, BRLM stacks shared encoder\nover both side sentences separately. In particular, we design\ntwo network structures for BRLM, which are divided into']",nan,multi_context,TRUE
54,What is the role of the improved relation detection model in the KBQA process and how does it contribute to achieving state-of-the-art results?,"['\nWe use the same example in Fig 1(a) to illustrate\nthe idea. Given the input question in the exam-\nple, a relation detector is very likely to assign high\nscores to relations such as “ episodes written ”,\n“author of” and “ profession ”. Then, according\nto the connections of entity candidates in KB,\nwe ﬁnd that the TV writer “ Mike Kelley ” will\nbe scored higher than the baseball player “ Mike\nKelley ”, because the former has the relations\n“episodes written ” and “ profession ”. This method\ncan be viewed as exploiting entity-relation collo-\ncation for entity linking.\n5.2 Relation Detection\nIn this step, for each candidate entity e∈\nEL′\nK(q), we use the question text as the input to a\nrelation detector to score all the relations r∈Re\nthat are associated to the entity ein the KB.4Be-\ncause we have a single topic entity input in this\nstep, we do the following question reformatting:\nwe replace the the candidate e’s entity mention in\n4Note that the number of entities and the number of rela-\ntion candidates will be much smaller than those in the previ-\nous step.', 'Question: what episode was mike kelleythe writer of\nKnowledgeBaseMike\tKelley(American\ttelevision\twriter/producer)Mike\tKelley(American\tbaseball\tplayer)…Entity Linking \nLove\tWill\tFind\ta\tWayUSA…First\tbaseman…episodes_written\nposition_playedRelation Detection(a)(b)Question: what tvshow did grantshowplay onin2008Mike\tKelley?episodes_writtenEntity Linking RelationDetectionGrantShow?starring_rolesseries(date)from2008ConstraintDetectionGrantShow\t(American\tactor)SwingTown\nBigLoveepisodesScoundrelsseries2011from20102008Figure 1: KBQA examples and its three key components. (a) A single relation example. We ﬁrst identify the topic entity with\nentity linking and then detect the relation asked by the question with relation detection (from all relations connecting the topic\nentity). Based on the detected entity and relation, we form a query to search the KB for the correct answer “ Love Will Find a\nWay”. (b) A more complex question containing two entities. By using “ Grant Show ” as the topic entity, we could detect a chain\nof relations “ starring roles-series ” pointing to the answer. An additional constraint detection takes the other entity “ 2008 ” as\na constraint, to ﬁlter the correct answer “ SwingTown ” from all candidates found by the topic entity and relation.\nthat original relation names can sometimes help\nto match longer question contexts, we propose\nto build both relation-level and word-level rela-\ntion representations. Third, we use deep bidirec-\ntional LSTMs ( BiLSTM s) to learn different levels\nof question representations in order to match the\ndifferent levels of relation information. Finally, we\npropose a residual learning method for sequence\nmatching, which makes the model training easier\nand results in more abstract (deeper) question rep-\nresentations, thus improves hierarchical matching.\nIn order to assess how the proposed improved\nrelation detection could beneﬁt the KBQA end\ntask, we also propose a simple KBQA implemen-\ntation composed of two-step relation detection .\nGiven an input question and a set of candidate enti-\nties retrieved by an entity linker based on the ques-\ntion, our proposed relation detection model plays a\nkey role in the KBQA process: (1) Re-ranking the\nentity candidates according to whether they con-\nnect to high conﬁdent relations detected from the\nraw question text by the relation detection model.\nThis step is important to deal with the ambigui-\nties normally present in entity linking results. (2)\nFinding the core relation (chains) for each topic\nentity2selection from a much smaller candidate\nentity set after re-ranking. The above steps are\nfollowed by an optional constraint detection step,\nwhen the question cannot be answered by single\nrelations (e.g., multiple entities in the question).\nFinally the highest scored query from the above\n2Following Yih et al. (2015), here topic entity refers to\nthe root of the (directed) query tree; and core-chain is the\ndirected path of relation from root to the answer node.steps is used to query the KB for answers.\nOur main contributions include: (i) An im-\nproved relation detection model by hierarchical\nmatching between questions and relations with\nresidual learning; (ii) We demonstrate that the im-\nproved relation detector enables our simple KBQA\nsystem to achieve state-of-the-art results on both\nsingle-relation and multi-relation KBQA tasks.\n2 Related Work\nRelation Extraction Relation extraction (RE) is\nan important sub-ﬁeld of information extraction.\nGeneral research in this ﬁeld usually works on a\n(small) pre-deﬁned relation set, where given a text\nparagraph and two target entities, the goal is to\ndetermine whether the text indicates any types of\nrelations between the entities or not. As a result\nRE is usually formulated as a classiﬁcation task .\nTraditional RE methods rely on large amount of\nhand-crafted features (Zhou et al., 2005; Rink and\nHarabagiu, 2010; Sun et al., 2011). Recent re-\nsearch beneﬁts a lot from the advancement of deep\nlearning: from word embeddings (Nguyen and Gr-\nishman, 2014; Gormley et al., 2015) to deep net-\nworks']","The improved relation detection model plays a key role in the KBQA process by re-ranking the entity candidates based on high confident relations detected from the raw question text. It helps deal with ambiguities in entity linking results. Additionally, the model finds the core relation for each topic entity, which is selected from a smaller candidate entity set after re-ranking. The improved relation detection model contributes to achieving state-of-the-art results in both single-relation and multi-relation KBQA tasks.",multi_context,TRUE
55,"""What model is used for sensitive information detection and classification in Spanish clinical text?","['Sensitive Data Detection and Classiﬁcation in Spanish Clinical Text:\nExperiments with BERT\nAitor Garc ´ıa-Pablos, Naiara Perez, Montse Cuadros\nSNLT group at Vicomtech Foundation, Basque Research and Technology Alliance (BRTA)\nDonostia/San-Sebasti ´an, 20009, Spain\n{agarciap, nperez, mcuadros }@vicomtech.org\nAbstract\nMassive digital data processing provides a wide range of opportunities and beneﬁts, but at the cost of endangering personal data privacy.\nAnonymisation consists in removing or replacing sensitive information from data, enabling its exploitation for different purposes\nwhile preserving the privacy of individuals. Over the years, a lot of automatic anonymisation systems have been proposed; however,\ndepending on the type of data, the target language or the availability of training documents, the task remains challenging still. The\nemergence of novel deep-learning models during the last two years has brought large improvements to the state of the art in the ﬁeld\nof Natural Language Processing. These advancements have been most noticeably led by BERT, a model proposed by Google in 2018,\nand the shared language models pre-trained on millions of documents. In this paper, we use a BERT-based sequence labelling model to\nconduct a series of anonymisation experiments on several clinical datasets in Spanish. We also compare BERT to other algorithms. The\nexperiments show that a simple BERT-based model with general-domain pre-training obtains highly competitive results without any\ndomain speciﬁc feature engineering.\nKeywords: Anonymisation, De-identiﬁcation, PHI, Clinical Data, BERT\n1. Introduction\nDuring the ﬁrst two decades of the 21st century, the sharing\nand processing of vast amounts of data has become per-\nvasive. This expansion of data sharing and processing ca-\npabilities is both a blessing and a curse. Data helps build\nbetter information systems for the digital era and enables\nfurther research for advanced data management that ben-\neﬁts the society in general. But the use of this very data\ncontaining sensitive information conﬂicts with private data\nprotection, both from an ethical and a legal perspective.\nThere are several application domains on which this situ-\nation is particularly acute. This is the case of the medical\ndomain (Abouelmehdi et al., 2018). There are plenty of po-\ntential applications for advanced medical data management\nthat can only be researched and developed using real data;\nyet, the use of medical data is severely limited –when not\nentirely prohibited– due to data privacy protection policies.\nOne way of circumventing this problem is to anonymise\nthe data by removing, replacing or obfuscating the personal\ninformation mentioned, as exempliﬁed in Table 1. This task\ncan be done by hand, having people read and anonymise\nthe documents one by one. Despite being a reliable and\nsimple solution, this approach is tedious, expensive, time\nconsuming and difﬁcult to scale to the potentially thousands\nor millions of documents that need to be anonymised.\nFor this reason, numerous of systems and approaches have\nbeen developed during the last decades to attempt to auto-\nmate the anonymisation of sensitive content, starting with\nthe automatic detection and classiﬁcation of sensitive infor-\nmation. Some of these systems rely on rules, patterns and\ndictionaries, while others use more advanced techniques re-\nlated to machine learning and, more recently, deep learning.\nGiven that this paper is concerned with text documents\n(e.g. medical records), the involved techniques are related\nto Natural Language Processing (NLP). When using NLP\napproaches, it is common to pose the problem of documentanonymisation as a sequence labelling problem, i.e. clas-\nsifying each token within a sequence as being sensitive in-\nformation or not. Further, depending on the objective of\nthe anonymisation task, it is also important to determine\nthe type of sensitive information (names of individuals, ad-\ndresses, age, sex, etc.).\nThe anonymisation systems based on NLP techniques per-\nform reasonably well, but are far from perfect. Depend-\ning on the difﬁculty posed by each dataset or the amount\nof available data for training machine learning models,\nthe performance achieved by these methods is not enough\nto fully rely on them in certain situations (Abouelmehdi\net al., 2018). However, in the last two years, the NLP\ncommunity has reached an important milestone thanks to\nthe', ' appearance of the so-called Transformers neural net-\nwork architectures (Wolf et al., 2019). In this paper, we\nconduct several experiments in sensitive information de-\ntection and classiﬁcation on Spanish clinical text using\nBERT (from ‘Bidirectional Encoder Representations from\nTransformers’) (Devlin et al., 2019) as the base for a se-\nquence labelling approach. The experiments are carried\nout on two datasets: the MEDDOCAN: Medical Document\nAnonymization shared task dataset (Marimon et al., 2019),\nand NUB ES(Lima et al., 2019), a corpus of real medical\nreports in Spanish. In these experiments, we compare the\nperformance of BERT with other machine-learning-based\nsystems, some of which use language-speciﬁc features. Our\naim is to evaluate how good a BERT-based model performs\nwithout language nor domain specialisation apart from the\ntraining data labelled for the task at hand.\nThe rest of the paper is structured as follows: the next sec-\ntion describes related work about data anonymisation in\ngeneral and clinical data anonymisation in particular; it also\nprovides a more detailed explanation and background about\nthe Transformers architecture and BERT. Section 3. de-\nscribes the data involved in the experiments and the systemsarXiv:2003.03106v2  [cs.CL]  17 Mar 2020', 'original Paciente de 64 a ˜nosoperado de una hernia el 12/01/2016 por la Dra Lopez\nexample 1 Paciente de XXXXXXX operado de una hernia el XXXXXXXXXX por XXXXXXXXXXXX\nexample 2 Paciente de [-AGE-] operado de una hernia el [--DATE--] por [--DOCTOR--]\nexample 3 Paciente de 59 a ˜nosoperado de una hernia el 05/06/2019 por el Dr Sancho\nTable 1: Anonymization examples of “64-year-old patient operated on a hernia on the 12/01/2016 by Dr Lopez”; sensitive\ndata and their substitutions are highlighted in bold.\nevaluated in this paper, including the BERT-based system;\nﬁnally, it details the experimental design. Section 4. intro-\nduces the results for each set of experiments. Finally, Sec-\ntion 5. contains the conclusions and future lines of work.\n2. Related Work\nThe state of the art in the ﬁeld of Natural Language Pro-\ncessing (NLP) has reached an important milestone in the\nlast couple of years thanks to deep-learning architectures,\nincreasing in several points the performance of new models\nfor almost any text processing task.\nThe major change started with the Transformers model pro-\nposed by Vaswani et al. (2017). It substituted the widely\nused recurrent and convolutional neural network architec-\ntures by another approach based solely on self-attention,\nobtaining an impressive performance gain. The original\nproposal was focused on an encoder-decoder architecture\nfor machine translation, but soon the use of Transformers\nwas made more general (Wolf et al., 2019). There are sev-\neral other popular models that use Transformers, such as\nOpen AI’s GPT and GPT2 (Radford et al., 2019), RoBERTa\n(Liu et al., 2019) and the most recent XLNet (Yang et al.,\n2019); still, BERT (Devlin et al., 2019) is one of the most\nwidespread Transformer-based models.\nBERT trains its unsupervised language model using a\nMasked Language Model and Next Sentence Prediction.\nA common problem in NLP is the lack of enough training\ndata. BERT can be pre-trained to learn general or speciﬁc\nlanguage models using very large amounts of unlabelled\ntext (e.g. web content, Wikipedia, etc.), and this knowl-\nedge can be transferred to a different downstream task in a\nprocess that receives the name ﬁne-tuning .\nDevlin et al. (2019) have used ﬁne-tuning to achieve state-\nof-the-art results on a wide variety of challenging natural\nlanguage tasks, such as text classiﬁcation, Question An-\nswering (QA) and Named Entity Recognition and Classi-\nﬁcation (NERC). BERT has also been used successfully by\nother community practitioners for a wide range of NLP-\nrelated tasks (Liu and Lapata, 2019; Nogueira and Cho,\n2019, among others).\nRegarding the task of data anonymisation in particular,\nanonymisation systems may follow different approaches\nand pursue different objectives (Cormode and Srivastava,\n2009). The ﬁrst objective of these systems is to detect\nand classify the sensitive information contained in the doc-\numents to be anonymised. In order to achieve that, they\nuse rule-based approaches, Machine Learning (ML) ap-\nproaches, or a combination of both.\nAlthough most of these efforts are for English texts –\nsee, among others, the i2b2 de-identiﬁcation challenges(Uzuner et al., 2007; Stubbs et al., 2015), Dernoncourt et\nal. (2016), or Khin et al. (2018)–, other languages are also\nattracting growing interest. Some examples are Mamede\net al. (2016) for Portuguese and Tveit et al. (2004) for\nNorwegian. With respect to the anonymisation of text writ-\nten in Spanish, recent studies include Medina and Turmo\n(2018), Hassan et al. (2018) and Garc ´ıa-Sardi ˜na (2018).\nMost notably, in 2019 the ﬁrst community challenge about\nanonymisation of medical documents in Spanish, MED-\nDOCAN1(Marimon et al., 2019), was held as part of\nthe IberLEF initiative.', ' appearance of the so-called Transformers neural net-\nwork architectures (Wolf et al., 2019). In this paper, we\nconduct several experiments in sensitive information de-\ntection and classiﬁcation on Spanish clinical text using\nBERT (from ‘Bidirectional Encoder Representations from\nTransformers’) (Devlin et al., 2019) as the base for a se-\nquence labelling approach. The experiments are carried\nout on two datasets: the MEDDOCAN: Medical Document\nAnonymization shared task dataset (Marimon et al., 2019),\nand NUB ES(Lima et al., 2019), a corpus of real medical\nreports in Spanish. In these experiments, we compare the\nperformance of BERT with other machine-learning-based\nsystems, some of which use language-speciﬁc features. Our\naim is to evaluate how good a BERT-based model performs\nwithout language nor domain specialisation apart from the\ntraining data labelled for the task at hand.\nThe rest of the paper is structured as follows: the next sec-\ntion describes related work about data anonymisation in\ngeneral and clinical data anonymisation in particular; it also\nprovides a more detailed explanation and background about\nthe Transformers architecture and BERT. Section 3. de-\nscribes the data involved in the experiments and the systemsarXiv:2003.03106v2  [cs.CL]  17 Mar 2020']",The model used for sensitive information detection and classification in Spanish clinical text is BERT (Bidirectional Encoder Representations from Transformers). ,multi_context,TRUE
56,"""What is the proposed solution for word-order divergence in multilingual NMT in a low-resource setting, and how does it involve pre-ordering English sentences?""","[' 1999) which enables the model to learn\nthe word-order of the source language when sufﬁ-\ncient child task parallel corpus is available.\nWe also compare the performance of the ﬁne-\ntuned model with the model trained only on the\navailable source-target parallel corpus with ran-\ndomly initialized weights (No Transfer Learning).\nTransfer learning, with and without pre-ordering,\nis better compared to training only on the small\nsource-target parallel corpus.\n6 Conclusion\nIn this paper, we show that handling word-order\ndivergence between the source and assisting lan-\nguages is crucial for the success of multilingual\nNMT in an extremely low-resource setting. We\nshow that pre-ordering the assisting language to\nmatch the word order of the source language sig-\nniﬁcantly improves translation quality in an ex-\ntremely low-resource setting. If pre-ordering is\nnot possible, ﬁne-tuning on a small source-target', '2 Related Work\nTo the best of our knowledge, no work has ad-\ndressed word order divergence in transfer learn-\ning for multilingual NMT. However, some work\nexists for other NLP tasks in a multilingual set-\nting. For Named Entity Recognition (NER), Xie\net al. (2018) use a self-attention layer after the\nBi-LSTM layer to address word-order divergence\nfor Named Entity Recognition (NER) task. The\napproach does not show any signiﬁcant improve-\nments, possibly because the divergence has to be\naddressed before/during construction of the con-\ntextual embeddings in the Bi-LSTM layer. Joty\net al. (2017) use adversarial training for cross-\nlingual question-question similarity ranking. The\nadversarial training tries to force the sentence rep-\nresentation generated by the encoder of similar\nsentences from different input languages to have\nsimilar representations.\nPre-ordering the source language sentences to\nmatch the target language word order has been\nfound useful in addressing word-order divergence\nfor Phrase-Based SMT (Collins et al., 2005; Ra-\nmanathan et al., 2008; Navratil et al., 2012; Chat-\nterjee et al., 2014). For NMT, Ponti et al. (2018)\nand Kawara et al. (2018) have explored pre-\nordering. Ponti et al. (2018) demonstrated that\nby reducing the syntactic divergence between the\nsource and the target languages, consistent im-\nprovements in NMT performance can be obtained.\nOn the contrary, Kawara et al. (2018) reported\ndrop in NMT performance due to pre-ordering.\nNote that these works address source-target diver-\ngence, not divergence between source languages\nin multilingual NMT scenario.\n3 Proposed Solution\nConsider the task of translating for an extremely\nlow-resource language pair. The parallel corpus\nbetween the two languages, if available may be\ntoo small to train an NMT model. Similar to Zoph\net al. (2016), we use transfer learning to over-\ncome data sparsity between the source and the\ntarget languages. We choose English as the as-\nsisting language in all our experiments. In our\nresource-scarce scenario, we have no parallel cor-\npus for training the child model. Hence, at test\ntime, the source language sentence is translated\nusing the parent model after performing a word-\nby-word translation from source to the assisting\nlanguage using a bilingual dictionary.Before Reordering After Reordering\nS\nNP0 VP\nV NP 1S\nNP0 VP\nNP1V\nS\nNP\nNNP\nAnuragVP\nMD\nwillVP\nVB\nmeetNP\nNNP\nThakurS\nNP\nNNP\nAnuragVP\nNP\nNNP\nThakurVP\nMD\nwillVP\nVB\nmeet\nTable 1: Example showing transitive verb before and\nafter reordering (Adapted from Chatterjee et al. (2014))\nSince the source language and the assisting lan-\nguage (English) have different word order, we hy-\npothesize that it leads to inconsistencies in the\ncontextual representations generated by the en-\ncoder for the two languages. Speciﬁcally, given an\nEnglish sentence (SVO word order) and its transla-\ntion in the source language (SOV word order), the\nencoder representations for words in the two sen-\ntences will be different due to different contexts\nof synonymous words. This could lead to the at-\ntention and the decoder layers generating different\ntranslations from the same (parallel) sentence in\nthe source or assisting language. This is undesir-\nable as we want the knowledge to be transferred\nfrom the parent model (assisting source →target)\nto the child model (source →target).\nIn this paper, we propose to pre-order English\nsentences (assisting language sentences) to match\nthe source language word-order and train the par-\nent model on the pre-ordered corpus. Table 1\nshows one of the pre-ordering rules (Ramanathan\net al., 2008) used along with an example sentence\nillustrating the effect of pre-ordering. This will en-\nsure that context of words in the parallel source\nand assisting language sentences are similar, lead-\ning to consistent contextual representations across\nthe source languages. Pre-ordering may also be\nbeneﬁcial for other word order divergence scenar-\nios (e.g., SOV to SVO), but we leave']","The proposed solution for word-order divergence in multilingual NMT in a low-resource setting involves pre-ordering English sentences to match the source language word-order. This ensures that the context of words in the parallel source and assisting language sentences are similar, leading to consistent contextual representations across the source languages.",multi_context,TRUE
57,"""What factors contribute to a user's dogmatism on Reddit?""","[')\nDogmatism is widely considered to be a domain-\nspeciﬁc attitude (for example, oriented towards re-\nligion or politics) as opposed to a deeper personality\ntrait (Rokeach, 1954). Here we use Reddit as a lens\nto examine this idea more closely. Are users who\nare dogmatic about one topic likely to be dogmatic\nabout others? Do clusters of dogmatism exist around\nparticular topics? To ﬁnd out, we examine the re-', 'Libertarianism business conspiracy science Christianity lgbt\nAnarcho Capitalism Bitcoin Republican Christianity DebateAChristian feminisms\nBitcoin economy conspiritard relationship advice DebateReligion Equality\nronpaul entertainment ronpaul worldpolitics science SubredditDrama\nConservative TrueReddit collapse MensRights videos TwoXChromosomes\nAndroid socialism guns IAmA news MensRights\nukpolitics bestof worldpolitics TwoXChromosomes Libertarianism offbeat\nEquality philosophy occupywallstreet WTF atheism fffffffuuuuuuuuuuuu\nTable 4: Clusters of subreddits that share dogmatic users. For example, users who are dogmatic on the conspiracy subreddit (a\nplace to discuss conspiracy theories) are also likely to be dogmatic on guns oroccupywallstreet .\nlationships between subreddits over which individ-\nual users are dogmatic. For example, if many users\noften post dogmatic comments on both the politics\nandChristianity subreddits, but less often on world-\nnews , that would suggest politics andChristianity\nare linked per a boost in likelihood of individuals\nbeing dogmatic in both.\nWe sample 1000 Reddit users who posted at least\nonce a year between 2007 and 2015 to construct a\ncorpus of 10 million posts that constitute their entire\npost history. We then annotate these posts using the\nclassiﬁer and compute the average dogmatism score\nper subreddit per user. For example, one user might\nhave an average dogmatism level of 0.55 for the pol-\nitics subreddit and 0.45 for the economics subred-\ndit. Most users do not post in all subreddits, so we\ntrack only subreddits for which a user had posted at\nleast 10 times. Any subreddits with an average dog-\nmatism score higher than 0.50 we consider to be a\nuser’s dogmatic subreddits. We then count all pairs\nof these dogmatic subreddits. For example, 45 users\nhave politics andtechnology among their dogmatic\nsubreddits, so we consider politics andtechnology\nas linked 45 times. We compute the mutual informa-\ntion (Church and Hanks, 1990) between these links,\nwhich gives us a measure of the subreddits that are\nmost related through dogmatism.\nWe present the results of this analysis in Table 4,\nchoosing clusters that represent a diverse set of top-\nics. For example, Libertarianism is linked through\ndogmatism to other political communities like An-\narcho Capitalism ,ronpaul , orukpolitics , as well as\nother topical subreddits like guns oreconomy . Sim-\nilarly, people who are dogmatic in the business sub-\nreddit also tend to be dogmatic in subreddits for Bit-\ncoin,socialism , and technology . Notably, when we\napply the same mutual information analysis to links\ndeﬁned by subreddits posted in by the same user, weFeature Direction\ntotal user posts ↑\nproportion of posts in most active subreddit ↑\nnumber of subreddits posted in ↓\naverage number of posts in active articles ↓\nTable 5: User behavioral features that are positively and nega-\ntively associated with dogmatism. ↑means the feature is pos-\nitively predictive with dogmatism, and ↓means the feature is\nnegatively predictive. For example, the more subreddits a user\nposts in, the less likely they are to be dogmatic. All features are\nstatistically signiﬁcant ( p < 0.001).\nsee dramatically different results. For example, the\nsubreddits most linked to science through user posts\nareUpliftingNews ,photoshopbattles , and ﬁrstworl-\ndanarchist , and millionairemakers .\nFinally, we see less obvious connections between\nsubreddits that suggest some people may be dog-\nmatic by nature. For example, among the users who\nare dogmatic on politics , they are also disproportion-\nately dogmatic on unrelated subreddits such as sci-\nence (p < 0.001),technology (p < 0.001),IAmA\n(p < 0.001), and AskReddit (p < 0.05), with p-\nvalues computed under a binomial test.\n5.3 What user behaviors are predictive of\ndogmatism? (R3)\nWe have shown dogmatism is captured by many lin-\nguistic features, but can we discover other high-level\nuser behaviors that are similarly predictive?\nTo ﬁnd out, we compute metrics of user behavior\nusing the data sample of 1000 users and 10 million\nposts described in Section 5.2. Speciﬁcally, we cal-\nculate (1) activity : a user’s total number of posts,', 'Libertarianism business conspiracy science Christianity lgbt\nAnarcho Capitalism Bitcoin Republican Christianity DebateAChristian feminisms\nBitcoin economy conspiritard relationship advice DebateReligion Equality\nronpaul entertainment ronpaul worldpolitics science SubredditDrama\nConservative TrueReddit collapse MensRights videos TwoXChromosomes\nAndroid socialism guns IAmA news MensRights\nukpolitics bestof worldpolitics TwoXChromosomes Libertarianism offbeat\nEquality philosophy occupywallstreet WTF atheism fffffffuuuuuuuuuuuu\nTable 4: Clusters of subreddits that share dogmatic users. For example, users who are dogmatic on the conspiracy subreddit (a\nplace to discuss conspiracy theories) are also likely to be dogmatic on guns oroccupywallstreet .\nlationships between subreddits over which individ-\nual users are dogmatic. For example, if many users\noften post dogmatic comments on both the politics\nandChristianity subreddits, but less often on world-\nnews , that would suggest politics andChristianity\nare linked per a boost in likelihood of individuals\nbeing dogmatic in both.\nWe sample 1000 Reddit users who posted at least\nonce a year between 2007 and 2015 to construct a\ncorpus of 10 million posts that constitute their entire\npost history. We then annotate these posts using the\nclassiﬁer and compute the average dogmatism score\nper subreddit per user. For example, one user might\nhave an average dogmatism level of 0.55 for the pol-\nitics subreddit and 0.45 for the economics subred-\ndit. Most users do not post in all subreddits, so we\ntrack only subreddits for which a user had posted at\nleast 10 times. Any subreddits with an average dog-\nmatism score higher than 0.50 we consider to be a\nuser’s dogmatic subreddits. We then count all pairs\nof these dogmatic subreddits. For example, 45 users\nhave politics andtechnology among their dogmatic\nsubreddits, so we consider politics andtechnology\nas linked 45 times. We compute the mutual informa-\ntion (Church and Hanks, 1990) between these links,\nwhich gives us a measure of the subreddits that are\nmost related through dogmatism.\nWe present the results of this analysis in Table 4,\nchoosing clusters that represent a diverse set of top-\nics. For example, Libertarianism is linked through\ndogmatism to other political communities like An-\narcho Capitalism ,ronpaul , orukpolitics , as well as\nother topical subreddits like guns oreconomy . Sim-\nilarly, people who are dogmatic in the business sub-\nreddit also tend to be dogmatic in subreddits for Bit-\ncoin,socialism , and technology . Notably, when we\napply the same mutual information analysis to links\ndeﬁned by subreddits posted in by the same user, weFeature Direction\ntotal user posts ↑\nproportion of posts in most active subreddit ↑\nnumber of subreddits posted in ↓\naverage number of posts in active articles ↓\nTable 5: User behavioral features that are positively and nega-\ntively associated with dogmatism. ↑means the feature is pos-\nitively predictive with dogmatism, and ↓means the feature is\nnegatively predictive. For example, the more subreddits a user\nposts in, the less likely they are to be dogmatic. All features are\nstatistically signiﬁcant ( p < 0.001).\nsee dramatically different results. For example, the\nsubreddits most linked to science through user posts\nareUpliftingNews ,photoshopbattles , and ﬁrstworl-\ndanarchist , and millionairemakers .\nFinally, we see less obvious connections between\nsubreddits that suggest some people may be dog-\nmatic by nature. For example, among the users who\nare dogmatic on politics , they are also disproportion-\nately dogmatic on unrelated subreddits such as sci-\nence (p < 0.001),technology (p < 0.001),IAmA\n(p < 0.001), and AskReddit (p < 0.05), with p-\nvalues computed under a binomial test.\n5.3 What user behaviors are predictive of\ndogmatism? (R3)\nWe have shown dogmatism is captured by many lin-\nguistic features, but can we discover other high-level\nuser behaviors that are similarly predictive?\nTo ﬁnd out, we compute metrics of user behavior\nusing the data sample of 1000 users and 10 million\nposts described in Section 5.2. Speciﬁcally, we cal-\nculate (1) activity : a user’s total number of posts,']",nan,multi_context,TRUE
58,What structures extend MLM into a bilingual scenario and introduce alignment information during model training?,"[', MNMT\nusually performs worse than the pivot-based method in\nzero-shot translation setting (Arivazhagan et al. 2018).\n•Unsupervised NMT (UNMT) considers a harder setting,\nin which only large-scale monolingual corpora are avail-\nable for training. Recently, many methods have been pro-\nposed to improve the performance of UNMT, including\nusing denoising auto-encoder, statistic machine transla-\ntion (SMT) and unsupervised pre-training (Artetxe et\nal. 2017; Lample et al. 2018; Ren et al. 2019; Lample\nand Conneau 2019). Since UNMT performs well between\nsimilar languages (e.g., English-German translation), its\nperformance between distant languages is still far from\nexpectation.\nOur proposed method belongs to the transfer learning,\nbut it is different from traditional transfer methods which\ntrain a parent model as starting point. Before training a par-\nent model, our approach fully leverages cross-lingual pre-\ntraining methods to make all source languages share the\nsame feature space and thus enables a smooth transition for\nzero-shot translation.\nApproach\nIn this section, we will present a cross-lingual pre-\ntraining based transfer approach. This method is designed\nfor a common zero-shot scenario where there are a lot\nof source↔pivot and pivot↔target bilingual data but no\nsource↔target parallel data, and the whole training process\ncan be summarized as follows step by step:\n•Pre-train a universal encoder with source/pivot monolin-\ngual or source↔pivot bilingual data.\n•Train a pivot→target parent model built on the pre-trained\nuniversal encoder with the available parallel data. Dur-\ning the training process, we freeze several layers of the\npre-trained universal encoder to avoid the degeneracy is-\nsue (Howard and Ruder 2018).', 'Figure 2: The overview of BRidge Language Modeling (BRLM). The BRLM extends MLM (Lample and Conneau 2019) to\npairs of parallel sentences and leverages explicit alignment information obtained by external aligner tool or additional attention\nlayer to encourage word representation alignment across different languages.\n•Directly translate source sentences into target sentences\nwith the parent model, which beneﬁts from the availabil-\nity of the universal encoder.\nThe key difﬁculty of this method is to ensure the intermedi-\nate representations of the universal encoder are language in-\nvariant. In the rest of this section, we ﬁrst present two exist-\ning methods yet to be explored in zero-shot translation, and\nthen propose a straightforward but effective cross-lingual\npre-training method. In the end, we present the whole train-\ning and inference protocol for transfer.\nMasked and Translation Language Model\nPretraining\nTwo existing cross-lingual pre-training methods, Masked\nLanguage Modeling (MLM) and Translation Language\nModeling (TLM), have shown their effectiveness on XNLI\ncross-lingual classiﬁcation task (Lample and Conneau 2019;\nHuang et al. 2019), but these methods have not been well\nstudied on cross-lingual generation tasks in zero-shot condi-\ntion. We attempt to take advantage of the cross-lingual abil-\nity of the two methods for zero-shot translation.\nSpeciﬁcally, MLM adopts the Cloze objective of\nBERT (Devlin et al. 2018) and predicts the masked words\nthat are randomly selected and replaced with [MASK] to-\nken on monolingual corpus. In practice, MLM takes dif-\nferent language monolingual corpora as input to ﬁnd fea-\ntures shared across different languages. With this method,\nword pieces shared in all languages have been mapped into\na shared space, which makes the sentence representationsacross different languages close (Pires, Schlinger, and Gar-\nrette 2019).\nSince MLM objective is unsupervised and only requires\nmonolingual data, TLM is designed to leverage parallel data\nwhen it is available. Actually, TLM is a simple extension of\nMLM, with the difference that TLM concatenates sentence\npair into a whole sentence, and then randomly masks words\nin both the source and target sentences. In this way, the\nmodel can either attend to surrounding words or to the trans-\nlation sentence, implicitly encouraging the model to align\nthe source and target language representations. Note that al-\nthough each sentence pair is formed into one sentence, the\npositions of the target sentence are reset to count form zero.\nBridge Language Model Pretraining\nAside from MLM and TLM, we propose BRidge Language\nModeling (BRLM) to further obtain word-level representa-\ntion alignment between different languages. This method is\ninspired by the assumption that if the feature spaces of dif-\nferent languages are aligned very well, the masked words\nin the corrupted sentence can also be guessed by the con-\ntext of the correspondingly aligned words on the other side.\nTo achieve this goal, BRLM is designed to strengthen the\nability to infer words across languages based on alignment\ninformation, instead of inferring words within monolingual\nsentence as in MLM or within the pseudo sentence formed\nby concatenating sentence pair as in TLM.\nAs illustrated in Figure 2, BRLM stacks shared encoder\nover both side sentences separately. In particular, we design\ntwo network structures for BRLM, which are divided into', 'Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-\nSA) according to the way of generating the alignment infor-\nmation. These two structures actually extend MLM into a\nbilingual scenario, with the difference that BRLM leverages\nexternal aligner tool or additional attention layer to explic-\nitly introduce alignment information during model training.\n•Hard Alignment (BRLM-HA). We ﬁrst use exter-\nnal aligner tool on source ↔pivot parallel data to ex-\ntract the alignment information of sentence pair. Dur-\ning model training, given source ↔pivot sentence pair,\nBRLM-HA randomly masks some words in source sen-\ntence and leverages alignment information to obtain the\naligned words in pivot sentence for masked words. Based\non the processed input, BRLM-HA adopts the Trans-\nformer (Vaswani et al. 2017) encoder to gain the hid-\nden states for source and pivot sentences respectively.\nThen the training objective of BRLM-HA is to predict\nthe masked words by not only the surrounding words\nin source sentence but also the encoder outputs of the\naligned words. Note that this training process is also car-\nried out in a symmetric situation, in which we mask some\nwords in pivot sentence and obtain the aligned words in\nthe source sentence.\n•Soft Alignment (BRLM-SA). Instead of using external\naligner tool, BRLM-SA introduces an additional atten-\ntion layer to learn the alignment information together with\nmodel training. In this way, BRLM-SA avoids the effect\ncaused by external wrong alignment information and en-\nables many-to-one soft alignment during model training.\nSimilar with BRLM-HA, the training objective of BRLM-\nSA is to predict the masked words by not only the sur-\nrounding words in source sentence but also the outputs of\nattention layer. In our implementation, the attention layer\nis a multi-head attention layer adopted in Transformer,\nwhere the queries come from the masked source sentence,\nthe keys and values come from the pivot sentence.\nIn principle, MLM and TLM can learn some implicit align-\nment information during model training. However, the align-\nment process in MLM is inefﬁcient since the shared word\npieces only account for a small proportion of the whole cor-\npus, resulting in the difﬁculty of expanding the shared in-\nformation to align the whole corpus. TLM also lacks effort\nin alignment between the source and target sentences since\nTLM concatenates the sentence pair into one sequence, mak-\ning the explicit alignment between the source and target in-\nfeasible. BRLM fully utilizes the alignment information to\nobtain better word-level representation alignment between\ndifferent languages, which better relieves the burden of the\ndomain shift problem .\nTransfer Protocol\nWe consider the typical zero-shot translation scenario in\nwhich a high resource pivot language has parallel data with\nboth source and target languages, while source and target\nlanguages has no parallel data between themselves. Our pro-\nposed cross-lingual pretraining based transfer approach for\nsource→target zero-shot translation is mainly divided into\ntwo phrases: the pretraining phase and the transfer phase.Corpus Language Train Dev Test\nEuroparlDe-En,En-Fr 1M,1M 2,000 2,000\nFr-En,En-Es 1M,1M 2,000 2,000\nRo-En,En-De 0.6M,1.5M 2,000 1,000\nMultiUNAr-En,En-Es\nEn-Ru9.7M,11.3M\n11.6M4,000 4,000\nTable 1: Data Statistics.\nIn the pretraining phase, we ﬁrst pretrain MLM on mono-\nlingual corpora of both source and pivot languages, and con-\ntinue to pretrain TLM or the proposed BRLM on the avail-\nable parallel data between source and pivot languages, in\norder to build a cross-lingual encoder shared by the source\nand pivot languages.\nIn the transfer phase, we train pivot →target NMT model\ninitialized by the cross-lingually pre-trained encoder, and\nﬁnally transfer the trained NMT model to source →target\ntranslation thanks to the shared encoder. Note that during\ntraining pivot→target NMT model, we freeze several layers\nof the cross-lingually pre-trained encoder to avoid the de-\ngeneracy issue.\nFor the', ' more complicated scenario that either the source\nside or the target side has multiple languages, the encoder\nand the decoder are also shared across each side languages\nfor efﬁcient deployment of translation between multiple lan-\nguages.\nExperiments\nSetup\nWe evaluate our cross-lingual pre-training based transfer ap-\nproach against several strong baselines on two public datat-\nsets, Europarl (Koehn 2005) and MultiUN (Eisele and Chen\n2010), which contain multi-parallel evaluation data to assess\nthe zero-shot performance. In all experiments, we use BLEU\nas the automatic metric for translation evaluation.1\nDatasets. The statistics of Europarl and MultiUN cor-\npora are summarized in Table 1. For Europarl corpus, we\nevaluate on French-English-Spanish (Fr-En-Es), German-\nEnglish-French (De-En-Fr) and Romanian-English-German\n(Ro-En-De), where English acts as the pivot language, its\nleft side is the source language, and its right side is the target\nlanguage. We remove the multi-parallel sentences between\ndifferent training corpora to ensure zero-shot settings. We\nuse the devtest2006 as the validation set and the test2006 as\nthe test set for Fr→Es and De→Fr. For distant language pair\nRo→De, we extract 1,000 overlapping sentences from new-\nstest2016 as the test set and the 2,000 overlapping sentences\nsplit from the training set as the validation set since there is\nno ofﬁcial validation and test sets. For vocabulary, we use\n60K sub-word tokens based on Byte Pair Encoding (BPE)\n(Sennrich, Haddow, and Birch 2015).\nFor MultiUN corpus, we use four languages: English\n(En) is set as the pivot language, which has parallel data\n1We calculate BLEU scores with the multi-bleu.perl script.', 'Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-\nSA) according to the way of generating the alignment infor-\nmation. These two structures actually extend MLM into a\nbilingual scenario, with the difference that BRLM leverages\nexternal aligner tool or additional attention layer to explic-\nitly introduce alignment information during model training.\n•Hard Alignment (BRLM-HA). We ﬁrst use exter-\nnal aligner tool on source ↔pivot parallel data to ex-\ntract the alignment information of sentence pair. Dur-\ning model training, given source ↔pivot sentence pair,\nBRLM-HA randomly masks some words in source sen-\ntence and leverages alignment information to obtain the\naligned words in pivot sentence for masked words. Based\non the processed input, BRLM-HA adopts the Trans-\nformer (Vaswani et al. 2017) encoder to gain the hid-\nden states for source and pivot sentences respectively.\nThen the training objective of BRLM-HA is to predict\nthe masked words by not only the surrounding words\nin source sentence but also the encoder outputs of the\naligned words. Note that this training process is also car-\nried out in a symmetric situation, in which we mask some\nwords in pivot sentence and obtain the aligned words in\nthe source sentence.\n•Soft Alignment (BRLM-SA). Instead of using external\naligner tool, BRLM-SA introduces an additional atten-\ntion layer to learn the alignment information together with\nmodel training. In this way, BRLM-SA avoids the effect\ncaused by external wrong alignment information and en-\nables many-to-one soft alignment during model training.\nSimilar with BRLM-HA, the training objective of BRLM-\nSA is to predict the masked words by not only the sur-\nrounding words in source sentence but also the outputs of\nattention layer. In our implementation, the attention layer\nis a multi-head attention layer adopted in Transformer,\nwhere the queries come from the masked source sentence,\nthe keys and values come from the pivot sentence.\nIn principle, MLM and TLM can learn some implicit align-\nment information during model training. However, the align-\nment process in MLM is inefﬁcient since the shared word\npieces only account for a small proportion of the whole cor-\npus, resulting in the difﬁculty of expanding the shared in-\nformation to align the whole corpus. TLM also lacks effort\nin alignment between the source and target sentences since\nTLM concatenates the sentence pair into one sequence, mak-\ning the explicit alignment between the source and target in-\nfeasible. BRLM fully utilizes the alignment information to\nobtain better word-level representation alignment between\ndifferent languages, which better relieves the burden of the\ndomain shift problem .\nTransfer Protocol\nWe consider the typical zero-shot translation scenario in\nwhich a high resource pivot language has parallel data with\nboth source and target languages, while source and target\nlanguages has no parallel data between themselves. Our pro-\nposed cross-lingual pretraining based transfer approach for\nsource→target zero-shot translation is mainly divided into\ntwo phrases: the pretraining phase and the transfer phase.Corpus Language Train Dev Test\nEuroparlDe-En,En-Fr 1M,1M 2,000 2,000\nFr-En,En-Es 1M,1M 2,000 2,000\nRo-En,En-De 0.6M,1.5M 2,000 1,000\nMultiUNAr-En,En-Es\nEn-Ru9.7M,11.3M\n11.6M4,000 4,000\nTable 1: Data Statistics.\nIn the pretraining phase, we ﬁrst pretrain MLM on mono-\nlingual corpora of both source and pivot languages, and con-\ntinue to pretrain TLM or the proposed BRLM on the avail-\nable parallel data between source and pivot languages, in\norder to build a cross-lingual encoder shared by the source\nand pivot languages.\nIn the transfer phase, we train pivot →target NMT model\ninitialized by the cross-lingually pre-trained encoder, and\nﬁnally transfer the trained NMT model to source →target\ntranslation thanks to the shared encoder. Note that during\ntraining pivot→target NMT model, we freeze several layers\nof the cross-lingually pre-trained encoder to avoid the de-\ngeneracy issue.\nFor the']",Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-SA) are the two structures that extend MLM into a bilingual scenario and introduce alignment information during model training.,multi_context,TRUE
59,"What is the context-speciﬁcity of stopwords in ELMo, BERT, and GPT-2?","['Figure 1: In almost all layers of BERT, ELMo, and GPT-2, the word representations are anisotropic (i.e., not\ndirectionally uniform): the average cosine similarity between uniformly randomly sampled words is non-zero.\nThe one exception is ELMo’s input layer; this is not surprising given that it generates character-level embeddings\nwithout using context. Representations in higher layers are generally more anisotropic than those in lower ones.\nELMo as well, though there are exceptions: for ex-\nample, the anisotropy in BERT’s penultimate layer\nis much higher than in its ﬁnal layer.\nIsotropy has both theoretical and empirical ben-\neﬁts for static word embeddings. In theory, it\nallows for stronger “self-normalization” during\ntraining (Arora et al., 2017), and in practice, sub-\ntracting the mean vector from static embeddings\nleads to improvements on several downstream\nNLP tasks (Mu et al., 2018). Thus the extreme\ndegree of anisotropy seen in contextualized word\nrepresentations – particularly in higher layers –\nis surprising. As seen in Figure 1, for all three\nmodels, the contextualized hidden layer represen-\ntations are almost all more anisotropic than the in-\nput layer representations, which do not incorpo-\nrate context. This suggests that high anisotropy is\ninherent to, or least a by-product of, the process of\ncontextualization.\n4.2 Context-Speciﬁcity\nContextualized word representations are more\ncontext-speciﬁc in higher layers. Recall from\nDeﬁnition 1 that the self-similarity of a word, in\na given layer of a given model, is the average co-\nsine similarity between its representations in dif-\nferent contexts, adjusted for anisotropy. If the\nself-similarity is 1, then the representations are\nnot context-speciﬁc at all; if the self-similarity is\n0, that the representations are maximally context-\nspeciﬁc. In Figure 2, we plot the average self-\nsimilarity of uniformly randomly sampled wordsin each layer of BERT, ELMo, and GPT-2. For\nexample, the self-similarity is 1.0 in ELMo’s in-\nput layer because representations in that layer are\nstatic character-level embeddings.\nIn all three models, the higher the layer, the\nlower the self-similarity is on average. In other\nwords, the higher the layer, the more context-\nspeciﬁc the contextualized representations. This\nﬁnding makes intuitive sense. In image classiﬁca-\ntion models, lower layers recognize more generic\nfeatures such as edges while upper layers recog-\nnize more class-speciﬁc features (Yosinski et al.,\n2014). Similarly, upper layers of LSTMs trained\non NLP tasks learn more task-speciﬁc represen-\ntations (Liu et al., 2019a). Therefore, it fol-\nlows that upper layers of neural language mod-\nels learn more context-speciﬁc representations, so\nas to predict the next word for a given context\nmore accurately. Of all three models, representa-\ntions in GPT-2 are the most context-speciﬁc, with\nthose in GPT-2’s last layer being almost maxi-\nmally context-speciﬁc.\nStopwords (e.g., ‘the’, ‘of’, ‘to’ ) have among the\nmost context-speciﬁc representations. Across\nall layers, stopwords have among the lowest self-\nsimilarity of all words, implying that their con-\ntextualized representations are among the most\ncontext-speciﬁc. For example, the words with the\nlowest average self-similarity across ELMo’s lay-\ners are ‘and’, ‘of’, ‘’s’, ‘the’ , and ‘to’. This is rel-\natively surprising, given that these words are not\npolysemous. This ﬁnding suggests that the variety', 'Figure 2: The average cosine similarity between representations of the same word in different contexts is called\nthe word’s self-similarity (see Deﬁnition 1). Above, we plot the average self-similarity of uniformly randomly\nsampled words after adjusting for anisotropy (see section 3.4). In all three models, the higher the layer, the lower\nthe self-similarity, suggesting that contextualized word representations are more context-speciﬁc in higher layers.\nof contexts a word appears in, rather than its inher-\nent polysemy, is what drives variation in its con-\ntextualized representations. This answers one of\nthe questions we posed in the introduction: ELMo,\nBERT, and GPT-2 are not simply assigning one of\na ﬁnite number of word-sense representations to\neach word; otherwise, there would not be so much\nvariation in the representations of words with so\nfew word senses.\nContext-speciﬁcity manifests very differently in\nELMo, BERT, and GPT-2. As noted earlier,\ncontextualized representations are more context-\nspeciﬁc in upper layers of ELMo, BERT, and GPT-\n2. However, how does this increased context-\nspeciﬁcity manifest in the vector space? Do word\nrepresentations in the same sentence converge to a\nsingle point, or do they remain distinct from one\nanother while still being distinct from their repre-\nsentations in other contexts? To answer this ques-\ntion, we can measure a sentence’s intra-sentence\nsimilarity. Recall from Deﬁnition 2 that the intra-\nsentence similarity of a sentence, in a given layer\nof a given model, is the average cosine similarity\nbetween each of its word representations and their\nmean, adjusted for anisotropy. In Figure 3, we plot\nthe average intra-sentence similarity of 500 uni-\nformly randomly sampled sentences.\nIn ELMo, words in the same sentence are more\nsimilar to one another in upper layers. As\nword representations in a sentence become more\ncontext-speciﬁc in upper layers, the intra-sentencesimilarity also rises. This suggests that, in prac-\ntice, ELMo ends up extending the intuition behind\nFirth’s (1957) distributional hypothesis to the sen-\ntence level: that because words in the same sen-\ntence share the same context, their contextualized\nrepresentations should also be similar.\nIn BERT, words in the same sentence are more\ndissimilar to one another in upper layers. As\nword representations in a sentence become more\ncontext-speciﬁc in upper layers, they drift away\nfrom one another, although there are exceptions\n(see layer 12 in Figure 3). However, in all lay-\ners, the average similarity between words in the\nsame sentence is still greater than the average sim-\nilarity between randomly chosen words (i.e., the\nanisotropy baseline). This suggests a more nu-\nanced contextualization than in ELMo, with BERT\nrecognizing that although the surrounding sen-\ntence informs a word’s meaning, two words in the\nsame sentence do not necessarily have a similar\nmeaning because they share the same context.\nIn GPT-2, word representations in the same\nsentence are no more similar to each other than\nrandomly sampled words. On average, the un-\nadjusted intra-sentence similarity is roughly the\nsame as the anisotropic baseline, so as seen in Fig-\nure 3, the anisotropy-adjusted intra-sentence simi-\nlarity is close to 0 in most layers of GPT-2. In fact,\nthe intra-sentence similarity is highest in the input\nlayer, which does not contextualize words at all.\nThis is in contrast to ELMo and BERT, where the']","Stopwords have among the most context-specific representations in ELMo, BERT, and GPT-2.",multi_context,TRUE
60,How is sequence labeling used in NLP tasks and what are some related research areas?,"['ing the NER loss. Thus the resulting features of the common\nBi-LSTM are worker invariant and NER sensitive.\nFor evaluation, we create two Chinese NER datasets in\ntwo domains: dialog and e-commerce. We require the crowd\nannotators to label the types of entities, including person,\nsong, brand, product, and so on. Identifying these entities\nis useful for chatbot and e-commerce platforms (Kl ¨uwer\n2011). Then we conduct experiments on the newly created\ndatasets to verify the effectiveness of the proposed adversar-\nial neural network model. The results show that our system\noutperforms very strong baseline systems. In summary, we\nmake the following contributions:\n•We propose a crowd-annotation learning model based on\nadversarial neural networks. The model uses labeled data\ncreated by non-experts to train a NER classiﬁer and simul-\ntaneously learns the common and private features among\nthe non-expert annotators.\n•We create two data sets in dialog and e-commerce do-\nmains by crowd annotations. The experimental results\nshow that the proposed approach performs the best among\nall the comparison systems.\nRelated Work\nOur work is related to three lines of research: Sequence la-\nbeling, Adversarial training, and Crowdsourcing.\nSequence labeling. NER is widely treated as a sequence la-\nbeling problem, by assigning a unique label over each sen-\ntential word (Ratinov and Roth 2009). Early studies on se-\nquence labeling often use the models of HMM, MEMM,\nand CRF (Lafferty et al. 2001) based on manually-crafted\ndiscrete features, which can suffer the feature sparsity prob-\nlem and require heavy feature engineering. Recently, neural\nnetwork models have been successfully applied to sequence\nlabeling (Collobert et al. 2011; Huang, Xu, and Yu 2015;\nLample et al. 2016). Among these work, the model which\nuses Bi-LSTM for feature extraction and CRF for decoding\nhas achieved state-of-the-art performances (Huang, Xu, and\nYu 2015; Lample et al. 2016), which is exploited as the base-\nline model in our work.\nAdversarial Training. Adversarial Networks have achieved\ngreat success in computer vision such as image genera-\ntion (Denton et al. 2015; Ganin et al. 2016). In the NLP\ncommunity, the method is mainly exploited under the set-\ntings of domain adaption (Zhang, Barzilay, and Jaakkola\n2017; Gui et al. 2017), cross-lingual (Chen et al. 2016;\nKim et al. 2017) and multi-task learning (Chen et al. 2017;\nLiu, Qiu, and Huang 2017). All these settings involve the\nfeature divergences between the training and test examples,\nand aim to learn invariant features across the divergences by\nan additional adversarial discriminator, such as domain dis-\ncriminator. Our work is similar to these work but is applies\non crowdsourcing learning, aiming to ﬁnd invariant features\namong different crowdsourcing workers.\nCrowdsourcing. Most NLP tasks require a massive amount\nof labeled training data which are annotated by experts.\nHowever, hiring experts is costly and non-scalable, both in\nterms of time and money. Instead, crowdsourcing is anothersolution to obtain labeled data at a lower cost but with rela-\ntive lower quality than those from experts. Snow et al. (2008)\ncollected labeled results for several NLP tasks from Amazon\nMechanical Turk and demonstrated that non-experts annota-\ntions were quite useful for training new systems. In recent\nyears, a series of work have focused on how to use crowd-\nsourcing data efﬁciently in tasks such as classiﬁcation (Felt\net al. 2015; Bi et al. 2014), and compare quality of crowd\nand expert labels (Dumitrache, Aroyo, and Welty 2017).\nIn sequence labeling tasks, Dredze, Talukdar, and Cram-\nmer (2009) viewed this task as a multi-label problem while\nRodrigues, Pereira, and Ribeiro (2014) took workers iden-\ntities into account by assuming that each sentential word\nwas tagged correctly by one of the crowdsourcing workers\nand proposed a CRF-based model with multiple annotators.\nNguyen et al. (2017) introduced a crowd representation in\nwhich the crowd vectors were added into']","Sequence labeling is widely used in NLP tasks, where it involves assigning a unique label to each word in a sentence. It has been applied to tasks such as named entity recognition (NER). Early studies used models like HMM, MEMM, and CRF with manually-crafted features. More recently, neural network models, particularly those using Bi-LSTM for feature extraction and CRF for decoding, have achieved state-of-the-art performance. Adversarial training has also been applied in NLP, particularly in domain adaptation, cross-lingual learning, and multi-task learning. Crowdsourcing is another approach used to obtain labeled training data at a lower cost, although the quality may be lower than that of expert annotations. Several studies have focused on using crowdsourcing data efficiently, including in sequence labeling tasks.",reasoning,TRUE
61,"What potential benefits can be derived from constructing a corpus of social media posts annotated with dogmatism scores, and how can this corpus be utilized to encourage pro-social behavior in online communities?","['model may allow future researchers to probe these\nquestions more deeply.\n7 Conclusion\nWe have constructed the ﬁrst corpus of social me-\ndia posts annotated with dogmatism scores, allowing\nus to explore linguistic features of dogmatism and\nbuild a predictive model that analyzes new content.\nWe apply this model to Reddit, where we discover\nbehavioral predictors of dogmatism and topical pat-\nterns in the comments of dogmatic users.\nCould we use this computational model to help\nusers shed their dogmatic beliefs? Looking forward,\nour work makes possible new avenues for encourag-\ning pro-social behavior in online communities.\nReferences\n[Cheng et al.2015] Justin Cheng, Cristian Danescu-\nNiculescu-Mizil, and Jure Leskovec. 2015. Antisocial\nbehavior in online discussion communities. arXiv\npreprint arXiv:1504.00680 .\n[Church and Hanks1990] Kenneth Ward Church and\nPatrick Hanks. 1990. Word association norms,\nmutual information, and lexicography. Computational\nlinguistics , 16(1):22–29.\n[Crowson2009] H Michael Crowson. 2009. Does the\ndog scale measure dogmatism? another look at con-\nstruct validity. The Journal of social psychology ,\n149(3):365–383.\n[Danescu-Niculescu-Mizil et al.2013] Cristian Danescu-\nNiculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure\nLeskovec, and Christopher Potts. 2013. A computa-\ntional approach to politeness with application to social\nfactors. arXiv preprint arXiv:1306.6078 .\n[Doroudi et al.2016] Shayan Doroudi, Ece Kamar, Emma\nBrunskill, and Eric Horvitz. 2016. Toward a learning\nscience for complex crowdsourcing tasks. In Proceed-\nings of the 2016 CHI Conference on Human Factors in\nComputing Systems , pages 2623–2634. ACM.\n[El-Nawawy and Powers2010] Mohammed El-Nawawy\nand Shawn Powers. 2010. Al-jazeera english a con-\nciliatory medium in a conﬂict-driven environment?\nGlobal Media and Communication , 6(1):61–84.\n[Ertel1985] S Ertel. 1985. Content analysis: An alter-\nnative approach to open and closed minds. The High\nSchool Journal , 68(4):229–240.\n[Gilbert2012] Eric Gilbert. 2012. Phrases that signal\nworkplace hierarchy. In Proceedings of the ACM 2012\nconference on Computer Supported Cooperative Work ,\npages 1037–1046. ACM.[Gurney et al.2013] Daniel J Gurney, Shelley McKeown,\nJamie Churchyard, and Neil Howlett. 2013. Believe it\nor not: Exploring the relationship between dogmatism\nand openness within non-religious samples. Personal-\nity and Individual Differences , 55(8):936–940.\n[Hasan and Ng2014] Kazi Saidul Hasan and Vincent Ng.\n2014. Why are you taking this stance? identifying and\nclassifying reasons in ideological debates. In EMNLP ,\npages 751–762.\n[Hayes and Krippendorff2007] Andrew F Hayes and\nKlaus Krippendorff. 2007. Answering the call\nfor a standard reliability measure for coding data.\nCommunication methods and measures , 1(1):77–89.\n[Lohman2010] Margaret C Lohman. 2010. An unex-\namined triumvirate: dogmatism, problem solving, and\nhrd. Human Resource Development Review .\n[Martin et al.2011] Matthew M Martin, Sydney M Stag-\ngers, and Carolyn M Anderson. 2011. The relation-\nships between cognitive ﬂexibility with dogmatism,\nintellectual ﬂexibility, preference for consistency, and\nself-compassion. Communication Research Reports ,\n28(3):275–280.\n[McCluskey and Hmielowski2012] Michael McCluskey\nand Jay Hmielowski. 2012. Opinion expression dur-\ning social conﬂict: Comparing online reader com-\nments and letters to the editor. Journalism , 13(3):303–\n319.\n[McKenny2005] John McKenny. 2005. Content analy-\nsis of dogmatism compared with corpus analysis of\nepistemic stance in student essays']","The potential benefits of constructing a corpus of social media posts annotated with dogmatism scores include exploring linguistic features of dogmatism and building a predictive model to analyze new content. This model can be utilized to identify behavioral predictors of dogmatism and topical patterns in the comments of dogmatic users on platforms like Reddit. Looking forward, this work makes possible new avenues for encouraging pro-social behavior in online communities.",reasoning,TRUE
62,"According to the context, which cross-lingual pre-training method demonstrates the most stable and similar cross-lingual representations of sentence pairs on all layers, and achieves the best performance in zero-shot translation?","['SA with back translation also achieves better performance\nthan the original supervised Transformer.\nAnalysis\nSentence Representation. We ﬁrst evaluate the represen-\ntational invariance across languages for all cross-lingual pre-\ntraining methods. Following Arivazhagan et al. (2018), we\nadopt max-pooling operation to collect the sentence rep-\nresentation of each encoder layer for all source-pivot sen-\ntence pairs in the Europarl validation sets. Then we calcu-\nlate the cosine similarity for each sentence pair and aver-\nage all cosine scores. As shown in Figure 3, we can ob-\nserve that, MLM+BRLM-SA has the most stable and similar\ncross-lingual representations of sentence pairs on all layers,\nwhile it achieves the best performance in zero-shot transla-\ntion. This demonstrates that better cross-lingual representa-\ntions can beneﬁt for the process of transfer learning. Besides,\nMLM+BRLM-HA is not as superior as MLM+BRLM-\nSA and even worse than MLM+TLM on Fr-En, since\nMLM+BRLM-HA may suffer from the wrong alignment\nknowledge from an external aligner tool. We also ﬁnd an in-\nteresting phenomenon that as the number of layers increases,\nthe cosine similarity decreases.\nContextualized Word Representation. We further sam-\nple an English-Russian sentence pair from the MultiUN\nvalidation sets and visualize the cosine similarity between\nhidden states of the top encoder layer to further investi-\ngate the difference of all cross-lingual pre-training meth-\nods. As shown in Figure 4, the hidden states generated by\nMLM+BRLM-SA have higher similarity for two aligned\nwords. It indicates that MLM+BRLM-SA can gain bet-\nter word-level representation alignment between source and\npivot languages, which better relieves the burden of the do-\nmain shift problem .\nThe Effect of Freezing Parameters. To freeze parame-\nters is a common strategy to avoid catastrophic forgetting in\ntransfer learning (Howard and Ruder 2018). Table 4 shows\nthe performance of transfer learning with freezing different\nlayers on MultiUN test set, in which En →Ru denotes the\nparent model, Ar→Ru and Es→Ru are two child models,\nand all models are based on MLM+BRLM-SA. We can ﬁnd\nthat updating all parameters during training will cause a no-\ntable drop on the zero-shot direction due to the catastrophic\nforgetting. On the contrary, freezing all the parameters leads\nto the decline on supervised direction because the language\nfeatures extracted during pre-training is not sufﬁcient for\nMT task. Freezing the ﬁrst four layers of the transformer\nshows the best performance and keeps the balance between\npre-training and ﬁne-tuning.\nConclusion\nIn this paper, we propose a cross-lingual pretraining based\ntransfer approach for the challenging zero-shot translation\ntask, in which source and target languages have no parallel\ndata, while they both have parallel data with a high resource\n(a) MLM\n (b) MLM+TLM\n(c) MLM+BRLM-HA\n (d) MLM+BRLM-SA\nFigure 4: Cosine similarity visualization at word level given\nan English-Russian sentence pair from the MultiUN valida-\ntion sets. Brighter indicates higher similarity.\nFreezing Layers En→Ru Ar→Ru Es→Ru\nNone 37.80 16.09 19.80\n2 37.79 21.47 28.35\n4 37.55 25.49 30.47\n6 35.31 22.90 28.22\nTable 4: BLEU score of freezing different layers. The num-\nber in Freezing Layers column denotes that the number of\nencoder layers will not be updated.\npivot language. With the aim of building the language in-\nvariant representation between source and pivot languages\nfor smooth transfer of the parent model of pivot →target di-\nrection to the child model of source →target direction, we in-\ntroduce one monolingual pretraining method and two bilin-\ngual pretraining methods to construct an universal encoder\nfor the source and pivot languages. Experiments on public\ndatasets show that our approaches signiﬁcantly outperforms\nseveral strong baseline systems, and manifest the language\ninvariance characteristics in both sentence level and word\nlevel neural representations.\nAcknowledgments\nWe would like', 'MultiUN Ar,Es,Ru↔En\nDirection Ar→Es Es→Ar Ar→Ru Ru→Ar Es→Ru Ru→Es A-ZST A-ST\nBaselines\nCross-lingual Transfer 10.26 12.44 4.58 4.42 13.80 7.93 8.90 44.73\nMNMT(Johnson et al. 2016) 27.40 20.18 15.12 16.19 17.88 27.93 20.78 43.95\nPivoting m 42.29 30.15 27.23 26.16 29.57 40.08 32.58 43.95\nProposed Cross-lingual Pretraining Based Transfer\nMLM 16.50 23.41 9.61 14.23 22.80 23.66 18.36 44.25\nMLM+TLM 25.98 26.55 16.84 20.07 25.91 29.52 24.14 43.71\nMLM+BRLM-HA 29.05 27.58 18.10 20.42 25.39 30.96 25.25 44.67\nMLM+BRLM-SA 36.01 31.08 25.49 25.06 30.47 36.01 30.68 44.54\nAdding Back Translation\nMNMT* (Gu et al. 2019) 39.72 28.05 24.67 24.43 27.41 38.01 30.38 43.98\nMLM 40.98 31.53 26.06 26.69 31.28 40.02 32.76 44.28\nMLM+TLM 41.15 29.77 27.61 27.74 31.02 40.37 32.39 44.14\nMLM+BRLM-HA 41.74 31.89 27.24 27.54 31.29 40.34 33.35 44.52\nMLM+BRLM-SA 44.17 33.20 29.01 28.91 32.53 41.93 34.95 45.49\nTable 3: Results on MultiUN test sets. The six zero-shot translation directions are evaluated. The column “A-ZST"" reports av-\neraged BLEU of zero-shot translation, while the column “A-ST"" reports averaged BLEU of supervised pivot →target direction.\n(a) Fr-En\n (b) De-En\n (c) Ro-En\nFigure 3: Cosine similarity between sentence representation of each encoder layer across all source-pivot sentence pairs in the\nEuroparl validation set.\ncoder trained on both large-scale monolingual data and par-\nallel data between multiple languages.\nMLM alone that does not use source ↔pivot parallel data\nperforms much better than the cross-lingual transfer, and\nachieves comparable results to pivoting. When MLM is\ncombined with TLM or the proposed BRLM, the perfor-\nmance is further improved. MLM+BRLM-SA performs the\nbest, and is better than MLM+BRLM-HA indicating that\nsoft alignment is helpful than hard alignment for the cross-\nlingual pretraining.\nResults on MultiUN Dataset. Like experimental results\non Europarl, MLM+BRLM-SA performs the best among\nall proposed cross-lingual pretraining based transfer ap-\nproaches as shown in Table 3. When comparing systems\nconsisting of one encoder-decoder model for all zero-shot\ntranslation, our approaches performs signiﬁcantly better\nthan MNMT (Johnson et al. 2016).Although it is challenging for one model to translate all\nzero-shot directions between multiple distant language pairs\nof MultiUN, MLM+BRLM-SA still achieves better perfor-\nmances on Es→Ar and Es→Ru than strong pivoting m,\nwhich uses MNMT to translate source to pivot then to tar-\nget in two separate steps with each step receiving supervised\nsignal of parallel corpora. Our approaches surpass pivoting m\nin all zero-shot directions by adding back translation (Sen-\nnrich, Haddow, and Birch 2015) to generate pseudo parallel\nsentences for all zero-shot directions based on our pretrained\nmodels such as MLM+BRLM-SA, and further training our\nuniversal encoder-decoder model with these pseudo data.\nGu et al. (2019) introduces back translation into MNMT,\nwhile we adopt it in our transfer approaches. Finally, our\nbest MLM+BRLM-SA with back translation outperforms\npivoting mby 2.4 BLEU points averagely, and outperforms\nMNMT (Gu et al. 2019) by 4.6']",MLM+BRLM-SA,reasoning,TRUE
63,"What regularities are observed in the phoneme embeddings of the multilingual model, and how do these embeddings relate to the graphemes used in different languages?","['ilingual\nmodel, on the other hand, could potentially gener-', 'ate phonemes from the inventories of any language\nit has been trained on.\nEven if LangID-High does not present a more\naccurate result, it does present a more compact\none: LangID-High is 15.4 MB, while the com-\nbined wFST high resource models are 197.5 MB.\nModel WER WER 100 PER\nwFST 44.17 21.97 14.70\nLangID-High 47.88 15.50 16.89\nLangID-All 48.76 15.78 17.35\nNoLangID-High 69.72 29.24 35.16\nNoLangID-All 69.82 29.27 35.47\nTable 5: High Resource Results\n6.6 Results on Unseen Languages\nFinally, we report our models’ results on unseen\nlanguages in Table 6. The unseen languages are\nany that are present in the test corpus but absent\nfrom the training data. Deri and Knight did not\nreport results speciﬁcally on these languages. Al-\nthough the NoLangID models sometimes do better\non WER 100, even here the LangID models have a\nslight advantage in WER and PER. This is some-\nwhat surprising because the LangID models have\nnot learned embeddings for the language ID to-\nkens of unseen languages. Perhaps negative asso-\nciations are also being learned, driving the model\ntowards predicting more common pronunciations\nfor unseen languages.\nModel WER WER 100 PER\nLangID-High 85.94 58.10 53.06\nLangID-Adapted 87.78 68.40 65.62\nLangID-All 86.27 62.31 54.33\nNoLangID-High 88.52 58.21 62.02\nNoLangID-Adapted 91.27 57.61 74.07\nNoLangID-All 89.96 56.29 62.79\nTable 6: Results on languages not in the training\ncorpus\n7 Discussion\n7.1 Language ID Tokens\nAdding a language ID token always improves\nresults in cases where an embedding has been\nlearned for that token. The power of these em-\nbeddings is demonstrated by what happens whenone feeds the same input word to the model with\ndifferent language tokens, as is seen in Table 7.\nImpressively, this even works when the source se-\nquence is in the wrong script for the language, as\nis seen in the entry for Arabic.\nLanguage Pronunciation\nEnglish d Z u: æI s\nGerman j U t s @\nSpanish x w i T eﬂ\nItalian d Z u i t S e\nPortuguese Z w i s ˜i\nTurkish Z U I d ” Z E\nArabic j u: i s\nTable 7: The word ‘juice’ translated by the\nLangID-All model with various language ID to-\nkens. The incorrect English pronunciation rhymes\nwith the system’s result for ‘ice’\n7.2 Language Embeddings\nBecause these language ID tokens are so useful,\nit would be good if they could be effectively es-\ntimated for unseen languages. ¨Ostling and Tiede-\nmann (2017) found that the language vectors their\nmodels learned correlated well to genetic relation-\nships, so it would be interesting to see if the em-\nbeddings our source encoder learned for the lan-\nguage ID tokens showed anything similar. In a few\ncases they do (the languages closest to German\nin the vector space are Luxembourgish, Bavarian,\nand Yiddish, all close relatives). However, for the\nmost part the structure of these vectors is not in-\nterpretable. Therefore, it would be difﬁcult to esti-\nmate the embedding for an unseen language, or to\n“borrow” the language ID token of a similar lan-\nguage. A more promising way forward is to ﬁnd a\nmodel that uses an externally constructed typolog-\nical representation of the language.\n7.3 Phoneme Embeddings\nIn contrast to the language embeddings, the\nphoneme embeddings appear to show many reg-\nularities (see Table 8). This is a sign that our\nmultilingual model learns similar embeddings for\nphonemes that are written with the same grapheme\nin different languages. These phonemes tend to be\nphonetically similar to each other.\nPerhaps the structure of the phoneme embed-\nding space is what leads to our']",The phoneme embeddings of the multilingual model show regularities and tend to be phonetically similar to each other.,reasoning,TRUE
64,How does the context-specificity of contextualized word representations change across the layers of neural language models?,"['Figure 1: In almost all layers of BERT, ELMo, and GPT-2, the word representations are anisotropic (i.e., not\ndirectionally uniform): the average cosine similarity between uniformly randomly sampled words is non-zero.\nThe one exception is ELMo’s input layer; this is not surprising given that it generates character-level embeddings\nwithout using context. Representations in higher layers are generally more anisotropic than those in lower ones.\nELMo as well, though there are exceptions: for ex-\nample, the anisotropy in BERT’s penultimate layer\nis much higher than in its ﬁnal layer.\nIsotropy has both theoretical and empirical ben-\neﬁts for static word embeddings. In theory, it\nallows for stronger “self-normalization” during\ntraining (Arora et al., 2017), and in practice, sub-\ntracting the mean vector from static embeddings\nleads to improvements on several downstream\nNLP tasks (Mu et al., 2018). Thus the extreme\ndegree of anisotropy seen in contextualized word\nrepresentations – particularly in higher layers –\nis surprising. As seen in Figure 1, for all three\nmodels, the contextualized hidden layer represen-\ntations are almost all more anisotropic than the in-\nput layer representations, which do not incorpo-\nrate context. This suggests that high anisotropy is\ninherent to, or least a by-product of, the process of\ncontextualization.\n4.2 Context-Speciﬁcity\nContextualized word representations are more\ncontext-speciﬁc in higher layers. Recall from\nDeﬁnition 1 that the self-similarity of a word, in\na given layer of a given model, is the average co-\nsine similarity between its representations in dif-\nferent contexts, adjusted for anisotropy. If the\nself-similarity is 1, then the representations are\nnot context-speciﬁc at all; if the self-similarity is\n0, that the representations are maximally context-\nspeciﬁc. In Figure 2, we plot the average self-\nsimilarity of uniformly randomly sampled wordsin each layer of BERT, ELMo, and GPT-2. For\nexample, the self-similarity is 1.0 in ELMo’s in-\nput layer because representations in that layer are\nstatic character-level embeddings.\nIn all three models, the higher the layer, the\nlower the self-similarity is on average. In other\nwords, the higher the layer, the more context-\nspeciﬁc the contextualized representations. This\nﬁnding makes intuitive sense. In image classiﬁca-\ntion models, lower layers recognize more generic\nfeatures such as edges while upper layers recog-\nnize more class-speciﬁc features (Yosinski et al.,\n2014). Similarly, upper layers of LSTMs trained\non NLP tasks learn more task-speciﬁc represen-\ntations (Liu et al., 2019a). Therefore, it fol-\nlows that upper layers of neural language mod-\nels learn more context-speciﬁc representations, so\nas to predict the next word for a given context\nmore accurately. Of all three models, representa-\ntions in GPT-2 are the most context-speciﬁc, with\nthose in GPT-2’s last layer being almost maxi-\nmally context-speciﬁc.\nStopwords (e.g., ‘the’, ‘of’, ‘to’ ) have among the\nmost context-speciﬁc representations. Across\nall layers, stopwords have among the lowest self-\nsimilarity of all words, implying that their con-\ntextualized representations are among the most\ncontext-speciﬁc. For example, the words with the\nlowest average self-similarity across ELMo’s lay-\ners are ‘and’, ‘of’, ‘’s’, ‘the’ , and ‘to’. This is rel-\natively surprising, given that these words are not\npolysemous. This ﬁnding suggests that the variety']",The context-specificity of contextualized word representations decreases as the layers of neural language models increase.,reasoning,TRUE
65,"According to the context, what is the performance of the BERT-based model in the MEDDOCAN 2019 shared task compared to other systems?","['be more dangerous than the unintended over-obfuscation of\nnon-sensitive text.\nFurther, we have conducted an additional experiment on\nthis dataset by progressively reducing the training data for\nall the compared systems. The BERT-based model shows\nthe highest robustness to training-data scarcity, loosing only\n7 points of F1-score when trained on 230 instances instead\nof 21,371. These observation are in line with the results ob-\ntained by the NLP community using BERT for other tasks.\nThe experiments with the MEDDOCAN 2019 shared task\ndataset follow the same pattern. In this case, the BERT-\nbased model falls 0.3 F1-score points behind the shared task\nwinning system, but it would have achieved the second po-\nsition in the competition with no further reﬁnement.\nSince we have used a pre-trained multilingual BERT model,\nthe same approach is likely to work for other languages just\nby providing some labelled training data. Further, this is the\nsimplest ﬁne-tuning that can be performed based on BERT.\nMore sophisticated ﬁne-tuning layers could help improve\nthe results. For example, it could be expected that a CRF\nlayer helped enforce better BIO tagging sequence predic-\ntions. Precisely, Mao and Liu (2019) participated in the\nMEDDOCAN competition using a BERT+CRF architec-\nture, but their reported scores are about 3 points lower than\nour implementation. From the description of their work, it\nis unclear what the source of this score difference could be.\nFurther, at the time of writing this paper, new multilingual\npre-trained models and Transformer architectures have be-\ncome available. It would not come as a surprise that these\nnew resources and systems –e.g., XLM-RoBERTa (Con-\nneau et al., 2019) or BETO (Wu and Dredze, 2019), a BERT\nmodel fully pre-trained on Spanish texts– further advanced\nthe state of the art in this task.\n6. Acknowledgements\nThis work has been supported by Vicomtech and partially\nfunded by the project DeepReading (RTI2018-096846-B-\nC21, MCIU/AEI/FEDER,UE).\n7. Bibliographical References\nAbouelmehdi, K., Beni-Hessane, A., and Khalouﬁ, H.\n(2018). Big healthcare data: preserving security and pri-\nvacy. Journal of Big Data , 5(1):1–18.\nAgerri, R., Bermudez, J., and Rigau, G. (2014). IXA\npipeline: Efﬁcient and Ready to Use Multilingual NLP\ntools. In Proceedings of the 9th Language Resources and\nEvaluation Conference (LREC 2014) , pages 3823–3828.\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary,\nV ., Wenzek, G., Guzm ´an, F., Grave, E., Ott, M.,\nZettlemoyer, L., and Stoyanov, V . (2019). Unsuper-\nvised Cross-lingual Representation Learning at Scale.\narXiv:1911.02116 .\nDernoncourt, F., Lee, J. Y ., Uzuner, ¨O., and Szolovits, P.\n(2016). De-identiﬁcation of Patient Notes with Recur-\nrent Neural Networks. Journal of the American Medical\nInformatics Association , 24(3):596–606.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K.\n(2019). BERT: Pre-training of Deep Bidirectional Trans-\nformers for Language Understanding. In Proceedings of\nthe 2019 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Pa-\npers) , pages 4171–4186.\nGarc ´ıa-Sardi ˜na, L. (2018). Automating the anonymisa-\ntion of textual corpora. Master’s thesis, University of the\nBasque Country (UPV/EHU).\nHassan, F., Domingo-Ferrer, J., and Soria-Comas, J.\n(2018). Anonimizaci ´on de datos no estructurados a\ntrav´es del reconocimiento de entidades nominadas. In', '0 50 1000.40.60.81\ntrain data (%)precision\n0 50 1000.40.60.81\ntrain data (%)recall\n0 50 1000.40.60.81\ntrain data (%)F1-score\nCRF spaCy BERT\nFigure 2: Performance decay with decreasing amounts of training data on the sensitive information detection task in the\nNUB ES-PHI corpus\nDetection Classiﬁcation\nPrec Rec F1 Prec Rec F1\nCRF (Perez et al., 2019) 0.977 0.943 0.960 0.971 0.937 0.954\nspaCy (Perez et al., 2019) 0.967 0.953 0.965 0.965 0.947 0.956\nNLND S2 (Lange et al., 2019) 0.976 0.973 0.974 0.971 0.968 0.970\nNLND S3 (Lange et al., 2019) 0.975 0.975 0.975 0.970 0.969 0.970\nBERT + CRF (Mao and Liu, 2019) 0.968 0.919 0.943 0.965 0.912 0.937\nBERT 0.973 0.972 0.972 0.968 0.967 0.967\nTable 8: Results of Experiment B: MEDDOCAN\na larger extent from recall decline, which is not that marked\nin the case of BERT. Overall, these results indicate that the\ntransfer-learning achieved through the BERT multilingual\npre-trained model not only helps obtain better results, but\nalso lowers the need of manually labelled data for this ap-\nplication domain.\n4.2. Experiment B: MEDDOCAN\nThe results of the two MEDDOCAN scenarios –detection\nand classiﬁcation– are shown in Table 8. These results fol-\nlow the same pattern as in the previous experiments, with\nthe CRF classiﬁer being the most precise of all, and BERT\noutperforming both the CRF and spaCy classiﬁers thanks to\nits greater recall. We also show the results of Mao and Liu\n(2019) who, despite of having used a BERT-based system,\nachieve lower scores than our models. The reason why it\nshould be so remain unclear.\nWith regard to the winner of the MEDDOCAN shared\ntask, the BERT-based model has not improved the scores\nobtained by neither the domain-dependent (S3) nor the\ndomain-independent (S2) NLNDE model. However, at-\ntending to the obtained results, BERT remains only 0.3 F1-\nscore points behind, and would have achieved the second\nposition among all the MEDDOCAN shared task competi-\ntors. Taking into account that only 3% of the gold labels\nremain incorrectly annotated, the task can be considered al-\nmost solved, and it is not clear if the differences among the\nsystems are actually signiﬁcant, or whether they stem from\nminor variations in initialisation or a long-tail of minor la-belling inconsistencies.\n5. Conclusions and Future Work\nIn this work we have brieﬂy introduced the problems re-\nlated to data privacy protection in clinical domain. We have\nalso described some of the groundbreaking advances on the\nNatural Language Processing ﬁeld due to the appearance of\nTransformers-based deep-learning architectures and trans-\nfer learning from very large general-domain multilingual\ncorpora, focusing our attention in one of its most represen-\ntative examples, Google’s BERT model.\nIn order to assess the performance of BERT for Span-\nish clinical data anonymisation, we have conducted sev-\neral experiments with a BERT-based sequence labelling\napproach using the pre-trained multilingual BERT model\nshared by Google as the starting point for the model train-\ning. We have compared this BERT-based sequence la-\nbelling against other methods and systems. One of the ex-\nperiments uses the MEDDOCAN 2019 shared task dataset,\nwhile the other uses a novel Spanish clinical reports dataset\ncalled NUB ES-PHI.\nThe results of the experiments show that, in NUB ES-PHI,\nthe BERT-based model outperforms the other systems with-\nout requiring any adaptation or domain-speciﬁc feature en-\ngineering, just by being trained on the provided labelled\ndata. Interestingly, the BERT-based model obtains a re-\n']",The BERT-based model falls 0.3 F1-score points behind the shared task winning system in the MEDDOCAN 2019 shared task.,reasoning,TRUE
66,What does the confusion matrix in Table VI illustrate about the trained recurrent models' ability to recognize tokens that start named entities compared to descriptor words for organizations and locations?,"['TABLE III: Evaluation results for named entity recognition algorithms\nAlgorithmdev test\nPrecision Recall F1 Precision Recall F1\nStanford NER 76.86 70.62 73.61 78.46 46.52 58.41\nspaCy 2.0 68.19 71.86 69.98 64.83 55.77 59.96\nChar-biLSTM+biLSTM+CRF 77.21 74.81 75.99 73.27 54.14 62.23\nTABLE IV: Evaluation results for Char-biLSTM+biLSTM+CRF\nWord embeddingstrain embeddings=False train embeddings=True\ndev F1 test F1 dev F1 test F1\nGloVe (dim=50) 74.18 56.46 75.99 62.23\nGloVe (dim=100) 73.94 58.52 74.83 61.54\nGloVe (dim=200) 75.00 58.37 74.97 59.78\nGloVe (dim=300) 74.21 59.66 74.75 59.92\nTABLE V: Evaluation results for spaCy 2.0 NER\nWord embeddingsdev test\nPrecision Recall F1 Precision Recall F1\nGloVe (dim=50) 69.31 71.26 70.27 66.52 51.72 58.20\nGloVe (dim=100) 70.12 72.91 71.49 66.34 53.35 59.14\nGloVe (dim=200) 68.19 71.86 69.98 64.83 55.77 59.96\nGloVe (dim=300) 70.08 71.80 70.93 66.61 52.94 59.00\nVI. D ISCUSSION\nTable III shows the average scores of evaluated models. The\nhighest F1 score was achieved by the recurrent model using\na batch size of 8 and Adam optimizer with an initial learning\nrate of 0.001. Updating word embeddings during training\nalso noticeably improved the performance. GloVe word vecto r\nmodels of four different sizes (50, 100, 200, and 300) were\ntested, with vectors of size 50 producing the best results (T able\nIV).\nFor spaCy 2.0 named entity recognizer, the same word\nembedding models were tested. However, in this case the per-\nformance of 200-dimensional embeddings was highest (Table\nV). Unsurprisingly, both deep learning models outperforme d\nthe feature-based Stanford recognizer in recall, the latte r\nhowever demonstrated noticeably higher precision.\nIt is clear that the development set of automatically gen-\nerated examples was not an ideal indicator of models’ per-\nformance on gold-standard test set. Higher development set\nscores often led to lower test scores as seen in the evalua-\ntion results for spaCy 2.0 and Char-biLSTM+biLSTM+CRF\n(Tables V and IV). Analysis of errors on the development\nset revealed that many were caused by the incomplete-\nness of annotations, when named entity recognizers correct ly\npredicted entities that were absent from annotations (e.g.\n[/Armkhe/Armse/Armho/Armmen/armendash/armini LOC ] (USSR’s), [ /Armda/armini/armnu/armayb/armmen/armvo/armnu ORG ] (the_Dinamo),\n[/Armpe/armini/armre/armyech/armnu/armyech/armhi/armayb/armnu /armto/armyech/armre/armayb/armken/armghat/armza/armvo/armvyun LOC ] (Iberian Peninsula’s) etc).\nSimilarly, the recognizers often correctly ignored non-en tities\nthat are incorrectly labeled in data (e.g. [ /armo/armse/armmen/armayb/armnu/armnu/armyech/armre/armini PER ],\n[/armken/armvo/armnu/armse/armyech/armre/armvev/armayb/armtyun/armvo/armre/armini/armayb/armnu ORG ] etc).\nGenerally, tested models demonstrated relatively high pre -cision of recognizing tokens that started named entities, b ut\nfailed to do so with descriptor words for organizations and,\nto a certain degree, locations. The confusion matrix for one\nof the trained recurrent models illustrates that differenc e\n(Table VI). This can be partly attributed to the quality of\ngenerated']",nan,reasoning,TRUE
67,"What is the main problem that hinders transfer learning in zero-shot translation for Neural Machine Translation, and how does the proposed approach address this problem?","['Cross-lingual Pre-training Based Transfer for Zero-shot Neural\nMachine Translation\nBaijun Ji‡, Zhirui Zhang§, Xiangyu Duan†‡∗, Min Zhang†‡, Boxing Chen§and Weihua Luo§\n†Institute of Artiﬁcial Intelligence, Soochow University, Suzhou, China\n‡School of Computer Science and Technology, Soochow University, Suzhou, China\n§Alibaba DAMO Academy, Hangzhou, China\n‡bjji@stu.suda.edu.cn†{xiangyuduan, minzhang}@suda.edu.cn\n§{zhirui.zzr,boxing.cbx,weihua.luowh}@alibaba-inc.com\nAbstract\nTransfer learning between different language pairs has shown\nits effectiveness for Neural Machine Translation (NMT) in\nlow-resource scenario. However, existing transfer methods\ninvolving a common target language are far from success in\nthe extreme scenario of zero-shot translation, due to the lan-\nguage space mismatch problem between transferor (the par-\nent model) and transferee (the child model) on the source\nside. To address this challenge, we propose an effective trans-\nfer learning approach based on cross-lingual pre-training. Our\nkey idea is to make all source languages share the same fea-\nture space and thus enable a smooth transition for zero-shot\ntranslation. To this end, we introduce one monolingual pre-\ntraining method and two bilingual pre-training methods to\nobtain a universal encoder for different languages. Once the\nuniversal encoder is constructed, the parent model built on\nsuch encoder is trained with large-scale annotated data and\nthen directly applied in zero-shot translation scenario. Exper-\niments on two public datasets show that our approach signif-\nicantly outperforms strong pivot-based baseline and various\nmultilingual NMT approaches.\nIntroduction\nAlthough Neural Machine Translation (NMT) has domi-\nnated recent research on translation tasks (Wu et al. 2016;\nVaswani et al. 2017; Hassan et al. 2018), NMT heavily relies\non large-scale parallel data, resulting in poor performance\non low-resource or zero-resource language pairs (Koehn\nand Knowles 2017). Translation between these low-resource\nlanguages (e.g., Arabic →Spanish) is usually accomplished\nwith pivoting through a rich-resource language (such as En-\nglish), i.e., Arabic (source) sentence is translated to En-\nglish (pivot) ﬁrst which is later translated to Spanish (tar-\nget) (Kauers et al. 2002; de Gispert and Mariño 2006).\nHowever, the pivot-based method requires doubled decoding\ntime and suffers from the propagation of translation errors.\nOne common alternative to avoid pivoting in NMT is\ntransfer learning (Zoph et al. 2016; Nguyen and Chiang\n2017; Kocmi and Bojar 2018; Kim et al. 2019) which lever-\nages a high-resource pivot →target model ( parent ) to ini-\n∗Corresponding Author.\nCopyright c⃝2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The circle and triangle dots represent source sen-\ntences in different language l1andl2, and the square dots\nmeans target sentences in language l3. A sample of transla-\ntion pairs is connected by the dashed line. We would like to\nforce each of the translation pairs has the same latent rep-\nresentation as the right part of the ﬁgure so as to transfer\nl1→l3model directly to l2→l3model.\ntialize a low-resource source →target model ( child ) that is\nfurther optimized with a small amount of available paral-\nlel data. Although this approach has achieved success in\nsome low-resource language pairs, it still performs very\npoorly in extremely low-resource or zero-resource transla-\ntion scenario. Speciﬁcally, Kocmi and Bojar (2018) reports\nthat without any child model training data, the performance\nof the parent model on the child test set is miserable.\nIn this work, we argue that the language space mis-\nmatch problem, also named domain shift problem (Fu et al.\n2015), brings about the zero-shot translation failure in trans-\nfer learning. It is because transfer learning has no explicit\ntraining process to guarantee that the source and pivot', 'as shown in the right of Figure 1. One way to achieve this\ngoal is the ﬁne-tuning technique, which forces the model to\nforget the speciﬁc knowledge from parent data and learn new\nfeatures from child data. However, the domain shift problem\nstill exists, and the demand of parallel child data for ﬁne-\ntuning heavily hinders transfer learning for NMT towards\nthe zero-resource setting.\nIn this paper, we explore the transfer learning in\na common zero-shot scenario where there are a lot\nof source↔pivot and pivot↔target parallel data but no\nsource↔target parallel data. In this scenario, we propose\na simple but effective transfer approach, the key idea\nof which is to relieve the burden of the domain shift\nproblem by means of cross-lingual pre-training. To this\nend, we ﬁrstly investigate the performance of two exist-\ning cross-lingual pre-training methods proposed by Lam-\nple and Conneau (2019) in zero-shot translation scenario.\nBesides, a novel pre-training method called BRidge Lan-\nguage Modeling (BRLM) is designed to make full use of the\nsource↔pivot bilingual data to obtain a universal encoder\nfor different languages. Once the universal encoder is con-\nstructed, we only need to train the pivot →target model and\nthen test this model in source →target direction directly. The\nmain contributions of this paper are as follows:\n•We propose a new transfer learning approach for NMT\nwhich uses the cross-lingual language model pre-training\nto enable a high performance on zero-shot translation.\n•We propose a novel pre-training method called BRLM,\nwhich can effectively alleviates the distance between dif-\nferent source language spaces.\n•Our proposed approach signiﬁcantly improves zero-shot\ntranslation performance, consistently surpassing pivot-\ning and multilingual approaches. Meanwhile, the perfor-\nmance on supervised translation direction remains the\nsame level or even better when using our method.\nRelated Work\nIn recent years, zero-shot translation in NMT has attracted\nwidespread attention in academic research. Existing meth-\nods are mainly divided into four categories: pivot-based\nmethod, transfer learning, multilingual NMT, and unsuper-\nvised NMT.\n•Pivot-based Method is a common strategy to obtain a\nsource→target model by introducing a pivot language.\nThis approach is further divided into pivoting and pivot-\nsynthetic. While the former ﬁrstly translates a source lan-\nguage into the pivot language which is later translated\nto the target language (Kauers et al. 2002; de Gispert\nand Mariño 2006; Utiyama and Isahara 2007), the lat-\nter trains a source→target model with pseudo data gener-\nated from source-pivot or pivot-target parallel data (Chen\net al. 2017; Zheng, Cheng, and Liu 2017). Although the\npivot-based methods can achieve not bad performance, it\nalways falls into a computation-expensive and parameter-\nvast dilemma of quadratic growth in the number of source\nlanguages, and suffers from the error propagation prob-\nlem (Zhu et al. 2013).•Transfer Learning is ﬁrstly introduced for NMT by\nZoph et al. (2016), which leverages a high-resource par-\nent model to initialize the low-resource child model. On\nthis basis, Nguyen and Chiang (2017) and Kocmi and\nBojar (2018) use shared vocabularies for source/target\nlanguage to improve transfer learning, while Kim, Gao,\nand Ney (2019) relieve the vocabulary mismatch by\nmainly using cross-lingual word embedding. Although\nthese methods are successful in the low-resource scene,\nthey have limited effects in zero-shot translation.\n•Multilingual NMT (MNMT) enables training a single\nmodel that supports translation from multiple source lan-\nguages into multiple target languages, even those unseen\nlanguage pairs (Firat, Cho, and Bengio 2016; Firat et al.\n2016; Johnson et al. 2016; Al-Shedivat and Parikh 2019;\nAharoni, Johnson, and Firat 2019). Aside from sim-\npler deployment, MNMT beneﬁts from transfer learning\nwhere low-resource language pairs are trained together\nwith high-resource ones. However, Gu et al. (2019) point\nout that MNMT for zero-shot translation easily fails, and\nis sensitive to the hyper-parameter setting. Also']",nan,reasoning,TRUE
68,How does the proposed improved relation detection model benefit the KBQA end task and what are its main contributions?,"['cation .arXiv:1704.06194v2  [cs.CL]  27 May 2017', 'Question: what episode was mike kelleythe writer of\nKnowledgeBaseMike\tKelley(American\ttelevision\twriter/producer)Mike\tKelley(American\tbaseball\tplayer)…Entity Linking \nLove\tWill\tFind\ta\tWayUSA…First\tbaseman…episodes_written\nposition_playedRelation Detection(a)(b)Question: what tvshow did grantshowplay onin2008Mike\tKelley?episodes_writtenEntity Linking RelationDetectionGrantShow?starring_rolesseries(date)from2008ConstraintDetectionGrantShow\t(American\tactor)SwingTown\nBigLoveepisodesScoundrelsseries2011from20102008Figure 1: KBQA examples and its three key components. (a) A single relation example. We ﬁrst identify the topic entity with\nentity linking and then detect the relation asked by the question with relation detection (from all relations connecting the topic\nentity). Based on the detected entity and relation, we form a query to search the KB for the correct answer “ Love Will Find a\nWay”. (b) A more complex question containing two entities. By using “ Grant Show ” as the topic entity, we could detect a chain\nof relations “ starring roles-series ” pointing to the answer. An additional constraint detection takes the other entity “ 2008 ” as\na constraint, to ﬁlter the correct answer “ SwingTown ” from all candidates found by the topic entity and relation.\nthat original relation names can sometimes help\nto match longer question contexts, we propose\nto build both relation-level and word-level rela-\ntion representations. Third, we use deep bidirec-\ntional LSTMs ( BiLSTM s) to learn different levels\nof question representations in order to match the\ndifferent levels of relation information. Finally, we\npropose a residual learning method for sequence\nmatching, which makes the model training easier\nand results in more abstract (deeper) question rep-\nresentations, thus improves hierarchical matching.\nIn order to assess how the proposed improved\nrelation detection could beneﬁt the KBQA end\ntask, we also propose a simple KBQA implemen-\ntation composed of two-step relation detection .\nGiven an input question and a set of candidate enti-\nties retrieved by an entity linker based on the ques-\ntion, our proposed relation detection model plays a\nkey role in the KBQA process: (1) Re-ranking the\nentity candidates according to whether they con-\nnect to high conﬁdent relations detected from the\nraw question text by the relation detection model.\nThis step is important to deal with the ambigui-\nties normally present in entity linking results. (2)\nFinding the core relation (chains) for each topic\nentity2selection from a much smaller candidate\nentity set after re-ranking. The above steps are\nfollowed by an optional constraint detection step,\nwhen the question cannot be answered by single\nrelations (e.g., multiple entities in the question).\nFinally the highest scored query from the above\n2Following Yih et al. (2015), here topic entity refers to\nthe root of the (directed) query tree; and core-chain is the\ndirected path of relation from root to the answer node.steps is used to query the KB for answers.\nOur main contributions include: (i) An im-\nproved relation detection model by hierarchical\nmatching between questions and relations with\nresidual learning; (ii) We demonstrate that the im-\nproved relation detector enables our simple KBQA\nsystem to achieve state-of-the-art results on both\nsingle-relation and multi-relation KBQA tasks.\n2 Related Work\nRelation Extraction Relation extraction (RE) is\nan important sub-ﬁeld of information extraction.\nGeneral research in this ﬁeld usually works on a\n(small) pre-deﬁned relation set, where given a text\nparagraph and two target entities, the goal is to\ndetermine whether the text indicates any types of\nrelations between the entities or not. As a result\nRE is usually formulated as a classiﬁcation task .\nTraditional RE methods rely on large amount of\nhand-crafted features (Zhou et al., 2005; Rink and\nHarabagiu, 2010; Sun et al., 2011). Recent re-\nsearch beneﬁts a lot from the advancement of deep\nlearning: from word embeddings (Nguyen and Gr-\nishman, 2014; Gormley et al., 2015) to deep net-\nworks', ' like CNNs and LSTMs (Zeng et al., 2014;\ndos Santos et al., 2015; Vu et al., 2016) and atten-\ntion models (Zhou et al., 2016; Wang et al., 2016).\nThe above research assumes there is a ﬁxed\n(closed) set of relation types, thus no zero-shot\nlearning capability is required. The number\nof relations is usually not large: The widely\nused ACE2005 has 11/32 coarse/ﬁne-grained rela-\ntions; SemEval2010 Task8 has 19 relations; TAC-', 'Question: what episode was mike kelleythe writer of\nKnowledgeBaseMike\tKelley(American\ttelevision\twriter/producer)Mike\tKelley(American\tbaseball\tplayer)…Entity Linking \nLove\tWill\tFind\ta\tWayUSA…First\tbaseman…episodes_written\nposition_playedRelation Detection(a)(b)Question: what tvshow did grantshowplay onin2008Mike\tKelley?episodes_writtenEntity Linking RelationDetectionGrantShow?starring_rolesseries(date)from2008ConstraintDetectionGrantShow\t(American\tactor)SwingTown\nBigLoveepisodesScoundrelsseries2011from20102008Figure 1: KBQA examples and its three key components. (a) A single relation example. We ﬁrst identify the topic entity with\nentity linking and then detect the relation asked by the question with relation detection (from all relations connecting the topic\nentity). Based on the detected entity and relation, we form a query to search the KB for the correct answer “ Love Will Find a\nWay”. (b) A more complex question containing two entities. By using “ Grant Show ” as the topic entity, we could detect a chain\nof relations “ starring roles-series ” pointing to the answer. An additional constraint detection takes the other entity “ 2008 ” as\na constraint, to ﬁlter the correct answer “ SwingTown ” from all candidates found by the topic entity and relation.\nthat original relation names can sometimes help\nto match longer question contexts, we propose\nto build both relation-level and word-level rela-\ntion representations. Third, we use deep bidirec-\ntional LSTMs ( BiLSTM s) to learn different levels\nof question representations in order to match the\ndifferent levels of relation information. Finally, we\npropose a residual learning method for sequence\nmatching, which makes the model training easier\nand results in more abstract (deeper) question rep-\nresentations, thus improves hierarchical matching.\nIn order to assess how the proposed improved\nrelation detection could beneﬁt the KBQA end\ntask, we also propose a simple KBQA implemen-\ntation composed of two-step relation detection .\nGiven an input question and a set of candidate enti-\nties retrieved by an entity linker based on the ques-\ntion, our proposed relation detection model plays a\nkey role in the KBQA process: (1) Re-ranking the\nentity candidates according to whether they con-\nnect to high conﬁdent relations detected from the\nraw question text by the relation detection model.\nThis step is important to deal with the ambigui-\nties normally present in entity linking results. (2)\nFinding the core relation (chains) for each topic\nentity2selection from a much smaller candidate\nentity set after re-ranking. The above steps are\nfollowed by an optional constraint detection step,\nwhen the question cannot be answered by single\nrelations (e.g., multiple entities in the question).\nFinally the highest scored query from the above\n2Following Yih et al. (2015), here topic entity refers to\nthe root of the (directed) query tree; and core-chain is the\ndirected path of relation from root to the answer node.steps is used to query the KB for answers.\nOur main contributions include: (i) An im-\nproved relation detection model by hierarchical\nmatching between questions and relations with\nresidual learning; (ii) We demonstrate that the im-\nproved relation detector enables our simple KBQA\nsystem to achieve state-of-the-art results on both\nsingle-relation and multi-relation KBQA tasks.\n2 Related Work\nRelation Extraction Relation extraction (RE) is\nan important sub-ﬁeld of information extraction.\nGeneral research in this ﬁeld usually works on a\n(small) pre-deﬁned relation set, where given a text\nparagraph and two target entities, the goal is to\ndetermine whether the text indicates any types of\nrelations between the entities or not. As a result\nRE is usually formulated as a classiﬁcation task .\nTraditional RE methods rely on large amount of\nhand-crafted features (Zhou et al., 2005; Rink and\nHarabagiu, 2010; Sun et al., 2011). Recent re-\nsearch beneﬁts a lot from the advancement of deep\nlearning: from word embeddings (Nguyen and Gr-\nishman, 2014; Gormley et al., 2015) to deep net-\nworks']",nan,reasoning,TRUE
69,"What is the hypothesis regarding the difficulty of training deep BiLSTMs without shortcut connections in relation to KB relation detection, based on the experiments conducted?","[' guar-\nantee that the two-levels of question hidden rep-\nresentations are comparable . This is evidenced\nby that during training one layer usually gets a\nweight close to 0 thus is ignored. For exam-\nple, one run gives us weights of -75.39/0.14 for\nthe two layers (we take exponential for the ﬁnal\nweighted sum). It also gives much lower train-\ning accuracy (91.94%) compared to HR-BiLSTM\n(95.67%), suffering from training difﬁculty.\nSecond, compared to our deep BiLSTM with\nshortcut connections, we have the hypothesis that\nfor KB relation detection, training deep BiLSTMs\nis more difﬁcult without shortcut connections . Our\nexperiments suggest that deeper BiLSTM does not\nalways result in lower training accuracy. In the\nexperiments a two-layer BiLSTM converges to\n94.99%, even lower than the 95.25% achieved by a']",The hypothesis is that training deep BiLSTMs without shortcut connections is more difficult for KB relation detection.,reasoning,TRUE
70,How does the proposed adversarial learning module incorporate crowd-annotated NE labels to predict the exact worker?,"['-\nlikelihood objective as the loss function. We apply softmax\nover all candidate output label sequences, thus the probabil-\nity of the crowd-annotated label sequence is computed by:\np(¯ y|X) =exp(\nscore (X,¯ y))\n∑\ny∈YXexp(\nscore (X,y)), (3)\nwhere ¯ yis the crowd-annotated label sequences and YXis\nall candidate label sequence of input X.\nBased on the above formula, the loss function of our base-\nline model is:\nloss(Θ,X,¯ y) =−logp(¯ y|X), (4)\nwhere Θis the set of all model parameters. We use standard\nback-propagation method to minimize the loss function of\nthe baseline CRF model.', 'Worker Adversarial\nAdversarial learning has been an effective mechanism to re-\nsolve the problem of the input features between the training\nand test examples having large divergences (Goodfellow et\nal. 2014; Ganin et al. 2016). It has been successfully applied\non domain adaption (Gui et al. 2017), cross-lingual learn-\ning (Chen et al. 2016) and multi-task learning (Liu, Qiu, and\nHuang 2017). All settings involve feature shifting between\nthe training and testing.\nIn this paper, our setting is different. We are using the\nannotations from non-experts, which are noise and can in-\nﬂuence the ﬁnal performances if they are not properly pro-\ncessed. Directly learning based on the resulting corpus may\nadapt the neural feature extraction into the biased annota-\ntions. In this work, we assume that individual workers have\ntheir own guidelines in mind after short training. For exam-\nple, a perfect worker can annotate highly consistently with\nan expert, while common crowdsourcing workers may be\nconfused and have different understandings on certain con-\ntexts. Based on the assumption, we make an adaption for the\noriginal adversarial neural network to our setting.\nOur adaption is very simple. Brieﬂy speaking, the original\nadversarial learning adds an additional discriminator to clas-\nsify the type of source inputs, for example, the domain cate-\ngory in the domain adaption setting, while we add a discrim-\ninator to classify the annotation workers. Solely the features\nfrom the input sentence is not enough for worker classiﬁ-\ncation. The annotation result of the worker is also required.\nThus the inputs of our discriminator are different. Here we\nexploit both the source sentences and the crowd-annotated\nNE labels as basic inputs for the worker discrimination.\nIn the following, we describe the proposed adversarial\nlearning module, including both the submodels and the train-\ning method. As shown by the left part of Figure 1, the\nsubmodel consists of four parts: (1) a common Bi-LSTM\nover input characters; (2) an additional Bi-LSTM to en-\ncode crowd-annotated NE label sequence; (3) a convolu-\ntional neural network (CNN) to extract features for worker\ndiscriminator; (4) output and prediction.\nCommon Bi-LSTM over Characters\nTo build the adversarial part, ﬁrst we create a new bi-\ndirectional LSTM, named by the common Bi-LSTM:\nhcommon\n1hcommon\n2···hcommon\nn =Bi-LSTM (x1x2···xn).(5)\nAs shown in Figure 1, this Bi-LSTM is constructed over\nthe same input character representations of the private Bi-\nLSTM, in order to extract worker independent features.\nThe resulting features of the common Bi-LSTM are used\nfor both NER and the worker discriminator, different with\nthe features of private Bi-LSTM which are used for NER\nonly. As shown in Figure 1, we concatenate the outputs of\nthe common and private Bi-LSTMs together, and then feed\nthe results into the feed-forward combination layer of the\nNER part. Thus Formula 1 can be rewritten as:\nhner\nt=W(hcommon\nt⊕hprivate\nt) +b, (6)\nwhere Wis wider than the original combination because the\nnewly-added hcommon\nt .Noticeably, although the resulting common features are\nused for the worker discriminator, they actually have no ca-\npability to distinguish the workers. Because this part is ex-\nploited to maximize the loss of the worker discriminator, it\nwill be interpreted in the later training subsection. These fea-\ntures are invariant among different workers, thus they can\nhave less noises for NER. This is the goal of adversarial\nlearning, and we hope the NER being able to ﬁnd useful\nfeatures from these worker independent features.\nAdditional Bi-LSTM over Annotated NER Labels\nIn order to incorporate the annotated NE labels to predict the\nexact worker, we build another bi-directional LSTM (named\nby label Bi-LSTM) based on the crowd-annotated NE label\nsequence. This Bi-LSTM is used for worker discriminator\nonly. During the decoding of the testing phase, we will never\nhave this Bi-', '-\nlikelihood objective as the loss function. We apply softmax\nover all candidate output label sequences, thus the probabil-\nity of the crowd-annotated label sequence is computed by:\np(¯ y|X) =exp(\nscore (X,¯ y))\n∑\ny∈YXexp(\nscore (X,y)), (3)\nwhere ¯ yis the crowd-annotated label sequences and YXis\nall candidate label sequence of input X.\nBased on the above formula, the loss function of our base-\nline model is:\nloss(Θ,X,¯ y) =−logp(¯ y|X), (4)\nwhere Θis the set of all model parameters. We use standard\nback-propagation method to minimize the loss function of\nthe baseline CRF model.']",nan,reasoning,TRUE
71,What is the performance of the BERT-based model compared to other systems in the MEDDOCAN corpus?,"[' The winners of the challenge –the\nNeither-Language-nor-Domain-Experts (NLNDE) (Lange\net al., 2019)– achieved F1-scores as high as 0.975 in the\ntask of sensitive information detection and categorisation\nby using recurrent neural networks with Conditional Ran-\ndom Field (CRF) output layers.\nAt the same challenge, Mao and Liu (2019) occupied the\n8thposition among 18 participants using BERT. According\nto the description of the system, the authors used BERT-\nBase Multilingual Cased and an output CRF layer. How-\never, their system is ∼3 F1-score points below our imple-\nmentation without the CRF layer.\n3. Materials and Methods\nThe aim of this paper is to evaluate BERT’s multilingual\nmodel and compare it to other established machine-learning\nalgorithms in a speciﬁc task: sensitive data detection and\nclassiﬁcation in Spanish clinical free text. This section de-\nscribes the data involved in the experiments and the systems\nevaluated. Finally, we introduce the experimental setup.\n3.1. Data\nTwo datasets are exploited in this article. Both datasets\nconsist of plain text containing clinical narrative written in\nSpanish, and their respective manual annotations of sensi-\ntive information in BRAT (Stenetorp et al., 2012) standoff\nformat2. In order to feed the data to the different algorithms\npresented in Section 3.2., these datasets were transformed\nto comply with the commonly used BIO sequence repre-\nsentation scheme (Ramshaw and Marcus, 1999).\n3.1.1. NUB ES-PHI\nNUB ES(Lima et al., 2019) is a corpus of around 7,000 real\nmedical reports written in Spanish and annotated with nega-\ntion and uncertainty information. Before being published,\nsensitive information had to be manually annotated and re-\nplaced for the corpus to be safely shared. In this article,\n1http://temu.bsc.es/meddocan/\n2https://brat.nlplab.org/standoff.html', 'we work with the NUB ESversion prior to its anonymisa-\ntion, that is, with the manual annotations of sensitive in-\nformation. It follows that the version we work with is not\npublicly available and, due to contractual restrictions, we\ncannot reveal the provenance of the data. In order to avoid\nconfusion between the two corpus versions, we henceforth\nrefer to the version relevant in this paper as NUB ES-PHI\n(from ‘NUB ESwith Personal Health Information’).\nNUB ES-PHI consists of 32,055 sentences annotated for 11\ndifferent sensitive information categories. Overall, it con-\ntains 7,818 annotations. The corpus has been randomly\nsplit into train (72%), development (8%) and test (20%) sets\nto conduct the experiments described in this paper. The size\nof each split and the distribution of the annotations can be\nconsulted in Tables 2 and 3, respectively.\ntrain dev test\n# sentences 23,079 2,565 6,411\n# tokens 379,401 41,936 107,024\nvocabulary 25,304 7,483 12,750\n# annotations 5,562 677 1,579\nTable 2: Size of the NUB ES-PHI corpus\nThe majority of sensitive information in NUB ES-PHI are\ntemporal expressions (‘Date’ and ‘Time’), followed by\nhealthcare facility mentions (‘Hospital’), and the age of the\npatient. Mentions of people are not that frequent, with\nphysician names (‘Doctor’) occurring much more often\nthan patient names (‘Patient’). The least frequent sensitive\ninformation types, which account for ∼10% of the remain-\ning annotations, consist of the patient’s sex, job, and kin-\nship, and locations other than healthcare facilities (‘Loca-\ntion’). Finally, the tag ‘Other’ includes, for instance, men-\ntions to institutions unrelated to healthcare and whether the\npatient is right- or left-handed. It occurs just 36 times.\ntrain dev test\n# % # % # %\nDate 2,165 39 251 37 660 41\nHospital 1,012 18 105 16 275 17\nAge 701 13 133 20 200 13\nTime 608 11 63 9 155 10\nDoctor 486 9 44 6 134 8\nSex 270 5 35 5 71 4\nKinship 158 3 20 3 44 3\nLocation 71 1 10 1 19 1\nPatient 48 1 5 1 11 1\nJob 31 1 3 0 9 1\nOther 12 0 8 1 16 1\nTotal 5,562 100 677 100 1,579 100\nTable 3: Label distribution in the NUB ES-PHI corpus\n3.1.2. The MEDDOCAN corpus\nThe organisers of the MEDDOCAN shared task (Marimon\net al., 2019) curated a synthetic corpus of clinical cases en-\nriched with sensitive information by health documentalists.In this regard, the MEDDOCAN evaluation scenario could\nbe said to be somewhat far from the real use case the tech-\nnology developed for the shared task is supposed to be ap-\nplied in. However, at the moment it also provides the only\npublic means for a rigorous comparison between systems\nfor sensitive health information detection in Spanish texts.\nThe size of the MEDDOCAN corpus is shown in Table 4.\nCompared to NUB ES-PHI (Table 2), this corpus contains\nmore sensitive information annotations, both in absolute\nand relative terms.\ntrain dev test\n# documents 500 250 250\n# tokens 360,407 138,812 132,961\nvocabulary 26,355 15,985 15,397\n# annotations 11,333 5,801 5,661\nTable 4: Size of the MEDDOCAN corpus\nThe sensitive annotation categories considered in MED-\nDOCAN differ in part from those in NUB ES-PHI. Most\nnotably, it contains ﬁner-grained labels for location-related\nmentions –namely, ‘Address’, ‘Territory’, and ‘Country’–,\nand other sensitive information categories that we did not\nencounter in NUB ES-PHI (e.g., identiﬁers, phone num-\nbers, e-mail addresses, etc.). In total, the MEDDOCAN\ncorpus has 21 sensitive information categories. We refer\nthe reader to the organisers’ article (Marimon et al., 2019)\nfor more detailed information about this corpus.\n3.2. Systems\nApart from experimenting with a pre-trained BERT model,\nwe have run experiments with other systems and base-\nlines, to', ' compare them and obtain a better perspective about\nBERT’s performance in these datasets.\n3.2.1. Baseline\nAs the simplest baseline, a sensitive data recogniser and\nclassiﬁer has been developed that consists of regular-\nexpressions and dictionary look-ups. For each category to\ndetect a speciﬁc method has been implemented. For in-\nstance, the Date, Age, Time and Doctor detectors are based\non regular-expressions; Hospital, Sex, Kinship, Location,\nPatient and Job are looked up in dictionaries. The dic-\ntionaries are hand-crafted from the training data available,\nexcept for the Patient’s case, for which the possible can-\ndidates considered are the 100 most common female and\nmale names in Spain according to the Instituto Nacional de\nEstad ´ıstica (INE; Spanish Statistical Ofﬁce ).\n3.2.2. CRF\nConditional Random Fields (CRF) (Lafferty et al., 2001)\nhave been extensively used for tasks of sequential nature. In\nthis paper, we propose as one of the competitive baselines\na CRF classiﬁer trained with sklearn-crfsuite3for Python\n3.5 and the following conﬁguration: algorithm = lbfgs ;\nmaximum iterations = 100; c1 = c2 = 0.1; all transitions\n=true ; optimise = false . The features extracted from\neach token are as follows:\n3https://sklearn-crfsuite.readthedocs.io', '– preﬁxes and sufﬁxes of 2 and 3 characters;\n– the length of the token in characters and the length of\nthe sentence in tokens;\n– whether the token is all-letters, a number, or a se-\nquence of punctuation marks;\n– whether the token contains the character ‘@’;\n– whether the token is the start or end of the sentence;\n– the token’s casing and the ratio of uppercase charac-\nters, digits, and punctuation marks to its length;\n– and, the lemma, part-of-speech tag, and named-entity\ntag given by ixa-pipes4(Agerri et al., 2014) upon\nanalysing the sentence the token belongs to.\nNoticeably, none of the features used to train the CRF clas-\nsiﬁer is domain-dependent. However, the latter group of\nfeatures is language dependent.\n3.2.3. spaCy\nspaCy5is a widely used NLP library that implements state-\nof-the-art text processing pipelines, including a sequence-\nlabelling pipeline similar to the one described by Strubell\net al. (2017). spaCy offers several pre-trained models in\nSpanish, which perform basic NLP tasks such as Named\nEntity Recognition (NER). In this paper, we have trained a\nnew NER model to detect NUB ES-PHI labels. For this\npurpose, the new model uses all the labels of the train-\ning corpus coded with its context at sentence level. The\nnetwork optimisation parameters and dropout values are\nthe ones recommended in the documentation for small\ndatasets6. Finally, the model is trained using batches of\nsize 64. No more features are included, so the classiﬁer is\nlanguage-dependent but not domain-dependent.\n3.2.4. BERT\nAs introduced earlier, BERT has shown an outstanding\nperformance in NERC-like tasks, improving the start-of-\nthe-art results for almost every dataset and language. We\ntake the same approach here, by using the model BERT-\nBase Multilingual Cased7with a Fully Connected (FC)\nlayer on top to perform a ﬁne-tuning of the whole model\nfor an anonymisation task in Spanish clinical data. Our\nimplementation is built on PyTorch8and the PyTorch-\nTransformers library9(Wolf et al., 2019). The training\nphase consists in the following steps (roughly depicted in\nFigure 1):\n1.Pre-processing: since we are relying on a pre-trained\nBERT model, we must match the same conﬁguration\nby using a speciﬁc tokenisation and vocabulary. BERT\nalso needs that the inputs contains special tokens to\nsignal the beginning and the end of each sequence.\n2.Fine-tuning: the pre-processed sequence is fed into\nthe model. BERT outputs the contextual embeddings\nthat encode each of the inputted tokens. This embed-\nding representation for each token is fed into the FC\n4https://ixa2.si.ehu.es/ixa-pipes\n5https://spacy.io\n6https://spacy.io/usage/training\n7https://github.com/google-research/bert\n8https://pytorch.org\n9https://github.com/huggingface/transformers\nFigure 1: Pre-trained BERT with a Fully Connected layer\non top to perform the ﬁne-tuning\nlinear layer after a dropout layer (with a 0.1 dropout\nprobability), which in turn outputs the logits for each\npossible class. The cross-entropy loss function is cal-\nculated comparing the logits and the gold labels, and\nthe error is back-propagated to adjust the model pa-\nrameters.\nWe have trained the model using an AdamW optimiser\n(Loshchilov and Hutter, 2019) with the learning rate set to\n3e-5, as recommended by Devlin et al. (2019), and with\na gradient clipping of 1.0. We also applied a learning-rate\nscheduler that warms up the learning rate from zero to its\nmaximum value as the training progresses, which is also a\ncommon practice. For each experiment set proposed below,\nthe training was run with an early-stopping patience of 15\nepochs. Then, the model that performed best against the\ndevelopment set was used to produce the reported results.\nThe experiments were run on a 64-', 'core server with operat-\ning system Ubuntu 16.04, 250GB of RAM memory, and 4\nGeForce RTX 2080 GPUs with 11GB of memory. The max-\nimum sequence length was set at 500 and the batch size at\n12. In this setting, each epoch –a full pass through all the\ntraining data– required about 10 minutes to complete.\n3.3. Experimental design\nWe have conducted experiments with BERT in the two\ndatasets of Spanish clinical narrative presented in Section\n3.1. The ﬁrst experiment set uses NUB ES-PHI, a corpus\nof real medical reports manually annotated with sensitive\ninformation. Because this corpus is not publicly available,\nand in order to compare the BERT-based model to other re-\nlated published systems, the second set of experiments uses\nthe MEDDOCAN 2019 shared task competition dataset.\nThe following sections provide greater detail about the two\nexperimental setups.\n3.3.1. Experiment A: NUB ES-PHI\nIn this experiment set, we evaluate all the systems presented\nin Section 3.2., namely, the rule-based baseline, the CRF', '0 50 1000.40.60.81\ntrain data (%)precision\n0 50 1000.40.60.81\ntrain data (%)recall\n0 50 1000.40.60.81\ntrain data (%)F1-score\nCRF spaCy BERT\nFigure 2: Performance decay with decreasing amounts of training data on the sensitive information detection task in the\nNUB ES-PHI corpus\nDetection Classiﬁcation\nPrec Rec F1 Prec Rec F1\nCRF (Perez et al., 2019) 0.977 0.943 0.960 0.971 0.937 0.954\nspaCy (Perez et al., 2019) 0.967 0.953 0.965 0.965 0.947 0.956\nNLND S2 (Lange et al., 2019) 0.976 0.973 0.974 0.971 0.968 0.970\nNLND S3 (Lange et al., 2019) 0.975 0.975 0.975 0.970 0.969 0.970\nBERT + CRF (Mao and Liu, 2019) 0.968 0.919 0.943 0.965 0.912 0.937\nBERT 0.973 0.972 0.972 0.968 0.967 0.967\nTable 8: Results of Experiment B: MEDDOCAN\na larger extent from recall decline, which is not that marked\nin the case of BERT. Overall, these results indicate that the\ntransfer-learning achieved through the BERT multilingual\npre-trained model not only helps obtain better results, but\nalso lowers the need of manually labelled data for this ap-\nplication domain.\n4.2. Experiment B: MEDDOCAN\nThe results of the two MEDDOCAN scenarios –detection\nand classiﬁcation– are shown in Table 8. These results fol-\nlow the same pattern as in the previous experiments, with\nthe CRF classiﬁer being the most precise of all, and BERT\noutperforming both the CRF and spaCy classiﬁers thanks to\nits greater recall. We also show the results of Mao and Liu\n(2019) who, despite of having used a BERT-based system,\nachieve lower scores than our models. The reason why it\nshould be so remain unclear.\nWith regard to the winner of the MEDDOCAN shared\ntask, the BERT-based model has not improved the scores\nobtained by neither the domain-dependent (S3) nor the\ndomain-independent (S2) NLNDE model. However, at-\ntending to the obtained results, BERT remains only 0.3 F1-\nscore points behind, and would have achieved the second\nposition among all the MEDDOCAN shared task competi-\ntors. Taking into account that only 3% of the gold labels\nremain incorrectly annotated, the task can be considered al-\nmost solved, and it is not clear if the differences among the\nsystems are actually signiﬁcant, or whether they stem from\nminor variations in initialisation or a long-tail of minor la-belling inconsistencies.\n5. Conclusions and Future Work\nIn this work we have brieﬂy introduced the problems re-\nlated to data privacy protection in clinical domain. We have\nalso described some of the groundbreaking advances on the\nNatural Language Processing ﬁeld due to the appearance of\nTransformers-based deep-learning architectures and trans-\nfer learning from very large general-domain multilingual\ncorpora, focusing our attention in one of its most represen-\ntative examples, Google’s BERT model.\nIn order to assess the performance of BERT for Span-\nish clinical data anonymisation, we have conducted sev-\neral experiments with a BERT-based sequence labelling\napproach using the pre-trained multilingual BERT model\nshared by Google as the starting point for the model train-\ning. We have compared this BERT-based sequence la-\nbelling against other methods and systems. One of the ex-\nperiments uses the MEDDOCAN 2019 shared task dataset,\nwhile the other uses a novel Spanish clinical reports dataset\ncalled NUB ES-PHI.\nThe results of the experiments show that, in NUB ES-PHI,\nthe BERT-based model outperforms the other systems with-\nout requiring any adaptation or domain-speciﬁc feature en-\ngineering, just by being trained on the provided labelled\ndata. Interestingly, the BERT-based model obtains a re-\n']",The BERT-based model outperforms the other systems in the MEDDOCAN corpus.,reasoning,TRUE
72,What is the proposed key concept to address the language space mismatch problem in zero-shot neural machine translation?,"['Cross-lingual Pre-training Based Transfer for Zero-shot Neural\nMachine Translation\nBaijun Ji‡, Zhirui Zhang§, Xiangyu Duan†‡∗, Min Zhang†‡, Boxing Chen§and Weihua Luo§\n†Institute of Artiﬁcial Intelligence, Soochow University, Suzhou, China\n‡School of Computer Science and Technology, Soochow University, Suzhou, China\n§Alibaba DAMO Academy, Hangzhou, China\n‡bjji@stu.suda.edu.cn†{xiangyuduan, minzhang}@suda.edu.cn\n§{zhirui.zzr,boxing.cbx,weihua.luowh}@alibaba-inc.com\nAbstract\nTransfer learning between different language pairs has shown\nits effectiveness for Neural Machine Translation (NMT) in\nlow-resource scenario. However, existing transfer methods\ninvolving a common target language are far from success in\nthe extreme scenario of zero-shot translation, due to the lan-\nguage space mismatch problem between transferor (the par-\nent model) and transferee (the child model) on the source\nside. To address this challenge, we propose an effective trans-\nfer learning approach based on cross-lingual pre-training. Our\nkey idea is to make all source languages share the same fea-\nture space and thus enable a smooth transition for zero-shot\ntranslation. To this end, we introduce one monolingual pre-\ntraining method and two bilingual pre-training methods to\nobtain a universal encoder for different languages. Once the\nuniversal encoder is constructed, the parent model built on\nsuch encoder is trained with large-scale annotated data and\nthen directly applied in zero-shot translation scenario. Exper-\niments on two public datasets show that our approach signif-\nicantly outperforms strong pivot-based baseline and various\nmultilingual NMT approaches.\nIntroduction\nAlthough Neural Machine Translation (NMT) has domi-\nnated recent research on translation tasks (Wu et al. 2016;\nVaswani et al. 2017; Hassan et al. 2018), NMT heavily relies\non large-scale parallel data, resulting in poor performance\non low-resource or zero-resource language pairs (Koehn\nand Knowles 2017). Translation between these low-resource\nlanguages (e.g., Arabic →Spanish) is usually accomplished\nwith pivoting through a rich-resource language (such as En-\nglish), i.e., Arabic (source) sentence is translated to En-\nglish (pivot) ﬁrst which is later translated to Spanish (tar-\nget) (Kauers et al. 2002; de Gispert and Mariño 2006).\nHowever, the pivot-based method requires doubled decoding\ntime and suffers from the propagation of translation errors.\nOne common alternative to avoid pivoting in NMT is\ntransfer learning (Zoph et al. 2016; Nguyen and Chiang\n2017; Kocmi and Bojar 2018; Kim et al. 2019) which lever-\nages a high-resource pivot →target model ( parent ) to ini-\n∗Corresponding Author.\nCopyright c⃝2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The circle and triangle dots represent source sen-\ntences in different language l1andl2, and the square dots\nmeans target sentences in language l3. A sample of transla-\ntion pairs is connected by the dashed line. We would like to\nforce each of the translation pairs has the same latent rep-\nresentation as the right part of the ﬁgure so as to transfer\nl1→l3model directly to l2→l3model.\ntialize a low-resource source →target model ( child ) that is\nfurther optimized with a small amount of available paral-\nlel data. Although this approach has achieved success in\nsome low-resource language pairs, it still performs very\npoorly in extremely low-resource or zero-resource transla-\ntion scenario. Speciﬁcally, Kocmi and Bojar (2018) reports\nthat without any child model training data, the performance\nof the parent model on the child test set is miserable.\nIn this work, we argue that the language space mis-\nmatch problem, also named domain shift problem (Fu et al.\n2015), brings about the zero-shot translation failure in trans-\nfer learning. It is because transfer learning has no explicit\ntraining process to guarantee that the source and pivot']",nan,reasoning,TRUE
73,"What is the performance of the BERT-based model in the MEDDOCAN shared task compared to other systems, and how does it achieve its results?","['00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00\nO0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00\n(b) spaCy\npredicted\nDat Hos Age Tim Doc Sex Kin Loc Pat Job Oth OtrueDat 0.96 0.00 0.01 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.02\nHos 0.00 0.96 0.00 0.00 0.01 0.00 0.00 0.01 0.00 0.00 0.00 0.03\nAge 0.00 0.00 0.99 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nTim 0.01 0.00 0.00 0.99 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nDoc 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\nSex 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00\nKin 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.00 0.00\nLoc 0.00 0.23 0.00 0.00 0.00 0.00 0.00 0.50 0.00 0.08 0.00 0.19\nPat 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00 0.00 0.00 0.00\nJob 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.41 0.00 0.59\nOth 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00\nO0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.00\n(c) BERT\nTable 6: Confusion matrices for the sensitive information\nclassiﬁcation task on the NUB ES-PHI corpusare shown in Table 6. As can bee seen, BERT has less dif-\nﬁculty in predicting correctly less frequent categories, such\nas ‘Location’, ‘Job’, and ‘Patient’. One of the most com-\nmon mistakes according to the confusion matrices is classi-\nfying hospital names as ‘Location’ instead of the more ac-\ncurate ‘Hospital’; this is hardly a harmful error, given that a\nhospital is actually a location. Last, the category ‘Other’ is\ncompletely leaked by all the compared systems, most likely\ndue to its almost total lack of support in both training and\nevaluation datasets.\nTo ﬁnish with this experiment set, Table 5 also shows the\nstrict classiﬁcation precision, recall and F1-score for the\ncompared systems. Despite the fact that, in general, the\nsystems obtain high values, BERT outperforms them again.\nBERT’s F1-score is 1.9 points higher than the next most\ncompetitive result in the comparison. More remarkably, the\nrecall obtained by BERT is about 5 points above.\nUpon manual inspection of the errors committed by the\nBERT-based model, we discovered that it has a slight ten-\ndency towards producing ill-formed BIO sequences (e.g,\nstarting a sensitive span with ‘Inside’ instead of ‘Begin’;\nsee Table 7). We could expect that complementing the\nBERT-based model with a CRF layer on top would help en-\nforce the emission of valid sequences, alleviating this kind\nof errors and further improving its results.\nAcudir´a a la Cl ´ınica Marseille\ntrue O O B I I\npredicted O O B B I\n(a) Example 1: “[The patient] will attend the Marseille Clinic”\ncontrol ( 15 y 22 de junio', ' )\ntrue O O B O BI I O\npredicted O O B O II I O\n(b) Example 2: “inspection (15 and 22 of june)”\nNi˜no de 4 a ˜nos y medio\ntrue BO B I I I\npredicted OO B I O O\n(c) Example 3: “4 and a half years-old boy”\nTable 7: BERT error examples (only BIO-tags are shown;\ndifferences between gold annotations and predictions are\nhighlighted in bold)\nFinally, Figure 2 shows the impact of decreasing the\namount of training data in the detection scenario. It shows\nthe difference in precision, recall, and F1-score with respect\nto that obtained using 100% of the training data. A general\ndownward trend can be observed, as one would expect: less\ntraining data leads to less accurate predictions. However,\nthe BERT-based model is the most robust to training-data\nreduction, showing an steadily low performance loss. With\n1% of the dataset (230 training instances), the BERT-based\nmodel only suffers a striking 7-point F1-score loss, in con-\ntrast to the 32 and 39 points lost by the CRF and spaCy\nmodels, respectively. This steep performance drop stems to', '0 50 1000.40.60.81\ntrain data (%)precision\n0 50 1000.40.60.81\ntrain data (%)recall\n0 50 1000.40.60.81\ntrain data (%)F1-score\nCRF spaCy BERT\nFigure 2: Performance decay with decreasing amounts of training data on the sensitive information detection task in the\nNUB ES-PHI corpus\nDetection Classiﬁcation\nPrec Rec F1 Prec Rec F1\nCRF (Perez et al., 2019) 0.977 0.943 0.960 0.971 0.937 0.954\nspaCy (Perez et al., 2019) 0.967 0.953 0.965 0.965 0.947 0.956\nNLND S2 (Lange et al., 2019) 0.976 0.973 0.974 0.971 0.968 0.970\nNLND S3 (Lange et al., 2019) 0.975 0.975 0.975 0.970 0.969 0.970\nBERT + CRF (Mao and Liu, 2019) 0.968 0.919 0.943 0.965 0.912 0.937\nBERT 0.973 0.972 0.972 0.968 0.967 0.967\nTable 8: Results of Experiment B: MEDDOCAN\na larger extent from recall decline, which is not that marked\nin the case of BERT. Overall, these results indicate that the\ntransfer-learning achieved through the BERT multilingual\npre-trained model not only helps obtain better results, but\nalso lowers the need of manually labelled data for this ap-\nplication domain.\n4.2. Experiment B: MEDDOCAN\nThe results of the two MEDDOCAN scenarios –detection\nand classiﬁcation– are shown in Table 8. These results fol-\nlow the same pattern as in the previous experiments, with\nthe CRF classiﬁer being the most precise of all, and BERT\noutperforming both the CRF and spaCy classiﬁers thanks to\nits greater recall. We also show the results of Mao and Liu\n(2019) who, despite of having used a BERT-based system,\nachieve lower scores than our models. The reason why it\nshould be so remain unclear.\nWith regard to the winner of the MEDDOCAN shared\ntask, the BERT-based model has not improved the scores\nobtained by neither the domain-dependent (S3) nor the\ndomain-independent (S2) NLNDE model. However, at-\ntending to the obtained results, BERT remains only 0.3 F1-\nscore points behind, and would have achieved the second\nposition among all the MEDDOCAN shared task competi-\ntors. Taking into account that only 3% of the gold labels\nremain incorrectly annotated, the task can be considered al-\nmost solved, and it is not clear if the differences among the\nsystems are actually signiﬁcant, or whether they stem from\nminor variations in initialisation or a long-tail of minor la-belling inconsistencies.\n5. Conclusions and Future Work\nIn this work we have brieﬂy introduced the problems re-\nlated to data privacy protection in clinical domain. We have\nalso described some of the groundbreaking advances on the\nNatural Language Processing ﬁeld due to the appearance of\nTransformers-based deep-learning architectures and trans-\nfer learning from very large general-domain multilingual\ncorpora, focusing our attention in one of its most represen-\ntative examples, Google’s BERT model.\nIn order to assess the performance of BERT for Span-\nish clinical data anonymisation, we have conducted sev-\neral experiments with a BERT-based sequence labelling\napproach using the pre-trained multilingual BERT model\nshared by Google as the starting point for the model train-\ning. We have compared this BERT-based sequence la-\nbelling against other methods and systems. One of the ex-\nperiments uses the MEDDOCAN 2019 shared task dataset,\nwhile the other uses a novel Spanish clinical reports dataset\ncalled NUB ES-PHI.\nThe results of the experiments show that, in NUB ES-PHI,\nthe BERT-based model outperforms the other systems with-\nout requiring any adaptation or domain-speciﬁc feature en-\ngineering, just by being trained on the provided labelled\ndata. Interestingly, the BERT-based model obtains a re-\n']",The BERT-based model outperforms other systems in the MEDDOCAN shared task. It achieves its results by using the pre-trained multilingual BERT model and training on the provided labelled data.,reasoning,TRUE
74,"What problems does this work aim to address in relation to Armenian named entity recognition, and what resources are provided to tackle these problems?","['arXiv:1810.08699v1  [cs.CL]  19 Oct 2018pioNER: Datasets and Baselines for Armenian\nNamed Entity Recognition\nTsolak Ghukasyan1, Garnik Davtyan2, Karen Avetisyan3, Ivan Andrianov4\nIvannikov Laboratory for System Programming at Russian-Ar menian University1,2,3, Yerevan, Armenia\nIvannikov Institute for System Programming of the Russian A cademy of Sciences4, Moscow, Russia\nEmail: {1tsggukasyan,2garnik.davtyan,3karavet,4ivan.andrianov}@ispras.ru\nAbstract —In this work, we tackle the problem of Armenian\nnamed entity recognition, providing silver- and gold-stan dard\ndatasets as well as establishing baseline results on popula r mod-\nels. We present a 163000-token named entity corpus automati cally\ngenerated and annotated from Wikipedia, and another 53400-\ntoken corpus of news sentences with manual annotation of peo ple,\norganization and location named entities. The corpora were used\nto train and evaluate several popular named entity recognit ion\nmodels. Alongside the datasets, we release 50-, 100-, 200-, 300-\ndimensional GloVe word embeddings trained on a collection o f\nArmenian texts from Wikipedia, news, blogs, and encycloped ia.\nIndex Terms —machine learning, deep learning, natural lan-\nguage processing, named entity recognition, word embeddin gs\nI. I NTRODUCTION\nNamed entity recognition is an important task of natural\nlanguage processing, featuring in many popular text proces sing\ntoolkits. This area of natural language processing has been\nactively studied in the latest decades and the advent of deep\nlearning reinvigorated the research on more effective and\naccurate models. However, most of existing approaches requ ire\nlarge annotated corpora. To the best of our knowledge, no suc h\nwork has been done for the Armenian language, and in this\nwork we address several problems, including the creation of a\ncorpus for training machine learning models, the developme nt\nof gold-standard test corpus and evaluation of the effectiv eness\nof established approaches for the Armenian language.\nConsidering the cost of creating manually annotated named\nentity corpus, we focused on alternative approaches. Lack\nof named entity corpora is a common problem for many\nlanguages, thus bringing the attention of many researchers\naround the globe. Projection based transfer schemes have be en\nshown to be very effective (e.g. [1], [2], [3]), using resour ce-\nrich language’s corpora to generate annotated data for the l ow-\nresource language. In this approach, the annotations of hig h-\nresource language are projected over the corresponding tok ens\nof the parallel low-resource language’s texts. This strate gy\ncan be applied for language pairs that have parallel corpora .\nHowever, that approach would not work for Armenian as we\ndid not have access to sufﬁciently large parallel corpus wit h a\nresource-rich language.\nAnother popular approach is using Wikipedia. Klesti\nHoxha and Artur Baxhaku employ gazetteers extracted from\nWikipedia to generate an annotated corpus for Albanian [4],\nand Weber and Pötzl propose a rule-based system for Germanthat leverages the information from Wikipedia [5]. However ,\nthe latter relies on external tools such as part-of-speech t ag-\ngers, making it nonviable for the Armenian language.\nNothman et al. generated a silver-standard corpus for 9\nlanguages by extracting Wikipedia article texts with outgo ing\nlinks and turning those links into named entity annotations\nbased on the target article’s type [6]. Sysoev and Andrianov\nused a similar approach for the Russian language [7] [8].\nBased on its success for a wide range of languages, our choice\nfell on this model to tackle automated data generation and\nannotation for the Armenian language.\nAside from the lack of training data, we also address the\nabsence of a benchmark dataset of Armenian texts for named\nentity recognition. We propose a gold-standard corpus with\nmanual annotation of CoNLL named entity categories: person ,\nlocation, and organization [9] [10], hoping it will be used t o\nevaluate future named entity recognition models.\nFurthermore, popular entity recognition models were ap-\nplied to the mentioned data in order to obtain baseline resul ts\nfor future research in the area. Along with the datasets, we\n', 'developed GloVe [11] word embeddings to train and evaluate\nthe deep learning models in our experiments.\nThe contributions of this work are (i) the silver-standard\ntraining corpus, (ii) the gold-standard test corpus, (iii) GloVe\nword embeddings, (iv) baseline results for 3 different mode ls\non the proposed benchmark data set. All aforementioned\nresources are available on GitHub1.\nII. A UTOMATED TRAINING CORPUS GENERATION\nWe used Sysoev and Andrianov’s modiﬁcation of the\nNothman et al. approach to automatically generate data for\ntraining a named entity recognizer. This approach uses link s\nbetween Wikipedia articles to generate sequences of named-\nentity annotated tokens.\nA. Dataset extraction\nThe main steps of the dataset extraction system are de-\nscribed in Figure 1.\nFirst, each Wikipedia article is assigned a named entity\nclass (e.g. the article /Armke/armini/armmen /Armke/armayb/armsha/armke/armayb/armsha/armhi/armayb/armnu (Kim Kashkashian) is\nclassiﬁed as PER (person), /Armayb/armza/armgim/armyech/armre/armini /armlyun/armini/armgim/armayb (League of Nations)\nasORG (organization), /Armse/armini/armre/armini/armayb (Syria) as LOC etc). One of the\n1https://github.com/ispras-texterra/pioner\n© 2018 IEEE. Personal use of this material is permitted. Perm ission from IEEE must be obtained for all other uses, in any cu rrent or future media, including\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for r esale or redistribution to servers or lists, or\nreuse of any copyrighted component of this work in other work s.', 'Fig. 1: Steps of automatic dataset extraction from Wikipedi a\nClassiﬁcation of Wikipedia articles into NE types\nLabelling common article aliases to increase coverage\nExtraction of text fragments with outgoing links\nLabelling links according to their target article’s type\nAdjustment of labeled entities’ boundaries\ncore differences between our approach and Nothman’s system\nis that we do not rely on manual classiﬁcation of articles and\ndo not use inter-language links to project article classiﬁc ations\nacross languages. Instead, our classiﬁcation algorithm us es\nonly an article’s Wikidata entry’s ﬁrst instance of label’s\nparent subclass of labels, which are, incidentally, language\nindependent and thus can be used for any language.\nThen, outgoing links in articles are assigned the article’s\ntype they are leading to. Sentences are included in the train ing\ncorpus only if they contain at least one named entity and\nall contained capitalized words have an outgoing link to an\narticle of known type. Since in Wikipedia articles only the ﬁ rst\nmention of each entity is linked, this approach becomes very\nrestrictive and in order to include more sentences, additio nal\nlinks are inferred. This is accomplished by compiling a list of\ncommon aliases for articles corresponding to named entitie s,\nand then ﬁnding text fragments matching those aliases to\nassign a named entity label. An article’s aliases include it s\ntitle, titles of disambiguation pages with the article, and texts\nof links leading to the article (e.g. /Armlyun/armyech/armnu/armini/armnu/armgim/armre/armayb/armda (Leningrad),\n/Armpe/armyech/armtyun/armre/armvo/armgim/armre/armayb/armda (Petrograd), /Armpe/armyech/armtyun/armyech/armre/armben/armvo/armvyun/armre/armgim (Peterburg) are aliases\nfor/Armse/armayb/armnu/armken/armtyun /Armpe/armyech/armtyun/armyech/armre/armben/armvo/armvyun/armre/armgim (Saint Petersburg)). The list of\naliases is compiled for all PER ,ORG ,LOC articles.\nAfter that, link boundaries are adjusted by removing the\nlabels for expressions in parentheses, the text after a comm a,\nand in some cases breaking into separate named entities if th e\nlinked text contains a comma. For example, [LOC/Armayb/armben/armvo/armvev/armhi/armayb/armnu\n/armparenleft/armke/armayb/armghat/armayb/armke/armparenright ](Abovyan (town)) is reworked into [LOC/Armayb/armben/armvo/armvev/armhi/armayb/armnu ]\n/armparenleft/armke/armayb/armghat/armayb/armke/armparenright .\nB. Using Wikidata to classify Wikipedia\nInstead of manually classifying Wikipedia articles as it wa s\ndone in Nothman et al., we developed a rule-based classiﬁer\nthat used an article’s Wikidata instance of andsubclass of\nattributes to ﬁnd the corresponding named entity type.The classiﬁcation could be done using solely instance\noflabels, but these labels are unnecessarily speciﬁc for the\ntask and building a mapping on it would require a more\ntime-consuming and meticulous work. Therefore, we classiﬁ ed\narticles based on their ﬁrst instance of attribute’s subclass of\nvalues. Table I displays the mapping between these values an d\nnamed entity types. Using higher-level subclass of values was\nnot an option as their values often were too general, making\nit impossible to derive the correct named entity category.\nC. Generated data\nUsing the algorithm described above, we generated 7455\nannotated sentences with 163247 tokens based on 20 February\n2018 dump of Armenian Wikipedia.\nThe generated data is still signiﬁcantly smaller than the\nmanually annotated corpora from CoNLL 2002 and 2003.\nFor comparison, the train set of English CoNLL 2003 corpus\ncontains 203621 tokens and the German one 2069', '31, while\nthe Spanish and Dutch corpora from CoNLL 2002 respectively\n273037 and 218737 lines. The smaller size of our generated\ndata can be attributed to the strict selection of candidate\nsentences as well as simply to the relatively small size of\nArmenian Wikipedia.\nThe accuracy of annotation in the generated corpus heavily\nrelies on the quality of links in Wikipedia articles. During gen-\neration, we assumed that ﬁrst mentions of all named entities\nhave an outgoing link to their article, however this was not a l-\nways the case in actual source data and as a result the train se t\ncontained sentences where not all named entities are labele d.\nAnnotation inaccuracies also stemmed from wrongly assigne d\nlink boundaries (for example, in Wikipedia article /Armayb/armre/armto/armvo/armvyun/armre\n/Armvo/armvyun/armyech/armlyun/armse/armlyun/armini /Armvev/armyech/armlyun/armini/armnu/armgim/armto/armvo/armnu (Arthur Wellesley) there is a link to the\nNapoleon article with the text "" /arme/Armnu/armayb/armpe/armvo/armlyun/armyech/armvo/armnu/armat "" (""Napoleon is""),\nwhen it should be "" /Armnu/armayb/armpe/armvo/armlyun/armyech/armvo/armnu/armat "" (""Napoleon"")). Another kind\nof common annotation errors occurred when a named entity\nappeared inside a link not targeting a LOC ,ORG , orPER\narticle (e.g. "" /Armayb/Armmen/Armnu /armnu/armayb/armkhe/armayb/armgim/armayb/armho/armayb/armken/armayb/armnu /armat/armnu/armtyun/armre/armvo/armvyun/armto/armhi/armvo/armvyun/armnu/armnu/armyech/armre/armvo/armvyun/armmen ""\n(""USA presidential elections"") is linked to the article /Armayb/Armmen/Armnu\n/armnu/armayb/armkhe/armayb/armgim/armayb/armho/armayb/armken/armayb/armnu /armat/armnu/armtyun/armre/armvo/armvyun/armto/armhi/armvo/armvyun/armnu/armnu/armyech/armre 2016 (United States presi-\ndential election, 2016) and as a result [ LOC/Armayb/Armmen/Armnu ] (USA) is\nlost).\nIII. T EST DATASET\nIn order to evaluate the models trained on generated data,\nwe manually annotated a named entities dataset comprising\n53453 tokens and 2566 sentences selected from over 250 news\ntexts from ilur.am2. This dataset is comparable in size with\nthe test sets of other languages (Table II). Included senten ces\nare from political, sports, local and world news (Figures 2,\n3), covering the period between August 2012 and July 2018.\nThe dataset provides annotations for 3 popular named entity\nclasses: people ( PER ), organizations ( ORG ), and locations\n(LOC ), and is released in CoNLL03 format with IOB tagging\nscheme. Tokens and sentences were segmented according to\nthe UD standards for the Armenian language [12].\n2http://ilur.am/news/newsline.html']",nan,reasoning,TRUE
75,"How are behavioral features used to predict dogmatism levels in users, and what impact does interacting with a dogmatic comment have on a conversation?","['We then ﬁt these behavioral features to a linear re-\ngression model where we predict each user’s average\ndogmatism level. Positive coefﬁcients in this model\nare positively predictive of dogmatism, while nega-\ntive coefﬁcients are negatively predictive.\nWe ﬁnd this model is signiﬁcantly predicitive of\ndogmatism ( R2= 0.1,p< 0.001), with all features\nreaching statistical signiﬁcance ( p < 0.001).Activ-\nityandfocus are positively associated with dogma-\ntism, while breadth andengagement are negatively\nassociated (Table 5). Together, these results suggest\ndogmatic users tend to post frequently and in spe-\nciﬁc communities, but are not as inclined to continue\nto engage with a discussion, once it has begun.\n5.4 How does dogmatism impact a\nconversation? (R4)\nHow does interacting with a dogmatic comment im-\npact a conversation? Are users able to shrug it off?\nOr do otherwise non-dogmatic users become more\ndogmatic themselves?\nTo answer this question, we sample 600,000 con-\nversations triples from Reddit. These conversations\nconsist of two people (A and B) talking, with the\nstructure: A1 →B→A2. This allows us to mea-\nsure the impact of B’s dogmatism on A’s response,\nwhile also controlling for the dogmatism level ini-\ntially set by A. Concretely, we model the impact of\ndogmatism on these conversations through a linear\nregression. This model takes two features, the dog-\nmatism levels of A1 and B, and predicts the dogma-\ntism response of A2. If B’s dogmatism has no effect\non A’s response, the coefﬁcient that corresponds to\nB will not be signiﬁcant in the model. Alternatively,\nif B’s dogmatism does have some effect, it will be\ncaptured by the model’s coefﬁcient.\nWe ﬁnd the coefﬁcient of the B feature in the\nmodel is positively associated with dogmatism ( p<\n0.001). In other words, engagement with a dog-\nmatic comment tends to make a user more dogmatic\nthemselves. This effect holds when we run the same\nmodel on data subsets consisting only of dogmatic\nor non-dogmatic users, and also when we conserva-\ntively remove all words used by B from A’s response\n(i.e., controlling for quoting effects).6 Related Work\nIn contrast to the computational models we have pre-\nsented, dogmatism is usually measured in psychol-\nogy through survey scales, in which study partic-\nipants answer questions designed to reveal under-\nlying personality attributes (Rokeach, 1954). Over\ntime, these surveys have been updated (Shearman\nand Levine, 2006) and improved to meet standards\nof psychometric validity (Crowson, 2009).\nThese surveys are often used to study the rela-\ntionship between dogmatism and other psychologi-\ncal phenomena. For example, dogmatic people tend\nto show an increased tendency for confrontation (El-\nNawawy and Powers, 2010) or moral conviction and\nreligiosity (Swink, 2011), and less likelihood of cog-\nnitive ﬂexibility (Martin et al., 2011), even among\nstereotypically non-dogmatic groups like atheists\n(Gurney et al., 2013). From a behavioral standpoint,\ndogmatic people solve problems differently, spend-\ning less time framing a problem and expressing more\ncertainty in their solution (Lohman, 2010). Here we\nsimilarly examine how user behaviors on Reddit re-\nlate to a language model of dogmatism.\nErtel sought to capture dogmatism linguistically,\nthough a small lexicon of words that correspond\nwith high-level concepts like certainty and compro-\nmise (1985). McKenny then used this dictionary to\nrelate dogmatism to argument quality in student es-\nsays (2005). Our work expands on this approach,\napplying supervised models based on a broader set\nof linguistic categories to identify dogmatism in text.\nOther researchers have studied topics similar to\ndogmatism, such as signals of cognitive style in\nright-wing political thought (Van Hiel et al., 2010),\nthe language used by trolls on social media (']","Behavioral features are used to fit a linear regression model to predict dogmatism levels in users. Positive coefficients in the model are positively predictive of dogmatism, while negative coefficients are negatively predictive. Interacting with a dogmatic comment tends to make a user more dogmatic themselves.",reasoning,TRUE
76,"In the context of multilingual NMT in a low-resource setting, what is the proposed solution for handling word-order divergence between the source and assisting languages?","[' 1999) which enables the model to learn\nthe word-order of the source language when sufﬁ-\ncient child task parallel corpus is available.\nWe also compare the performance of the ﬁne-\ntuned model with the model trained only on the\navailable source-target parallel corpus with ran-\ndomly initialized weights (No Transfer Learning).\nTransfer learning, with and without pre-ordering,\nis better compared to training only on the small\nsource-target parallel corpus.\n6 Conclusion\nIn this paper, we show that handling word-order\ndivergence between the source and assisting lan-\nguages is crucial for the success of multilingual\nNMT in an extremely low-resource setting. We\nshow that pre-ordering the assisting language to\nmatch the word order of the source language sig-\nniﬁcantly improves translation quality in an ex-\ntremely low-resource setting. If pre-ordering is\nnot possible, ﬁne-tuning on a small source-target', '2 Related Work\nTo the best of our knowledge, no work has ad-\ndressed word order divergence in transfer learn-\ning for multilingual NMT. However, some work\nexists for other NLP tasks in a multilingual set-\nting. For Named Entity Recognition (NER), Xie\net al. (2018) use a self-attention layer after the\nBi-LSTM layer to address word-order divergence\nfor Named Entity Recognition (NER) task. The\napproach does not show any signiﬁcant improve-\nments, possibly because the divergence has to be\naddressed before/during construction of the con-\ntextual embeddings in the Bi-LSTM layer. Joty\net al. (2017) use adversarial training for cross-\nlingual question-question similarity ranking. The\nadversarial training tries to force the sentence rep-\nresentation generated by the encoder of similar\nsentences from different input languages to have\nsimilar representations.\nPre-ordering the source language sentences to\nmatch the target language word order has been\nfound useful in addressing word-order divergence\nfor Phrase-Based SMT (Collins et al., 2005; Ra-\nmanathan et al., 2008; Navratil et al., 2012; Chat-\nterjee et al., 2014). For NMT, Ponti et al. (2018)\nand Kawara et al. (2018) have explored pre-\nordering. Ponti et al. (2018) demonstrated that\nby reducing the syntactic divergence between the\nsource and the target languages, consistent im-\nprovements in NMT performance can be obtained.\nOn the contrary, Kawara et al. (2018) reported\ndrop in NMT performance due to pre-ordering.\nNote that these works address source-target diver-\ngence, not divergence between source languages\nin multilingual NMT scenario.\n3 Proposed Solution\nConsider the task of translating for an extremely\nlow-resource language pair. The parallel corpus\nbetween the two languages, if available may be\ntoo small to train an NMT model. Similar to Zoph\net al. (2016), we use transfer learning to over-\ncome data sparsity between the source and the\ntarget languages. We choose English as the as-\nsisting language in all our experiments. In our\nresource-scarce scenario, we have no parallel cor-\npus for training the child model. Hence, at test\ntime, the source language sentence is translated\nusing the parent model after performing a word-\nby-word translation from source to the assisting\nlanguage using a bilingual dictionary.Before Reordering After Reordering\nS\nNP0 VP\nV NP 1S\nNP0 VP\nNP1V\nS\nNP\nNNP\nAnuragVP\nMD\nwillVP\nVB\nmeetNP\nNNP\nThakurS\nNP\nNNP\nAnuragVP\nNP\nNNP\nThakurVP\nMD\nwillVP\nVB\nmeet\nTable 1: Example showing transitive verb before and\nafter reordering (Adapted from Chatterjee et al. (2014))\nSince the source language and the assisting lan-\nguage (English) have different word order, we hy-\npothesize that it leads to inconsistencies in the\ncontextual representations generated by the en-\ncoder for the two languages. Speciﬁcally, given an\nEnglish sentence (SVO word order) and its transla-\ntion in the source language (SOV word order), the\nencoder representations for words in the two sen-\ntences will be different due to different contexts\nof synonymous words. This could lead to the at-\ntention and the decoder layers generating different\ntranslations from the same (parallel) sentence in\nthe source or assisting language. This is undesir-\nable as we want the knowledge to be transferred\nfrom the parent model (assisting source →target)\nto the child model (source →target).\nIn this paper, we propose to pre-order English\nsentences (assisting language sentences) to match\nthe source language word-order and train the par-\nent model on the pre-ordered corpus. Table 1\nshows one of the pre-ordering rules (Ramanathan\net al., 2008) used along with an example sentence\nillustrating the effect of pre-ordering. This will en-\nsure that context of words in the parallel source\nand assisting language sentences are similar, lead-\ning to consistent contextual representations across\nthe source languages. Pre-ordering may also be\nbeneﬁcial for other word order divergence scenar-\nios (e.g., SOV to SVO), but we leave']",nan,reasoning,TRUE
77,"According to the provided context, what are some user behaviors that are predictive of dogmatism?","[')\nDogmatism is widely considered to be a domain-\nspeciﬁc attitude (for example, oriented towards re-\nligion or politics) as opposed to a deeper personality\ntrait (Rokeach, 1954). Here we use Reddit as a lens\nto examine this idea more closely. Are users who\nare dogmatic about one topic likely to be dogmatic\nabout others? Do clusters of dogmatism exist around\nparticular topics? To ﬁnd out, we examine the re-', 'Libertarianism business conspiracy science Christianity lgbt\nAnarcho Capitalism Bitcoin Republican Christianity DebateAChristian feminisms\nBitcoin economy conspiritard relationship advice DebateReligion Equality\nronpaul entertainment ronpaul worldpolitics science SubredditDrama\nConservative TrueReddit collapse MensRights videos TwoXChromosomes\nAndroid socialism guns IAmA news MensRights\nukpolitics bestof worldpolitics TwoXChromosomes Libertarianism offbeat\nEquality philosophy occupywallstreet WTF atheism fffffffuuuuuuuuuuuu\nTable 4: Clusters of subreddits that share dogmatic users. For example, users who are dogmatic on the conspiracy subreddit (a\nplace to discuss conspiracy theories) are also likely to be dogmatic on guns oroccupywallstreet .\nlationships between subreddits over which individ-\nual users are dogmatic. For example, if many users\noften post dogmatic comments on both the politics\nandChristianity subreddits, but less often on world-\nnews , that would suggest politics andChristianity\nare linked per a boost in likelihood of individuals\nbeing dogmatic in both.\nWe sample 1000 Reddit users who posted at least\nonce a year between 2007 and 2015 to construct a\ncorpus of 10 million posts that constitute their entire\npost history. We then annotate these posts using the\nclassiﬁer and compute the average dogmatism score\nper subreddit per user. For example, one user might\nhave an average dogmatism level of 0.55 for the pol-\nitics subreddit and 0.45 for the economics subred-\ndit. Most users do not post in all subreddits, so we\ntrack only subreddits for which a user had posted at\nleast 10 times. Any subreddits with an average dog-\nmatism score higher than 0.50 we consider to be a\nuser’s dogmatic subreddits. We then count all pairs\nof these dogmatic subreddits. For example, 45 users\nhave politics andtechnology among their dogmatic\nsubreddits, so we consider politics andtechnology\nas linked 45 times. We compute the mutual informa-\ntion (Church and Hanks, 1990) between these links,\nwhich gives us a measure of the subreddits that are\nmost related through dogmatism.\nWe present the results of this analysis in Table 4,\nchoosing clusters that represent a diverse set of top-\nics. For example, Libertarianism is linked through\ndogmatism to other political communities like An-\narcho Capitalism ,ronpaul , orukpolitics , as well as\nother topical subreddits like guns oreconomy . Sim-\nilarly, people who are dogmatic in the business sub-\nreddit also tend to be dogmatic in subreddits for Bit-\ncoin,socialism , and technology . Notably, when we\napply the same mutual information analysis to links\ndeﬁned by subreddits posted in by the same user, weFeature Direction\ntotal user posts ↑\nproportion of posts in most active subreddit ↑\nnumber of subreddits posted in ↓\naverage number of posts in active articles ↓\nTable 5: User behavioral features that are positively and nega-\ntively associated with dogmatism. ↑means the feature is pos-\nitively predictive with dogmatism, and ↓means the feature is\nnegatively predictive. For example, the more subreddits a user\nposts in, the less likely they are to be dogmatic. All features are\nstatistically signiﬁcant ( p < 0.001).\nsee dramatically different results. For example, the\nsubreddits most linked to science through user posts\nareUpliftingNews ,photoshopbattles , and ﬁrstworl-\ndanarchist , and millionairemakers .\nFinally, we see less obvious connections between\nsubreddits that suggest some people may be dog-\nmatic by nature. For example, among the users who\nare dogmatic on politics , they are also disproportion-\nately dogmatic on unrelated subreddits such as sci-\nence (p < 0.001),technology (p < 0.001),IAmA\n(p < 0.001), and AskReddit (p < 0.05), with p-\nvalues computed under a binomial test.\n5.3 What user behaviors are predictive of\ndogmatism? (R3)\nWe have shown dogmatism is captured by many lin-\nguistic features, but can we discover other high-level\nuser behaviors that are similarly predictive?\nTo ﬁnd out, we compute metrics of user behavior\nusing the data sample of 1000 users and 10 million\nposts described in Section 5.2. Speciﬁcally, we cal-\nculate (1) activity : a user’s total number of posts,', 'Libertarianism business conspiracy science Christianity lgbt\nAnarcho Capitalism Bitcoin Republican Christianity DebateAChristian feminisms\nBitcoin economy conspiritard relationship advice DebateReligion Equality\nronpaul entertainment ronpaul worldpolitics science SubredditDrama\nConservative TrueReddit collapse MensRights videos TwoXChromosomes\nAndroid socialism guns IAmA news MensRights\nukpolitics bestof worldpolitics TwoXChromosomes Libertarianism offbeat\nEquality philosophy occupywallstreet WTF atheism fffffffuuuuuuuuuuuu\nTable 4: Clusters of subreddits that share dogmatic users. For example, users who are dogmatic on the conspiracy subreddit (a\nplace to discuss conspiracy theories) are also likely to be dogmatic on guns oroccupywallstreet .\nlationships between subreddits over which individ-\nual users are dogmatic. For example, if many users\noften post dogmatic comments on both the politics\nandChristianity subreddits, but less often on world-\nnews , that would suggest politics andChristianity\nare linked per a boost in likelihood of individuals\nbeing dogmatic in both.\nWe sample 1000 Reddit users who posted at least\nonce a year between 2007 and 2015 to construct a\ncorpus of 10 million posts that constitute their entire\npost history. We then annotate these posts using the\nclassiﬁer and compute the average dogmatism score\nper subreddit per user. For example, one user might\nhave an average dogmatism level of 0.55 for the pol-\nitics subreddit and 0.45 for the economics subred-\ndit. Most users do not post in all subreddits, so we\ntrack only subreddits for which a user had posted at\nleast 10 times. Any subreddits with an average dog-\nmatism score higher than 0.50 we consider to be a\nuser’s dogmatic subreddits. We then count all pairs\nof these dogmatic subreddits. For example, 45 users\nhave politics andtechnology among their dogmatic\nsubreddits, so we consider politics andtechnology\nas linked 45 times. We compute the mutual informa-\ntion (Church and Hanks, 1990) between these links,\nwhich gives us a measure of the subreddits that are\nmost related through dogmatism.\nWe present the results of this analysis in Table 4,\nchoosing clusters that represent a diverse set of top-\nics. For example, Libertarianism is linked through\ndogmatism to other political communities like An-\narcho Capitalism ,ronpaul , orukpolitics , as well as\nother topical subreddits like guns oreconomy . Sim-\nilarly, people who are dogmatic in the business sub-\nreddit also tend to be dogmatic in subreddits for Bit-\ncoin,socialism , and technology . Notably, when we\napply the same mutual information analysis to links\ndeﬁned by subreddits posted in by the same user, weFeature Direction\ntotal user posts ↑\nproportion of posts in most active subreddit ↑\nnumber of subreddits posted in ↓\naverage number of posts in active articles ↓\nTable 5: User behavioral features that are positively and nega-\ntively associated with dogmatism. ↑means the feature is pos-\nitively predictive with dogmatism, and ↓means the feature is\nnegatively predictive. For example, the more subreddits a user\nposts in, the less likely they are to be dogmatic. All features are\nstatistically signiﬁcant ( p < 0.001).\nsee dramatically different results. For example, the\nsubreddits most linked to science through user posts\nareUpliftingNews ,photoshopbattles , and ﬁrstworl-\ndanarchist , and millionairemakers .\nFinally, we see less obvious connections between\nsubreddits that suggest some people may be dog-\nmatic by nature. For example, among the users who\nare dogmatic on politics , they are also disproportion-\nately dogmatic on unrelated subreddits such as sci-\nence (p < 0.001),technology (p < 0.001),IAmA\n(p < 0.001), and AskReddit (p < 0.05), with p-\nvalues computed under a binomial test.\n5.3 What user behaviors are predictive of\ndogmatism? (R3)\nWe have shown dogmatism is captured by many lin-\nguistic features, but can we discover other high-level\nuser behaviors that are similarly predictive?\nTo ﬁnd out, we compute metrics of user behavior\nusing the data sample of 1000 users and 10 million\nposts described in Section 5.2. Speciﬁcally, we cal-\nculate (1) activity : a user’s total number of posts,']",nan,reasoning,TRUE
78,How does BRLM utilize alignment information to improve word-level representation alignment between different languages in zero-shot translation?,"[', MNMT\nusually performs worse than the pivot-based method in\nzero-shot translation setting (Arivazhagan et al. 2018).\n•Unsupervised NMT (UNMT) considers a harder setting,\nin which only large-scale monolingual corpora are avail-\nable for training. Recently, many methods have been pro-\nposed to improve the performance of UNMT, including\nusing denoising auto-encoder, statistic machine transla-\ntion (SMT) and unsupervised pre-training (Artetxe et\nal. 2017; Lample et al. 2018; Ren et al. 2019; Lample\nand Conneau 2019). Since UNMT performs well between\nsimilar languages (e.g., English-German translation), its\nperformance between distant languages is still far from\nexpectation.\nOur proposed method belongs to the transfer learning,\nbut it is different from traditional transfer methods which\ntrain a parent model as starting point. Before training a par-\nent model, our approach fully leverages cross-lingual pre-\ntraining methods to make all source languages share the\nsame feature space and thus enables a smooth transition for\nzero-shot translation.\nApproach\nIn this section, we will present a cross-lingual pre-\ntraining based transfer approach. This method is designed\nfor a common zero-shot scenario where there are a lot\nof source↔pivot and pivot↔target bilingual data but no\nsource↔target parallel data, and the whole training process\ncan be summarized as follows step by step:\n•Pre-train a universal encoder with source/pivot monolin-\ngual or source↔pivot bilingual data.\n•Train a pivot→target parent model built on the pre-trained\nuniversal encoder with the available parallel data. Dur-\ning the training process, we freeze several layers of the\npre-trained universal encoder to avoid the degeneracy is-\nsue (Howard and Ruder 2018).', 'Figure 2: The overview of BRidge Language Modeling (BRLM). The BRLM extends MLM (Lample and Conneau 2019) to\npairs of parallel sentences and leverages explicit alignment information obtained by external aligner tool or additional attention\nlayer to encourage word representation alignment across different languages.\n•Directly translate source sentences into target sentences\nwith the parent model, which beneﬁts from the availabil-\nity of the universal encoder.\nThe key difﬁculty of this method is to ensure the intermedi-\nate representations of the universal encoder are language in-\nvariant. In the rest of this section, we ﬁrst present two exist-\ning methods yet to be explored in zero-shot translation, and\nthen propose a straightforward but effective cross-lingual\npre-training method. In the end, we present the whole train-\ning and inference protocol for transfer.\nMasked and Translation Language Model\nPretraining\nTwo existing cross-lingual pre-training methods, Masked\nLanguage Modeling (MLM) and Translation Language\nModeling (TLM), have shown their effectiveness on XNLI\ncross-lingual classiﬁcation task (Lample and Conneau 2019;\nHuang et al. 2019), but these methods have not been well\nstudied on cross-lingual generation tasks in zero-shot condi-\ntion. We attempt to take advantage of the cross-lingual abil-\nity of the two methods for zero-shot translation.\nSpeciﬁcally, MLM adopts the Cloze objective of\nBERT (Devlin et al. 2018) and predicts the masked words\nthat are randomly selected and replaced with [MASK] to-\nken on monolingual corpus. In practice, MLM takes dif-\nferent language monolingual corpora as input to ﬁnd fea-\ntures shared across different languages. With this method,\nword pieces shared in all languages have been mapped into\na shared space, which makes the sentence representationsacross different languages close (Pires, Schlinger, and Gar-\nrette 2019).\nSince MLM objective is unsupervised and only requires\nmonolingual data, TLM is designed to leverage parallel data\nwhen it is available. Actually, TLM is a simple extension of\nMLM, with the difference that TLM concatenates sentence\npair into a whole sentence, and then randomly masks words\nin both the source and target sentences. In this way, the\nmodel can either attend to surrounding words or to the trans-\nlation sentence, implicitly encouraging the model to align\nthe source and target language representations. Note that al-\nthough each sentence pair is formed into one sentence, the\npositions of the target sentence are reset to count form zero.\nBridge Language Model Pretraining\nAside from MLM and TLM, we propose BRidge Language\nModeling (BRLM) to further obtain word-level representa-\ntion alignment between different languages. This method is\ninspired by the assumption that if the feature spaces of dif-\nferent languages are aligned very well, the masked words\nin the corrupted sentence can also be guessed by the con-\ntext of the correspondingly aligned words on the other side.\nTo achieve this goal, BRLM is designed to strengthen the\nability to infer words across languages based on alignment\ninformation, instead of inferring words within monolingual\nsentence as in MLM or within the pseudo sentence formed\nby concatenating sentence pair as in TLM.\nAs illustrated in Figure 2, BRLM stacks shared encoder\nover both side sentences separately. In particular, we design\ntwo network structures for BRLM, which are divided into', 'Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-\nSA) according to the way of generating the alignment infor-\nmation. These two structures actually extend MLM into a\nbilingual scenario, with the difference that BRLM leverages\nexternal aligner tool or additional attention layer to explic-\nitly introduce alignment information during model training.\n•Hard Alignment (BRLM-HA). We ﬁrst use exter-\nnal aligner tool on source ↔pivot parallel data to ex-\ntract the alignment information of sentence pair. Dur-\ning model training, given source ↔pivot sentence pair,\nBRLM-HA randomly masks some words in source sen-\ntence and leverages alignment information to obtain the\naligned words in pivot sentence for masked words. Based\non the processed input, BRLM-HA adopts the Trans-\nformer (Vaswani et al. 2017) encoder to gain the hid-\nden states for source and pivot sentences respectively.\nThen the training objective of BRLM-HA is to predict\nthe masked words by not only the surrounding words\nin source sentence but also the encoder outputs of the\naligned words. Note that this training process is also car-\nried out in a symmetric situation, in which we mask some\nwords in pivot sentence and obtain the aligned words in\nthe source sentence.\n•Soft Alignment (BRLM-SA). Instead of using external\naligner tool, BRLM-SA introduces an additional atten-\ntion layer to learn the alignment information together with\nmodel training. In this way, BRLM-SA avoids the effect\ncaused by external wrong alignment information and en-\nables many-to-one soft alignment during model training.\nSimilar with BRLM-HA, the training objective of BRLM-\nSA is to predict the masked words by not only the sur-\nrounding words in source sentence but also the outputs of\nattention layer. In our implementation, the attention layer\nis a multi-head attention layer adopted in Transformer,\nwhere the queries come from the masked source sentence,\nthe keys and values come from the pivot sentence.\nIn principle, MLM and TLM can learn some implicit align-\nment information during model training. However, the align-\nment process in MLM is inefﬁcient since the shared word\npieces only account for a small proportion of the whole cor-\npus, resulting in the difﬁculty of expanding the shared in-\nformation to align the whole corpus. TLM also lacks effort\nin alignment between the source and target sentences since\nTLM concatenates the sentence pair into one sequence, mak-\ning the explicit alignment between the source and target in-\nfeasible. BRLM fully utilizes the alignment information to\nobtain better word-level representation alignment between\ndifferent languages, which better relieves the burden of the\ndomain shift problem .\nTransfer Protocol\nWe consider the typical zero-shot translation scenario in\nwhich a high resource pivot language has parallel data with\nboth source and target languages, while source and target\nlanguages has no parallel data between themselves. Our pro-\nposed cross-lingual pretraining based transfer approach for\nsource→target zero-shot translation is mainly divided into\ntwo phrases: the pretraining phase and the transfer phase.Corpus Language Train Dev Test\nEuroparlDe-En,En-Fr 1M,1M 2,000 2,000\nFr-En,En-Es 1M,1M 2,000 2,000\nRo-En,En-De 0.6M,1.5M 2,000 1,000\nMultiUNAr-En,En-Es\nEn-Ru9.7M,11.3M\n11.6M4,000 4,000\nTable 1: Data Statistics.\nIn the pretraining phase, we ﬁrst pretrain MLM on mono-\nlingual corpora of both source and pivot languages, and con-\ntinue to pretrain TLM or the proposed BRLM on the avail-\nable parallel data between source and pivot languages, in\norder to build a cross-lingual encoder shared by the source\nand pivot languages.\nIn the transfer phase, we train pivot →target NMT model\ninitialized by the cross-lingually pre-trained encoder, and\nﬁnally transfer the trained NMT model to source →target\ntranslation thanks to the shared encoder. Note that during\ntraining pivot→target NMT model, we freeze several layers\nof the cross-lingually pre-trained encoder to avoid the de-\ngeneracy issue.\nFor the', ' more complicated scenario that either the source\nside or the target side has multiple languages, the encoder\nand the decoder are also shared across each side languages\nfor efﬁcient deployment of translation between multiple lan-\nguages.\nExperiments\nSetup\nWe evaluate our cross-lingual pre-training based transfer ap-\nproach against several strong baselines on two public datat-\nsets, Europarl (Koehn 2005) and MultiUN (Eisele and Chen\n2010), which contain multi-parallel evaluation data to assess\nthe zero-shot performance. In all experiments, we use BLEU\nas the automatic metric for translation evaluation.1\nDatasets. The statistics of Europarl and MultiUN cor-\npora are summarized in Table 1. For Europarl corpus, we\nevaluate on French-English-Spanish (Fr-En-Es), German-\nEnglish-French (De-En-Fr) and Romanian-English-German\n(Ro-En-De), where English acts as the pivot language, its\nleft side is the source language, and its right side is the target\nlanguage. We remove the multi-parallel sentences between\ndifferent training corpora to ensure zero-shot settings. We\nuse the devtest2006 as the validation set and the test2006 as\nthe test set for Fr→Es and De→Fr. For distant language pair\nRo→De, we extract 1,000 overlapping sentences from new-\nstest2016 as the test set and the 2,000 overlapping sentences\nsplit from the training set as the validation set since there is\nno ofﬁcial validation and test sets. For vocabulary, we use\n60K sub-word tokens based on Byte Pair Encoding (BPE)\n(Sennrich, Haddow, and Birch 2015).\nFor MultiUN corpus, we use four languages: English\n(En) is set as the pivot language, which has parallel data\n1We calculate BLEU scores with the multi-bleu.perl script.', 'Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-\nSA) according to the way of generating the alignment infor-\nmation. These two structures actually extend MLM into a\nbilingual scenario, with the difference that BRLM leverages\nexternal aligner tool or additional attention layer to explic-\nitly introduce alignment information during model training.\n•Hard Alignment (BRLM-HA). We ﬁrst use exter-\nnal aligner tool on source ↔pivot parallel data to ex-\ntract the alignment information of sentence pair. Dur-\ning model training, given source ↔pivot sentence pair,\nBRLM-HA randomly masks some words in source sen-\ntence and leverages alignment information to obtain the\naligned words in pivot sentence for masked words. Based\non the processed input, BRLM-HA adopts the Trans-\nformer (Vaswani et al. 2017) encoder to gain the hid-\nden states for source and pivot sentences respectively.\nThen the training objective of BRLM-HA is to predict\nthe masked words by not only the surrounding words\nin source sentence but also the encoder outputs of the\naligned words. Note that this training process is also car-\nried out in a symmetric situation, in which we mask some\nwords in pivot sentence and obtain the aligned words in\nthe source sentence.\n•Soft Alignment (BRLM-SA). Instead of using external\naligner tool, BRLM-SA introduces an additional atten-\ntion layer to learn the alignment information together with\nmodel training. In this way, BRLM-SA avoids the effect\ncaused by external wrong alignment information and en-\nables many-to-one soft alignment during model training.\nSimilar with BRLM-HA, the training objective of BRLM-\nSA is to predict the masked words by not only the sur-\nrounding words in source sentence but also the outputs of\nattention layer. In our implementation, the attention layer\nis a multi-head attention layer adopted in Transformer,\nwhere the queries come from the masked source sentence,\nthe keys and values come from the pivot sentence.\nIn principle, MLM and TLM can learn some implicit align-\nment information during model training. However, the align-\nment process in MLM is inefﬁcient since the shared word\npieces only account for a small proportion of the whole cor-\npus, resulting in the difﬁculty of expanding the shared in-\nformation to align the whole corpus. TLM also lacks effort\nin alignment between the source and target sentences since\nTLM concatenates the sentence pair into one sequence, mak-\ning the explicit alignment between the source and target in-\nfeasible. BRLM fully utilizes the alignment information to\nobtain better word-level representation alignment between\ndifferent languages, which better relieves the burden of the\ndomain shift problem .\nTransfer Protocol\nWe consider the typical zero-shot translation scenario in\nwhich a high resource pivot language has parallel data with\nboth source and target languages, while source and target\nlanguages has no parallel data between themselves. Our pro-\nposed cross-lingual pretraining based transfer approach for\nsource→target zero-shot translation is mainly divided into\ntwo phrases: the pretraining phase and the transfer phase.Corpus Language Train Dev Test\nEuroparlDe-En,En-Fr 1M,1M 2,000 2,000\nFr-En,En-Es 1M,1M 2,000 2,000\nRo-En,En-De 0.6M,1.5M 2,000 1,000\nMultiUNAr-En,En-Es\nEn-Ru9.7M,11.3M\n11.6M4,000 4,000\nTable 1: Data Statistics.\nIn the pretraining phase, we ﬁrst pretrain MLM on mono-\nlingual corpora of both source and pivot languages, and con-\ntinue to pretrain TLM or the proposed BRLM on the avail-\nable parallel data between source and pivot languages, in\norder to build a cross-lingual encoder shared by the source\nand pivot languages.\nIn the transfer phase, we train pivot →target NMT model\ninitialized by the cross-lingually pre-trained encoder, and\nﬁnally transfer the trained NMT model to source →target\ntranslation thanks to the shared encoder. Note that during\ntraining pivot→target NMT model, we freeze several layers\nof the cross-lingually pre-trained encoder to avoid the de-\ngeneracy issue.\nFor the']","BRLM utilizes alignment information by using an external aligner tool or an additional attention layer during model training. It randomly masks words in the source sentence and uses alignment information to obtain the aligned words in the pivot sentence. The training objective is to predict the masked words using both the surrounding words in the source sentence and the encoder outputs of the aligned words. This process is carried out symmetrically, masking words in the pivot sentence and obtaining aligned words in the source sentence. BRLM also introduces a soft alignment approach using an attention layer to learn alignment information. The attention layer is a multi-head attention layer where the queries come from the masked source sentence and the keys and values come from the pivot sentence.",reasoning,TRUE
79,How does the context-specificity of contextualized word representations change across the layers of neural language models?,"['Figure 1: In almost all layers of BERT, ELMo, and GPT-2, the word representations are anisotropic (i.e., not\ndirectionally uniform): the average cosine similarity between uniformly randomly sampled words is non-zero.\nThe one exception is ELMo’s input layer; this is not surprising given that it generates character-level embeddings\nwithout using context. Representations in higher layers are generally more anisotropic than those in lower ones.\nELMo as well, though there are exceptions: for ex-\nample, the anisotropy in BERT’s penultimate layer\nis much higher than in its ﬁnal layer.\nIsotropy has both theoretical and empirical ben-\neﬁts for static word embeddings. In theory, it\nallows for stronger “self-normalization” during\ntraining (Arora et al., 2017), and in practice, sub-\ntracting the mean vector from static embeddings\nleads to improvements on several downstream\nNLP tasks (Mu et al., 2018). Thus the extreme\ndegree of anisotropy seen in contextualized word\nrepresentations – particularly in higher layers –\nis surprising. As seen in Figure 1, for all three\nmodels, the contextualized hidden layer represen-\ntations are almost all more anisotropic than the in-\nput layer representations, which do not incorpo-\nrate context. This suggests that high anisotropy is\ninherent to, or least a by-product of, the process of\ncontextualization.\n4.2 Context-Speciﬁcity\nContextualized word representations are more\ncontext-speciﬁc in higher layers. Recall from\nDeﬁnition 1 that the self-similarity of a word, in\na given layer of a given model, is the average co-\nsine similarity between its representations in dif-\nferent contexts, adjusted for anisotropy. If the\nself-similarity is 1, then the representations are\nnot context-speciﬁc at all; if the self-similarity is\n0, that the representations are maximally context-\nspeciﬁc. In Figure 2, we plot the average self-\nsimilarity of uniformly randomly sampled wordsin each layer of BERT, ELMo, and GPT-2. For\nexample, the self-similarity is 1.0 in ELMo’s in-\nput layer because representations in that layer are\nstatic character-level embeddings.\nIn all three models, the higher the layer, the\nlower the self-similarity is on average. In other\nwords, the higher the layer, the more context-\nspeciﬁc the contextualized representations. This\nﬁnding makes intuitive sense. In image classiﬁca-\ntion models, lower layers recognize more generic\nfeatures such as edges while upper layers recog-\nnize more class-speciﬁc features (Yosinski et al.,\n2014). Similarly, upper layers of LSTMs trained\non NLP tasks learn more task-speciﬁc represen-\ntations (Liu et al., 2019a). Therefore, it fol-\nlows that upper layers of neural language mod-\nels learn more context-speciﬁc representations, so\nas to predict the next word for a given context\nmore accurately. Of all three models, representa-\ntions in GPT-2 are the most context-speciﬁc, with\nthose in GPT-2’s last layer being almost maxi-\nmally context-speciﬁc.\nStopwords (e.g., ‘the’, ‘of’, ‘to’ ) have among the\nmost context-speciﬁc representations. Across\nall layers, stopwords have among the lowest self-\nsimilarity of all words, implying that their con-\ntextualized representations are among the most\ncontext-speciﬁc. For example, the words with the\nlowest average self-similarity across ELMo’s lay-\ners are ‘and’, ‘of’, ‘’s’, ‘the’ , and ‘to’. This is rel-\natively surprising, given that these words are not\npolysemous. This ﬁnding suggests that the variety', 'Figure 2: The average cosine similarity between representations of the same word in different contexts is called\nthe word’s self-similarity (see Deﬁnition 1). Above, we plot the average self-similarity of uniformly randomly\nsampled words after adjusting for anisotropy (see section 3.4). In all three models, the higher the layer, the lower\nthe self-similarity, suggesting that contextualized word representations are more context-speciﬁc in higher layers.\nof contexts a word appears in, rather than its inher-\nent polysemy, is what drives variation in its con-\ntextualized representations. This answers one of\nthe questions we posed in the introduction: ELMo,\nBERT, and GPT-2 are not simply assigning one of\na ﬁnite number of word-sense representations to\neach word; otherwise, there would not be so much\nvariation in the representations of words with so\nfew word senses.\nContext-speciﬁcity manifests very differently in\nELMo, BERT, and GPT-2. As noted earlier,\ncontextualized representations are more context-\nspeciﬁc in upper layers of ELMo, BERT, and GPT-\n2. However, how does this increased context-\nspeciﬁcity manifest in the vector space? Do word\nrepresentations in the same sentence converge to a\nsingle point, or do they remain distinct from one\nanother while still being distinct from their repre-\nsentations in other contexts? To answer this ques-\ntion, we can measure a sentence’s intra-sentence\nsimilarity. Recall from Deﬁnition 2 that the intra-\nsentence similarity of a sentence, in a given layer\nof a given model, is the average cosine similarity\nbetween each of its word representations and their\nmean, adjusted for anisotropy. In Figure 3, we plot\nthe average intra-sentence similarity of 500 uni-\nformly randomly sampled sentences.\nIn ELMo, words in the same sentence are more\nsimilar to one another in upper layers. As\nword representations in a sentence become more\ncontext-speciﬁc in upper layers, the intra-sentencesimilarity also rises. This suggests that, in prac-\ntice, ELMo ends up extending the intuition behind\nFirth’s (1957) distributional hypothesis to the sen-\ntence level: that because words in the same sen-\ntence share the same context, their contextualized\nrepresentations should also be similar.\nIn BERT, words in the same sentence are more\ndissimilar to one another in upper layers. As\nword representations in a sentence become more\ncontext-speciﬁc in upper layers, they drift away\nfrom one another, although there are exceptions\n(see layer 12 in Figure 3). However, in all lay-\ners, the average similarity between words in the\nsame sentence is still greater than the average sim-\nilarity between randomly chosen words (i.e., the\nanisotropy baseline). This suggests a more nu-\nanced contextualization than in ELMo, with BERT\nrecognizing that although the surrounding sen-\ntence informs a word’s meaning, two words in the\nsame sentence do not necessarily have a similar\nmeaning because they share the same context.\nIn GPT-2, word representations in the same\nsentence are no more similar to each other than\nrandomly sampled words. On average, the un-\nadjusted intra-sentence similarity is roughly the\nsame as the anisotropic baseline, so as seen in Fig-\nure 3, the anisotropy-adjusted intra-sentence simi-\nlarity is close to 0 in most layers of GPT-2. In fact,\nthe intra-sentence similarity is highest in the input\nlayer, which does not contextualize words at all.\nThis is in contrast to ELMo and BERT, where the']",nan,reasoning,TRUE
80,How does the proposed solution for low-resource language translation utilize pre-ordering to address word-order divergence between the source and assisting languages?,"['2 Related Work\nTo the best of our knowledge, no work has ad-\ndressed word order divergence in transfer learn-\ning for multilingual NMT. However, some work\nexists for other NLP tasks in a multilingual set-\nting. For Named Entity Recognition (NER), Xie\net al. (2018) use a self-attention layer after the\nBi-LSTM layer to address word-order divergence\nfor Named Entity Recognition (NER) task. The\napproach does not show any signiﬁcant improve-\nments, possibly because the divergence has to be\naddressed before/during construction of the con-\ntextual embeddings in the Bi-LSTM layer. Joty\net al. (2017) use adversarial training for cross-\nlingual question-question similarity ranking. The\nadversarial training tries to force the sentence rep-\nresentation generated by the encoder of similar\nsentences from different input languages to have\nsimilar representations.\nPre-ordering the source language sentences to\nmatch the target language word order has been\nfound useful in addressing word-order divergence\nfor Phrase-Based SMT (Collins et al., 2005; Ra-\nmanathan et al., 2008; Navratil et al., 2012; Chat-\nterjee et al., 2014). For NMT, Ponti et al. (2018)\nand Kawara et al. (2018) have explored pre-\nordering. Ponti et al. (2018) demonstrated that\nby reducing the syntactic divergence between the\nsource and the target languages, consistent im-\nprovements in NMT performance can be obtained.\nOn the contrary, Kawara et al. (2018) reported\ndrop in NMT performance due to pre-ordering.\nNote that these works address source-target diver-\ngence, not divergence between source languages\nin multilingual NMT scenario.\n3 Proposed Solution\nConsider the task of translating for an extremely\nlow-resource language pair. The parallel corpus\nbetween the two languages, if available may be\ntoo small to train an NMT model. Similar to Zoph\net al. (2016), we use transfer learning to over-\ncome data sparsity between the source and the\ntarget languages. We choose English as the as-\nsisting language in all our experiments. In our\nresource-scarce scenario, we have no parallel cor-\npus for training the child model. Hence, at test\ntime, the source language sentence is translated\nusing the parent model after performing a word-\nby-word translation from source to the assisting\nlanguage using a bilingual dictionary.Before Reordering After Reordering\nS\nNP0 VP\nV NP 1S\nNP0 VP\nNP1V\nS\nNP\nNNP\nAnuragVP\nMD\nwillVP\nVB\nmeetNP\nNNP\nThakurS\nNP\nNNP\nAnuragVP\nNP\nNNP\nThakurVP\nMD\nwillVP\nVB\nmeet\nTable 1: Example showing transitive verb before and\nafter reordering (Adapted from Chatterjee et al. (2014))\nSince the source language and the assisting lan-\nguage (English) have different word order, we hy-\npothesize that it leads to inconsistencies in the\ncontextual representations generated by the en-\ncoder for the two languages. Speciﬁcally, given an\nEnglish sentence (SVO word order) and its transla-\ntion in the source language (SOV word order), the\nencoder representations for words in the two sen-\ntences will be different due to different contexts\nof synonymous words. This could lead to the at-\ntention and the decoder layers generating different\ntranslations from the same (parallel) sentence in\nthe source or assisting language. This is undesir-\nable as we want the knowledge to be transferred\nfrom the parent model (assisting source →target)\nto the child model (source →target).\nIn this paper, we propose to pre-order English\nsentences (assisting language sentences) to match\nthe source language word-order and train the par-\nent model on the pre-ordered corpus. Table 1\nshows one of the pre-ordering rules (Ramanathan\net al., 2008) used along with an example sentence\nillustrating the effect of pre-ordering. This will en-\nsure that context of words in the parallel source\nand assisting language sentences are similar, lead-\ning to consistent contextual representations across\nthe source languages. Pre-ordering may also be\nbeneﬁcial for other word order divergence scenar-\nios (e.g., SOV to SVO), but we leave']",nan,conditional,TRUE
81,"How does the proposed method enhance KB relation detection in Knowledge Base Question Answering, considering the challenges of unseen relations and predicting chains of relations?","['Improved Neural Relation Detection for Knowledge Base Question\nAnswering\nMo Yu†Wenpeng Yin⋆Kazi Saidul Hasan‡Cicero dos Santos†\nBing Xiang‡Bowen Zhou†\n†AI Foundations, IBM Research, USA\n⋆Center for Information and Language Processing, LMU Munich\n‡IBM Watson, USA\n{yum,kshasan,cicerons,bingxia,zhou }@us.ibm.com, wenpeng@cis.lmu.de\nAbstract\nRelation detection is a core component of\nmany NLP applications including Knowl-\nedge Base Question Answering (KBQA).\nIn this paper, we propose a hierarchi-\ncal recurrent neural network enhanced by\nresidual learning which detects KB re-\nlations given an input question. Our\nmethod uses deep residual bidirectional\nLSTMs to compare questions and rela-\ntion names via different levels of abstrac-\ntion. Additionally, we propose a sim-\nple KBQA system that integrates entity\nlinking and our proposed relation detec-\ntor to make the two components enhance\neach other. Our experimental results show\nthat our approach not only achieves out-\nstanding relation detection performance,\nbut more importantly, it helps our KBQA\nsystem achieve state-of-the-art accuracy\nfor both single-relation (SimpleQuestions)\nand multi-relation (WebQSP) QA bench-\nmarks.\n1 Introduction\nKnowledge Base Question Answering (KBQA)\nsystems answer questions by obtaining informa-\ntion from KB tuples (Berant et al., 2013; Yao et al.,\n2014; Bordes et al., 2015; Bast and Haussmann,\n2015; Yih et al., 2015; Xu et al., 2016). For an\ninput question, these systems typically generate a\nKB query, which can be executed to retrieve the\nanswers from a KB. Figure 1 illustrates the process\nused to parse two sample questions in a KBQA\nsystem: (a) a single-relation question, which can\nbe answered with a single <head-entity, relation,\ntail-entity>KB tuple (Fader et al., 2013; Yih et al.,\n2014; Bordes et al., 2015); and (b) a more complex\ncase, where some constraints need to be handledfor multiple entities in the question. The KBQA\nsystem in the ﬁgure performs two key tasks: (1)\nentity linking , which links n-grams in questions\nto KB entities, and (2) relation detection , which\nidentiﬁes the KB relation(s) a question refers to.\nThe main focus of this work is to improve the\nrelation detection subtask and further explore how\nit can contribute to the KBQA system. Although\ngeneral relation detection1methods are well stud-\nied in the NLP community, such studies usually\ndo not take the end task of KBQA into considera-\ntion. As a result, there is a signiﬁcant gap between\ngeneral relation detection studies and KB-speciﬁc\nrelation detection. First, in most general relation\ndetection tasks, the number of target relations is\nlimited, normally smaller than 100. In contrast, in\nKBQA even a small KB, like Freebase2M (Bor-\ndes et al., 2015), contains more than 6,000 relation\ntypes. Second, relation detection for KBQA often\nbecomes a zero-shot learning task, since some test\ninstances may have unseen relations in the training\ndata. For example, the SimpleQuestions (Bordes\net al., 2015) data set has 14% of the golden test\nrelations not observed in golden training tuples.\nThird, as shown in Figure 1(b), for some KBQA\ntasks like WebQuestions (Berant et al., 2013), we\nneed to predict a chain of relations instead of a\nsingle relation. This increases the number of tar-\nget relation types and the sizes of candidate rela-\ntion pools, further increasing the difﬁculty of KB\nrelation detection. Owing to these reasons, KB re-\nlation detection is signiﬁcantly more challenging\ncompared to general relation detection tasks.\nThis paper improves KB relation detection to\ncope with the problems mentioned above. First, in\norder to deal with the unseen relations, we propose\nto break the relation names into word sequences\nfor question-relation matching. Second, noticing\n1In the information extraction ﬁeld such tasks are usually\ncalled relation extraction orrelation classiﬁ', 'cation .arXiv:1704.06194v2  [cs.CL]  27 May 2017']",nan,conditional,TRUE
82,What is the impact of freezing parameters on the performance of transfer learning in zero-shot translation using MLM+BRLM-SA?,"['SA with back translation also achieves better performance\nthan the original supervised Transformer.\nAnalysis\nSentence Representation. We ﬁrst evaluate the represen-\ntational invariance across languages for all cross-lingual pre-\ntraining methods. Following Arivazhagan et al. (2018), we\nadopt max-pooling operation to collect the sentence rep-\nresentation of each encoder layer for all source-pivot sen-\ntence pairs in the Europarl validation sets. Then we calcu-\nlate the cosine similarity for each sentence pair and aver-\nage all cosine scores. As shown in Figure 3, we can ob-\nserve that, MLM+BRLM-SA has the most stable and similar\ncross-lingual representations of sentence pairs on all layers,\nwhile it achieves the best performance in zero-shot transla-\ntion. This demonstrates that better cross-lingual representa-\ntions can beneﬁt for the process of transfer learning. Besides,\nMLM+BRLM-HA is not as superior as MLM+BRLM-\nSA and even worse than MLM+TLM on Fr-En, since\nMLM+BRLM-HA may suffer from the wrong alignment\nknowledge from an external aligner tool. We also ﬁnd an in-\nteresting phenomenon that as the number of layers increases,\nthe cosine similarity decreases.\nContextualized Word Representation. We further sam-\nple an English-Russian sentence pair from the MultiUN\nvalidation sets and visualize the cosine similarity between\nhidden states of the top encoder layer to further investi-\ngate the difference of all cross-lingual pre-training meth-\nods. As shown in Figure 4, the hidden states generated by\nMLM+BRLM-SA have higher similarity for two aligned\nwords. It indicates that MLM+BRLM-SA can gain bet-\nter word-level representation alignment between source and\npivot languages, which better relieves the burden of the do-\nmain shift problem .\nThe Effect of Freezing Parameters. To freeze parame-\nters is a common strategy to avoid catastrophic forgetting in\ntransfer learning (Howard and Ruder 2018). Table 4 shows\nthe performance of transfer learning with freezing different\nlayers on MultiUN test set, in which En →Ru denotes the\nparent model, Ar→Ru and Es→Ru are two child models,\nand all models are based on MLM+BRLM-SA. We can ﬁnd\nthat updating all parameters during training will cause a no-\ntable drop on the zero-shot direction due to the catastrophic\nforgetting. On the contrary, freezing all the parameters leads\nto the decline on supervised direction because the language\nfeatures extracted during pre-training is not sufﬁcient for\nMT task. Freezing the ﬁrst four layers of the transformer\nshows the best performance and keeps the balance between\npre-training and ﬁne-tuning.\nConclusion\nIn this paper, we propose a cross-lingual pretraining based\ntransfer approach for the challenging zero-shot translation\ntask, in which source and target languages have no parallel\ndata, while they both have parallel data with a high resource\n(a) MLM\n (b) MLM+TLM\n(c) MLM+BRLM-HA\n (d) MLM+BRLM-SA\nFigure 4: Cosine similarity visualization at word level given\nan English-Russian sentence pair from the MultiUN valida-\ntion sets. Brighter indicates higher similarity.\nFreezing Layers En→Ru Ar→Ru Es→Ru\nNone 37.80 16.09 19.80\n2 37.79 21.47 28.35\n4 37.55 25.49 30.47\n6 35.31 22.90 28.22\nTable 4: BLEU score of freezing different layers. The num-\nber in Freezing Layers column denotes that the number of\nencoder layers will not be updated.\npivot language. With the aim of building the language in-\nvariant representation between source and pivot languages\nfor smooth transfer of the parent model of pivot →target di-\nrection to the child model of source →target direction, we in-\ntroduce one monolingual pretraining method and two bilin-\ngual pretraining methods to construct an universal encoder\nfor the source and pivot languages. Experiments on public\ndatasets show that our approaches signiﬁcantly outperforms\nseveral strong baseline systems, and manifest the language\ninvariance characteristics in both sentence level and word\nlevel neural representations.\nAcknowledgments\nWe would like', 'MultiUN Ar,Es,Ru↔En\nDirection Ar→Es Es→Ar Ar→Ru Ru→Ar Es→Ru Ru→Es A-ZST A-ST\nBaselines\nCross-lingual Transfer 10.26 12.44 4.58 4.42 13.80 7.93 8.90 44.73\nMNMT(Johnson et al. 2016) 27.40 20.18 15.12 16.19 17.88 27.93 20.78 43.95\nPivoting m 42.29 30.15 27.23 26.16 29.57 40.08 32.58 43.95\nProposed Cross-lingual Pretraining Based Transfer\nMLM 16.50 23.41 9.61 14.23 22.80 23.66 18.36 44.25\nMLM+TLM 25.98 26.55 16.84 20.07 25.91 29.52 24.14 43.71\nMLM+BRLM-HA 29.05 27.58 18.10 20.42 25.39 30.96 25.25 44.67\nMLM+BRLM-SA 36.01 31.08 25.49 25.06 30.47 36.01 30.68 44.54\nAdding Back Translation\nMNMT* (Gu et al. 2019) 39.72 28.05 24.67 24.43 27.41 38.01 30.38 43.98\nMLM 40.98 31.53 26.06 26.69 31.28 40.02 32.76 44.28\nMLM+TLM 41.15 29.77 27.61 27.74 31.02 40.37 32.39 44.14\nMLM+BRLM-HA 41.74 31.89 27.24 27.54 31.29 40.34 33.35 44.52\nMLM+BRLM-SA 44.17 33.20 29.01 28.91 32.53 41.93 34.95 45.49\nTable 3: Results on MultiUN test sets. The six zero-shot translation directions are evaluated. The column “A-ZST"" reports av-\neraged BLEU of zero-shot translation, while the column “A-ST"" reports averaged BLEU of supervised pivot →target direction.\n(a) Fr-En\n (b) De-En\n (c) Ro-En\nFigure 3: Cosine similarity between sentence representation of each encoder layer across all source-pivot sentence pairs in the\nEuroparl validation set.\ncoder trained on both large-scale monolingual data and par-\nallel data between multiple languages.\nMLM alone that does not use source ↔pivot parallel data\nperforms much better than the cross-lingual transfer, and\nachieves comparable results to pivoting. When MLM is\ncombined with TLM or the proposed BRLM, the perfor-\nmance is further improved. MLM+BRLM-SA performs the\nbest, and is better than MLM+BRLM-HA indicating that\nsoft alignment is helpful than hard alignment for the cross-\nlingual pretraining.\nResults on MultiUN Dataset. Like experimental results\non Europarl, MLM+BRLM-SA performs the best among\nall proposed cross-lingual pretraining based transfer ap-\nproaches as shown in Table 3. When comparing systems\nconsisting of one encoder-decoder model for all zero-shot\ntranslation, our approaches performs signiﬁcantly better\nthan MNMT (Johnson et al. 2016).Although it is challenging for one model to translate all\nzero-shot directions between multiple distant language pairs\nof MultiUN, MLM+BRLM-SA still achieves better perfor-\nmances on Es→Ar and Es→Ru than strong pivoting m,\nwhich uses MNMT to translate source to pivot then to tar-\nget in two separate steps with each step receiving supervised\nsignal of parallel corpora. Our approaches surpass pivoting m\nin all zero-shot directions by adding back translation (Sen-\nnrich, Haddow, and Birch 2015) to generate pseudo parallel\nsentences for all zero-shot directions based on our pretrained\nmodels such as MLM+BRLM-SA, and further training our\nuniversal encoder-decoder model with these pseudo data.\nGu et al. (2019) introduces back translation into MNMT,\nwhile we adopt it in our transfer approaches. Finally, our\nbest MLM+BRLM-SA with back translation outperforms\npivoting mby 2.4 BLEU points averagely, and outperforms\nMNMT (Gu et al. 2019) by 4.6']","Freezing different layers of the transformer shows different impacts on the performance of transfer learning in zero-shot translation using MLM+BRLM-SA. Freezing all parameters leads to a decline in the supervised direction, while updating all parameters during training causes a notable drop in the zero-shot direction. Freezing the first four layers of the transformer shows the best performance and maintains a balance between pre-training and fine-tuning.",conditional,TRUE
83,What additional information does the newly developed corpus for automatic readability assessment and text simplification of German provide?,"['arXiv:1909.09067v1  [cs.CL]  19 Sep 2019A Corpus for Automatic Readability Assessment and Text Simp liﬁcation\nof German\nAlessia Battisti\nInstitute of Computational Linguistics\nUniversity of Zurich\nAndreasstrasse 15, 8050 Zurich\nalessia.battisti@uzh.chSarah Ebling\nInstitute of Computational Linguistics\nUniversity of Zurich\nAndreasstrasse 15, 8050 Zurich\nebling@cl.uzh.ch\nAbstract\nIn this paper, we present a corpus for use in\nautomatic readability assessment and auto-\nmatic text simpliﬁcation of German. The\ncorpus is compiled from web sources and\nconsists of approximately 211,000 sen-\ntences. As a novel contribution, it con-\ntains information on text structure, typog-\nraphy, and images, which can be exploited\nas part of machine learning approaches to\nreadability assessment and text simpliﬁca-\ntion. The focus of this publication is on\nrepresenting such information as an exten-\nsion to an existing corpus standard.\n1 Introduction\nSimpliﬁed language is a variety of standard lan-\nguage characterized by reduced lexical and syn-\ntactic complexity, the addition of explanations\nfor difﬁcult concepts, and clearly structured lay-\nout.1Among the target groups of simpliﬁed lan-\nguage commonly mentioned are persons with cog-\nnitive impairment or learning disabilities, prelin-\ngually deaf persons, functionally illiterate persons,\nand foreign language learners (Bredel and Maaß,\n2016).\nTwo natural language processing tasks deal with\nthe concept of simpliﬁed language: automatic\nreadability assessment and automatic text simpli-\nﬁcation. Readability assessment refers to the pro-\ncess of determining the level of difﬁculty of a text,\ne.g., along readability measures, school grades, or\nlevels of the Common European Framework of\nReference for Languages (CEFR) (Council of Eu-\nrope, 2009). Readability measures, in their tra-\nditional form, take into account only surface fea-\ntures. For example, the Flesch Reading Ease Score\n1The term plain language is avoided, as it refers to a spe-\nciﬁc level of simpliﬁcation. Simpliﬁed language subsumes all\nefforts of reducing the complexity of a piece of text.(Flesch, 1948) measures the length of words (in\nsyllables) and sentences (in words). While read-\nability has been shown to correlate with such fea-\ntures to some extent (Just and Carpenter, 1980), a\nconsensus has emerged according to which they\nare not sufﬁcient to account for all of the com-\nplexity inherent in a text. As Kauchak et al.\n(2014, p. 2618) state, “the usability of readabil-\nity formulas is limited and there is little evidence\nthat the output of these tools directly results in\nimproved understanding by readers”. Recently,\nmore sophisticated models employing (deeper) lin-\nguistic features such as lexical, semantic, mor-\nphological, morphosyntactic, syntactic, pragmatic,\ndiscourse, psycholinguistic, and language model\nfeatures have been proposed (Collins-Thompson,\n2014; Heimann M¨ uhlenbock, 2013; Pitler and\nNenkova, 2008; Schwarm and Ostendorf, 2005;\nTanaka et al., 2013).\nAutomatic text simpliﬁcation was initiated in\nthe late 1990s (Carroll et al., 1998; Chandrasekar\net al., 1996) and since then has been approached\nby means of rule-based and statistical methods. As\npart of a rule-based approach, the operations car-\nried out typically include replacing complex lex-\nical and syntactic units by simpler ones. A sta-\ntistical approach generally conceptualizes the sim-\npliﬁcation task as one of converting a standard-\nlanguage into a simpliﬁed-language text using ma-\nchine translation. Nisioi et al. (2017) introduced\nneural machine translation to automatic text sim-\npliﬁcation. Research on automatic text simpliﬁ-\ncation is comparatively widespread for languages\nsuch as English, Swedish, Spanish, and Brazilian\nPortuguese. To the authors’ knowledge,', ' no pro-\nductive system exists for German. Suter (2015),\nSuter et al. (2016) presented a prototype of a rule-\nbased system for German.\nMachine learning approaches to both readabil-\nity assessment and text simpliﬁcation rely on\ndata systematically prepared in the form of cor-', 'pora. Speciﬁcally, for automatic text simpliﬁca-\ntion via machine translation, pairs of standard-\nlanguage/simpliﬁed-language texts aligned at the\nsentence level (i.e., parallel corpora) are needed.\nThe paper at hand introduces a corpus devel-\noped for use in automatic readability assessment\nand automatic text simpliﬁcation of German. The\nfocus of this publication is on representing infor-\nmation that is valuable for these tasks but that hith-\nerto has largely been ignored in machine learning\napproaches centering around simpliﬁed language,\nspeciﬁcally, text structure (e.g., paragraphs, lines),\ntypography (e.g., font type, font style), and im-\nage (content, position, and dimensions) informa-\ntion. The importance of considering such infor-\nmation has repeatedly been asserted theoretically\n(Arf´ e et al., 2018; Bock, 2018; Bredel and Maaß,\n2016).\nThe remainder of this paper is structured as fol-\nlows: Section 2 presents previous corpora used for\nautomatic readability assessment and text simpliﬁ-\ncation. Section 3 describes our corpus, introduc-\ning its novel aspects and presenting the primary\ndata (Section 3.1), the metadata (Section 3.2), the\nsecondary data (Section 3.3), the proﬁle (Section\n3.4), and the results of machine learning experi-\nments carried out on the corpus (Section 3.5).\n2 Previous Corpora for Automatic\nReadability Assessment and Automatic\nText Simpliﬁcation\nA number of corpora for use in automatic read-\nability assessment and automatic text simpliﬁca-\ntion exist. The most well-known example is the\nParallel Wikipedia Simpliﬁcation Corpus (PWKP)\ncompiled from parallel articles of the English\nWikipedia and Simple English Wikipedia (Zhu et\nal., 2010) and consisting of around 108,000 sen-\ntence pairs. The corpus proﬁle is shown in Table 1.\nWhile the corpus represents the largest dataset in-\nvolving simpliﬁed language to date, its applica-\ntion has been criticized for various reasons (Aman-\ncio and Specia, 2014; Xu et al., 2015; ˇStajner et\nal., 2018); among these, the fact that Simple En-\nglish Wikipedia articles are not necessarily direct\ntranslations of articles from the English Wikipedia\nstands out. Hwang et al. (2015) provided an up-\ndated version of the corpus that includes a total\nof 280,000 full and partial matches between the\ntwo Wikipedia versions. Another frequently used\ndata collection for English is the Newsela Corpus(Xu et al., 2015) consisting of 1,130 news articles,\neach simpliﬁed into four school grade levels by\nprofessional editors. Table 2 shows the proﬁle of\nthe Newsela Corpus. The table obviates that the\ndifference in vocabulary size between the English\nand the simpliﬁed English side of the PWKP Cor-\npus amounts to only 18%, while the corresponding\nnumber for the English side and the level repre-\nsenting the highest amount of simpliﬁcation in the\nNewsela Corpus (Simple-4) is 50.8%. V ocabulary\nsize as an indicator of lexical richness is generally\ntaken to correlate positively with complexity (Vaj-\njala and Meurers, 2012).\nGasperin et al. (2010) compiled the PorSimples\nCorpus consisting of Brazilian Portuguese texts\n(2,116 sentences), each with a natural and a strong\nsimpliﬁcation, resulting in around 4,500 aligned\nsentences. Drndarevi´ c and Saggion (2012), Bott\net al. (2012), Bott and Saggion (2012) produced\nthe Simplext Corpus consisting of 200 Span-\nish/simpliﬁed Spanish document pairs, amount-\ning to a total of 1,149 (Spanish)/1,808 (simpliﬁed\nSpanish) sentences (approximately 1,000 aligned\nsentences).\nKlaper et al. (2013) created the ﬁrst parallel cor-\npus for German/simpliﬁed German, consisting of\n256 parallel texts downloaded from the web (ap-\nproximately 70,000 tokens).\n3 Building a Corpus for Automatic\nRead', 'pora. Speciﬁcally, for automatic text simpliﬁca-\ntion via machine translation, pairs of standard-\nlanguage/simpliﬁed-language texts aligned at the\nsentence level (i.e., parallel corpora) are needed.\nThe paper at hand introduces a corpus devel-\noped for use in automatic readability assessment\nand automatic text simpliﬁcation of German. The\nfocus of this publication is on representing infor-\nmation that is valuable for these tasks but that hith-\nerto has largely been ignored in machine learning\napproaches centering around simpliﬁed language,\nspeciﬁcally, text structure (e.g., paragraphs, lines),\ntypography (e.g., font type, font style), and im-\nage (content, position, and dimensions) informa-\ntion. The importance of considering such infor-\nmation has repeatedly been asserted theoretically\n(Arf´ e et al., 2018; Bock, 2018; Bredel and Maaß,\n2016).\nThe remainder of this paper is structured as fol-\nlows: Section 2 presents previous corpora used for\nautomatic readability assessment and text simpliﬁ-\ncation. Section 3 describes our corpus, introduc-\ning its novel aspects and presenting the primary\ndata (Section 3.1), the metadata (Section 3.2), the\nsecondary data (Section 3.3), the proﬁle (Section\n3.4), and the results of machine learning experi-\nments carried out on the corpus (Section 3.5).\n2 Previous Corpora for Automatic\nReadability Assessment and Automatic\nText Simpliﬁcation\nA number of corpora for use in automatic read-\nability assessment and automatic text simpliﬁca-\ntion exist. The most well-known example is the\nParallel Wikipedia Simpliﬁcation Corpus (PWKP)\ncompiled from parallel articles of the English\nWikipedia and Simple English Wikipedia (Zhu et\nal., 2010) and consisting of around 108,000 sen-\ntence pairs. The corpus proﬁle is shown in Table 1.\nWhile the corpus represents the largest dataset in-\nvolving simpliﬁed language to date, its applica-\ntion has been criticized for various reasons (Aman-\ncio and Specia, 2014; Xu et al., 2015; ˇStajner et\nal., 2018); among these, the fact that Simple En-\nglish Wikipedia articles are not necessarily direct\ntranslations of articles from the English Wikipedia\nstands out. Hwang et al. (2015) provided an up-\ndated version of the corpus that includes a total\nof 280,000 full and partial matches between the\ntwo Wikipedia versions. Another frequently used\ndata collection for English is the Newsela Corpus(Xu et al., 2015) consisting of 1,130 news articles,\neach simpliﬁed into four school grade levels by\nprofessional editors. Table 2 shows the proﬁle of\nthe Newsela Corpus. The table obviates that the\ndifference in vocabulary size between the English\nand the simpliﬁed English side of the PWKP Cor-\npus amounts to only 18%, while the corresponding\nnumber for the English side and the level repre-\nsenting the highest amount of simpliﬁcation in the\nNewsela Corpus (Simple-4) is 50.8%. V ocabulary\nsize as an indicator of lexical richness is generally\ntaken to correlate positively with complexity (Vaj-\njala and Meurers, 2012).\nGasperin et al. (2010) compiled the PorSimples\nCorpus consisting of Brazilian Portuguese texts\n(2,116 sentences), each with a natural and a strong\nsimpliﬁcation, resulting in around 4,500 aligned\nsentences. Drndarevi´ c and Saggion (2012), Bott\net al. (2012), Bott and Saggion (2012) produced\nthe Simplext Corpus consisting of 200 Span-\nish/simpliﬁed Spanish document pairs, amount-\ning to a total of 1,149 (Spanish)/1,808 (simpliﬁed\nSpanish) sentences (approximately 1,000 aligned\nsentences).\nKlaper et al. (2013) created the ﬁrst parallel cor-\npus for German/simpliﬁed German, consisting of\n256 parallel texts downloaded from the web (ap-\nproximately 70,000 tokens).\n3 Building a Corpus for Automatic\nRead']","The newly developed corpus for automatic readability assessment and text simplification of German provides information on text structure, typography, and images.",conditional,TRUE
84,How does dogmatism impact a conversation on Reddit? Does engaging with a dogmatic comment make users more dogmatic themselves?,"['We then ﬁt these behavioral features to a linear re-\ngression model where we predict each user’s average\ndogmatism level. Positive coefﬁcients in this model\nare positively predictive of dogmatism, while nega-\ntive coefﬁcients are negatively predictive.\nWe ﬁnd this model is signiﬁcantly predicitive of\ndogmatism ( R2= 0.1,p< 0.001), with all features\nreaching statistical signiﬁcance ( p < 0.001).Activ-\nityandfocus are positively associated with dogma-\ntism, while breadth andengagement are negatively\nassociated (Table 5). Together, these results suggest\ndogmatic users tend to post frequently and in spe-\nciﬁc communities, but are not as inclined to continue\nto engage with a discussion, once it has begun.\n5.4 How does dogmatism impact a\nconversation? (R4)\nHow does interacting with a dogmatic comment im-\npact a conversation? Are users able to shrug it off?\nOr do otherwise non-dogmatic users become more\ndogmatic themselves?\nTo answer this question, we sample 600,000 con-\nversations triples from Reddit. These conversations\nconsist of two people (A and B) talking, with the\nstructure: A1 →B→A2. This allows us to mea-\nsure the impact of B’s dogmatism on A’s response,\nwhile also controlling for the dogmatism level ini-\ntially set by A. Concretely, we model the impact of\ndogmatism on these conversations through a linear\nregression. This model takes two features, the dog-\nmatism levels of A1 and B, and predicts the dogma-\ntism response of A2. If B’s dogmatism has no effect\non A’s response, the coefﬁcient that corresponds to\nB will not be signiﬁcant in the model. Alternatively,\nif B’s dogmatism does have some effect, it will be\ncaptured by the model’s coefﬁcient.\nWe ﬁnd the coefﬁcient of the B feature in the\nmodel is positively associated with dogmatism ( p<\n0.001). In other words, engagement with a dog-\nmatic comment tends to make a user more dogmatic\nthemselves. This effect holds when we run the same\nmodel on data subsets consisting only of dogmatic\nor non-dogmatic users, and also when we conserva-\ntively remove all words used by B from A’s response\n(i.e., controlling for quoting effects).6 Related Work\nIn contrast to the computational models we have pre-\nsented, dogmatism is usually measured in psychol-\nogy through survey scales, in which study partic-\nipants answer questions designed to reveal under-\nlying personality attributes (Rokeach, 1954). Over\ntime, these surveys have been updated (Shearman\nand Levine, 2006) and improved to meet standards\nof psychometric validity (Crowson, 2009).\nThese surveys are often used to study the rela-\ntionship between dogmatism and other psychologi-\ncal phenomena. For example, dogmatic people tend\nto show an increased tendency for confrontation (El-\nNawawy and Powers, 2010) or moral conviction and\nreligiosity (Swink, 2011), and less likelihood of cog-\nnitive ﬂexibility (Martin et al., 2011), even among\nstereotypically non-dogmatic groups like atheists\n(Gurney et al., 2013). From a behavioral standpoint,\ndogmatic people solve problems differently, spend-\ning less time framing a problem and expressing more\ncertainty in their solution (Lohman, 2010). Here we\nsimilarly examine how user behaviors on Reddit re-\nlate to a language model of dogmatism.\nErtel sought to capture dogmatism linguistically,\nthough a small lexicon of words that correspond\nwith high-level concepts like certainty and compro-\nmise (1985). McKenny then used this dictionary to\nrelate dogmatism to argument quality in student es-\nsays (2005). Our work expands on this approach,\napplying supervised models based on a broader set\nof linguistic categories to identify dogmatism in text.\nOther researchers have studied topics similar to\ndogmatism, such as signals of cognitive style in\nright-wing political thought (Van Hiel et al., 2010),\nthe language used by trolls on social media (', 'Cheng\net al., 2015), or what makes for impartial language\non twitter (Zafar et al., 2016). A similar ﬂavor of\nwork has examined linguistic models that capture\npoliteness (Danescu-Niculescu-Mizil et al., 2013),\ndeception (Ott et al., 2011), and authority (Gilbert,\n2012). We took inspiration from these models when\nconstructing the feature sets in our work.\nFinally, while we examine what makes an opin-\nion dogmatic, other work has pushed further into the\nstructure of arguments, for example classifying their\njustiﬁcations (Hasan and Ng, 2014), or what makes\nan argument likely to win (Tan et al., 2016). Our', 'model may allow future researchers to probe these\nquestions more deeply.\n7 Conclusion\nWe have constructed the ﬁrst corpus of social me-\ndia posts annotated with dogmatism scores, allowing\nus to explore linguistic features of dogmatism and\nbuild a predictive model that analyzes new content.\nWe apply this model to Reddit, where we discover\nbehavioral predictors of dogmatism and topical pat-\nterns in the comments of dogmatic users.\nCould we use this computational model to help\nusers shed their dogmatic beliefs? Looking forward,\nour work makes possible new avenues for encourag-\ning pro-social behavior in online communities.\nReferences\n[Cheng et al.2015] Justin Cheng, Cristian Danescu-\nNiculescu-Mizil, and Jure Leskovec. 2015. Antisocial\nbehavior in online discussion communities. arXiv\npreprint arXiv:1504.00680 .\n[Church and Hanks1990] Kenneth Ward Church and\nPatrick Hanks. 1990. Word association norms,\nmutual information, and lexicography. Computational\nlinguistics , 16(1):22–29.\n[Crowson2009] H Michael Crowson. 2009. Does the\ndog scale measure dogmatism? another look at con-\nstruct validity. The Journal of social psychology ,\n149(3):365–383.\n[Danescu-Niculescu-Mizil et al.2013] Cristian Danescu-\nNiculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure\nLeskovec, and Christopher Potts. 2013. A computa-\ntional approach to politeness with application to social\nfactors. arXiv preprint arXiv:1306.6078 .\n[Doroudi et al.2016] Shayan Doroudi, Ece Kamar, Emma\nBrunskill, and Eric Horvitz. 2016. Toward a learning\nscience for complex crowdsourcing tasks. In Proceed-\nings of the 2016 CHI Conference on Human Factors in\nComputing Systems , pages 2623–2634. ACM.\n[El-Nawawy and Powers2010] Mohammed El-Nawawy\nand Shawn Powers. 2010. Al-jazeera english a con-\nciliatory medium in a conﬂict-driven environment?\nGlobal Media and Communication , 6(1):61–84.\n[Ertel1985] S Ertel. 1985. Content analysis: An alter-\nnative approach to open and closed minds. The High\nSchool Journal , 68(4):229–240.\n[Gilbert2012] Eric Gilbert. 2012. Phrases that signal\nworkplace hierarchy. In Proceedings of the ACM 2012\nconference on Computer Supported Cooperative Work ,\npages 1037–1046. ACM.[Gurney et al.2013] Daniel J Gurney, Shelley McKeown,\nJamie Churchyard, and Neil Howlett. 2013. Believe it\nor not: Exploring the relationship between dogmatism\nand openness within non-religious samples. Personal-\nity and Individual Differences , 55(8):936–940.\n[Hasan and Ng2014] Kazi Saidul Hasan and Vincent Ng.\n2014. Why are you taking this stance? identifying and\nclassifying reasons in ideological debates. In EMNLP ,\npages 751–762.\n[Hayes and Krippendorff2007] Andrew F Hayes and\nKlaus Krippendorff. 2007. Answering the call\nfor a standard reliability measure for coding data.\nCommunication methods and measures , 1(1):77–89.\n[Lohman2010] Margaret C Lohman. 2010. An unex-\namined triumvirate: dogmatism, problem solving, and\nhrd. Human Resource Development Review .\n[Martin et al.2011] Matthew M Martin, Sydney M Stag-\ngers, and Carolyn M Anderson. 2011. The relation-\nships between cognitive ﬂexibility with dogmatism,\nintellectual ﬂexibility, preference for consistency, and\nself-compassion. Communication Research Reports ,\n28(3):275–280.\n[McCluskey and Hmielowski2012] Michael McCluskey\nand Jay Hmielowski. 2012. Opinion expression dur-\ning social conﬂict: Comparing online reader com-\nments and letters to the editor. Journalism , 13(3):303–\n319.\n[McKenny2005] John McKenny. 2005. Content analy-\nsis of dogmatism compared with corpus analysis of\nepistemic stance in student essays']",Engaging with a dogmatic comment tends to make a user more dogmatic themselves.,conditional,TRUE
85,"What is the effect of reducing training data on the performance of the BERT-based model, and how does it compare to other systems in terms of robustness and F1-score?","['be more dangerous than the unintended over-obfuscation of\nnon-sensitive text.\nFurther, we have conducted an additional experiment on\nthis dataset by progressively reducing the training data for\nall the compared systems. The BERT-based model shows\nthe highest robustness to training-data scarcity, loosing only\n7 points of F1-score when trained on 230 instances instead\nof 21,371. These observation are in line with the results ob-\ntained by the NLP community using BERT for other tasks.\nThe experiments with the MEDDOCAN 2019 shared task\ndataset follow the same pattern. In this case, the BERT-\nbased model falls 0.3 F1-score points behind the shared task\nwinning system, but it would have achieved the second po-\nsition in the competition with no further reﬁnement.\nSince we have used a pre-trained multilingual BERT model,\nthe same approach is likely to work for other languages just\nby providing some labelled training data. Further, this is the\nsimplest ﬁne-tuning that can be performed based on BERT.\nMore sophisticated ﬁne-tuning layers could help improve\nthe results. For example, it could be expected that a CRF\nlayer helped enforce better BIO tagging sequence predic-\ntions. Precisely, Mao and Liu (2019) participated in the\nMEDDOCAN competition using a BERT+CRF architec-\nture, but their reported scores are about 3 points lower than\nour implementation. From the description of their work, it\nis unclear what the source of this score difference could be.\nFurther, at the time of writing this paper, new multilingual\npre-trained models and Transformer architectures have be-\ncome available. It would not come as a surprise that these\nnew resources and systems –e.g., XLM-RoBERTa (Con-\nneau et al., 2019) or BETO (Wu and Dredze, 2019), a BERT\nmodel fully pre-trained on Spanish texts– further advanced\nthe state of the art in this task.\n6. Acknowledgements\nThis work has been supported by Vicomtech and partially\nfunded by the project DeepReading (RTI2018-096846-B-\nC21, MCIU/AEI/FEDER,UE).\n7. Bibliographical References\nAbouelmehdi, K., Beni-Hessane, A., and Khalouﬁ, H.\n(2018). Big healthcare data: preserving security and pri-\nvacy. Journal of Big Data , 5(1):1–18.\nAgerri, R., Bermudez, J., and Rigau, G. (2014). IXA\npipeline: Efﬁcient and Ready to Use Multilingual NLP\ntools. In Proceedings of the 9th Language Resources and\nEvaluation Conference (LREC 2014) , pages 3823–3828.\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary,\nV ., Wenzek, G., Guzm ´an, F., Grave, E., Ott, M.,\nZettlemoyer, L., and Stoyanov, V . (2019). Unsuper-\nvised Cross-lingual Representation Learning at Scale.\narXiv:1911.02116 .\nDernoncourt, F., Lee, J. Y ., Uzuner, ¨O., and Szolovits, P.\n(2016). De-identiﬁcation of Patient Notes with Recur-\nrent Neural Networks. Journal of the American Medical\nInformatics Association , 24(3):596–606.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K.\n(2019). BERT: Pre-training of Deep Bidirectional Trans-\nformers for Language Understanding. In Proceedings of\nthe 2019 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Pa-\npers) , pages 4171–4186.\nGarc ´ıa-Sardi ˜na, L. (2018). Automating the anonymisa-\ntion of textual corpora. Master’s thesis, University of the\nBasque Country (UPV/EHU).\nHassan, F., Domingo-Ferrer, J., and Soria-Comas, J.\n(2018). Anonimizaci ´on de datos no estructurados a\ntrav´es del reconocimiento de entidades nominadas. In', '0 50 1000.40.60.81\ntrain data (%)precision\n0 50 1000.40.60.81\ntrain data (%)recall\n0 50 1000.40.60.81\ntrain data (%)F1-score\nCRF spaCy BERT\nFigure 2: Performance decay with decreasing amounts of training data on the sensitive information detection task in the\nNUB ES-PHI corpus\nDetection Classiﬁcation\nPrec Rec F1 Prec Rec F1\nCRF (Perez et al., 2019) 0.977 0.943 0.960 0.971 0.937 0.954\nspaCy (Perez et al., 2019) 0.967 0.953 0.965 0.965 0.947 0.956\nNLND S2 (Lange et al., 2019) 0.976 0.973 0.974 0.971 0.968 0.970\nNLND S3 (Lange et al., 2019) 0.975 0.975 0.975 0.970 0.969 0.970\nBERT + CRF (Mao and Liu, 2019) 0.968 0.919 0.943 0.965 0.912 0.937\nBERT 0.973 0.972 0.972 0.968 0.967 0.967\nTable 8: Results of Experiment B: MEDDOCAN\na larger extent from recall decline, which is not that marked\nin the case of BERT. Overall, these results indicate that the\ntransfer-learning achieved through the BERT multilingual\npre-trained model not only helps obtain better results, but\nalso lowers the need of manually labelled data for this ap-\nplication domain.\n4.2. Experiment B: MEDDOCAN\nThe results of the two MEDDOCAN scenarios –detection\nand classiﬁcation– are shown in Table 8. These results fol-\nlow the same pattern as in the previous experiments, with\nthe CRF classiﬁer being the most precise of all, and BERT\noutperforming both the CRF and spaCy classiﬁers thanks to\nits greater recall. We also show the results of Mao and Liu\n(2019) who, despite of having used a BERT-based system,\nachieve lower scores than our models. The reason why it\nshould be so remain unclear.\nWith regard to the winner of the MEDDOCAN shared\ntask, the BERT-based model has not improved the scores\nobtained by neither the domain-dependent (S3) nor the\ndomain-independent (S2) NLNDE model. However, at-\ntending to the obtained results, BERT remains only 0.3 F1-\nscore points behind, and would have achieved the second\nposition among all the MEDDOCAN shared task competi-\ntors. Taking into account that only 3% of the gold labels\nremain incorrectly annotated, the task can be considered al-\nmost solved, and it is not clear if the differences among the\nsystems are actually signiﬁcant, or whether they stem from\nminor variations in initialisation or a long-tail of minor la-belling inconsistencies.\n5. Conclusions and Future Work\nIn this work we have brieﬂy introduced the problems re-\nlated to data privacy protection in clinical domain. We have\nalso described some of the groundbreaking advances on the\nNatural Language Processing ﬁeld due to the appearance of\nTransformers-based deep-learning architectures and trans-\nfer learning from very large general-domain multilingual\ncorpora, focusing our attention in one of its most represen-\ntative examples, Google’s BERT model.\nIn order to assess the performance of BERT for Span-\nish clinical data anonymisation, we have conducted sev-\neral experiments with a BERT-based sequence labelling\napproach using the pre-trained multilingual BERT model\nshared by Google as the starting point for the model train-\ning. We have compared this BERT-based sequence la-\nbelling against other methods and systems. One of the ex-\nperiments uses the MEDDOCAN 2019 shared task dataset,\nwhile the other uses a novel Spanish clinical reports dataset\ncalled NUB ES-PHI.\nThe results of the experiments show that, in NUB ES-PHI,\nthe BERT-based model outperforms the other systems with-\nout requiring any adaptation or domain-speciﬁc feature en-\ngineering, just by being trained on the provided labelled\ndata. Interestingly, the BERT-based model obtains a re-\n']","The BERT-based model shows the highest robustness to training-data scarcity, losing only 7 points of F1-score when trained on 230 instances instead of 21,371. These observations are in line with the results obtained by the NLP community using BERT for other tasks.",conditional,TRUE
86,"What does the anisotropy of contextualized word representations in ELMo, BERT, and GPT-2 suggest about the range of context-specific representations for each word?","['How Contextual are Contextualized Word Representations?\nComparing the Geometry of BERT, ELMo, and GPT-2 Embeddings\nKawin Ethayarajh∗\nStanford University\nkawin@stanford.edu\nAbstract\nReplacing static word embeddings with con-\ntextualized word representations has yielded\nsigniﬁcant improvements on many NLP tasks.\nHowever, just how contextual are the contex-\ntualized representations produced by models\nsuch as ELMo and BERT? Are there inﬁnitely\nmany context-speciﬁc representations for each\nword, or are words essentially assigned one of\na ﬁnite number of word-sense representations?\nFor one, we ﬁnd that the contextualized rep-\nresentations of all words are not isotropic in\nany layer of the contextualizing model. While\nrepresentations of the same word in differ-\nent contexts still have a greater cosine simi-\nlarity than those of two different words, this\nself-similarity is much lower in upper layers.\nThis suggests that upper layers of contextu-\nalizing models produce more context-speciﬁc\nrepresentations, much like how upper layers\nof LSTMs produce more task-speciﬁc repre-\nsentations. In all layers of ELMo, BERT, and\nGPT-2, on average, less than 5% of the vari-\nance in a word’s contextualized representa-\ntions can be explained by a static embedding\nfor that word, providing some justiﬁcation for\nthe success of contextualized representations.\n1 Introduction\nThe application of deep learning methods to NLP\nis made possible by representing words as vec-\ntors in a low-dimensional continuous space. Tradi-\ntionally, these word embeddings were static : each\nword had a single vector, regardless of context\n(Mikolov et al., 2013a; Pennington et al., 2014).\nThis posed several problems, most notably that\nall senses of a polysemous word had to share the\nsame representation. More recent work, namely\ndeep neural language models such as ELMo (Pe-\nters et al., 2018) and BERT (Devlin et al., 2018),\n∗Work partly done at the University of Toronto.have successfully created contextualized word rep-\nresentations , word vectors that are sensitive to\nthe context in which they appear. Replacing\nstatic embeddings with contextualized representa-\ntions has yielded signiﬁcant improvements on a di-\nverse array of NLP tasks, ranging from question-\nanswering to coreference resolution.\nThe success of contextualized word represen-\ntations suggests that despite being trained with\nonly a language modelling task, they learn highly\ntransferable and task-agnostic properties of lan-\nguage. In fact, linear probing models trained on\nfrozen contextualized representations can predict\nlinguistic properties of words (e.g., part-of-speech\ntags) almost as well as state-of-the-art models (Liu\net al., 2019a; Hewitt and Manning, 2019). Still,\nthese representations remain poorly understood.\nFor one, just how contextual are these contextu-\nalized word representations? Are there inﬁnitely\nmany context-speciﬁc representations that BERT\nand ELMo can assign to each word, or are words\nessentially assigned one of a ﬁnite number of\nword-sense representations?\nWe answer this question by studying the geom-\netry of the representation space for each layer of\nELMo, BERT, and GPT-2. Our analysis yields\nsome surprising ﬁndings:\n1. In all layers of all three models, the con-\ntextualized word representations of all words\nare not isotropic: they are not uniformly dis-\ntributed with respect to direction. Instead,\nthey are anisotropic , occupying a narrow\ncone in the vector space. The anisotropy in\nGPT-2’s last layer is so extreme that two ran-\ndom words will on average have almost per-\nfect cosine similarity! Given that isotropy\nhas both theoretical and empirical beneﬁts for\nstatic embeddings (Mu et al., 2018), the ex-\ntent of anisotropy in contextualized represen-arXiv:1909.00512v1  [cs.CL]  2 Sep 2019']","The anisotropy of contextualized word representations in ELMo, BERT, and GPT-2 suggests that there are a finite number of context-specific representations for each word.",conditional,TRUE
87,"What is the main challenge in zero-shot translation for transfer learning in Neural Machine Translation, considering the language space mismatch problem between the transferor and transferee models?","['Cross-lingual Pre-training Based Transfer for Zero-shot Neural\nMachine Translation\nBaijun Ji‡, Zhirui Zhang§, Xiangyu Duan†‡∗, Min Zhang†‡, Boxing Chen§and Weihua Luo§\n†Institute of Artiﬁcial Intelligence, Soochow University, Suzhou, China\n‡School of Computer Science and Technology, Soochow University, Suzhou, China\n§Alibaba DAMO Academy, Hangzhou, China\n‡bjji@stu.suda.edu.cn†{xiangyuduan, minzhang}@suda.edu.cn\n§{zhirui.zzr,boxing.cbx,weihua.luowh}@alibaba-inc.com\nAbstract\nTransfer learning between different language pairs has shown\nits effectiveness for Neural Machine Translation (NMT) in\nlow-resource scenario. However, existing transfer methods\ninvolving a common target language are far from success in\nthe extreme scenario of zero-shot translation, due to the lan-\nguage space mismatch problem between transferor (the par-\nent model) and transferee (the child model) on the source\nside. To address this challenge, we propose an effective trans-\nfer learning approach based on cross-lingual pre-training. Our\nkey idea is to make all source languages share the same fea-\nture space and thus enable a smooth transition for zero-shot\ntranslation. To this end, we introduce one monolingual pre-\ntraining method and two bilingual pre-training methods to\nobtain a universal encoder for different languages. Once the\nuniversal encoder is constructed, the parent model built on\nsuch encoder is trained with large-scale annotated data and\nthen directly applied in zero-shot translation scenario. Exper-\niments on two public datasets show that our approach signif-\nicantly outperforms strong pivot-based baseline and various\nmultilingual NMT approaches.\nIntroduction\nAlthough Neural Machine Translation (NMT) has domi-\nnated recent research on translation tasks (Wu et al. 2016;\nVaswani et al. 2017; Hassan et al. 2018), NMT heavily relies\non large-scale parallel data, resulting in poor performance\non low-resource or zero-resource language pairs (Koehn\nand Knowles 2017). Translation between these low-resource\nlanguages (e.g., Arabic →Spanish) is usually accomplished\nwith pivoting through a rich-resource language (such as En-\nglish), i.e., Arabic (source) sentence is translated to En-\nglish (pivot) ﬁrst which is later translated to Spanish (tar-\nget) (Kauers et al. 2002; de Gispert and Mariño 2006).\nHowever, the pivot-based method requires doubled decoding\ntime and suffers from the propagation of translation errors.\nOne common alternative to avoid pivoting in NMT is\ntransfer learning (Zoph et al. 2016; Nguyen and Chiang\n2017; Kocmi and Bojar 2018; Kim et al. 2019) which lever-\nages a high-resource pivot →target model ( parent ) to ini-\n∗Corresponding Author.\nCopyright c⃝2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The circle and triangle dots represent source sen-\ntences in different language l1andl2, and the square dots\nmeans target sentences in language l3. A sample of transla-\ntion pairs is connected by the dashed line. We would like to\nforce each of the translation pairs has the same latent rep-\nresentation as the right part of the ﬁgure so as to transfer\nl1→l3model directly to l2→l3model.\ntialize a low-resource source →target model ( child ) that is\nfurther optimized with a small amount of available paral-\nlel data. Although this approach has achieved success in\nsome low-resource language pairs, it still performs very\npoorly in extremely low-resource or zero-resource transla-\ntion scenario. Speciﬁcally, Kocmi and Bojar (2018) reports\nthat without any child model training data, the performance\nof the parent model on the child test set is miserable.\nIn this work, we argue that the language space mis-\nmatch problem, also named domain shift problem (Fu et al.\n2015), brings about the zero-shot translation failure in trans-\nfer learning. It is because transfer learning has no explicit\ntraining process to guarantee that the source and pivot', 'as shown in the right of Figure 1. One way to achieve this\ngoal is the ﬁne-tuning technique, which forces the model to\nforget the speciﬁc knowledge from parent data and learn new\nfeatures from child data. However, the domain shift problem\nstill exists, and the demand of parallel child data for ﬁne-\ntuning heavily hinders transfer learning for NMT towards\nthe zero-resource setting.\nIn this paper, we explore the transfer learning in\na common zero-shot scenario where there are a lot\nof source↔pivot and pivot↔target parallel data but no\nsource↔target parallel data. In this scenario, we propose\na simple but effective transfer approach, the key idea\nof which is to relieve the burden of the domain shift\nproblem by means of cross-lingual pre-training. To this\nend, we ﬁrstly investigate the performance of two exist-\ning cross-lingual pre-training methods proposed by Lam-\nple and Conneau (2019) in zero-shot translation scenario.\nBesides, a novel pre-training method called BRidge Lan-\nguage Modeling (BRLM) is designed to make full use of the\nsource↔pivot bilingual data to obtain a universal encoder\nfor different languages. Once the universal encoder is con-\nstructed, we only need to train the pivot →target model and\nthen test this model in source →target direction directly. The\nmain contributions of this paper are as follows:\n•We propose a new transfer learning approach for NMT\nwhich uses the cross-lingual language model pre-training\nto enable a high performance on zero-shot translation.\n•We propose a novel pre-training method called BRLM,\nwhich can effectively alleviates the distance between dif-\nferent source language spaces.\n•Our proposed approach signiﬁcantly improves zero-shot\ntranslation performance, consistently surpassing pivot-\ning and multilingual approaches. Meanwhile, the perfor-\nmance on supervised translation direction remains the\nsame level or even better when using our method.\nRelated Work\nIn recent years, zero-shot translation in NMT has attracted\nwidespread attention in academic research. Existing meth-\nods are mainly divided into four categories: pivot-based\nmethod, transfer learning, multilingual NMT, and unsuper-\nvised NMT.\n•Pivot-based Method is a common strategy to obtain a\nsource→target model by introducing a pivot language.\nThis approach is further divided into pivoting and pivot-\nsynthetic. While the former ﬁrstly translates a source lan-\nguage into the pivot language which is later translated\nto the target language (Kauers et al. 2002; de Gispert\nand Mariño 2006; Utiyama and Isahara 2007), the lat-\nter trains a source→target model with pseudo data gener-\nated from source-pivot or pivot-target parallel data (Chen\net al. 2017; Zheng, Cheng, and Liu 2017). Although the\npivot-based methods can achieve not bad performance, it\nalways falls into a computation-expensive and parameter-\nvast dilemma of quadratic growth in the number of source\nlanguages, and suffers from the error propagation prob-\nlem (Zhu et al. 2013).•Transfer Learning is ﬁrstly introduced for NMT by\nZoph et al. (2016), which leverages a high-resource par-\nent model to initialize the low-resource child model. On\nthis basis, Nguyen and Chiang (2017) and Kocmi and\nBojar (2018) use shared vocabularies for source/target\nlanguage to improve transfer learning, while Kim, Gao,\nand Ney (2019) relieve the vocabulary mismatch by\nmainly using cross-lingual word embedding. Although\nthese methods are successful in the low-resource scene,\nthey have limited effects in zero-shot translation.\n•Multilingual NMT (MNMT) enables training a single\nmodel that supports translation from multiple source lan-\nguages into multiple target languages, even those unseen\nlanguage pairs (Firat, Cho, and Bengio 2016; Firat et al.\n2016; Johnson et al. 2016; Al-Shedivat and Parikh 2019;\nAharoni, Johnson, and Firat 2019). Aside from sim-\npler deployment, MNMT beneﬁts from transfer learning\nwhere low-resource language pairs are trained together\nwith high-resource ones. However, Gu et al. (2019) point\nout that MNMT for zero-shot translation easily fails, and\nis sensitive to the hyper-parameter setting. Also']",The main challenge in zero-shot translation for transfer learning in Neural Machine Translation is the language space mismatch problem between the transferor and transferee models.,conditional,TRUE
88,What is the impact of original relation names on matching longer question contexts in relation detection for KBQA?,"['cation .arXiv:1704.06194v2  [cs.CL]  27 May 2017', 'Question: what episode was mike kelleythe writer of\nKnowledgeBaseMike\tKelley(American\ttelevision\twriter/producer)Mike\tKelley(American\tbaseball\tplayer)…Entity Linking \nLove\tWill\tFind\ta\tWayUSA…First\tbaseman…episodes_written\nposition_playedRelation Detection(a)(b)Question: what tvshow did grantshowplay onin2008Mike\tKelley?episodes_writtenEntity Linking RelationDetectionGrantShow?starring_rolesseries(date)from2008ConstraintDetectionGrantShow\t(American\tactor)SwingTown\nBigLoveepisodesScoundrelsseries2011from20102008Figure 1: KBQA examples and its three key components. (a) A single relation example. We ﬁrst identify the topic entity with\nentity linking and then detect the relation asked by the question with relation detection (from all relations connecting the topic\nentity). Based on the detected entity and relation, we form a query to search the KB for the correct answer “ Love Will Find a\nWay”. (b) A more complex question containing two entities. By using “ Grant Show ” as the topic entity, we could detect a chain\nof relations “ starring roles-series ” pointing to the answer. An additional constraint detection takes the other entity “ 2008 ” as\na constraint, to ﬁlter the correct answer “ SwingTown ” from all candidates found by the topic entity and relation.\nthat original relation names can sometimes help\nto match longer question contexts, we propose\nto build both relation-level and word-level rela-\ntion representations. Third, we use deep bidirec-\ntional LSTMs ( BiLSTM s) to learn different levels\nof question representations in order to match the\ndifferent levels of relation information. Finally, we\npropose a residual learning method for sequence\nmatching, which makes the model training easier\nand results in more abstract (deeper) question rep-\nresentations, thus improves hierarchical matching.\nIn order to assess how the proposed improved\nrelation detection could beneﬁt the KBQA end\ntask, we also propose a simple KBQA implemen-\ntation composed of two-step relation detection .\nGiven an input question and a set of candidate enti-\nties retrieved by an entity linker based on the ques-\ntion, our proposed relation detection model plays a\nkey role in the KBQA process: (1) Re-ranking the\nentity candidates according to whether they con-\nnect to high conﬁdent relations detected from the\nraw question text by the relation detection model.\nThis step is important to deal with the ambigui-\nties normally present in entity linking results. (2)\nFinding the core relation (chains) for each topic\nentity2selection from a much smaller candidate\nentity set after re-ranking. The above steps are\nfollowed by an optional constraint detection step,\nwhen the question cannot be answered by single\nrelations (e.g., multiple entities in the question).\nFinally the highest scored query from the above\n2Following Yih et al. (2015), here topic entity refers to\nthe root of the (directed) query tree; and core-chain is the\ndirected path of relation from root to the answer node.steps is used to query the KB for answers.\nOur main contributions include: (i) An im-\nproved relation detection model by hierarchical\nmatching between questions and relations with\nresidual learning; (ii) We demonstrate that the im-\nproved relation detector enables our simple KBQA\nsystem to achieve state-of-the-art results on both\nsingle-relation and multi-relation KBQA tasks.\n2 Related Work\nRelation Extraction Relation extraction (RE) is\nan important sub-ﬁeld of information extraction.\nGeneral research in this ﬁeld usually works on a\n(small) pre-deﬁned relation set, where given a text\nparagraph and two target entities, the goal is to\ndetermine whether the text indicates any types of\nrelations between the entities or not. As a result\nRE is usually formulated as a classiﬁcation task .\nTraditional RE methods rely on large amount of\nhand-crafted features (Zhou et al., 2005; Rink and\nHarabagiu, 2010; Sun et al., 2011). Recent re-\nsearch beneﬁts a lot from the advancement of deep\nlearning: from word embeddings (Nguyen and Gr-\nishman, 2014; Gormley et al., 2015) to deep net-\nworks', ' like CNNs and LSTMs (Zeng et al., 2014;\ndos Santos et al., 2015; Vu et al., 2016) and atten-\ntion models (Zhou et al., 2016; Wang et al., 2016).\nThe above research assumes there is a ﬁxed\n(closed) set of relation types, thus no zero-shot\nlearning capability is required. The number\nof relations is usually not large: The widely\nused ACE2005 has 11/32 coarse/ﬁne-grained rela-\ntions; SemEval2010 Task8 has 19 relations; TAC-', 'Question: what episode was mike kelleythe writer of\nKnowledgeBaseMike\tKelley(American\ttelevision\twriter/producer)Mike\tKelley(American\tbaseball\tplayer)…Entity Linking \nLove\tWill\tFind\ta\tWayUSA…First\tbaseman…episodes_written\nposition_playedRelation Detection(a)(b)Question: what tvshow did grantshowplay onin2008Mike\tKelley?episodes_writtenEntity Linking RelationDetectionGrantShow?starring_rolesseries(date)from2008ConstraintDetectionGrantShow\t(American\tactor)SwingTown\nBigLoveepisodesScoundrelsseries2011from20102008Figure 1: KBQA examples and its three key components. (a) A single relation example. We ﬁrst identify the topic entity with\nentity linking and then detect the relation asked by the question with relation detection (from all relations connecting the topic\nentity). Based on the detected entity and relation, we form a query to search the KB for the correct answer “ Love Will Find a\nWay”. (b) A more complex question containing two entities. By using “ Grant Show ” as the topic entity, we could detect a chain\nof relations “ starring roles-series ” pointing to the answer. An additional constraint detection takes the other entity “ 2008 ” as\na constraint, to ﬁlter the correct answer “ SwingTown ” from all candidates found by the topic entity and relation.\nthat original relation names can sometimes help\nto match longer question contexts, we propose\nto build both relation-level and word-level rela-\ntion representations. Third, we use deep bidirec-\ntional LSTMs ( BiLSTM s) to learn different levels\nof question representations in order to match the\ndifferent levels of relation information. Finally, we\npropose a residual learning method for sequence\nmatching, which makes the model training easier\nand results in more abstract (deeper) question rep-\nresentations, thus improves hierarchical matching.\nIn order to assess how the proposed improved\nrelation detection could beneﬁt the KBQA end\ntask, we also propose a simple KBQA implemen-\ntation composed of two-step relation detection .\nGiven an input question and a set of candidate enti-\nties retrieved by an entity linker based on the ques-\ntion, our proposed relation detection model plays a\nkey role in the KBQA process: (1) Re-ranking the\nentity candidates according to whether they con-\nnect to high conﬁdent relations detected from the\nraw question text by the relation detection model.\nThis step is important to deal with the ambigui-\nties normally present in entity linking results. (2)\nFinding the core relation (chains) for each topic\nentity2selection from a much smaller candidate\nentity set after re-ranking. The above steps are\nfollowed by an optional constraint detection step,\nwhen the question cannot be answered by single\nrelations (e.g., multiple entities in the question).\nFinally the highest scored query from the above\n2Following Yih et al. (2015), here topic entity refers to\nthe root of the (directed) query tree; and core-chain is the\ndirected path of relation from root to the answer node.steps is used to query the KB for answers.\nOur main contributions include: (i) An im-\nproved relation detection model by hierarchical\nmatching between questions and relations with\nresidual learning; (ii) We demonstrate that the im-\nproved relation detector enables our simple KBQA\nsystem to achieve state-of-the-art results on both\nsingle-relation and multi-relation KBQA tasks.\n2 Related Work\nRelation Extraction Relation extraction (RE) is\nan important sub-ﬁeld of information extraction.\nGeneral research in this ﬁeld usually works on a\n(small) pre-deﬁned relation set, where given a text\nparagraph and two target entities, the goal is to\ndetermine whether the text indicates any types of\nrelations between the entities or not. As a result\nRE is usually formulated as a classiﬁcation task .\nTraditional RE methods rely on large amount of\nhand-crafted features (Zhou et al., 2005; Rink and\nHarabagiu, 2010; Sun et al., 2011). Recent re-\nsearch beneﬁts a lot from the advancement of deep\nlearning: from word embeddings (Nguyen and Gr-\nishman, 2014; Gormley et al., 2015) to deep net-\nworks']",The impact of original relation names on matching longer question contexts in relation detection for KBQA is proposed to build both relation-level and word-level relation representations.,conditional,TRUE
89,"What potential advantages do static embeddings offer over contextualized word representations, considering their isotropy and ease of deployment?","['tations, such as those from Layer 1 of BERT, are\nmuch more effective.\n5 Future Work\nOur ﬁndings offer some new directions for future\nwork. For one, as noted earlier in the paper, Mu\net al. (2018) found that making static embeddings\nmore isotropic – by subtracting their mean from\neach embedding – leads to surprisingly large im-\nprovements in performance on downstream tasks.\nGiven that isotropy has beneﬁts for static embed-\ndings, it may also have beneﬁts for contextual-\nized word representations, although the latter have\nalready yielded signiﬁcant improvements despite\nbeing highly anisotropic. Therefore, adding an\nanisotropy penalty to the language modelling ob-\njective – to encourage the contextualized represen-\ntations to be more isotropic – may yield even better\nresults.\nAnother direction for future work is generat-\ning static word representations from contextual-\nized ones. While the latter offer superior per-\nformance, there are often challenges to deploying\nlarge models such as BERT in production, both\nwith respect to memory and run-time. In contrast,\nstatic representations are much easier to deploy.\nOur work in section 4.3 suggests that not only it is\npossible to extract static representations from con-\ntextualizing models, but that these extracted vec-\ntors often perform much better on a diverse array\nof tasks compared to traditional static embeddings\nsuch as GloVe and FastText. This may be a means\nof extracting some use from contextualizing mod-\nels without incurring the full cost of using them in\nproduction.\n6 Conclusion\nIn this paper, we investigated how contextual con-\ntextualized word representations truly are. For\none, we found that upper layers of ELMo, BERT,\nand GPT-2 produce more context-speciﬁc rep-\nresentations than lower layers. This increased\ncontext-speciﬁcity is always accompanied by in-\ncreased anisotropy. However, context-speciﬁcity\nalso manifests differently across the three models;\nthe anisotropy-adjusted similarity between words\nin the same sentence is highest in ELMo but al-\nmost non-existent in GPT-2. We ultimately found\nthat after adjusting for anisotropy, on average, less\nthan 5% of the variance in a word’s contextual-\nized representations could be explained by a staticembedding. This means that even in the best-case\nscenario, in all layers of all models, static word\nembeddings would be a poor replacement for con-\ntextualized ones. These insights help explain some\nof the remarkable success that contextualized rep-\nresentations have had on a diverse array of NLP\ntasks.\nAcknowledgments\nWe thank the anonymous reviewers for their in-\nsightful comments. We thank the Natural Sciences\nand Engineering Research Council of Canada\n(NSERC) for their ﬁnancial support.\nReferences\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel M\nCer, Mona T Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada\nMihalcea, et al. 2015. Semeval-2015 task 2: Seman-\ntic textual similarity, English, Spanish and pilot on\ninterpretability. In Proceedings SemEval@ NAACL-\nHLT . pages 252–263.\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel M\nCer, Mona T Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Rada Mihalcea, German Rigau, and Janyce\nWiebe. 2014. Semeval-2014 task 10: Multilin-\ngual semantic textual similarity. In Proceedings Se-\nmEval@ COLING . pages 81–91.\nEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-\nAgirre, and Weiwei Guo. 2013. Sem 2013 shared\ntask: Semantic textual similarity, including a pilot\non typed-similarity. In SEM 2013: The Second Joint\nConference on Lexical and Computational Seman-\ntics. Association for Computational Linguistics.\nEneko Agirre, Mona Diab, Daniel Cer, and Aitor\nGonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-\nlot on semantic textual similarity. In']",Static embeddings offer advantages in terms of isotropy and ease of deployment compared to contextualized word representations.,conditional,TRUE
90,"In the context of domain adaption, how has adversarial learning been modified to incorporate worker annotations for worker discrimination?","['-\nlikelihood objective as the loss function. We apply softmax\nover all candidate output label sequences, thus the probabil-\nity of the crowd-annotated label sequence is computed by:\np(¯ y|X) =exp(\nscore (X,¯ y))\n∑\ny∈YXexp(\nscore (X,y)), (3)\nwhere ¯ yis the crowd-annotated label sequences and YXis\nall candidate label sequence of input X.\nBased on the above formula, the loss function of our base-\nline model is:\nloss(Θ,X,¯ y) =−logp(¯ y|X), (4)\nwhere Θis the set of all model parameters. We use standard\nback-propagation method to minimize the loss function of\nthe baseline CRF model.', 'Worker Adversarial\nAdversarial learning has been an effective mechanism to re-\nsolve the problem of the input features between the training\nand test examples having large divergences (Goodfellow et\nal. 2014; Ganin et al. 2016). It has been successfully applied\non domain adaption (Gui et al. 2017), cross-lingual learn-\ning (Chen et al. 2016) and multi-task learning (Liu, Qiu, and\nHuang 2017). All settings involve feature shifting between\nthe training and testing.\nIn this paper, our setting is different. We are using the\nannotations from non-experts, which are noise and can in-\nﬂuence the ﬁnal performances if they are not properly pro-\ncessed. Directly learning based on the resulting corpus may\nadapt the neural feature extraction into the biased annota-\ntions. In this work, we assume that individual workers have\ntheir own guidelines in mind after short training. For exam-\nple, a perfect worker can annotate highly consistently with\nan expert, while common crowdsourcing workers may be\nconfused and have different understandings on certain con-\ntexts. Based on the assumption, we make an adaption for the\noriginal adversarial neural network to our setting.\nOur adaption is very simple. Brieﬂy speaking, the original\nadversarial learning adds an additional discriminator to clas-\nsify the type of source inputs, for example, the domain cate-\ngory in the domain adaption setting, while we add a discrim-\ninator to classify the annotation workers. Solely the features\nfrom the input sentence is not enough for worker classiﬁ-\ncation. The annotation result of the worker is also required.\nThus the inputs of our discriminator are different. Here we\nexploit both the source sentences and the crowd-annotated\nNE labels as basic inputs for the worker discrimination.\nIn the following, we describe the proposed adversarial\nlearning module, including both the submodels and the train-\ning method. As shown by the left part of Figure 1, the\nsubmodel consists of four parts: (1) a common Bi-LSTM\nover input characters; (2) an additional Bi-LSTM to en-\ncode crowd-annotated NE label sequence; (3) a convolu-\ntional neural network (CNN) to extract features for worker\ndiscriminator; (4) output and prediction.\nCommon Bi-LSTM over Characters\nTo build the adversarial part, ﬁrst we create a new bi-\ndirectional LSTM, named by the common Bi-LSTM:\nhcommon\n1hcommon\n2···hcommon\nn =Bi-LSTM (x1x2···xn).(5)\nAs shown in Figure 1, this Bi-LSTM is constructed over\nthe same input character representations of the private Bi-\nLSTM, in order to extract worker independent features.\nThe resulting features of the common Bi-LSTM are used\nfor both NER and the worker discriminator, different with\nthe features of private Bi-LSTM which are used for NER\nonly. As shown in Figure 1, we concatenate the outputs of\nthe common and private Bi-LSTMs together, and then feed\nthe results into the feed-forward combination layer of the\nNER part. Thus Formula 1 can be rewritten as:\nhner\nt=W(hcommon\nt⊕hprivate\nt) +b, (6)\nwhere Wis wider than the original combination because the\nnewly-added hcommon\nt .Noticeably, although the resulting common features are\nused for the worker discriminator, they actually have no ca-\npability to distinguish the workers. Because this part is ex-\nploited to maximize the loss of the worker discriminator, it\nwill be interpreted in the later training subsection. These fea-\ntures are invariant among different workers, thus they can\nhave less noises for NER. This is the goal of adversarial\nlearning, and we hope the NER being able to ﬁnd useful\nfeatures from these worker independent features.\nAdditional Bi-LSTM over Annotated NER Labels\nIn order to incorporate the annotated NE labels to predict the\nexact worker, we build another bi-directional LSTM (named\nby label Bi-LSTM) based on the crowd-annotated NE label\nsequence. This Bi-LSTM is used for worker discriminator\nonly. During the decoding of the testing phase, we will never\nhave this Bi-', '-\nlikelihood objective as the loss function. We apply softmax\nover all candidate output label sequences, thus the probabil-\nity of the crowd-annotated label sequence is computed by:\np(¯ y|X) =exp(\nscore (X,¯ y))\n∑\ny∈YXexp(\nscore (X,y)), (3)\nwhere ¯ yis the crowd-annotated label sequences and YXis\nall candidate label sequence of input X.\nBased on the above formula, the loss function of our base-\nline model is:\nloss(Θ,X,¯ y) =−logp(¯ y|X), (4)\nwhere Θis the set of all model parameters. We use standard\nback-propagation method to minimize the loss function of\nthe baseline CRF model.']",nan,conditional,TRUE
91,"Which system, among the rule-based baseline, CRF, spaCy, and BERT, performed the best in detecting sensitive information in the NUB ES-PHI dataset?","['we work with the NUB ESversion prior to its anonymisa-\ntion, that is, with the manual annotations of sensitive in-\nformation. It follows that the version we work with is not\npublicly available and, due to contractual restrictions, we\ncannot reveal the provenance of the data. In order to avoid\nconfusion between the two corpus versions, we henceforth\nrefer to the version relevant in this paper as NUB ES-PHI\n(from ‘NUB ESwith Personal Health Information’).\nNUB ES-PHI consists of 32,055 sentences annotated for 11\ndifferent sensitive information categories. Overall, it con-\ntains 7,818 annotations. The corpus has been randomly\nsplit into train (72%), development (8%) and test (20%) sets\nto conduct the experiments described in this paper. The size\nof each split and the distribution of the annotations can be\nconsulted in Tables 2 and 3, respectively.\ntrain dev test\n# sentences 23,079 2,565 6,411\n# tokens 379,401 41,936 107,024\nvocabulary 25,304 7,483 12,750\n# annotations 5,562 677 1,579\nTable 2: Size of the NUB ES-PHI corpus\nThe majority of sensitive information in NUB ES-PHI are\ntemporal expressions (‘Date’ and ‘Time’), followed by\nhealthcare facility mentions (‘Hospital’), and the age of the\npatient. Mentions of people are not that frequent, with\nphysician names (‘Doctor’) occurring much more often\nthan patient names (‘Patient’). The least frequent sensitive\ninformation types, which account for ∼10% of the remain-\ning annotations, consist of the patient’s sex, job, and kin-\nship, and locations other than healthcare facilities (‘Loca-\ntion’). Finally, the tag ‘Other’ includes, for instance, men-\ntions to institutions unrelated to healthcare and whether the\npatient is right- or left-handed. It occurs just 36 times.\ntrain dev test\n# % # % # %\nDate 2,165 39 251 37 660 41\nHospital 1,012 18 105 16 275 17\nAge 701 13 133 20 200 13\nTime 608 11 63 9 155 10\nDoctor 486 9 44 6 134 8\nSex 270 5 35 5 71 4\nKinship 158 3 20 3 44 3\nLocation 71 1 10 1 19 1\nPatient 48 1 5 1 11 1\nJob 31 1 3 0 9 1\nOther 12 0 8 1 16 1\nTotal 5,562 100 677 100 1,579 100\nTable 3: Label distribution in the NUB ES-PHI corpus\n3.1.2. The MEDDOCAN corpus\nThe organisers of the MEDDOCAN shared task (Marimon\net al., 2019) curated a synthetic corpus of clinical cases en-\nriched with sensitive information by health documentalists.In this regard, the MEDDOCAN evaluation scenario could\nbe said to be somewhat far from the real use case the tech-\nnology developed for the shared task is supposed to be ap-\nplied in. However, at the moment it also provides the only\npublic means for a rigorous comparison between systems\nfor sensitive health information detection in Spanish texts.\nThe size of the MEDDOCAN corpus is shown in Table 4.\nCompared to NUB ES-PHI (Table 2), this corpus contains\nmore sensitive information annotations, both in absolute\nand relative terms.\ntrain dev test\n# documents 500 250 250\n# tokens 360,407 138,812 132,961\nvocabulary 26,355 15,985 15,397\n# annotations 11,333 5,801 5,661\nTable 4: Size of the MEDDOCAN corpus\nThe sensitive annotation categories considered in MED-\nDOCAN differ in part from those in NUB ES-PHI. Most\nnotably, it contains ﬁner-grained labels for location-related\nmentions –namely, ‘Address’, ‘Territory’, and ‘Country’–,\nand other sensitive information categories that we did not\nencounter in NUB ES-PHI (e.g., identiﬁers, phone num-\nbers, e-mail addresses, etc.). In total, the MEDDOCAN\ncorpus has 21 sensitive information categories. We refer\nthe reader to the organisers’ article (Marimon et al., 2019)\nfor more detailed information about this corpus.\n3.2. Systems\nApart from experimenting with a pre-trained BERT model,\nwe have run experiments with other systems and base-\nlines, to', ' compare them and obtain a better perspective about\nBERT’s performance in these datasets.\n3.2.1. Baseline\nAs the simplest baseline, a sensitive data recogniser and\nclassiﬁer has been developed that consists of regular-\nexpressions and dictionary look-ups. For each category to\ndetect a speciﬁc method has been implemented. For in-\nstance, the Date, Age, Time and Doctor detectors are based\non regular-expressions; Hospital, Sex, Kinship, Location,\nPatient and Job are looked up in dictionaries. The dic-\ntionaries are hand-crafted from the training data available,\nexcept for the Patient’s case, for which the possible can-\ndidates considered are the 100 most common female and\nmale names in Spain according to the Instituto Nacional de\nEstad ´ıstica (INE; Spanish Statistical Ofﬁce ).\n3.2.2. CRF\nConditional Random Fields (CRF) (Lafferty et al., 2001)\nhave been extensively used for tasks of sequential nature. In\nthis paper, we propose as one of the competitive baselines\na CRF classiﬁer trained with sklearn-crfsuite3for Python\n3.5 and the following conﬁguration: algorithm = lbfgs ;\nmaximum iterations = 100; c1 = c2 = 0.1; all transitions\n=true ; optimise = false . The features extracted from\neach token are as follows:\n3https://sklearn-crfsuite.readthedocs.io', '– preﬁxes and sufﬁxes of 2 and 3 characters;\n– the length of the token in characters and the length of\nthe sentence in tokens;\n– whether the token is all-letters, a number, or a se-\nquence of punctuation marks;\n– whether the token contains the character ‘@’;\n– whether the token is the start or end of the sentence;\n– the token’s casing and the ratio of uppercase charac-\nters, digits, and punctuation marks to its length;\n– and, the lemma, part-of-speech tag, and named-entity\ntag given by ixa-pipes4(Agerri et al., 2014) upon\nanalysing the sentence the token belongs to.\nNoticeably, none of the features used to train the CRF clas-\nsiﬁer is domain-dependent. However, the latter group of\nfeatures is language dependent.\n3.2.3. spaCy\nspaCy5is a widely used NLP library that implements state-\nof-the-art text processing pipelines, including a sequence-\nlabelling pipeline similar to the one described by Strubell\net al. (2017). spaCy offers several pre-trained models in\nSpanish, which perform basic NLP tasks such as Named\nEntity Recognition (NER). In this paper, we have trained a\nnew NER model to detect NUB ES-PHI labels. For this\npurpose, the new model uses all the labels of the train-\ning corpus coded with its context at sentence level. The\nnetwork optimisation parameters and dropout values are\nthe ones recommended in the documentation for small\ndatasets6. Finally, the model is trained using batches of\nsize 64. No more features are included, so the classiﬁer is\nlanguage-dependent but not domain-dependent.\n3.2.4. BERT\nAs introduced earlier, BERT has shown an outstanding\nperformance in NERC-like tasks, improving the start-of-\nthe-art results for almost every dataset and language. We\ntake the same approach here, by using the model BERT-\nBase Multilingual Cased7with a Fully Connected (FC)\nlayer on top to perform a ﬁne-tuning of the whole model\nfor an anonymisation task in Spanish clinical data. Our\nimplementation is built on PyTorch8and the PyTorch-\nTransformers library9(Wolf et al., 2019). The training\nphase consists in the following steps (roughly depicted in\nFigure 1):\n1.Pre-processing: since we are relying on a pre-trained\nBERT model, we must match the same conﬁguration\nby using a speciﬁc tokenisation and vocabulary. BERT\nalso needs that the inputs contains special tokens to\nsignal the beginning and the end of each sequence.\n2.Fine-tuning: the pre-processed sequence is fed into\nthe model. BERT outputs the contextual embeddings\nthat encode each of the inputted tokens. This embed-\nding representation for each token is fed into the FC\n4https://ixa2.si.ehu.es/ixa-pipes\n5https://spacy.io\n6https://spacy.io/usage/training\n7https://github.com/google-research/bert\n8https://pytorch.org\n9https://github.com/huggingface/transformers\nFigure 1: Pre-trained BERT with a Fully Connected layer\non top to perform the ﬁne-tuning\nlinear layer after a dropout layer (with a 0.1 dropout\nprobability), which in turn outputs the logits for each\npossible class. The cross-entropy loss function is cal-\nculated comparing the logits and the gold labels, and\nthe error is back-propagated to adjust the model pa-\nrameters.\nWe have trained the model using an AdamW optimiser\n(Loshchilov and Hutter, 2019) with the learning rate set to\n3e-5, as recommended by Devlin et al. (2019), and with\na gradient clipping of 1.0. We also applied a learning-rate\nscheduler that warms up the learning rate from zero to its\nmaximum value as the training progresses, which is also a\ncommon practice. For each experiment set proposed below,\nthe training was run with an early-stopping patience of 15\nepochs. Then, the model that performed best against the\ndevelopment set was used to produce the reported results.\nThe experiments were run on a 64-', 'core server with operat-\ning system Ubuntu 16.04, 250GB of RAM memory, and 4\nGeForce RTX 2080 GPUs with 11GB of memory. The max-\nimum sequence length was set at 500 and the batch size at\n12. In this setting, each epoch –a full pass through all the\ntraining data– required about 10 minutes to complete.\n3.3. Experimental design\nWe have conducted experiments with BERT in the two\ndatasets of Spanish clinical narrative presented in Section\n3.1. The ﬁrst experiment set uses NUB ES-PHI, a corpus\nof real medical reports manually annotated with sensitive\ninformation. Because this corpus is not publicly available,\nand in order to compare the BERT-based model to other re-\nlated published systems, the second set of experiments uses\nthe MEDDOCAN 2019 shared task competition dataset.\nThe following sections provide greater detail about the two\nexperimental setups.\n3.3.1. Experiment A: NUB ES-PHI\nIn this experiment set, we evaluate all the systems presented\nin Section 3.2., namely, the rule-based baseline, the CRF']",BERT performed the best in detecting sensitive information in the NUB ES-PHI dataset.,conditional,TRUE
92,"How does the increased context-specificity in upper layers of ELMo, BERT, and GPT-2 affect the similarity between words in the same sentence?","['Figure 3: The intra-sentence similarity is the average cosine similarity between each word representation in a\nsentence and their mean (see Deﬁnition 2). Above, we plot the average intra-sentence similarity of uniformly\nrandomly sampled sentences, adjusted for anisotropy. This statistic reﬂects how context-speciﬁcity manifests in\nthe representation space, and as seen above, it manifests very differently for ELMo, BERT, and GPT-2.\naverage intra-sentence similarity is above 0.20 for\nall but one layer.\nAs noted earlier when discussing BERT, this be-\nhavior still makes intuitive sense: two words in the\nsame sentence do not necessarily have a similar\nmeaning simply because they share the same con-\ntext. The success of GPT-2 suggests that unlike\nanisotropy, which accompanies context-speciﬁcity\nin all three models, a high intra-sentence similar-\nity is not inherent to contextualization. Words in\nthe same sentence can have highly contextualized\nrepresentations without those representations be-\ning any more similar to each other than two ran-\ndom word representations. It is unclear, however,\nwhether these differences in intra-sentence simi-\nlarity can be traced back to differences in model\narchitecture; we leave this question as future work.\n4.3 Static vs. Contextualized\nOn average, less than 5% of the variance in\na word’s contextualized representations can be\nexplained by a static embedding. Recall from\nDeﬁnition 3 that the maximum explainable vari-\nance (MEV) of a word, for a given layer of a given\nmodel, is the proportion of variance in its con-\ntextualized representations that can be explained\nby their ﬁrst principal component. This gives us\nan upper bound on how well a static embedding\ncould replace a word’s contextualized representa-\ntions. Because contextualized representations are\nanisotropic (see section 4.1), much of the varia-\ntion across all words can be explained by a sin-gle vector. We adjust for anisotropy by calculating\nthe proportion of variance explained by the ﬁrst\nprincipal component of uniformly randomly sam-\npled word representations and subtracting this pro-\nportion from the raw MEV . In Figure 4, we plot\nthe average anisotropy-adjusted MEV across uni-\nformly randomly sampled words.\nIn no layer of ELMo, BERT, or GPT-2 can more\nthan 5% of the variance in a word’s contextual-\nized representations be explained by a static em-\nbedding, on average. Though not visible in Figure\n4, the raw MEV of many words is actually below\nthe anisotropy baseline: i.e., a greater proportion\nof the variance across all words can be explained\nby a single vector than can the variance across\nall representations of a single word. Note that\nthe 5% threshold represents the best-case scenario,\nand there is no theoretical guarantee that a word\nvector obtained using GloVe, for example, would\nbe similar to the static embedding that maximizes\nMEV . This suggests that contextualizing models\nare not simply assigning one of a ﬁnite number of\nword-sense representations to each word – other-\nwise, the proportion of variance explained would\nbe much higher. Even the average raw MEV is be-\nlow 5% for all layers of ELMo and BERT; only\nfor GPT-2 is the raw MEV non-negligible, being\naround 30% on average for layers 2 to 11 due to\nextremely high anisotropy.\nPrincipal components of contextualized repre-\nsentations in lower layers outperform GloVe\nand FastText on many benchmarks. As noted', 'Figure 2: The average cosine similarity between representations of the same word in different contexts is called\nthe word’s self-similarity (see Deﬁnition 1). Above, we plot the average self-similarity of uniformly randomly\nsampled words after adjusting for anisotropy (see section 3.4). In all three models, the higher the layer, the lower\nthe self-similarity, suggesting that contextualized word representations are more context-speciﬁc in higher layers.\nof contexts a word appears in, rather than its inher-\nent polysemy, is what drives variation in its con-\ntextualized representations. This answers one of\nthe questions we posed in the introduction: ELMo,\nBERT, and GPT-2 are not simply assigning one of\na ﬁnite number of word-sense representations to\neach word; otherwise, there would not be so much\nvariation in the representations of words with so\nfew word senses.\nContext-speciﬁcity manifests very differently in\nELMo, BERT, and GPT-2. As noted earlier,\ncontextualized representations are more context-\nspeciﬁc in upper layers of ELMo, BERT, and GPT-\n2. However, how does this increased context-\nspeciﬁcity manifest in the vector space? Do word\nrepresentations in the same sentence converge to a\nsingle point, or do they remain distinct from one\nanother while still being distinct from their repre-\nsentations in other contexts? To answer this ques-\ntion, we can measure a sentence’s intra-sentence\nsimilarity. Recall from Deﬁnition 2 that the intra-\nsentence similarity of a sentence, in a given layer\nof a given model, is the average cosine similarity\nbetween each of its word representations and their\nmean, adjusted for anisotropy. In Figure 3, we plot\nthe average intra-sentence similarity of 500 uni-\nformly randomly sampled sentences.\nIn ELMo, words in the same sentence are more\nsimilar to one another in upper layers. As\nword representations in a sentence become more\ncontext-speciﬁc in upper layers, the intra-sentencesimilarity also rises. This suggests that, in prac-\ntice, ELMo ends up extending the intuition behind\nFirth’s (1957) distributional hypothesis to the sen-\ntence level: that because words in the same sen-\ntence share the same context, their contextualized\nrepresentations should also be similar.\nIn BERT, words in the same sentence are more\ndissimilar to one another in upper layers. As\nword representations in a sentence become more\ncontext-speciﬁc in upper layers, they drift away\nfrom one another, although there are exceptions\n(see layer 12 in Figure 3). However, in all lay-\ners, the average similarity between words in the\nsame sentence is still greater than the average sim-\nilarity between randomly chosen words (i.e., the\nanisotropy baseline). This suggests a more nu-\nanced contextualization than in ELMo, with BERT\nrecognizing that although the surrounding sen-\ntence informs a word’s meaning, two words in the\nsame sentence do not necessarily have a similar\nmeaning because they share the same context.\nIn GPT-2, word representations in the same\nsentence are no more similar to each other than\nrandomly sampled words. On average, the un-\nadjusted intra-sentence similarity is roughly the\nsame as the anisotropic baseline, so as seen in Fig-\nure 3, the anisotropy-adjusted intra-sentence simi-\nlarity is close to 0 in most layers of GPT-2. In fact,\nthe intra-sentence similarity is highest in the input\nlayer, which does not contextualize words at all.\nThis is in contrast to ELMo and BERT, where the']","In ELMo, words in the same sentence are more similar to one another in upper layers. In BERT, words in the same sentence are more dissimilar to one another in upper layers. In GPT-2, word representations in the same sentence are no more similar to each other than randomly sampled words.",conditional,TRUE
93,"What is the purpose of the pretraining phase in the proposed cross-lingual transfer approach, and how does it enhance word-level representation alignment between different languages?","['Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-\nSA) according to the way of generating the alignment infor-\nmation. These two structures actually extend MLM into a\nbilingual scenario, with the difference that BRLM leverages\nexternal aligner tool or additional attention layer to explic-\nitly introduce alignment information during model training.\n•Hard Alignment (BRLM-HA). We ﬁrst use exter-\nnal aligner tool on source ↔pivot parallel data to ex-\ntract the alignment information of sentence pair. Dur-\ning model training, given source ↔pivot sentence pair,\nBRLM-HA randomly masks some words in source sen-\ntence and leverages alignment information to obtain the\naligned words in pivot sentence for masked words. Based\non the processed input, BRLM-HA adopts the Trans-\nformer (Vaswani et al. 2017) encoder to gain the hid-\nden states for source and pivot sentences respectively.\nThen the training objective of BRLM-HA is to predict\nthe masked words by not only the surrounding words\nin source sentence but also the encoder outputs of the\naligned words. Note that this training process is also car-\nried out in a symmetric situation, in which we mask some\nwords in pivot sentence and obtain the aligned words in\nthe source sentence.\n•Soft Alignment (BRLM-SA). Instead of using external\naligner tool, BRLM-SA introduces an additional atten-\ntion layer to learn the alignment information together with\nmodel training. In this way, BRLM-SA avoids the effect\ncaused by external wrong alignment information and en-\nables many-to-one soft alignment during model training.\nSimilar with BRLM-HA, the training objective of BRLM-\nSA is to predict the masked words by not only the sur-\nrounding words in source sentence but also the outputs of\nattention layer. In our implementation, the attention layer\nis a multi-head attention layer adopted in Transformer,\nwhere the queries come from the masked source sentence,\nthe keys and values come from the pivot sentence.\nIn principle, MLM and TLM can learn some implicit align-\nment information during model training. However, the align-\nment process in MLM is inefﬁcient since the shared word\npieces only account for a small proportion of the whole cor-\npus, resulting in the difﬁculty of expanding the shared in-\nformation to align the whole corpus. TLM also lacks effort\nin alignment between the source and target sentences since\nTLM concatenates the sentence pair into one sequence, mak-\ning the explicit alignment between the source and target in-\nfeasible. BRLM fully utilizes the alignment information to\nobtain better word-level representation alignment between\ndifferent languages, which better relieves the burden of the\ndomain shift problem .\nTransfer Protocol\nWe consider the typical zero-shot translation scenario in\nwhich a high resource pivot language has parallel data with\nboth source and target languages, while source and target\nlanguages has no parallel data between themselves. Our pro-\nposed cross-lingual pretraining based transfer approach for\nsource→target zero-shot translation is mainly divided into\ntwo phrases: the pretraining phase and the transfer phase.Corpus Language Train Dev Test\nEuroparlDe-En,En-Fr 1M,1M 2,000 2,000\nFr-En,En-Es 1M,1M 2,000 2,000\nRo-En,En-De 0.6M,1.5M 2,000 1,000\nMultiUNAr-En,En-Es\nEn-Ru9.7M,11.3M\n11.6M4,000 4,000\nTable 1: Data Statistics.\nIn the pretraining phase, we ﬁrst pretrain MLM on mono-\nlingual corpora of both source and pivot languages, and con-\ntinue to pretrain TLM or the proposed BRLM on the avail-\nable parallel data between source and pivot languages, in\norder to build a cross-lingual encoder shared by the source\nand pivot languages.\nIn the transfer phase, we train pivot →target NMT model\ninitialized by the cross-lingually pre-trained encoder, and\nﬁnally transfer the trained NMT model to source →target\ntranslation thanks to the shared encoder. Note that during\ntraining pivot→target NMT model, we freeze several layers\nof the cross-lingually pre-trained encoder to avoid the de-\ngeneracy issue.\nFor the', 'Figure 2: The overview of BRidge Language Modeling (BRLM). The BRLM extends MLM (Lample and Conneau 2019) to\npairs of parallel sentences and leverages explicit alignment information obtained by external aligner tool or additional attention\nlayer to encourage word representation alignment across different languages.\n•Directly translate source sentences into target sentences\nwith the parent model, which beneﬁts from the availabil-\nity of the universal encoder.\nThe key difﬁculty of this method is to ensure the intermedi-\nate representations of the universal encoder are language in-\nvariant. In the rest of this section, we ﬁrst present two exist-\ning methods yet to be explored in zero-shot translation, and\nthen propose a straightforward but effective cross-lingual\npre-training method. In the end, we present the whole train-\ning and inference protocol for transfer.\nMasked and Translation Language Model\nPretraining\nTwo existing cross-lingual pre-training methods, Masked\nLanguage Modeling (MLM) and Translation Language\nModeling (TLM), have shown their effectiveness on XNLI\ncross-lingual classiﬁcation task (Lample and Conneau 2019;\nHuang et al. 2019), but these methods have not been well\nstudied on cross-lingual generation tasks in zero-shot condi-\ntion. We attempt to take advantage of the cross-lingual abil-\nity of the two methods for zero-shot translation.\nSpeciﬁcally, MLM adopts the Cloze objective of\nBERT (Devlin et al. 2018) and predicts the masked words\nthat are randomly selected and replaced with [MASK] to-\nken on monolingual corpus. In practice, MLM takes dif-\nferent language monolingual corpora as input to ﬁnd fea-\ntures shared across different languages. With this method,\nword pieces shared in all languages have been mapped into\na shared space, which makes the sentence representationsacross different languages close (Pires, Schlinger, and Gar-\nrette 2019).\nSince MLM objective is unsupervised and only requires\nmonolingual data, TLM is designed to leverage parallel data\nwhen it is available. Actually, TLM is a simple extension of\nMLM, with the difference that TLM concatenates sentence\npair into a whole sentence, and then randomly masks words\nin both the source and target sentences. In this way, the\nmodel can either attend to surrounding words or to the trans-\nlation sentence, implicitly encouraging the model to align\nthe source and target language representations. Note that al-\nthough each sentence pair is formed into one sentence, the\npositions of the target sentence are reset to count form zero.\nBridge Language Model Pretraining\nAside from MLM and TLM, we propose BRidge Language\nModeling (BRLM) to further obtain word-level representa-\ntion alignment between different languages. This method is\ninspired by the assumption that if the feature spaces of dif-\nferent languages are aligned very well, the masked words\nin the corrupted sentence can also be guessed by the con-\ntext of the correspondingly aligned words on the other side.\nTo achieve this goal, BRLM is designed to strengthen the\nability to infer words across languages based on alignment\ninformation, instead of inferring words within monolingual\nsentence as in MLM or within the pseudo sentence formed\nby concatenating sentence pair as in TLM.\nAs illustrated in Figure 2, BRLM stacks shared encoder\nover both side sentences separately. In particular, we design\ntwo network structures for BRLM, which are divided into']",nan,conditional,TRUE
94,"What is the dataset used for Armenian named entity recognition, and how was it generated using Wikipedia and Wikidata?","['arXiv:1810.08699v1  [cs.CL]  19 Oct 2018pioNER: Datasets and Baselines for Armenian\nNamed Entity Recognition\nTsolak Ghukasyan1, Garnik Davtyan2, Karen Avetisyan3, Ivan Andrianov4\nIvannikov Laboratory for System Programming at Russian-Ar menian University1,2,3, Yerevan, Armenia\nIvannikov Institute for System Programming of the Russian A cademy of Sciences4, Moscow, Russia\nEmail: {1tsggukasyan,2garnik.davtyan,3karavet,4ivan.andrianov}@ispras.ru\nAbstract —In this work, we tackle the problem of Armenian\nnamed entity recognition, providing silver- and gold-stan dard\ndatasets as well as establishing baseline results on popula r mod-\nels. We present a 163000-token named entity corpus automati cally\ngenerated and annotated from Wikipedia, and another 53400-\ntoken corpus of news sentences with manual annotation of peo ple,\norganization and location named entities. The corpora were used\nto train and evaluate several popular named entity recognit ion\nmodels. Alongside the datasets, we release 50-, 100-, 200-, 300-\ndimensional GloVe word embeddings trained on a collection o f\nArmenian texts from Wikipedia, news, blogs, and encycloped ia.\nIndex Terms —machine learning, deep learning, natural lan-\nguage processing, named entity recognition, word embeddin gs\nI. I NTRODUCTION\nNamed entity recognition is an important task of natural\nlanguage processing, featuring in many popular text proces sing\ntoolkits. This area of natural language processing has been\nactively studied in the latest decades and the advent of deep\nlearning reinvigorated the research on more effective and\naccurate models. However, most of existing approaches requ ire\nlarge annotated corpora. To the best of our knowledge, no suc h\nwork has been done for the Armenian language, and in this\nwork we address several problems, including the creation of a\ncorpus for training machine learning models, the developme nt\nof gold-standard test corpus and evaluation of the effectiv eness\nof established approaches for the Armenian language.\nConsidering the cost of creating manually annotated named\nentity corpus, we focused on alternative approaches. Lack\nof named entity corpora is a common problem for many\nlanguages, thus bringing the attention of many researchers\naround the globe. Projection based transfer schemes have be en\nshown to be very effective (e.g. [1], [2], [3]), using resour ce-\nrich language’s corpora to generate annotated data for the l ow-\nresource language. In this approach, the annotations of hig h-\nresource language are projected over the corresponding tok ens\nof the parallel low-resource language’s texts. This strate gy\ncan be applied for language pairs that have parallel corpora .\nHowever, that approach would not work for Armenian as we\ndid not have access to sufﬁciently large parallel corpus wit h a\nresource-rich language.\nAnother popular approach is using Wikipedia. Klesti\nHoxha and Artur Baxhaku employ gazetteers extracted from\nWikipedia to generate an annotated corpus for Albanian [4],\nand Weber and Pötzl propose a rule-based system for Germanthat leverages the information from Wikipedia [5]. However ,\nthe latter relies on external tools such as part-of-speech t ag-\ngers, making it nonviable for the Armenian language.\nNothman et al. generated a silver-standard corpus for 9\nlanguages by extracting Wikipedia article texts with outgo ing\nlinks and turning those links into named entity annotations\nbased on the target article’s type [6]. Sysoev and Andrianov\nused a similar approach for the Russian language [7] [8].\nBased on its success for a wide range of languages, our choice\nfell on this model to tackle automated data generation and\nannotation for the Armenian language.\nAside from the lack of training data, we also address the\nabsence of a benchmark dataset of Armenian texts for named\nentity recognition. We propose a gold-standard corpus with\nmanual annotation of CoNLL named entity categories: person ,\nlocation, and organization [9] [10], hoping it will be used t o\nevaluate future named entity recognition models.\nFurthermore, popular entity recognition models were ap-\nplied to the mentioned data in order to obtain baseline resul ts\nfor future research in the area. Along with the datasets, we\n', 'developed GloVe [11] word embeddings to train and evaluate\nthe deep learning models in our experiments.\nThe contributions of this work are (i) the silver-standard\ntraining corpus, (ii) the gold-standard test corpus, (iii) GloVe\nword embeddings, (iv) baseline results for 3 different mode ls\non the proposed benchmark data set. All aforementioned\nresources are available on GitHub1.\nII. A UTOMATED TRAINING CORPUS GENERATION\nWe used Sysoev and Andrianov’s modiﬁcation of the\nNothman et al. approach to automatically generate data for\ntraining a named entity recognizer. This approach uses link s\nbetween Wikipedia articles to generate sequences of named-\nentity annotated tokens.\nA. Dataset extraction\nThe main steps of the dataset extraction system are de-\nscribed in Figure 1.\nFirst, each Wikipedia article is assigned a named entity\nclass (e.g. the article /Armke/armini/armmen /Armke/armayb/armsha/armke/armayb/armsha/armhi/armayb/armnu (Kim Kashkashian) is\nclassiﬁed as PER (person), /Armayb/armza/armgim/armyech/armre/armini /armlyun/armini/armgim/armayb (League of Nations)\nasORG (organization), /Armse/armini/armre/armini/armayb (Syria) as LOC etc). One of the\n1https://github.com/ispras-texterra/pioner\n© 2018 IEEE. Personal use of this material is permitted. Perm ission from IEEE must be obtained for all other uses, in any cu rrent or future media, including\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for r esale or redistribution to servers or lists, or\nreuse of any copyrighted component of this work in other work s.', 'Fig. 1: Steps of automatic dataset extraction from Wikipedi a\nClassiﬁcation of Wikipedia articles into NE types\nLabelling common article aliases to increase coverage\nExtraction of text fragments with outgoing links\nLabelling links according to their target article’s type\nAdjustment of labeled entities’ boundaries\ncore differences between our approach and Nothman’s system\nis that we do not rely on manual classiﬁcation of articles and\ndo not use inter-language links to project article classiﬁc ations\nacross languages. Instead, our classiﬁcation algorithm us es\nonly an article’s Wikidata entry’s ﬁrst instance of label’s\nparent subclass of labels, which are, incidentally, language\nindependent and thus can be used for any language.\nThen, outgoing links in articles are assigned the article’s\ntype they are leading to. Sentences are included in the train ing\ncorpus only if they contain at least one named entity and\nall contained capitalized words have an outgoing link to an\narticle of known type. Since in Wikipedia articles only the ﬁ rst\nmention of each entity is linked, this approach becomes very\nrestrictive and in order to include more sentences, additio nal\nlinks are inferred. This is accomplished by compiling a list of\ncommon aliases for articles corresponding to named entitie s,\nand then ﬁnding text fragments matching those aliases to\nassign a named entity label. An article’s aliases include it s\ntitle, titles of disambiguation pages with the article, and texts\nof links leading to the article (e.g. /Armlyun/armyech/armnu/armini/armnu/armgim/armre/armayb/armda (Leningrad),\n/Armpe/armyech/armtyun/armre/armvo/armgim/armre/armayb/armda (Petrograd), /Armpe/armyech/armtyun/armyech/armre/armben/armvo/armvyun/armre/armgim (Peterburg) are aliases\nfor/Armse/armayb/armnu/armken/armtyun /Armpe/armyech/armtyun/armyech/armre/armben/armvo/armvyun/armre/armgim (Saint Petersburg)). The list of\naliases is compiled for all PER ,ORG ,LOC articles.\nAfter that, link boundaries are adjusted by removing the\nlabels for expressions in parentheses, the text after a comm a,\nand in some cases breaking into separate named entities if th e\nlinked text contains a comma. For example, [LOC/Armayb/armben/armvo/armvev/armhi/armayb/armnu\n/armparenleft/armke/armayb/armghat/armayb/armke/armparenright ](Abovyan (town)) is reworked into [LOC/Armayb/armben/armvo/armvev/armhi/armayb/armnu ]\n/armparenleft/armke/armayb/armghat/armayb/armke/armparenright .\nB. Using Wikidata to classify Wikipedia\nInstead of manually classifying Wikipedia articles as it wa s\ndone in Nothman et al., we developed a rule-based classiﬁer\nthat used an article’s Wikidata instance of andsubclass of\nattributes to ﬁnd the corresponding named entity type.The classiﬁcation could be done using solely instance\noflabels, but these labels are unnecessarily speciﬁc for the\ntask and building a mapping on it would require a more\ntime-consuming and meticulous work. Therefore, we classiﬁ ed\narticles based on their ﬁrst instance of attribute’s subclass of\nvalues. Table I displays the mapping between these values an d\nnamed entity types. Using higher-level subclass of values was\nnot an option as their values often were too general, making\nit impossible to derive the correct named entity category.\nC. Generated data\nUsing the algorithm described above, we generated 7455\nannotated sentences with 163247 tokens based on 20 February\n2018 dump of Armenian Wikipedia.\nThe generated data is still signiﬁcantly smaller than the\nmanually annotated corpora from CoNLL 2002 and 2003.\nFor comparison, the train set of English CoNLL 2003 corpus\ncontains 203621 tokens and the German one 2069', '31, while\nthe Spanish and Dutch corpora from CoNLL 2002 respectively\n273037 and 218737 lines. The smaller size of our generated\ndata can be attributed to the strict selection of candidate\nsentences as well as simply to the relatively small size of\nArmenian Wikipedia.\nThe accuracy of annotation in the generated corpus heavily\nrelies on the quality of links in Wikipedia articles. During gen-\neration, we assumed that ﬁrst mentions of all named entities\nhave an outgoing link to their article, however this was not a l-\nways the case in actual source data and as a result the train se t\ncontained sentences where not all named entities are labele d.\nAnnotation inaccuracies also stemmed from wrongly assigne d\nlink boundaries (for example, in Wikipedia article /Armayb/armre/armto/armvo/armvyun/armre\n/Armvo/armvyun/armyech/armlyun/armse/armlyun/armini /Armvev/armyech/armlyun/armini/armnu/armgim/armto/armvo/armnu (Arthur Wellesley) there is a link to the\nNapoleon article with the text "" /arme/Armnu/armayb/armpe/armvo/armlyun/armyech/armvo/armnu/armat "" (""Napoleon is""),\nwhen it should be "" /Armnu/armayb/armpe/armvo/armlyun/armyech/armvo/armnu/armat "" (""Napoleon"")). Another kind\nof common annotation errors occurred when a named entity\nappeared inside a link not targeting a LOC ,ORG , orPER\narticle (e.g. "" /Armayb/Armmen/Armnu /armnu/armayb/armkhe/armayb/armgim/armayb/armho/armayb/armken/armayb/armnu /armat/armnu/armtyun/armre/armvo/armvyun/armto/armhi/armvo/armvyun/armnu/armnu/armyech/armre/armvo/armvyun/armmen ""\n(""USA presidential elections"") is linked to the article /Armayb/Armmen/Armnu\n/armnu/armayb/armkhe/armayb/armgim/armayb/armho/armayb/armken/armayb/armnu /armat/armnu/armtyun/armre/armvo/armvyun/armto/armhi/armvo/armvyun/armnu/armnu/armyech/armre 2016 (United States presi-\ndential election, 2016) and as a result [ LOC/Armayb/Armmen/Armnu ] (USA) is\nlost).\nIII. T EST DATASET\nIn order to evaluate the models trained on generated data,\nwe manually annotated a named entities dataset comprising\n53453 tokens and 2566 sentences selected from over 250 news\ntexts from ilur.am2. This dataset is comparable in size with\nthe test sets of other languages (Table II). Included senten ces\nare from political, sports, local and world news (Figures 2,\n3), covering the period between August 2012 and July 2018.\nThe dataset provides annotations for 3 popular named entity\nclasses: people ( PER ), organizations ( ORG ), and locations\n(LOC ), and is released in CoNLL03 format with IOB tagging\nscheme. Tokens and sentences were segmented according to\nthe UD standards for the Armenian language [12].\n2http://ilur.am/news/newsline.html']","The dataset used for Armenian named entity recognition was generated using Wikipedia and Wikidata. The algorithm used links between Wikipedia articles to generate sequences of named-entity annotated tokens. The articles were classified based on their Wikidata instance of and subclass of attributes. The generated dataset consists of 7,455 annotated sentences with 163,247 tokens.",conditional,TRUE
95,"What role do grapheme-phoneme mappings play in language processing, considering the various complexities and variations in writing systems?","['with the output of the wFST-based Phonetisaurus\nsystem (Novak et al., 2016) did better than either\nsystem alone.\nA different approach came from Kim and Sny-\nder (2012), who used supervised learning with\nan undirected graphical model to induce the\ngrapheme–phoneme mappings for languages writ-\nten in the Latin alphabet. Given a short text in\na language, the model predicts the language’s or-\nthographic rules. To create phonemic context fea-\ntures from the short text, the model na ¨ıvely maps\ngraphemes to IPA symbols written with the same\ncharacter, and uses the features of these symbols\nto learn an approximation of the phonotactic con-\nstraints of the language. In their experiments,\nthese phonotactic features proved to be more valu-\nable than geographical and genetic features drawn\nfrom WALS (Dryer and Haspelmath, 2013).\n2.2 Multilingual Neural NLP\nIn recent years, neural networks have emerged as\na common way to use data from several languages\nin a single system. Google’s zero-shot neural ma-\nchine translation system (Johnson et al., 2016)\nshares an encoder and decoder across all language\npairs. In order to facilitate this multi-way transla-\ntion, they prepend an artiﬁcial token to the begin-\nning of each source sentence at both training and\ntranslation time. The token identiﬁes what lan-\nguage the sentence should be translated to. This\napproach has three beneﬁts: it is far more efﬁcient\nthan building a separate model for each language\npair; it allows for translation between languages\nthat share no parallel data; and it improves re-\nsults on low-resource languages by allowing them\nto implicitly share parameters with high-resource\nlanguages. Our g2p system is inspired by this ap-\nproach, although it differs in that there is only one\ntarget “language”, IPA, and the artiﬁcial tokens\nidentify the language of the source instead of the\nlanguage of the target.\nOther work has also made use of multilingually-\ntrained neural networks. Phoneme-level polyglot\nlanguage models (Tsvetkov et al., 2016) train a\nsingle model on multiple languages and addition-\nally condition on externally constructed typolog-\nical data about the language. ¨Ostling and Tiede-\nmann (2017) used a similar approach, in which\na character-level neural language model is trained\non a massively multilingual corpus. A language\nembedding vector is concatenated to the input ateach time step. The language embeddings their\nsystem learned correlate closely to the genetic re-\nlationships between languages. However, neither\nof these models was applied to g2p.\n3 Grapheme-to-Phoneme\ng2p is the problem of converting the orthographic\nrepresentation of a word into a phonemic repre-\nsentation. A phoneme is an abstract unit of sound\nwhich may have different realizations in different\ncontexts. For example, the English phoneme /p/\nhas two phonetic realizations (or allophones):\n•[ph], as in the word ‘pain’ [pheI n]\n•[p], as in the word ‘Spain’ [s p eI n]\nEnglish speakers without linguistic training of-\nten struggle to perceive any difference between\nthese sounds. Writing systems usually do not dis-\ntinguish between allophones: [ph]and[p]are both\nwritten as⟨p⟩in English. The sounds are written\ndifferently in languages where they contrast, such\nas Hindi and Eastern Armenian.\nMost writing systems in use today are glot-\ntographic, meaning that their symbols encode\nsolely phonological information1. But despite\nbeing glottographic, in few writing systems do\ngraphemes correspond one-to-one with phonemes.\nThere are cases in which multiple graphemes rep-\nresent a single phoneme, as in the word thein En-\nglish:\nth e\nD @\nThere are cases in which a single grapheme rep-\nresents multiple phonemes, such as syllabaries, in\nwhich each symbol represents a syllable.\nIn many languages, there are silent letters, as in\nthe word hora in Spanish:\nh o r a\n-o R a\nThere are more complicated correspondences,\nsuch as the silent']","Grapheme-phoneme mappings play a crucial role in language processing as they help convert the orthographic representation of a word into its phonemic representation. This is important because writing systems often do not have a one-to-one correspondence between graphemes and phonemes. There can be cases where multiple graphemes represent a single phoneme or a single grapheme represents multiple phonemes. Additionally, there can be silent letters or more complicated correspondences. Understanding these mappings is essential for tasks like speech recognition, text-to-speech synthesis, and natural language processing.",conditional,TRUE
96,Why is pre-ordering the assisting language crucial for multilingual NMT in a low-resource setting to address word-order divergence?,"[' 1999) which enables the model to learn\nthe word-order of the source language when sufﬁ-\ncient child task parallel corpus is available.\nWe also compare the performance of the ﬁne-\ntuned model with the model trained only on the\navailable source-target parallel corpus with ran-\ndomly initialized weights (No Transfer Learning).\nTransfer learning, with and without pre-ordering,\nis better compared to training only on the small\nsource-target parallel corpus.\n6 Conclusion\nIn this paper, we show that handling word-order\ndivergence between the source and assisting lan-\nguages is crucial for the success of multilingual\nNMT in an extremely low-resource setting. We\nshow that pre-ordering the assisting language to\nmatch the word order of the source language sig-\nniﬁcantly improves translation quality in an ex-\ntremely low-resource setting. If pre-ordering is\nnot possible, ﬁne-tuning on a small source-target', '2 Related Work\nTo the best of our knowledge, no work has ad-\ndressed word order divergence in transfer learn-\ning for multilingual NMT. However, some work\nexists for other NLP tasks in a multilingual set-\nting. For Named Entity Recognition (NER), Xie\net al. (2018) use a self-attention layer after the\nBi-LSTM layer to address word-order divergence\nfor Named Entity Recognition (NER) task. The\napproach does not show any signiﬁcant improve-\nments, possibly because the divergence has to be\naddressed before/during construction of the con-\ntextual embeddings in the Bi-LSTM layer. Joty\net al. (2017) use adversarial training for cross-\nlingual question-question similarity ranking. The\nadversarial training tries to force the sentence rep-\nresentation generated by the encoder of similar\nsentences from different input languages to have\nsimilar representations.\nPre-ordering the source language sentences to\nmatch the target language word order has been\nfound useful in addressing word-order divergence\nfor Phrase-Based SMT (Collins et al., 2005; Ra-\nmanathan et al., 2008; Navratil et al., 2012; Chat-\nterjee et al., 2014). For NMT, Ponti et al. (2018)\nand Kawara et al. (2018) have explored pre-\nordering. Ponti et al. (2018) demonstrated that\nby reducing the syntactic divergence between the\nsource and the target languages, consistent im-\nprovements in NMT performance can be obtained.\nOn the contrary, Kawara et al. (2018) reported\ndrop in NMT performance due to pre-ordering.\nNote that these works address source-target diver-\ngence, not divergence between source languages\nin multilingual NMT scenario.\n3 Proposed Solution\nConsider the task of translating for an extremely\nlow-resource language pair. The parallel corpus\nbetween the two languages, if available may be\ntoo small to train an NMT model. Similar to Zoph\net al. (2016), we use transfer learning to over-\ncome data sparsity between the source and the\ntarget languages. We choose English as the as-\nsisting language in all our experiments. In our\nresource-scarce scenario, we have no parallel cor-\npus for training the child model. Hence, at test\ntime, the source language sentence is translated\nusing the parent model after performing a word-\nby-word translation from source to the assisting\nlanguage using a bilingual dictionary.Before Reordering After Reordering\nS\nNP0 VP\nV NP 1S\nNP0 VP\nNP1V\nS\nNP\nNNP\nAnuragVP\nMD\nwillVP\nVB\nmeetNP\nNNP\nThakurS\nNP\nNNP\nAnuragVP\nNP\nNNP\nThakurVP\nMD\nwillVP\nVB\nmeet\nTable 1: Example showing transitive verb before and\nafter reordering (Adapted from Chatterjee et al. (2014))\nSince the source language and the assisting lan-\nguage (English) have different word order, we hy-\npothesize that it leads to inconsistencies in the\ncontextual representations generated by the en-\ncoder for the two languages. Speciﬁcally, given an\nEnglish sentence (SVO word order) and its transla-\ntion in the source language (SOV word order), the\nencoder representations for words in the two sen-\ntences will be different due to different contexts\nof synonymous words. This could lead to the at-\ntention and the decoder layers generating different\ntranslations from the same (parallel) sentence in\nthe source or assisting language. This is undesir-\nable as we want the knowledge to be transferred\nfrom the parent model (assisting source →target)\nto the child model (source →target).\nIn this paper, we propose to pre-order English\nsentences (assisting language sentences) to match\nthe source language word-order and train the par-\nent model on the pre-ordered corpus. Table 1\nshows one of the pre-ordering rules (Ramanathan\net al., 2008) used along with an example sentence\nillustrating the effect of pre-ordering. This will en-\nsure that context of words in the parallel source\nand assisting language sentences are similar, lead-\ning to consistent contextual representations across\nthe source languages. Pre-ordering may also be\nbeneﬁcial for other word order divergence scenar-\nios (e.g., SOV to SVO), but we leave']","Pre-ordering the assisting language is crucial for multilingual NMT in a low-resource setting to address word-order divergence because it ensures that the contextual representations generated by the encoder for the source and assisting languages are consistent, leading to consistent translations and effective transfer of knowledge from the assisting language to the source language.",conditional,TRUE
97,"What are the subreddits that are linked to dogmatism in the context of Black hole, and how does dogmatism manifest in users' behavior on these subreddits?","[')\nDogmatism is widely considered to be a domain-\nspeciﬁc attitude (for example, oriented towards re-\nligion or politics) as opposed to a deeper personality\ntrait (Rokeach, 1954). Here we use Reddit as a lens\nto examine this idea more closely. Are users who\nare dogmatic about one topic likely to be dogmatic\nabout others? Do clusters of dogmatism exist around\nparticular topics? To ﬁnd out, we examine the re-', 'Libertarianism business conspiracy science Christianity lgbt\nAnarcho Capitalism Bitcoin Republican Christianity DebateAChristian feminisms\nBitcoin economy conspiritard relationship advice DebateReligion Equality\nronpaul entertainment ronpaul worldpolitics science SubredditDrama\nConservative TrueReddit collapse MensRights videos TwoXChromosomes\nAndroid socialism guns IAmA news MensRights\nukpolitics bestof worldpolitics TwoXChromosomes Libertarianism offbeat\nEquality philosophy occupywallstreet WTF atheism fffffffuuuuuuuuuuuu\nTable 4: Clusters of subreddits that share dogmatic users. For example, users who are dogmatic on the conspiracy subreddit (a\nplace to discuss conspiracy theories) are also likely to be dogmatic on guns oroccupywallstreet .\nlationships between subreddits over which individ-\nual users are dogmatic. For example, if many users\noften post dogmatic comments on both the politics\nandChristianity subreddits, but less often on world-\nnews , that would suggest politics andChristianity\nare linked per a boost in likelihood of individuals\nbeing dogmatic in both.\nWe sample 1000 Reddit users who posted at least\nonce a year between 2007 and 2015 to construct a\ncorpus of 10 million posts that constitute their entire\npost history. We then annotate these posts using the\nclassiﬁer and compute the average dogmatism score\nper subreddit per user. For example, one user might\nhave an average dogmatism level of 0.55 for the pol-\nitics subreddit and 0.45 for the economics subred-\ndit. Most users do not post in all subreddits, so we\ntrack only subreddits for which a user had posted at\nleast 10 times. Any subreddits with an average dog-\nmatism score higher than 0.50 we consider to be a\nuser’s dogmatic subreddits. We then count all pairs\nof these dogmatic subreddits. For example, 45 users\nhave politics andtechnology among their dogmatic\nsubreddits, so we consider politics andtechnology\nas linked 45 times. We compute the mutual informa-\ntion (Church and Hanks, 1990) between these links,\nwhich gives us a measure of the subreddits that are\nmost related through dogmatism.\nWe present the results of this analysis in Table 4,\nchoosing clusters that represent a diverse set of top-\nics. For example, Libertarianism is linked through\ndogmatism to other political communities like An-\narcho Capitalism ,ronpaul , orukpolitics , as well as\nother topical subreddits like guns oreconomy . Sim-\nilarly, people who are dogmatic in the business sub-\nreddit also tend to be dogmatic in subreddits for Bit-\ncoin,socialism , and technology . Notably, when we\napply the same mutual information analysis to links\ndeﬁned by subreddits posted in by the same user, weFeature Direction\ntotal user posts ↑\nproportion of posts in most active subreddit ↑\nnumber of subreddits posted in ↓\naverage number of posts in active articles ↓\nTable 5: User behavioral features that are positively and nega-\ntively associated with dogmatism. ↑means the feature is pos-\nitively predictive with dogmatism, and ↓means the feature is\nnegatively predictive. For example, the more subreddits a user\nposts in, the less likely they are to be dogmatic. All features are\nstatistically signiﬁcant ( p < 0.001).\nsee dramatically different results. For example, the\nsubreddits most linked to science through user posts\nareUpliftingNews ,photoshopbattles , and ﬁrstworl-\ndanarchist , and millionairemakers .\nFinally, we see less obvious connections between\nsubreddits that suggest some people may be dog-\nmatic by nature. For example, among the users who\nare dogmatic on politics , they are also disproportion-\nately dogmatic on unrelated subreddits such as sci-\nence (p < 0.001),technology (p < 0.001),IAmA\n(p < 0.001), and AskReddit (p < 0.05), with p-\nvalues computed under a binomial test.\n5.3 What user behaviors are predictive of\ndogmatism? (R3)\nWe have shown dogmatism is captured by many lin-\nguistic features, but can we discover other high-level\nuser behaviors that are similarly predictive?\nTo ﬁnd out, we compute metrics of user behavior\nusing the data sample of 1000 users and 10 million\nposts described in Section 5.2. Speciﬁcally, we cal-\nculate (1) activity : a user’s total number of posts,', 'Libertarianism business conspiracy science Christianity lgbt\nAnarcho Capitalism Bitcoin Republican Christianity DebateAChristian feminisms\nBitcoin economy conspiritard relationship advice DebateReligion Equality\nronpaul entertainment ronpaul worldpolitics science SubredditDrama\nConservative TrueReddit collapse MensRights videos TwoXChromosomes\nAndroid socialism guns IAmA news MensRights\nukpolitics bestof worldpolitics TwoXChromosomes Libertarianism offbeat\nEquality philosophy occupywallstreet WTF atheism fffffffuuuuuuuuuuuu\nTable 4: Clusters of subreddits that share dogmatic users. For example, users who are dogmatic on the conspiracy subreddit (a\nplace to discuss conspiracy theories) are also likely to be dogmatic on guns oroccupywallstreet .\nlationships between subreddits over which individ-\nual users are dogmatic. For example, if many users\noften post dogmatic comments on both the politics\nandChristianity subreddits, but less often on world-\nnews , that would suggest politics andChristianity\nare linked per a boost in likelihood of individuals\nbeing dogmatic in both.\nWe sample 1000 Reddit users who posted at least\nonce a year between 2007 and 2015 to construct a\ncorpus of 10 million posts that constitute their entire\npost history. We then annotate these posts using the\nclassiﬁer and compute the average dogmatism score\nper subreddit per user. For example, one user might\nhave an average dogmatism level of 0.55 for the pol-\nitics subreddit and 0.45 for the economics subred-\ndit. Most users do not post in all subreddits, so we\ntrack only subreddits for which a user had posted at\nleast 10 times. Any subreddits with an average dog-\nmatism score higher than 0.50 we consider to be a\nuser’s dogmatic subreddits. We then count all pairs\nof these dogmatic subreddits. For example, 45 users\nhave politics andtechnology among their dogmatic\nsubreddits, so we consider politics andtechnology\nas linked 45 times. We compute the mutual informa-\ntion (Church and Hanks, 1990) between these links,\nwhich gives us a measure of the subreddits that are\nmost related through dogmatism.\nWe present the results of this analysis in Table 4,\nchoosing clusters that represent a diverse set of top-\nics. For example, Libertarianism is linked through\ndogmatism to other political communities like An-\narcho Capitalism ,ronpaul , orukpolitics , as well as\nother topical subreddits like guns oreconomy . Sim-\nilarly, people who are dogmatic in the business sub-\nreddit also tend to be dogmatic in subreddits for Bit-\ncoin,socialism , and technology . Notably, when we\napply the same mutual information analysis to links\ndeﬁned by subreddits posted in by the same user, weFeature Direction\ntotal user posts ↑\nproportion of posts in most active subreddit ↑\nnumber of subreddits posted in ↓\naverage number of posts in active articles ↓\nTable 5: User behavioral features that are positively and nega-\ntively associated with dogmatism. ↑means the feature is pos-\nitively predictive with dogmatism, and ↓means the feature is\nnegatively predictive. For example, the more subreddits a user\nposts in, the less likely they are to be dogmatic. All features are\nstatistically signiﬁcant ( p < 0.001).\nsee dramatically different results. For example, the\nsubreddits most linked to science through user posts\nareUpliftingNews ,photoshopbattles , and ﬁrstworl-\ndanarchist , and millionairemakers .\nFinally, we see less obvious connections between\nsubreddits that suggest some people may be dog-\nmatic by nature. For example, among the users who\nare dogmatic on politics , they are also disproportion-\nately dogmatic on unrelated subreddits such as sci-\nence (p < 0.001),technology (p < 0.001),IAmA\n(p < 0.001), and AskReddit (p < 0.05), with p-\nvalues computed under a binomial test.\n5.3 What user behaviors are predictive of\ndogmatism? (R3)\nWe have shown dogmatism is captured by many lin-\nguistic features, but can we discover other high-level\nuser behaviors that are similarly predictive?\nTo ﬁnd out, we compute metrics of user behavior\nusing the data sample of 1000 users and 10 million\nposts described in Section 5.2. Speciﬁcally, we cal-\nculate (1) activity : a user’s total number of posts,']",nan,conditional,TRUE
98,What additional alignment methods are used in the proposed cross-lingual pre-training approach to ensure word-level representation alignment between different languages?,"[', MNMT\nusually performs worse than the pivot-based method in\nzero-shot translation setting (Arivazhagan et al. 2018).\n•Unsupervised NMT (UNMT) considers a harder setting,\nin which only large-scale monolingual corpora are avail-\nable for training. Recently, many methods have been pro-\nposed to improve the performance of UNMT, including\nusing denoising auto-encoder, statistic machine transla-\ntion (SMT) and unsupervised pre-training (Artetxe et\nal. 2017; Lample et al. 2018; Ren et al. 2019; Lample\nand Conneau 2019). Since UNMT performs well between\nsimilar languages (e.g., English-German translation), its\nperformance between distant languages is still far from\nexpectation.\nOur proposed method belongs to the transfer learning,\nbut it is different from traditional transfer methods which\ntrain a parent model as starting point. Before training a par-\nent model, our approach fully leverages cross-lingual pre-\ntraining methods to make all source languages share the\nsame feature space and thus enables a smooth transition for\nzero-shot translation.\nApproach\nIn this section, we will present a cross-lingual pre-\ntraining based transfer approach. This method is designed\nfor a common zero-shot scenario where there are a lot\nof source↔pivot and pivot↔target bilingual data but no\nsource↔target parallel data, and the whole training process\ncan be summarized as follows step by step:\n•Pre-train a universal encoder with source/pivot monolin-\ngual or source↔pivot bilingual data.\n•Train a pivot→target parent model built on the pre-trained\nuniversal encoder with the available parallel data. Dur-\ning the training process, we freeze several layers of the\npre-trained universal encoder to avoid the degeneracy is-\nsue (Howard and Ruder 2018).', 'Figure 2: The overview of BRidge Language Modeling (BRLM). The BRLM extends MLM (Lample and Conneau 2019) to\npairs of parallel sentences and leverages explicit alignment information obtained by external aligner tool or additional attention\nlayer to encourage word representation alignment across different languages.\n•Directly translate source sentences into target sentences\nwith the parent model, which beneﬁts from the availabil-\nity of the universal encoder.\nThe key difﬁculty of this method is to ensure the intermedi-\nate representations of the universal encoder are language in-\nvariant. In the rest of this section, we ﬁrst present two exist-\ning methods yet to be explored in zero-shot translation, and\nthen propose a straightforward but effective cross-lingual\npre-training method. In the end, we present the whole train-\ning and inference protocol for transfer.\nMasked and Translation Language Model\nPretraining\nTwo existing cross-lingual pre-training methods, Masked\nLanguage Modeling (MLM) and Translation Language\nModeling (TLM), have shown their effectiveness on XNLI\ncross-lingual classiﬁcation task (Lample and Conneau 2019;\nHuang et al. 2019), but these methods have not been well\nstudied on cross-lingual generation tasks in zero-shot condi-\ntion. We attempt to take advantage of the cross-lingual abil-\nity of the two methods for zero-shot translation.\nSpeciﬁcally, MLM adopts the Cloze objective of\nBERT (Devlin et al. 2018) and predicts the masked words\nthat are randomly selected and replaced with [MASK] to-\nken on monolingual corpus. In practice, MLM takes dif-\nferent language monolingual corpora as input to ﬁnd fea-\ntures shared across different languages. With this method,\nword pieces shared in all languages have been mapped into\na shared space, which makes the sentence representationsacross different languages close (Pires, Schlinger, and Gar-\nrette 2019).\nSince MLM objective is unsupervised and only requires\nmonolingual data, TLM is designed to leverage parallel data\nwhen it is available. Actually, TLM is a simple extension of\nMLM, with the difference that TLM concatenates sentence\npair into a whole sentence, and then randomly masks words\nin both the source and target sentences. In this way, the\nmodel can either attend to surrounding words or to the trans-\nlation sentence, implicitly encouraging the model to align\nthe source and target language representations. Note that al-\nthough each sentence pair is formed into one sentence, the\npositions of the target sentence are reset to count form zero.\nBridge Language Model Pretraining\nAside from MLM and TLM, we propose BRidge Language\nModeling (BRLM) to further obtain word-level representa-\ntion alignment between different languages. This method is\ninspired by the assumption that if the feature spaces of dif-\nferent languages are aligned very well, the masked words\nin the corrupted sentence can also be guessed by the con-\ntext of the correspondingly aligned words on the other side.\nTo achieve this goal, BRLM is designed to strengthen the\nability to infer words across languages based on alignment\ninformation, instead of inferring words within monolingual\nsentence as in MLM or within the pseudo sentence formed\nby concatenating sentence pair as in TLM.\nAs illustrated in Figure 2, BRLM stacks shared encoder\nover both side sentences separately. In particular, we design\ntwo network structures for BRLM, which are divided into', 'Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-\nSA) according to the way of generating the alignment infor-\nmation. These two structures actually extend MLM into a\nbilingual scenario, with the difference that BRLM leverages\nexternal aligner tool or additional attention layer to explic-\nitly introduce alignment information during model training.\n•Hard Alignment (BRLM-HA). We ﬁrst use exter-\nnal aligner tool on source ↔pivot parallel data to ex-\ntract the alignment information of sentence pair. Dur-\ning model training, given source ↔pivot sentence pair,\nBRLM-HA randomly masks some words in source sen-\ntence and leverages alignment information to obtain the\naligned words in pivot sentence for masked words. Based\non the processed input, BRLM-HA adopts the Trans-\nformer (Vaswani et al. 2017) encoder to gain the hid-\nden states for source and pivot sentences respectively.\nThen the training objective of BRLM-HA is to predict\nthe masked words by not only the surrounding words\nin source sentence but also the encoder outputs of the\naligned words. Note that this training process is also car-\nried out in a symmetric situation, in which we mask some\nwords in pivot sentence and obtain the aligned words in\nthe source sentence.\n•Soft Alignment (BRLM-SA). Instead of using external\naligner tool, BRLM-SA introduces an additional atten-\ntion layer to learn the alignment information together with\nmodel training. In this way, BRLM-SA avoids the effect\ncaused by external wrong alignment information and en-\nables many-to-one soft alignment during model training.\nSimilar with BRLM-HA, the training objective of BRLM-\nSA is to predict the masked words by not only the sur-\nrounding words in source sentence but also the outputs of\nattention layer. In our implementation, the attention layer\nis a multi-head attention layer adopted in Transformer,\nwhere the queries come from the masked source sentence,\nthe keys and values come from the pivot sentence.\nIn principle, MLM and TLM can learn some implicit align-\nment information during model training. However, the align-\nment process in MLM is inefﬁcient since the shared word\npieces only account for a small proportion of the whole cor-\npus, resulting in the difﬁculty of expanding the shared in-\nformation to align the whole corpus. TLM also lacks effort\nin alignment between the source and target sentences since\nTLM concatenates the sentence pair into one sequence, mak-\ning the explicit alignment between the source and target in-\nfeasible. BRLM fully utilizes the alignment information to\nobtain better word-level representation alignment between\ndifferent languages, which better relieves the burden of the\ndomain shift problem .\nTransfer Protocol\nWe consider the typical zero-shot translation scenario in\nwhich a high resource pivot language has parallel data with\nboth source and target languages, while source and target\nlanguages has no parallel data between themselves. Our pro-\nposed cross-lingual pretraining based transfer approach for\nsource→target zero-shot translation is mainly divided into\ntwo phrases: the pretraining phase and the transfer phase.Corpus Language Train Dev Test\nEuroparlDe-En,En-Fr 1M,1M 2,000 2,000\nFr-En,En-Es 1M,1M 2,000 2,000\nRo-En,En-De 0.6M,1.5M 2,000 1,000\nMultiUNAr-En,En-Es\nEn-Ru9.7M,11.3M\n11.6M4,000 4,000\nTable 1: Data Statistics.\nIn the pretraining phase, we ﬁrst pretrain MLM on mono-\nlingual corpora of both source and pivot languages, and con-\ntinue to pretrain TLM or the proposed BRLM on the avail-\nable parallel data between source and pivot languages, in\norder to build a cross-lingual encoder shared by the source\nand pivot languages.\nIn the transfer phase, we train pivot →target NMT model\ninitialized by the cross-lingually pre-trained encoder, and\nﬁnally transfer the trained NMT model to source →target\ntranslation thanks to the shared encoder. Note that during\ntraining pivot→target NMT model, we freeze several layers\nof the cross-lingually pre-trained encoder to avoid the de-\ngeneracy issue.\nFor the', ' more complicated scenario that either the source\nside or the target side has multiple languages, the encoder\nand the decoder are also shared across each side languages\nfor efﬁcient deployment of translation between multiple lan-\nguages.\nExperiments\nSetup\nWe evaluate our cross-lingual pre-training based transfer ap-\nproach against several strong baselines on two public datat-\nsets, Europarl (Koehn 2005) and MultiUN (Eisele and Chen\n2010), which contain multi-parallel evaluation data to assess\nthe zero-shot performance. In all experiments, we use BLEU\nas the automatic metric for translation evaluation.1\nDatasets. The statistics of Europarl and MultiUN cor-\npora are summarized in Table 1. For Europarl corpus, we\nevaluate on French-English-Spanish (Fr-En-Es), German-\nEnglish-French (De-En-Fr) and Romanian-English-German\n(Ro-En-De), where English acts as the pivot language, its\nleft side is the source language, and its right side is the target\nlanguage. We remove the multi-parallel sentences between\ndifferent training corpora to ensure zero-shot settings. We\nuse the devtest2006 as the validation set and the test2006 as\nthe test set for Fr→Es and De→Fr. For distant language pair\nRo→De, we extract 1,000 overlapping sentences from new-\nstest2016 as the test set and the 2,000 overlapping sentences\nsplit from the training set as the validation set since there is\nno ofﬁcial validation and test sets. For vocabulary, we use\n60K sub-word tokens based on Byte Pair Encoding (BPE)\n(Sennrich, Haddow, and Birch 2015).\nFor MultiUN corpus, we use four languages: English\n(En) is set as the pivot language, which has parallel data\n1We calculate BLEU scores with the multi-bleu.perl script.', 'Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-\nSA) according to the way of generating the alignment infor-\nmation. These two structures actually extend MLM into a\nbilingual scenario, with the difference that BRLM leverages\nexternal aligner tool or additional attention layer to explic-\nitly introduce alignment information during model training.\n•Hard Alignment (BRLM-HA). We ﬁrst use exter-\nnal aligner tool on source ↔pivot parallel data to ex-\ntract the alignment information of sentence pair. Dur-\ning model training, given source ↔pivot sentence pair,\nBRLM-HA randomly masks some words in source sen-\ntence and leverages alignment information to obtain the\naligned words in pivot sentence for masked words. Based\non the processed input, BRLM-HA adopts the Trans-\nformer (Vaswani et al. 2017) encoder to gain the hid-\nden states for source and pivot sentences respectively.\nThen the training objective of BRLM-HA is to predict\nthe masked words by not only the surrounding words\nin source sentence but also the encoder outputs of the\naligned words. Note that this training process is also car-\nried out in a symmetric situation, in which we mask some\nwords in pivot sentence and obtain the aligned words in\nthe source sentence.\n•Soft Alignment (BRLM-SA). Instead of using external\naligner tool, BRLM-SA introduces an additional atten-\ntion layer to learn the alignment information together with\nmodel training. In this way, BRLM-SA avoids the effect\ncaused by external wrong alignment information and en-\nables many-to-one soft alignment during model training.\nSimilar with BRLM-HA, the training objective of BRLM-\nSA is to predict the masked words by not only the sur-\nrounding words in source sentence but also the outputs of\nattention layer. In our implementation, the attention layer\nis a multi-head attention layer adopted in Transformer,\nwhere the queries come from the masked source sentence,\nthe keys and values come from the pivot sentence.\nIn principle, MLM and TLM can learn some implicit align-\nment information during model training. However, the align-\nment process in MLM is inefﬁcient since the shared word\npieces only account for a small proportion of the whole cor-\npus, resulting in the difﬁculty of expanding the shared in-\nformation to align the whole corpus. TLM also lacks effort\nin alignment between the source and target sentences since\nTLM concatenates the sentence pair into one sequence, mak-\ning the explicit alignment between the source and target in-\nfeasible. BRLM fully utilizes the alignment information to\nobtain better word-level representation alignment between\ndifferent languages, which better relieves the burden of the\ndomain shift problem .\nTransfer Protocol\nWe consider the typical zero-shot translation scenario in\nwhich a high resource pivot language has parallel data with\nboth source and target languages, while source and target\nlanguages has no parallel data between themselves. Our pro-\nposed cross-lingual pretraining based transfer approach for\nsource→target zero-shot translation is mainly divided into\ntwo phrases: the pretraining phase and the transfer phase.Corpus Language Train Dev Test\nEuroparlDe-En,En-Fr 1M,1M 2,000 2,000\nFr-En,En-Es 1M,1M 2,000 2,000\nRo-En,En-De 0.6M,1.5M 2,000 1,000\nMultiUNAr-En,En-Es\nEn-Ru9.7M,11.3M\n11.6M4,000 4,000\nTable 1: Data Statistics.\nIn the pretraining phase, we ﬁrst pretrain MLM on mono-\nlingual corpora of both source and pivot languages, and con-\ntinue to pretrain TLM or the proposed BRLM on the avail-\nable parallel data between source and pivot languages, in\norder to build a cross-lingual encoder shared by the source\nand pivot languages.\nIn the transfer phase, we train pivot →target NMT model\ninitialized by the cross-lingually pre-trained encoder, and\nﬁnally transfer the trained NMT model to source →target\ntranslation thanks to the shared encoder. Note that during\ntraining pivot→target NMT model, we freeze several layers\nof the cross-lingually pre-trained encoder to avoid the de-\ngeneracy issue.\nFor the']","The additional alignment methods used in the proposed cross-lingual pre-training approach are Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-SA). BRLM-HA uses an external aligner tool on source ↔ pivot parallel data to extract alignment information, while BRLM-SA introduces an additional attention layer to learn the alignment information during model training.",conditional,TRUE
99,How does the context-specificity of contextualized word representations vary across the layers of neural language models?,"['Figure 1: In almost all layers of BERT, ELMo, and GPT-2, the word representations are anisotropic (i.e., not\ndirectionally uniform): the average cosine similarity between uniformly randomly sampled words is non-zero.\nThe one exception is ELMo’s input layer; this is not surprising given that it generates character-level embeddings\nwithout using context. Representations in higher layers are generally more anisotropic than those in lower ones.\nELMo as well, though there are exceptions: for ex-\nample, the anisotropy in BERT’s penultimate layer\nis much higher than in its ﬁnal layer.\nIsotropy has both theoretical and empirical ben-\neﬁts for static word embeddings. In theory, it\nallows for stronger “self-normalization” during\ntraining (Arora et al., 2017), and in practice, sub-\ntracting the mean vector from static embeddings\nleads to improvements on several downstream\nNLP tasks (Mu et al., 2018). Thus the extreme\ndegree of anisotropy seen in contextualized word\nrepresentations – particularly in higher layers –\nis surprising. As seen in Figure 1, for all three\nmodels, the contextualized hidden layer represen-\ntations are almost all more anisotropic than the in-\nput layer representations, which do not incorpo-\nrate context. This suggests that high anisotropy is\ninherent to, or least a by-product of, the process of\ncontextualization.\n4.2 Context-Speciﬁcity\nContextualized word representations are more\ncontext-speciﬁc in higher layers. Recall from\nDeﬁnition 1 that the self-similarity of a word, in\na given layer of a given model, is the average co-\nsine similarity between its representations in dif-\nferent contexts, adjusted for anisotropy. If the\nself-similarity is 1, then the representations are\nnot context-speciﬁc at all; if the self-similarity is\n0, that the representations are maximally context-\nspeciﬁc. In Figure 2, we plot the average self-\nsimilarity of uniformly randomly sampled wordsin each layer of BERT, ELMo, and GPT-2. For\nexample, the self-similarity is 1.0 in ELMo’s in-\nput layer because representations in that layer are\nstatic character-level embeddings.\nIn all three models, the higher the layer, the\nlower the self-similarity is on average. In other\nwords, the higher the layer, the more context-\nspeciﬁc the contextualized representations. This\nﬁnding makes intuitive sense. In image classiﬁca-\ntion models, lower layers recognize more generic\nfeatures such as edges while upper layers recog-\nnize more class-speciﬁc features (Yosinski et al.,\n2014). Similarly, upper layers of LSTMs trained\non NLP tasks learn more task-speciﬁc represen-\ntations (Liu et al., 2019a). Therefore, it fol-\nlows that upper layers of neural language mod-\nels learn more context-speciﬁc representations, so\nas to predict the next word for a given context\nmore accurately. Of all three models, representa-\ntions in GPT-2 are the most context-speciﬁc, with\nthose in GPT-2’s last layer being almost maxi-\nmally context-speciﬁc.\nStopwords (e.g., ‘the’, ‘of’, ‘to’ ) have among the\nmost context-speciﬁc representations. Across\nall layers, stopwords have among the lowest self-\nsimilarity of all words, implying that their con-\ntextualized representations are among the most\ncontext-speciﬁc. For example, the words with the\nlowest average self-similarity across ELMo’s lay-\ners are ‘and’, ‘of’, ‘’s’, ‘the’ , and ‘to’. This is rel-\natively surprising, given that these words are not\npolysemous. This ﬁnding suggests that the variety', 'Figure 2: The average cosine similarity between representations of the same word in different contexts is called\nthe word’s self-similarity (see Deﬁnition 1). Above, we plot the average self-similarity of uniformly randomly\nsampled words after adjusting for anisotropy (see section 3.4). In all three models, the higher the layer, the lower\nthe self-similarity, suggesting that contextualized word representations are more context-speciﬁc in higher layers.\nof contexts a word appears in, rather than its inher-\nent polysemy, is what drives variation in its con-\ntextualized representations. This answers one of\nthe questions we posed in the introduction: ELMo,\nBERT, and GPT-2 are not simply assigning one of\na ﬁnite number of word-sense representations to\neach word; otherwise, there would not be so much\nvariation in the representations of words with so\nfew word senses.\nContext-speciﬁcity manifests very differently in\nELMo, BERT, and GPT-2. As noted earlier,\ncontextualized representations are more context-\nspeciﬁc in upper layers of ELMo, BERT, and GPT-\n2. However, how does this increased context-\nspeciﬁcity manifest in the vector space? Do word\nrepresentations in the same sentence converge to a\nsingle point, or do they remain distinct from one\nanother while still being distinct from their repre-\nsentations in other contexts? To answer this ques-\ntion, we can measure a sentence’s intra-sentence\nsimilarity. Recall from Deﬁnition 2 that the intra-\nsentence similarity of a sentence, in a given layer\nof a given model, is the average cosine similarity\nbetween each of its word representations and their\nmean, adjusted for anisotropy. In Figure 3, we plot\nthe average intra-sentence similarity of 500 uni-\nformly randomly sampled sentences.\nIn ELMo, words in the same sentence are more\nsimilar to one another in upper layers. As\nword representations in a sentence become more\ncontext-speciﬁc in upper layers, the intra-sentencesimilarity also rises. This suggests that, in prac-\ntice, ELMo ends up extending the intuition behind\nFirth’s (1957) distributional hypothesis to the sen-\ntence level: that because words in the same sen-\ntence share the same context, their contextualized\nrepresentations should also be similar.\nIn BERT, words in the same sentence are more\ndissimilar to one another in upper layers. As\nword representations in a sentence become more\ncontext-speciﬁc in upper layers, they drift away\nfrom one another, although there are exceptions\n(see layer 12 in Figure 3). However, in all lay-\ners, the average similarity between words in the\nsame sentence is still greater than the average sim-\nilarity between randomly chosen words (i.e., the\nanisotropy baseline). This suggests a more nu-\nanced contextualization than in ELMo, with BERT\nrecognizing that although the surrounding sen-\ntence informs a word’s meaning, two words in the\nsame sentence do not necessarily have a similar\nmeaning because they share the same context.\nIn GPT-2, word representations in the same\nsentence are no more similar to each other than\nrandomly sampled words. On average, the un-\nadjusted intra-sentence similarity is roughly the\nsame as the anisotropic baseline, so as seen in Fig-\nure 3, the anisotropy-adjusted intra-sentence simi-\nlarity is close to 0 in most layers of GPT-2. In fact,\nthe intra-sentence similarity is highest in the input\nlayer, which does not contextualize words at all.\nThis is in contrast to ELMo and BERT, where the']",nan,conditional,TRUE