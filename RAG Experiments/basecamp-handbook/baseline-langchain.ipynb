{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcea3a96-d866-4083-9626-14450bfb5fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55763470-dcdb-49ef-9ae9-3b1e3be9343e",
   "metadata": {},
   "source": [
    "# The Problem\n",
    "\n",
    "Figure out a Metrics-driven approach to make sense of this\n",
    "![](https://media.licdn.com/dms/image/D4D22AQEgjWxKXokOPA/feedshare-shrink_800/0/1708498751086?e=1711584000&v=beta&t=xaT95vKS8m4qTybofpKqQfXOGoFs8lQXBuOk2Fr45AE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147920f5-3448-440a-9bef-5665714df901",
   "metadata": {},
   "source": [
    "## Our Solution: `Metrics Driven Development with Ragas`\n",
    "\n",
    "![](https://docs.ragas.io/en/latest/_static/imgs/component-wise-metrics.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc41ad4-0293-4713-b247-9666cd37593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cfe471-eb5a-48d2-832b-84d0b9d24be1",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "000582e7-5b78-4d87-9826-a589ac6f6dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader(\"./data/\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b6d0cd7-ea54-4f5d-8229-7aa30a2d6cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in documents:\n",
    "    document.metadata['file_name'] = document.metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b068fd3e-dd29-4bc3-b452-6bfb75f7ab98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = documents\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1aa6746f-7740-4d26-a00b-219c122c51e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2dc9de-bc8b-4239-9cc6-0c9d6e113c0a",
   "metadata": {},
   "source": [
    "## Testset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ec07b286-56b0-49a3-bfb3-c990c40b4d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "711a6da32a854ab2867ccad4e7aa8302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "# generator with openai models\n",
    "generator = TestsetGenerator.with_openai()\n",
    "\n",
    "# generate testset\n",
    "testset = generator.generate_with_langchain_docs(\n",
    "    documents, \n",
    "    test_size=10, \n",
    "    distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9305ce2d-3ff1-4772-a206-306d1dc3d090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do the 6-week or 8-week cycles and the con...</td>\n",
       "      <td>[How We Work\\n\\nCycles\\n\\nWe work in 6-week or...</td>\n",
       "      <td>The 6-week or 8-week cycles and the concept of...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does it mean when it is said that \"market...</td>\n",
       "      <td>[37signals Is You\\n\\nEveryone working at 37sig...</td>\n",
       "      <td>When it is said that \"marketing is everyone's ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the meaning of \"PDI\" and how is it use...</td>\n",
       "      <td>[Vocabulary\\n\\n37signals has over time develop...</td>\n",
       "      <td>The meaning of \"PDI\" in the context of project...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does 37signals recognize mastery and how d...</td>\n",
       "      <td>[ how we recognize mastery, it’s by no means a...</td>\n",
       "      <td>37signals recognizes mastery by using titles t...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How does 37signals determine pay and promotion...</td>\n",
       "      <td>[ how we recognize mastery, it’s by no means a...</td>\n",
       "      <td>37signals determines pay and promotions based ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What are the criteria used to assess the level...</td>\n",
       "      <td>[Titles for Designers\\n\\nWe use the following ...</td>\n",
       "      <td>The criteria used to assess the level and titl...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does 37signals contribute to the success o...</td>\n",
       "      <td>[37signals Is You\\n\\nEveryone working at 37sig...</td>\n",
       "      <td>37signals contributes to the success of open s...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How does Basecamp help small businesses with p...</td>\n",
       "      <td>[ thinking. Since day one, we’ve always done t...</td>\n",
       "      <td>Basecamp helps small businesses with projects,...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"What is the connection between 37signals' ope...</td>\n",
       "      <td>[37signals Is You\\n\\nEveryone working at 37sig...</td>\n",
       "      <td>The connection between 37signals' open source ...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are some of the key influences that have ...</td>\n",
       "      <td>[What Influenced Us\\n\\nIf you want to learn th...</td>\n",
       "      <td>Some of the key influences that have shaped th...</td>\n",
       "      <td>simple</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  How do the 6-week or 8-week cycles and the con...   \n",
       "1  What does it mean when it is said that \"market...   \n",
       "2  What is the meaning of \"PDI\" and how is it use...   \n",
       "3  How does 37signals recognize mastery and how d...   \n",
       "4  How does 37signals determine pay and promotion...   \n",
       "5  What are the criteria used to assess the level...   \n",
       "6  How does 37signals contribute to the success o...   \n",
       "7  How does Basecamp help small businesses with p...   \n",
       "8  \"What is the connection between 37signals' ope...   \n",
       "9  What are some of the key influences that have ...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [How We Work\\n\\nCycles\\n\\nWe work in 6-week or...   \n",
       "1  [37signals Is You\\n\\nEveryone working at 37sig...   \n",
       "2  [Vocabulary\\n\\n37signals has over time develop...   \n",
       "3  [ how we recognize mastery, it’s by no means a...   \n",
       "4  [ how we recognize mastery, it’s by no means a...   \n",
       "5  [Titles for Designers\\n\\nWe use the following ...   \n",
       "6  [37signals Is You\\n\\nEveryone working at 37sig...   \n",
       "7  [ thinking. Since day one, we’ve always done t...   \n",
       "8  [37signals Is You\\n\\nEveryone working at 37sig...   \n",
       "9  [What Influenced Us\\n\\nIf you want to learn th...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  The 6-week or 8-week cycles and the concept of...         simple   \n",
       "1  When it is said that \"marketing is everyone's ...         simple   \n",
       "2  The meaning of \"PDI\" in the context of project...         simple   \n",
       "3  37signals recognizes mastery by using titles t...         simple   \n",
       "4  37signals determines pay and promotions based ...         simple   \n",
       "5  The criteria used to assess the level and titl...      reasoning   \n",
       "6  37signals contributes to the success of open s...      reasoning   \n",
       "7  Basecamp helps small businesses with projects,...  multi_context   \n",
       "8  The connection between 37signals' open source ...  multi_context   \n",
       "9  Some of the key influences that have shaped th...         simple   \n",
       "\n",
       "   episode_done  \n",
       "0          True  \n",
       "1          True  \n",
       "2          True  \n",
       "3          True  \n",
       "4          True  \n",
       "5          True  \n",
       "6          True  \n",
       "7          True  \n",
       "8          True  \n",
       "9          True  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = testset.to_pandas()\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bb52282f-4036-4d38-ba22-eabbffc878da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a new dataset:  basecamp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to batch ingest runs: LangSmithError('Failed to post https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Request body is not valid JSON\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to post https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Request body is not valid JSON\"}\\')')\n",
      "Failed to batch ingest runs: LangSmithError('Failed to post https://api.smith.langchain.com/runs/batch in LangSmith API. HTTPError(\\'400 Client Error: Bad Request for url: https://api.smith.langchain.com/runs/batch\\', \\'{\"detail\":\"Request body is not valid JSON\"}\\')')\n"
     ]
    }
   ],
   "source": [
    "# upload to langsmith\n",
    "\n",
    "from langsmith import Client\n",
    "from langsmith.utils import LangSmithNotFoundError\n",
    "\n",
    "client = Client()\n",
    "dataset_name = \"basecamp\"\n",
    "dataset_desc = \"Synthetic testset data for basecamp\"\n",
    "\n",
    "try:\n",
    "    # check if dataset exists\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    print(\"using existing dataset: \", dataset.name)\n",
    "except LangSmithError:\n",
    "    # if not create a new one with the generated query examples    \n",
    "    dataset = client.upload_dataframe(\n",
    "        df=test_df, \n",
    "        name=dataset_name, \n",
    "        input_keys=[\"question\"], \n",
    "        output_keys=[\"ground_truth\"],\n",
    "        description=dataset_desc\n",
    "    )\n",
    "    \n",
    "    print(\"Created a new dataset: \", dataset.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6567e4-4b22-405e-88f9-114b87b0e32d",
   "metadata": {},
   "source": [
    "Now you have a nice way to view them and select the ones you want."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c8271-63ee-4d9f-ac57-9df5874a8482",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "now lets build 2 baselines and compare them with metrics available through Ragas. The first metric will be `AnswerCorrectness`.\n",
    "\n",
    "The question I'm curious about is\n",
    "> Is RAG actually better than just LLM's for this data distribution?\n",
    "\n",
    "well - lets compare shall we! We'll take 2 examples\n",
    "1. Vanilla RAG from Langchain\n",
    "2. GPT 3.5 and my humble prompts\n",
    "\n",
    "first create both"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9728615b-14c9-4e43-8b14-8880e74fd4b4",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63fa1de7-6df5-4782-9c49-4c0f5a66a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ade7901a-540f-4820-a480-3d53434b308e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  How do the 6-week or 8-week cycles and the concept of a \"scope hammer\" contribute to the work process at 37signals?\n",
      "A:  The 6-week or 8-week cycles at 37signals provide a fixed cadence that creates an internal sense of urgency and serves as a scope hammer to prevent projects from expanding. The concept helps break big projects into smaller ones that can be completed within the designated time frame and encourages bundling smaller tasks into a manageable scope of work for discussion. This approach is particularly important for product teams to designate work with scope in mind upfront and avoid projects exceeding the budgeted time frame.\n"
     ]
    }
   ],
   "source": [
    "q = testset.to_pandas().question[0]\n",
    "print(\"Q: \", q)\n",
    "resp = rag_chain.invoke(q)\n",
    "print(\"A: \", resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053bc1cc-cd17-4a28-a7db-be4d5eb60df1",
   "metadata": {},
   "source": [
    "but we want the source documents also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c423bf18-22e1-4898-aeff-c591556d71b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='All that being said, you should still ensure that there is ample overlap with the people you work with most of the time. While most roadblocks can just as well be cleared in 15-30-60 minutes, they become real annoying if it’s a one-day turn-around every time.\\n\\nIn certain departments, like Support and Ops, it’s even more important that people are dependently available when they say they will be. That work has a lot of interrupt-based jobs that simply needs to be done right here, right now. So what applies to almost all work for design and programming and QA may well apply a little less frequently there.\\n\\nIn self-sufficient, independent teams\\n\\nOrganizational theory is thick with descriptions of the trade-offs between functional and project company structures. We seek to be more project than functional. This means a single project team should be able to go from idea to deploy as independently as possible.', metadata={'source': 'data/how-we-work.md'}),\n",
       "  Document(page_content='All that being said, you should still ensure that there is ample overlap with the people you work with most of the time. While most roadblocks can just as well be cleared in 15-30-60 minutes, they become real annoying if it’s a one-day turn-around every time.\\n\\nIn certain departments, like Support and Ops, it’s even more important that people are dependently available when they say they will be. That work has a lot of interrupt-based jobs that simply needs to be done right here, right now. So what applies to almost all work for design and programming and QA may well apply a little less frequently there.\\n\\nIn self-sufficient, independent teams\\n\\nOrganizational theory is thick with descriptions of the trade-offs between functional and project company structures. We seek to be more project than functional. This means a single project team should be able to go from idea to deploy as independently as possible.', metadata={'file_name': 'data/how-we-work.md', 'source': 'data/how-we-work.md'}),\n",
       "  Document(page_content='Vocabulary\\n\\n37signals has over time developed a vocabulary specific to our work and company.\\n\\nChowder\\n\\nWhen establishing the tasks under a project, there are must-haves and maybes. Chowder is a shorthand we use to refer to the maybes.\\n\\nJudo\\n\\nMost problems can be solved in a thousand different ways. One way might take 100 hours, another might take 10. Judo is the art of problem restatement. Turning that massive, scary 3-month looking problem into one that can be done in 3 weeks instead. It’s often used when we get frustrated trying to solve something hard and we aren’t making sufficient progress: “Let’s figure out a way to judo this!”.\\n\\nPDI\\n\\n“Please do investigate”. Seen most often on to-do lists in Basecamp to indicate uncertainty. It helps everyone on the project understand that we’re going to look into it but it may or may not be feasible, practical, or possible.\\n\\nScope Hammering', metadata={'source': 'data/vocabulary.md'}),\n",
       "  Document(page_content='Vocabulary\\n\\n37signals has over time developed a vocabulary specific to our work and company.\\n\\nChowder\\n\\nWhen establishing the tasks under a project, there are must-haves and maybes. Chowder is a shorthand we use to refer to the maybes.\\n\\nJudo\\n\\nMost problems can be solved in a thousand different ways. One way might take 100 hours, another might take 10. Judo is the art of problem restatement. Turning that massive, scary 3-month looking problem into one that can be done in 3 weeks instead. It’s often used when we get frustrated trying to solve something hard and we aren’t making sufficient progress: “Let’s figure out a way to judo this!”.\\n\\nPDI\\n\\n“Please do investigate”. Seen most often on to-do lists in Basecamp to indicate uncertainty. It helps everyone on the project understand that we’re going to look into it but it may or may not be feasible, practical, or possible.\\n\\nScope Hammering', metadata={'file_name': 'data/vocabulary.md', 'source': 'data/vocabulary.md'})],\n",
       " 'question': 'What is Task Decomposition',\n",
       " 'answer': 'Task decomposition is the process of breaking down a project into smaller, more manageable tasks. It allows for better organization and tracking of progress. This approach helps teams to work more efficiently and effectively towards project completion.'}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)\n",
    "\n",
    "resp = rag_chain_with_source.invoke(\"What is Task Decomposition\")\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d448d4a9-e6da-4dba-a8f4-9ae031a13167",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87fb71c0-622b-4cbc-99ae-c81c28c0cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "llm_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "just_llm = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | llm_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f1ae80c-a39d-42a1-9b8d-9e101f6fb1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:  What are the criteria for assessing the level and title of ops at 37signals, specifically for the Junior Site Reliability Engineer position?\n",
      "A:  The criteria for assessing the level and title of ops at 37signals, specifically for the Junior Site Reliability Engineer position, are based on technical skills, experience with relevant tools and technologies, and the ability to work effectively in a team. Thanks for asking!\n"
     ]
    }
   ],
   "source": [
    "q = testset.to_pandas().question[0]\n",
    "print(\"Q: \", q)\n",
    "resp = just_llm.invoke(q)\n",
    "print(\"A: \", resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92529864-e251-4b93-b968-b885c3d2f058",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8faeb587-b580-4732-8293-d326c6c9c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import faithfulness, answer_correctness, answer_similarity\n",
    "from ragas.llms import llm_factory\n",
    "from ragas.embeddings import embedding_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a23e7e9f-d25f-4e43-b88a-58be5589b171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "from langsmith.evaluation import EvaluationResult, run_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1a4ea499-84fa-4594-90cb-6095e15ed24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_similarity.embeddings=embedding_factory()\n",
    "answer_correctness.llm = llm_factory()\n",
    "answer_correctness.answer_similarity = answer_similarity\n",
    "\n",
    "@run_evaluator\n",
    "def ragas_scores(run, example) -> EvaluationResult:\n",
    "    pred = run.outputs.get(\"output\") or \"\"\n",
    "    q = example.inputs[\"question\"]\n",
    "    g = example.outputs[\"ground_truth\"]\n",
    "    if isinstance(pred, str):\n",
    "        contexts = [\"\"]\n",
    "        a = pred\n",
    "    else:\n",
    "        contexts = [d.page_context for d in pred.output[\"context\"]]\n",
    "        a = pred.outputs[\"answer\"]\n",
    "        \n",
    "    score = answer_correctness.score({\n",
    "            \"question\": example.inputs[\"question\"], \n",
    "            \"contexts\": contexts,\n",
    "            \"answer\": pred,\n",
    "            \"ground_truth\": g\n",
    "        })\n",
    "    return EvaluationResult(key=\"answer_correctness\", score=score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c544e0f3-68b3-48da-b878-d51cbd698bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'ordinary-building-39' at:\n",
      "https://smith.langchain.com/o/9bfbddc5-b88e-41e5-92df-2a62f0c64b4b/datasets/f3408987-0cab-46c2-9219-3dc52f525f0a/compare?selectedSessions=6d8cf192-919f-43c3-bf7e-4236ab739439\n",
      "\n",
      "View all tests for Dataset basecamp at:\n",
      "https://smith.langchain.com/o/9bfbddc5-b88e-41e5-92df-2a62f0c64b4b/datasets/f3408987-0cab-46c2-9219-3dc52f525f0a\n",
      "[------------------------------------------------->] 10/10"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.answer_correctness</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7810e0e1-0e31-4f56-a403-f9a72a2b79d0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.536848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.280200</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.172317</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.213458</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.215803</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.997127</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.422280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.086276</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.533798</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.278777</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.694764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.465362</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.742461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.588147</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.answer_correctness error  execution_time  \\\n",
       "count                     10.000000     0       10.000000   \n",
       "unique                          NaN     0             NaN   \n",
       "top                             NaN   NaN             NaN   \n",
       "freq                            NaN   NaN             NaN   \n",
       "mean                       0.536848   NaN        1.280200   \n",
       "std                        0.172317   NaN        0.213458   \n",
       "min                        0.215803   NaN        0.997127   \n",
       "25%                        0.422280   NaN        1.086276   \n",
       "50%                        0.533798   NaN        1.278777   \n",
       "75%                        0.694764   NaN        1.465362   \n",
       "max                        0.742461   NaN        1.588147   \n",
       "\n",
       "                                      run_id  \n",
       "count                                     10  \n",
       "unique                                    10  \n",
       "top     7810e0e1-0e31-4f56-a403-f9a72a2b79d0  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_config = RunEvalConfig(\n",
    "    custom_evaluators=[ragas_scores],\n",
    ")\n",
    "run = client.run_on_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=just_llm,\n",
    "    evaluation=eval_config,\n",
    "    verbose=True,\n",
    "    # Any experiment metadata can be specified here\n",
    "    project_metadata={\"version\": \"1.0.0\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82abcec8-449a-496c-95d6-63e07a9a6ad8",
   "metadata": {},
   "source": [
    "## Need help with improving RAG at your company? -> we are conducting Office-Hours\n",
    "\n",
    "More info will be available on bookface shortly but chat with us if you want to know more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e838b1e-f759-4044-9795-7907513d389a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
