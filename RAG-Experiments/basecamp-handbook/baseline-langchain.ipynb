{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe90efd1",
   "metadata": {},
   "source": [
    "# Build a Baseline with Langchain\n",
    "\n",
    "Having prepared a test dataset, we are now equipped to conduct experiments and iteratively enhance our RAG pipelines.\n",
    "\n",
    "## Introducing Metrics Driven Development (MDD)\n",
    "\n",
    "A key challenge is developing a systematic approach to measure and refine our RAG pipeline. To address this, we propose `Metrics Driven Development` (MDD), inspired by the popular Test Driven Development methodology. MDD advocates for employing various metrics to assess different facets of the LLM application and conducting targeted experiments for improvement based on specific use cases.\n",
    "\n",
    "Metrics Driven Development (MDD) offers a structured framework to tackle the complexities involved in optimizing RAG applications. Below is a mind map created by an engineer at AWS, illustrating potential strategies for enhancing a RAG pipeline.\n",
    "\n",
    "![](https://media.licdn.com/dms/image/D4D22AQEgjWxKXokOPA/feedshare-shrink_800/0/1708498751086?e=1711584000&v=beta&t=xaT95vKS8m4qTybofpKqQfXOGoFs8lQXBuOk2Fr45AE)\n",
    "\n",
    "The [original Miro mind map](https://miro.com/app/board/uXjVNvklNmc=/) is accessible for further exploration. MDD serves as a guiding light through the complexity of enhancing RAG applications.\n",
    "\n",
    "Let's proceed to see MDD in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d733a0-398f-43fc-a217-7146a082d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U \"unstructured[md]\" chromadb langchain_openai langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3ce85-ceb5-4b01-9205-b6c079469818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28504e01",
   "metadata": {},
   "source": [
    "## Building a baseline\n",
    "\n",
    "if you remember, in the [last notebook](./dataset.ipynb) we outline the steps and addressed a few\n",
    "\n",
    "1. Load the data as documents. ✅\n",
    "2. Generate the test set from these documents. ✅\n",
    "3. Upload and verify the test set with Langsmith. ✅\n",
    "4. Formulate experiments to improve you RAG pipeline. ⏳\n",
    "5. Choose the right metrics to evaluate the experiment ⏳\n",
    "6. Analyze the results using the Langsmith dashboard. ⏳\n",
    "\n",
    "So lets continue where we left off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ac315",
   "metadata": {},
   "source": [
    "## 4. Formulate experiments to improve your RAG pipeline\n",
    "\n",
    "Now for the baseline first thing we want to know is how effective is vanila GPT-3.5 compared to RAG based model. RAG should be superiour because there is specific information about companies but what exactly is the difference?\n",
    "\n",
    "> is RAG better than just using an LLM for our case?\n",
    "\n",
    "In order to compare that need to create 2 chains \n",
    "1. Just LLM - gpt-3.5\n",
    "2. LLM + Retriver "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18681df",
   "metadata": {},
   "source": [
    "### Building the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfc41ad4-0293-4713-b247-9666cd37593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d317b510",
   "metadata": {},
   "source": [
    "To build the RAG lets load the data, chunk it and add it to a vector store for retrieval. If you want more info on how to build RAG systems with langchain, check the [docs](https://python.langchain.com/docs/modules/data_connection/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "000582e7-5b78-4d87-9826-a589ac6f6dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the documents\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\"./data/\")\n",
    "documents = loader.load()\n",
    "\n",
    "# add filename as metadata\n",
    "for document in documents:\n",
    "    document.metadata[\"file_name\"] = document.metadata[\"source\"]\n",
    "\n",
    "# how many docs do we have?\n",
    "docs = documents\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa6746f-7740-4d26-a00b-219c122c51e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# create the vector store\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "548d590f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"What is the importance of levelheadedness in the company's values?\"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get one example question for the dataset for testing\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "examples = list(client.list_examples(dataset_name=\"basecamp\"))\n",
    "\n",
    "q = examples[0].inputs\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61a273b8-921a-4d4e-a255-2a5182f6899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63fa1de7-6df5-4782-9c49-4c0f5a66a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets from the docs\n",
    "vectorstore_retriever = vectorstore.as_retriever()\n",
    "# load a RAG prompt from Langchain HUB\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# our llm of choice\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def ragas_output_parser(docs):\n",
    "    return [doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a57d57",
   "metadata": {},
   "source": [
    "Now lets string together all the components together and make the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c423bf18-22e1-4898-aeff-c591556d71b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "generator = prompt | llm | StrOutputParser()\n",
    "\n",
    "retriever = RunnableParallel(\n",
    "    {\n",
    "        \"context\": vectorstore_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    ")\n",
    "\n",
    "filter_langsmith_dataset = RunnableLambda(\n",
    "    lambda x: x[\"question\"] if isinstance(x, dict) else x\n",
    ")\n",
    "\n",
    "rag_chain = RunnableParallel(\n",
    "    {\n",
    "        \"question\": filter_langsmith_dataset,\n",
    "        \"answer\": filter_langsmith_dataset | retriever | generator,\n",
    "        \"contexts\": filter_langsmith_dataset\n",
    "        | vectorstore_retriever\n",
    "        | ragas_output_parser,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c45e70b-1055-4317-bc61-80b8b8275810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Levelheadedness is important in the company's values because it promotes calm, considered, and thoughtful interactions with others. It helps prevent acting out of spite, rushing to judgment, or jumping to conclusions. This approach allows for clear communication and effective problem-solving within the company.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check with the example question to see if everything is working\n",
    "get_answer = RunnableLambda(lambda x: x[\"answer\"])\n",
    "resp = (rag_chain | get_answer).invoke(q)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f43bea6",
   "metadata": {},
   "source": [
    "Voilà! We have our RAG working with Langchain. Go on and try a few questions yourself from the `examples` we generated.\n",
    "\n",
    "Now, let's build the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d448d4a9-e6da-4dba-a8f4-9ae031a13167",
   "metadata": {},
   "source": [
    "### Just the LLM\n",
    "\n",
    "Setting this up is much easier as you could imagine, all you need are the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87fb71c0-622b-4cbc-99ae-c81c28c0cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "llm_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "just_llm = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | llm_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | RunnableParallel(\n",
    "        {\n",
    "            \"answer\": RunnablePassthrough(),\n",
    "            \"contexts\": RunnableLambda(lambda _: [\"\"]),\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e709c908-9e2d-4f2e-a664-b4df12215ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Levelheadedness is important in the company's values because it promotes rational decision-making, reduces conflict, and fosters a positive work environment. It helps employees stay calm under pressure and think critically about challenges they may face. Thanks for asking!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = (just_llm | get_answer).invoke(q)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d694cd",
   "metadata": {},
   "source": [
    "Try out a few `examples` from this chain also, see if you can spot any differences in performance by eyeballing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92529864-e251-4b93-b968-b885c3d2f058",
   "metadata": {},
   "source": [
    "## 5. Choose the right metrics to evaluate the experiment\n",
    "\n",
    "Ragas provides you with a different metrics that you can use to measure the different components of your RAG pipeline. You can see the entire list in the [docs](https://docs.ragas.io/en/latest/concepts/metrics/index.html).\n",
    "\n",
    "For this experiment we are going to choose [Answer Correctness](https://docs.ragas.io/en/latest/concepts/metrics/answer_correctness.html). `Answer Correctness` is an end-to-end metric that measures the accuracy of the generated answer when compared to the ground truth. This evaluation relies on the ground truth and the answer, with scores ranging from 0 to 1. A higher score indicates a closer alignment between the generated answer and the ground truth, signifying better correctness. Do check out the docs to learn more about how it works internally.\n",
    "\n",
    "To make evaluation of Langchain chains on Langsmith easier, Ragas provides you with 2 utils \n",
    "1. `EvaluatorChain`: which is a langchain chain that take a Ragas metric and creates a `Chain` which outputs the score.\n",
    "2. `evaluate()`: this is a util function for Langsmith that takes a dataset_name, chain and metrics to run the evaluations.\n",
    "\n",
    "Lets take a look at both of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391a54c",
   "metadata": {},
   "source": [
    "### `EvaluatorChain`\n",
    "\n",
    "Lets create one for `Answer Correctness` and evaluate both of the baselines we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cca237aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.integrations.langchain import EvaluatorChain\n",
    "\n",
    "# the metric we will be using\n",
    "from ragas.metrics import answer_correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90073a2f",
   "metadata": {},
   "source": [
    "### `evaluate()` Langsmith Dataset\n",
    "\n",
    "this utility function take the Langsmith dataset_name, RAG chain, the Ragas metrics you choose and runs the evaluations for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d43007aa-3e80-4039-b4e5-cff030bb89c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.integrations.langsmith import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e785b4a",
   "metadata": {},
   "source": [
    "Lets evaluate the `rag_chain` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8399a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'rag_chain_1' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/fe31443e-60c7-462f-87e7-35a85894055f/compare?selectedSessions=92cc8bb4-5b83-4033-ba6c-f5ddbfedab93\n",
      "\n",
      "View all tests for Dataset basecamp at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/fe31443e-60c7-462f-87e7-35a85894055f\n",
      "[>                                                 ] 0/21"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wfh/code/lc/community/ragas-notes/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 21/21"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.answer_correctness</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4ee9464b-8da9-40a1-ae64-528c2d7f04f2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.528093</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.032485</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.177913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.014589</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.181416</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.601767</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.405035</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.511995</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.562180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.725981</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.667870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.092583</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.779500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.275707</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.answer_correctness error  execution_time  \\\n",
       "count                     21.000000     0       21.000000   \n",
       "unique                          NaN     0             NaN   \n",
       "top                             NaN   NaN             NaN   \n",
       "freq                            NaN   NaN             NaN   \n",
       "mean                       0.528093   NaN        2.032485   \n",
       "std                        0.177913   NaN        1.014589   \n",
       "min                        0.181416   NaN        0.601767   \n",
       "25%                        0.405035   NaN        1.511995   \n",
       "50%                        0.562180   NaN        1.725981   \n",
       "75%                        0.667870   NaN        2.092583   \n",
       "max                        0.779500   NaN        5.275707   \n",
       "\n",
       "                                      run_id  \n",
       "count                                     21  \n",
       "unique                                    21  \n",
       "top     4ee9464b-8da9-40a1-ae64-528c2d7f04f2  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"basecamp\"\n",
    "# evaluate just llms\n",
    "run = evaluate(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=rag_chain,\n",
    "    experiment_name=\"rag_chain_1\",\n",
    "    metrics=[answer_correctness],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f9ce44",
   "metadata": {},
   "source": [
    "Now lets evaluate the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6620c4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'just_llm_1' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/fe31443e-60c7-462f-87e7-35a85894055f/compare?selectedSessions=5e1f0015-3911-403f-9ef2-f76ed83ff3e6\n",
      "\n",
      "View all tests for Dataset basecamp at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/fe31443e-60c7-462f-87e7-35a85894055f\n",
      "[----------------------------------------------->  ] 20/21"
     ]
    }
   ],
   "source": [
    "# evaluate rag_chain\n",
    "run = evaluate(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=just_llm,\n",
    "    experiment_name=\"just_llm_1\",\n",
    "    metrics=[answer_correctness],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7327e4-5f05-4b25-9a42-134070b87cce",
   "metadata": {},
   "source": [
    "Now you can check you langsmith dataset dashboard to view and analyise the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
