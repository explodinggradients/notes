{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe90efd1",
   "metadata": {},
   "source": [
    "# Build a Baseline with Langchain\n",
    "\n",
    "Having prepared a test dataset, we are now equipped to conduct experiments and iteratively enhance our RAG pipelines.\n",
    "\n",
    "## Introducing Evaluation Driven Development (EDD)\n",
    "\n",
    "A key challenge is developing a systematic approach to measure and refine our RAG pipeline. To address this, we propose `Evaluation Driven Development` (EDD), inspired by the popular Test Driven Development methodology. EDD advocates for employing various metrics to assess different facets of the LLM application and conducting targeted experiments for improvement based on specific use cases.\n",
    "\n",
    "Evaluation Driven Development (EDD) offers a structured framework to tackle the complexities involved in optimizing RAG applications. Below is a mind map created by an engineer at AWS, illustrating potential strategies for enhancing a RAG pipeline.\n",
    "\n",
    "![](./images/RAG_mindmap.jpeg)\n",
    "\n",
    "The [original Miro mind map](https://miro.com/app/board/uXjVNvklNmc=/) is accessible for further exploration. EDD serves as a guiding light through the complexity of enhancing RAG applications.\n",
    "\n",
    "Let's proceed to see EDD in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9d733a0-398f-43fc-a217-7146a082d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U \"unstructured[md]\" chromadb langchain_openai langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb3ce85-ceb5-4b01-9205-b6c079469818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28504e01",
   "metadata": {},
   "source": [
    "## Building a baseline\n",
    "\n",
    "if you remember, in the [last notebook](./dataset.ipynb) we outline the steps and addressed a few\n",
    "\n",
    "1. Load the data as documents. ✅\n",
    "2. Generate the test set from these documents. ✅\n",
    "3. Upload and verify the test set with Langsmith. ✅\n",
    "4. Formulate experiments to improve you RAG pipeline. ⏳\n",
    "5. Choose the right metrics to evaluate the experiment ⏳\n",
    "6. Analyze the results using the Langsmith dashboard. ⏳\n",
    "\n",
    "So lets continue where we left off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ac315",
   "metadata": {},
   "source": [
    "## 4. Formulate experiments to improve your RAG pipeline\n",
    "\n",
    "Now for the baseline first thing we want to know is how effective is vanila GPT-3.5 compared to RAG based model. RAG should be superiour because there is specific information about companies but what exactly is the difference?\n",
    "\n",
    "> is RAG better than just using an LLM for our case?\n",
    "\n",
    "In order to compare that need to create 2 chains \n",
    "1. Just LLM - gpt-3.5\n",
    "2. LLM + Retriver "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18681df",
   "metadata": {},
   "source": [
    "### Building the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfc41ad4-0293-4713-b247-9666cd37593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d317b510",
   "metadata": {},
   "source": [
    "To build the RAG lets load the data, chunk it and add it to a vector store for retrieval. If you want more info on how to build RAG systems with langchain, check the [docs](https://python.langchain.com/docs/modules/data_connection/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "000582e7-5b78-4d87-9826-a589ac6f6dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the documents\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\"./data/\")\n",
    "documents = loader.load()\n",
    "\n",
    "# add filename as metadata\n",
    "for document in documents:\n",
    "    document.metadata[\"file_name\"] = document.metadata[\"source\"]\n",
    "\n",
    "# how many docs do we have?\n",
    "docs = documents\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aa6746f-7740-4d26-a00b-219c122c51e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# create the vector store\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "548d590f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What does the 37signals Employee Handbook provide for new hires?'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get one example question for the dataset for testing\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "examples = list(client.list_examples(dataset_name=\"basecamp\"))\n",
    "\n",
    "q = examples[0].inputs\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61a273b8-921a-4d4e-a255-2a5182f6899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63fa1de7-6df5-4782-9c49-4c0f5a66a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets from the docs\n",
    "vectorstore_retriever = vectorstore.as_retriever()\n",
    "# load a RAG prompt from Langchain HUB\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# our llm of choice\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "def ragas_output_parser(docs):\n",
    "    return [doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a57d57",
   "metadata": {},
   "source": [
    "Now lets string together all the components together and make the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c423bf18-22e1-4898-aeff-c591556d71b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "generator = prompt | llm | StrOutputParser()\n",
    "\n",
    "retriever = RunnableParallel(\n",
    "    {\n",
    "        \"context\": vectorstore_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    ")\n",
    "\n",
    "filter_langsmith_dataset = RunnableLambda(\n",
    "    lambda x: x[\"question\"] if isinstance(x, dict) else x\n",
    ")\n",
    "\n",
    "rag_chain = RunnableParallel(\n",
    "    {\n",
    "        \"question\": filter_langsmith_dataset,\n",
    "        \"answer\": filter_langsmith_dataset | retriever | generator,\n",
    "        \"contexts\": filter_langsmith_dataset\n",
    "        | vectorstore_retriever\n",
    "        | ragas_output_parser,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c45e70b-1055-4317-bc61-80b8b8275810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The 37signals Employee Handbook provides guidance and clarity for new hires, helping them navigate the company's unique practices. It ensures that new employees feel supported and informed during their onboarding process. Prior to the handbook, new hires felt lost and isolated, making their first weeks or months stressful.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check with the example question to see if everything is working\n",
    "get_answer = RunnableLambda(lambda x: x[\"answer\"])\n",
    "resp = (rag_chain | get_answer).invoke(q)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f43bea6",
   "metadata": {},
   "source": [
    "Voilà! We have our RAG working with Langchain. Go on and try a few questions yourself from the `examples` we generated.\n",
    "\n",
    "Now, let's build the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d448d4a9-e6da-4dba-a8f4-9ae031a13167",
   "metadata": {},
   "source": [
    "### Just the LLM\n",
    "\n",
    "Setting this up is much easier as you could imagine, all you need are the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87fb71c0-622b-4cbc-99ae-c81c28c0cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "llm_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "just_llm = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | llm_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | RunnableParallel(\n",
    "        {\n",
    "            \"answer\": RunnablePassthrough(),\n",
    "            \"contexts\": RunnableLambda(lambda _: [\"\"]),\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e709c908-9e2d-4f2e-a664-b4df12215ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The 37signals Employee Handbook provides information on the company's values, culture, and expectations for new hires. It also outlines policies and procedures to help employees navigate their roles within the organization. Thanks for asking!\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = (just_llm | get_answer).invoke(q)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d694cd",
   "metadata": {},
   "source": [
    "Try out a few `examples` from this chain also, see if you can spot any differences in performance by eyeballing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92529864-e251-4b93-b968-b885c3d2f058",
   "metadata": {},
   "source": [
    "## 5. Choose the right metrics to evaluate the experiment\n",
    "\n",
    "Ragas provides you with a different metrics that you can use to measure the different components of your RAG pipeline. You can see the entire list in the [docs](https://docs.ragas.io/en/latest/concepts/metrics/index.html).\n",
    "\n",
    "For this experiment we are going to choose [Answer Correctness](https://docs.ragas.io/en/latest/concepts/metrics/answer_correctness.html). `Answer Correctness` is an end-to-end metric that measures the accuracy of the generated answer when compared to the ground truth. This evaluation relies on the ground truth and the answer, with scores ranging from 0 to 1. A higher score indicates a closer alignment between the generated answer and the ground truth, signifying better correctness. Do check out the docs to learn more about how it works internally.\n",
    "\n",
    "To make evaluation of Langchain chains on Langsmith easier, Ragas provides you with 2 utils \n",
    "1. `EvaluatorChain`: which is a langchain chain that take a Ragas metric and creates a `Chain` which outputs the score.\n",
    "2. `evaluate()`: this is a util function for Langsmith that takes a dataset_name, chain and metrics to run the evaluations.\n",
    "\n",
    "Lets take a look at both of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391a54c",
   "metadata": {},
   "source": [
    "### `EvaluatorChain`\n",
    "\n",
    "Lets create one for `Answer Correctness` and evaluate both of the baselines we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cca237aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.integrations.langchain import EvaluatorChain\n",
    "\n",
    "# the metric we will be using\n",
    "from ragas.metrics import answer_correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90073a2f",
   "metadata": {},
   "source": [
    "### `evaluate()` Langsmith Dataset\n",
    "\n",
    "this utility function take the Langsmith dataset_name, RAG chain, the Ragas metrics you choose and runs the evaluations for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d43007aa-3e80-4039-b4e5-cff030bb89c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.integrations.langsmith import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e785b4a",
   "metadata": {},
   "source": [
    "Lets evaluate the `rag_chain` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8399a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'stupendous-mark-31' at:\n",
      "https://smith.langchain.com/o/9bfbddc5-b88e-41e5-92df-2a62f0c64b4b/datasets/e9dc7bc8-9d47-4efd-8f4c-678a18a7aef5/compare?selectedSessions=69365437-afc2-4d0c-86ac-42f7d24654b7\n",
      "\n",
      "View all tests for Dataset basecamp at:\n",
      "https://smith.langchain.com/o/9bfbddc5-b88e-41e5-92df-2a62f0c64b4b/datasets/e9dc7bc8-9d47-4efd-8f4c-678a18a7aef5\n",
      "[>                                                 ] 0/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjmachan/.pyenv/versions/3.10.12/envs/notes/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 50/50"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.answer_correctness</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a46fc47d-fc6d-4278-ac58-a65746b66eb0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.521026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.498635</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.161262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.822136</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.180171</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.508961</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.446578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.902223</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.535243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.215940</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.616183</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.777203</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.845058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.818254</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.answer_correctness error  execution_time  \\\n",
       "count                     50.000000     0       50.000000   \n",
       "unique                          NaN     0             NaN   \n",
       "top                             NaN   NaN             NaN   \n",
       "freq                            NaN   NaN             NaN   \n",
       "mean                       0.521026   NaN        2.498635   \n",
       "std                        0.161262   NaN        0.822136   \n",
       "min                        0.180171   NaN        1.508961   \n",
       "25%                        0.446578   NaN        1.902223   \n",
       "50%                        0.535243   NaN        2.215940   \n",
       "75%                        0.616183   NaN        2.777203   \n",
       "max                        0.845058   NaN        4.818254   \n",
       "\n",
       "                                      run_id  \n",
       "count                                     50  \n",
       "unique                                    50  \n",
       "top     a46fc47d-fc6d-4278-ac58-a65746b66eb0  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"basecamp\"\n",
    "# evaluate just llms\n",
    "run = evaluate(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=rag_chain,\n",
    "    experiment_name=\"rag_chain_1\",\n",
    "    metrics=[answer_correctness],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f9ce44",
   "metadata": {},
   "source": [
    "Now lets evaluate the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6620c4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'worthwhile-connection-73' at:\n",
      "https://smith.langchain.com/o/9bfbddc5-b88e-41e5-92df-2a62f0c64b4b/datasets/e9dc7bc8-9d47-4efd-8f4c-678a18a7aef5/compare?selectedSessions=5eb768e6-7f39-4d15-8413-89cb525e868f\n",
      "\n",
      "View all tests for Dataset basecamp at:\n",
      "https://smith.langchain.com/o/9bfbddc5-b88e-41e5-92df-2a62f0c64b4b/datasets/e9dc7bc8-9d47-4efd-8f4c-678a18a7aef5\n",
      "[------------------------------------------------->] 50/50"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.answer_correctness</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0d67a0b4-cddc-4e1f-81ec-72fa2dd204d8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.477671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.784664</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.192345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.272940</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.177440</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.813902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.258132</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.245208</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.517662</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.503279</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.606184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.830282</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.962720</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.698655</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.answer_correctness error  execution_time  \\\n",
       "count                     50.000000     0       50.000000   \n",
       "unique                          NaN     0             NaN   \n",
       "top                             NaN   NaN             NaN   \n",
       "freq                            NaN   NaN             NaN   \n",
       "mean                       0.477671   NaN        1.784664   \n",
       "std                        0.192345   NaN        1.272940   \n",
       "min                        0.177440   NaN        0.813902   \n",
       "25%                        0.258132   NaN        1.245208   \n",
       "50%                        0.517662   NaN        1.503279   \n",
       "75%                        0.606184   NaN        1.830282   \n",
       "max                        0.962720   NaN        9.698655   \n",
       "\n",
       "                                      run_id  \n",
       "count                                     50  \n",
       "unique                                    50  \n",
       "top     0d67a0b4-cddc-4e1f-81ec-72fa2dd204d8  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate rag_chain\n",
    "run = evaluate(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=just_llm,\n",
    "    experiment_name=\"just_llm_1\",\n",
    "    metrics=[answer_correctness],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7327e4-5f05-4b25-9a42-134070b87cce",
   "metadata": {},
   "source": [
    "Now you can check you langsmith dataset dashboard to view and analyise the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c01be3",
   "metadata": {},
   "source": [
    "## 6. Analyze the results using the Langsmith dashboard\n",
    "\n",
    "The cool thing about Langsmith is that it provides a good UI to visualize the results and dig deeper into them if needed. In this section, we will do exactly that.\n",
    "\n",
    "If you open up the Datasets & Testing tab and choose the dataset you uploaded, you will be able to see the different runs.\n",
    "\n",
    "![](./images/test_dashboard_baseline.png)\n",
    "\n",
    "Here you can see the 2 experiments we ran: \"just_llm\" and \"rag_chain\". You can select the runs you want to compare against to dive deeper. You can also choose different measurements Langsmith provides like P50 Latency, P99 Latency, Cost, Error Rate, and the metrics Ragas provides, in our case, `Answer Correctness`. We can see a high-level overview of the score here.\n",
    "\n",
    "![](./images/test_dashboard_baseline_compare.png)\n",
    "\n",
    "As you can see, RAG is in fact better than Just_LLM, which makes sense because the contexts we provide have more information than what the model alone might know. But let's keep digging deeper. I want to see the different rows and the individual scores for each. Luckily, Langsmith makes reviewing this super simple.\n",
    "\n",
    "![](./images/baseline_compare_runs.png)\n",
    "\n",
    "This makes it easier to review each of the rows and compare outputs side by side and see the `Answer Correctness` score for each row. As you manually go through the results, you might notice patterns in the different runs, and you can click through and see more. You can also open the corresponding Langchain runs to debug even further.\n",
    "\n",
    "![](./images/baseline_compare_row.png)\n",
    "\n",
    "Last but not least, because Ragas metrics runs are also logged, you can click through the score and see the trace for `Answer Correctness`. This helps make Ragas scores explainable, and you can understand why you got the scores you got.\n",
    "\n",
    "![](./images/baseline_ragas_run_button.png)\n",
    "\n",
    "and the full `Answer Correctness` run\n",
    "\n",
    "![](./images/baseline_ragas_run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9204c11",
   "metadata": {},
   "source": [
    "And that is it, in this notebook we\n",
    "1. formulated an experiment to compare the performance between just using an LLM and using a RAG pipeline\n",
    "2. Chose the appropriate metric for the experiment - `Answer Correctness`\n",
    "3. Ran the experiments\n",
    "4. Analyzed the results and found that RAGs do perform better\n",
    "\n",
    "These steps are the core of Evaluation Driven Development, and in the next notebook, we will be using the same workflow to optimize our retriever."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
