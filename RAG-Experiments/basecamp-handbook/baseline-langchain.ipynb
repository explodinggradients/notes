{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe90efd1",
   "metadata": {},
   "source": [
    "# Build a Baseline with Langchain\n",
    "\n",
    "Having prepared a test dataset, we are now equipped to conduct experiments and iteratively enhance our RAG pipelines.\n",
    "\n",
    "## Introducing Metrics Driven Development (MDD)\n",
    "\n",
    "A key challenge is developing a systematic approach to measure and refine our RAG pipeline. To address this, we propose `Metrics Driven Development` (MDD), inspired by the popular Test Driven Development methodology. MDD advocates for employing various metrics to assess different facets of the LLM application and conducting targeted experiments for improvement based on specific use cases.\n",
    "\n",
    "Metrics Driven Development (MDD) offers a structured framework to tackle the complexities involved in optimizing RAG applications. Below is a mind map created by an engineer at AWS, illustrating potential strategies for enhancing a RAG pipeline.\n",
    "\n",
    "![](https://media.licdn.com/dms/image/D4D22AQEgjWxKXokOPA/feedshare-shrink_800/0/1708498751086?e=1711584000&v=beta&t=xaT95vKS8m4qTybofpKqQfXOGoFs8lQXBuOk2Fr45AE)\n",
    "\n",
    "The [original Miro mind map](https://miro.com/app/board/uXjVNvklNmc=/) is accessible for further exploration. MDD serves as a guiding light through the complexity of enhancing RAG applications.\n",
    "\n",
    "Let's proceed to see MDD in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d414d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for reloading code that changes locally\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28504e01",
   "metadata": {},
   "source": [
    "## Building a baseline\n",
    "\n",
    "if you remember, in the [last notebook](./dataset.ipynb) we outline the steps and addressed a few\n",
    "\n",
    "1. Load the data as documents. ✅\n",
    "2. Generate the test set from these documents. ✅\n",
    "3. Upload and verify the test set with Langsmith. ✅\n",
    "4. Formulate experiments to improve you RAG pipeline. ⏳\n",
    "5. Choose the right metrics to evaluate the experiment ⏳\n",
    "6. Analyze the results using the Langsmith dashboard. ⏳\n",
    "\n",
    "So lets continue where we left off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ac315",
   "metadata": {},
   "source": [
    "## 4. Formulate experiments to improve your RAG pipeline\n",
    "\n",
    "Now for the baseline first thing we want to know is how effective is vanila GPT-3.5 compared to RAG based model. RAG should be superiour because there is specific information about companies but what exactly is the difference?\n",
    "\n",
    "> is RAG better than just using an LLM for our case?\n",
    "\n",
    "In order to compare that need to create 2 chains \n",
    "1. Just LLM - gpt-3.5\n",
    "2. LLM + Retriver "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18681df",
   "metadata": {},
   "source": [
    "### Building the RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfc41ad4-0293-4713-b247-9666cd37593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d317b510",
   "metadata": {},
   "source": [
    "To build the RAG lets load the data, chunk it and add it to a vector store for retrieval. If you want more info on how to build RAG systems with langchain, check the [docs](https://python.langchain.com/docs/modules/data_connection/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "000582e7-5b78-4d87-9826-a589ac6f6dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the documents\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader(\"./data/\")\n",
    "documents = loader.load()\n",
    "\n",
    "# add filename as metadata\n",
    "for document in documents:\n",
    "    document.metadata['file_name'] = document.metadata['source']\n",
    "\n",
    "# how many docs do we have?\n",
    "docs = documents\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aa6746f-7740-4d26-a00b-219c122c51e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# create the vector store\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "548d590f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What does the 37signals Employee Handbook provide for new hires?'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get one example question for the dataset for testing\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "examples = list(client.list_examples(dataset_name=\"basecamp\"))\n",
    "\n",
    "q = examples[0].inputs\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61a273b8-921a-4d4e-a255-2a5182f6899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63fa1de7-6df5-4782-9c49-4c0f5a66a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets from the docs\n",
    "vectorstore_retriever = vectorstore.as_retriever()\n",
    "# load a RAG prompt from Langchain HUB\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# our llm of choice\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "def ragas_output_parser(docs):\n",
    "    return [doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a57d57",
   "metadata": {},
   "source": [
    "Now lets string together all the components together and make the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c423bf18-22e1-4898-aeff-c591556d71b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "generator = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "retriever = RunnableParallel({\n",
    "    \"context\": vectorstore_retriever | format_docs, \n",
    "    \"question\": RunnablePassthrough(),\n",
    "})\n",
    "\n",
    "filter_langsmith_dataset = RunnableLambda(lambda x: x[\"question\"] if isinstance(x, dict) else x)\n",
    "\n",
    "rag_chain = RunnableParallel({\n",
    "    \"question\": filter_langsmith_dataset,\n",
    "    \"answer\": filter_langsmith_dataset | retriever | generator,\n",
    "    \"contexts\": filter_langsmith_dataset | vectorstore_retriever | ragas_output_parser,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c45e70b-1055-4317-bc61-80b8b8275810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 37signals Employee Handbook provides new hires with information on life insurance, retirement plans, company culture, and benefits. It also includes sections on what the company stands for, how they work, and employee perks. New employees are encouraged to question things and take advantage of their new perspective.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check with the example question to see if everything is working\n",
    "get_answer = RunnableLambda(lambda x: x[\"answer\"])\n",
    "resp = (rag_chain | get_answer).invoke(q)\n",
    "resp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f43bea6",
   "metadata": {},
   "source": [
    "Voilà! We have our RAG working with Langchain. Go on and try a few questions yourself from the `examples` we generated.\n",
    "\n",
    "Now, let's build the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d448d4a9-e6da-4dba-a8f4-9ae031a13167",
   "metadata": {},
   "source": [
    "### Just the LLM\n",
    "\n",
    "Setting this up is much easier as you could imagine, all you need are the prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87fb71c0-622b-4cbc-99ae-c81c28c0cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "llm_prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "just_llm = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | llm_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | RunnableParallel({\n",
    "        \"answer\": RunnablePassthrough(),\n",
    "        \"contexts\": RunnableLambda(lambda _: [\"\"]),\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e709c908-9e2d-4f2e-a664-b4df12215ac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The 37signals Employee Handbook provides information on the company's values, culture, and expectations for new hires. It also outlines policies and procedures to help employees navigate their roles within the organization. Thanks for asking!\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = (just_llm | get_answer).invoke(q)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d694cd",
   "metadata": {},
   "source": [
    "Try out a few `examples` from this chain also, see if you can spot any differences in performance by eyeballing the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92529864-e251-4b93-b968-b885c3d2f058",
   "metadata": {},
   "source": [
    "## 5. Choose the right metrics to evaluate the experiment\n",
    "\n",
    "Ragas provides you with a different metrics that you can use to measure the different components of your RAG pipeline. You can see the entire list in the [docs](https://docs.ragas.io/en/latest/concepts/metrics/index.html).\n",
    "\n",
    "For this experiment we are going to choose [Answer Correctness](https://docs.ragas.io/en/latest/concepts/metrics/answer_correctness.html). `Answer Correctness` is an end-to-end metric that measures the accuracy of the generated answer when compared to the ground truth. This evaluation relies on the ground truth and the answer, with scores ranging from 0 to 1. A higher score indicates a closer alignment between the generated answer and the ground truth, signifying better correctness. Do check out the docs to learn more about how it works internally.\n",
    "\n",
    "To make evaluation of Langchain chains on Langsmith easier, Ragas provides you with 2 utils \n",
    "1. `EvaluatorChain`: which is a langchain chain that take a Ragas metric and creates a `Chain` which outputs the score.\n",
    "2. `evaluate()`: this is a util function for Langsmith that takes a dataset_name, chain and metrics to run the evaluations.\n",
    "\n",
    "Lets take a look at both of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391a54c",
   "metadata": {},
   "source": [
    "### `EvaluatorChain`\n",
    "\n",
    "Lets create one for `Answer Correctness` and evaluate both of the baselines we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca237aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.integrations.langchain import EvaluatorChain\n",
    "\n",
    "# the metric we will be using\n",
    "from ragas.metrics import answer_correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90073a2f",
   "metadata": {},
   "source": [
    "### `evaluate()` Langsmith Dataset\n",
    "\n",
    "this utility function take the Langsmith dataset_name, RAG chain, the Ragas metrics you choose and runs the evaluations for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d43007aa-3e80-4039-b4e5-cff030bb89c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.integrations.langsmith import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e785b4a",
   "metadata": {},
   "source": [
    "Lets evaluate the `rag_chain` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8399a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'rag_chain_1' at:\n",
      "https://smith.langchain.com/o/9bfbddc5-b88e-41e5-92df-2a62f0c64b4b/datasets/e9dc7bc8-9d47-4efd-8f4c-678a18a7aef5/compare?selectedSessions=e1c7b451-6331-476d-b360-7f488326df50\n",
      "\n",
      "View all tests for Dataset basecamp at:\n",
      "https://smith.langchain.com/o/9bfbddc5-b88e-41e5-92df-2a62f0c64b4b/datasets/e9dc7bc8-9d47-4efd-8f4c-678a18a7aef5\n",
      "[>                                                 ] 0/50"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjmachan/.pyenv/versions/3.10.12/envs/notes/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------->] 50/50"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.answer_correctness</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5b518dc8-243a-4098-a6d1-7aecf35ac98a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.533543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.664292</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.135102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.371598</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.180216</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.051612</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.485660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.341244</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.535136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.652268</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.616268</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.965620</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.787733</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.536122</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.answer_correctness error  execution_time  \\\n",
       "count                     50.000000     0       50.000000   \n",
       "unique                          NaN     0             NaN   \n",
       "top                             NaN   NaN             NaN   \n",
       "freq                            NaN   NaN             NaN   \n",
       "mean                       0.533543   NaN        1.664292   \n",
       "std                        0.135102   NaN        0.371598   \n",
       "min                        0.180216   NaN        1.051612   \n",
       "25%                        0.485660   NaN        1.341244   \n",
       "50%                        0.535136   NaN        1.652268   \n",
       "75%                        0.616268   NaN        1.965620   \n",
       "max                        0.787733   NaN        2.536122   \n",
       "\n",
       "                                      run_id  \n",
       "count                                     50  \n",
       "unique                                    50  \n",
       "top     5b518dc8-243a-4098-a6d1-7aecf35ac98a  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_name = \"basecamp\"\n",
    "# evaluate just llms\n",
    "run = evaluate(\n",
    "    dataset_name=dataset_name, \n",
    "    llm_or_chain_factory=rag_chain, \n",
    "    experiment_name=\"rag_chain_1\",\n",
    "    metrics=[answer_correctness],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f9ce44",
   "metadata": {},
   "source": [
    "Now lets evaluate the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6620c4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'just_llm_1' at:\n",
      "https://smith.langchain.com/o/9bfbddc5-b88e-41e5-92df-2a62f0c64b4b/datasets/e9dc7bc8-9d47-4efd-8f4c-678a18a7aef5/compare?selectedSessions=91e9d8cd-313b-4a29-892a-0d5f8c6a28e8\n",
      "\n",
      "View all tests for Dataset basecamp at:\n",
      "https://smith.langchain.com/o/9bfbddc5-b88e-41e5-92df-2a62f0c64b4b/datasets/e9dc7bc8-9d47-4efd-8f4c-678a18a7aef5\n",
      "[------------------------------------------------->] 50/50"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h3>Experiment Results:</h3>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback.answer_correctness</th>\n",
       "      <th>error</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e36cae5a-d19a-4917-9bb2-1bce78167127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.465834</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.936587</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.177573</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.206522</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.179124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.412590</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.270415</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.803660</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.500165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.925462</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.597824</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.060094</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.775874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.490873</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feedback.answer_correctness error  execution_time  \\\n",
       "count                     50.000000     0       50.000000   \n",
       "unique                          NaN     0             NaN   \n",
       "top                             NaN   NaN             NaN   \n",
       "freq                            NaN   NaN             NaN   \n",
       "mean                       0.465834   NaN        0.936587   \n",
       "std                        0.177573   NaN        0.206522   \n",
       "min                        0.179124   NaN        0.412590   \n",
       "25%                        0.270415   NaN        0.803660   \n",
       "50%                        0.500165   NaN        0.925462   \n",
       "75%                        0.597824   NaN        1.060094   \n",
       "max                        0.775874   NaN        1.490873   \n",
       "\n",
       "                                      run_id  \n",
       "count                                     50  \n",
       "unique                                    50  \n",
       "top     e36cae5a-d19a-4917-9bb2-1bce78167127  \n",
       "freq                                       1  \n",
       "mean                                     NaN  \n",
       "std                                      NaN  \n",
       "min                                      NaN  \n",
       "25%                                      NaN  \n",
       "50%                                      NaN  \n",
       "75%                                      NaN  \n",
       "max                                      NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluate rag_chain\n",
    "run = evaluate(\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=just_llm, \n",
    "    experiment_name=\"just_llm_1\",\n",
    "    metrics=[answer_correctness], \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7327e4-5f05-4b25-9a42-134070b87cce",
   "metadata": {},
   "source": [
    "Now you can check you langsmith dataset dashboard to view and analyise the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
