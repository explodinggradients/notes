{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2117a74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset qasper (/home/jjmachan/.cache/huggingface/datasets/allenai___qasper/qasper/0.3.0/2bfcd239e581ab83f9ab7b76a82e42c6bcf574a13246ae6cc5a6c357c35f96f9)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9edee11fece4b778e17289fdf843025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'abstract', 'full_text', 'qas', 'figures_and_tables'],\n",
       "        num_rows: 888\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'abstract', 'full_text', 'qas', 'figures_and_tables'],\n",
       "        num_rows: 281\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'abstract', 'full_text', 'qas', 'figures_and_tables'],\n",
       "        num_rows: 416\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "qasper_dataset = load_dataset(\"allenai/qasper\")\n",
    "qasper_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec8891",
   "metadata": {},
   "source": [
    "get the questions and answer as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "587ffae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'abstract', 'full_text', 'qas', 'figures_and_tables'],\n",
       "    num_rows: 281\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds = qasper_dataset[\"validation\"]\n",
    "val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d41c0594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['which multilingual approaches do they compare with?',\n",
       " 'what are the pivot-based baselines?',\n",
       " 'which datasets did they experiment with?',\n",
       " 'what language pairs are explored?']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds[0]['qas'][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "952413b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': [{'unanswerable': False,\n",
       "    'extractive_spans': ['BIBREF19', 'BIBREF20'],\n",
       "    'yes_no': None,\n",
       "    'free_form_answer': '',\n",
       "    'evidence': ['Table TABREF19 and TABREF26 report zero-shot results on Europarl and Multi-UN evaluation sets, respectively. We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting. Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin.'],\n",
       "    'highlighted_evidence': ['We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. ',\n",
       "     'The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23.']},\n",
       "   {'unanswerable': False,\n",
       "    'extractive_spans': ['multilingual NMT (MNMT) BIBREF19'],\n",
       "    'yes_no': None,\n",
       "    'free_form_answer': '',\n",
       "    'evidence': ['Table TABREF19 and TABREF26 report zero-shot results on Europarl and Multi-UN evaluation sets, respectively. We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting. Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin.'],\n",
       "    'highlighted_evidence': ['We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16.']}],\n",
       "  'annotation_id': ['819898a2daf67225307aaf59d8048987c06b0c03',\n",
       "   'bbc549d0d6a598c0a93d0dde01d8cc6ffe316adc'],\n",
       "  'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58',\n",
       "   '258ee4069f740c400c0049a2580945a1cc7f044c']},\n",
       " {'answer': [{'unanswerable': False,\n",
       "    'extractive_spans': ['pivoting', 'pivoting$_{\\\\rm m}$'],\n",
       "    'yes_no': None,\n",
       "    'free_form_answer': '',\n",
       "    'evidence': ['Table TABREF19 and TABREF26 report zero-shot results on Europarl and Multi-UN evaluation sets, respectively. We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16. The results show that our approaches consistently outperform other approaches across languages and datasets, especially surpass pivoting, which is a strong baseline in the zero-shot scenario that multilingual NMT systems often fail to beat BIBREF19, BIBREF20, BIBREF23. Pivoting translates source to pivot then to target in two steps, causing inefficient translation process. Our approaches use one encoder-decoder model to translate between any zero-shot directions, which is more efficient than pivoting. Regarding the comparison between transfer approaches, our cross-lingual pretraining based transfer outperforms transfer method that does not use pretraining by a large margin.',\n",
       "     'Although it is challenging for one model to translate all zero-shot directions between multiple distant language pairs of MultiUN, MLM+BRLM-SA still achieves better performances on Es $\\\\rightarrow $ Ar and Es $\\\\rightarrow $ Ru than strong pivoting$_{\\\\rm m}$, which uses MNMT to translate source to pivot then to target in two separate steps with each step receiving supervised signal of parallel corpora. Our approaches surpass pivoting$_{\\\\rm m}$ in all zero-shot directions by adding back translation BIBREF33 to generate pseudo parallel sentences for all zero-shot directions based on our pretrained models such as MLM+BRLM-SA, and further training our universal encoder-decoder model with these pseudo data. BIBREF22 gu2019improved introduces back translation into MNMT, while we adopt it in our transfer approaches. Finally, our best MLM+BRLM-SA with back translation outperforms pivoting$_{\\\\rm m}$ by 2.4 BLEU points averagely, and outperforms MNMT BIBREF22 by 4.6 BLEU points averagely. Again, in supervised translation directions, MLM+BRLM-SA with back translation also achieves better performance than the original supervised Transformer.'],\n",
       "    'highlighted_evidence': ['We compare our approaches with related approaches of pivoting, multilingual NMT (MNMT) BIBREF19, and cross-lingual transfer without pretraining BIBREF16.',\n",
       "     'Although it is challenging for one model to translate all zero-shot directions between multiple distant language pairs of MultiUN, MLM+BRLM-SA still achieves better performances on Es $\\\\rightarrow $ Ar and Es $\\\\rightarrow $ Ru than strong pivoting$_{\\\\rm m}$, which uses MNMT to translate source to pivot then to target in two separate steps with each step receiving supervised signal of parallel corpora. ']},\n",
       "   {'unanswerable': False,\n",
       "    'extractive_spans': ['firstly translates a source language into the pivot language which is later translated to the target language'],\n",
       "    'yes_no': None,\n",
       "    'free_form_answer': '',\n",
       "    'evidence': [\"We use traditional transfer learning, pivot-based method and multilingual NMT as our baselines. For the fair comparison, the Transformer-big model with 1024 embedding/hidden units, 4096 feed-forward filter size, 6 layers and 8 heads per layer is adopted for all translation models in our experiments. We set the batch size to 2400 per batch and limit sentence length to 100 BPE tokens. We set the $\\\\text{attn}\\\\_\\\\text{drop}=0$ (a dropout rate on each attention head), which is favorable to the zero-shot translation and has no effect on supervised translation directions BIBREF22. For the model initialization, we use Facebook's cross-lingual pretrained models released by XLM to initialize the encoder part, and the rest parameters are initialized with xavier uniform. We employ the Adam optimizer with $\\\\text{lr}=0.0001$, $t_{\\\\text{warm}\\\\_\\\\text{up}}=4000$ and $\\\\text{dropout}=0.1$. At decoding time, we generate greedily with length penalty $\\\\alpha =1.0$.\",\n",
       "     'Pivot-based Method is a common strategy to obtain a source$\\\\rightarrow $target model by introducing a pivot language. This approach is further divided into pivoting and pivot-synthetic. While the former firstly translates a source language into the pivot language which is later translated to the target language BIBREF4, BIBREF5, BIBREF12, the latter trains a source$\\\\rightarrow $target model with pseudo data generated from source-pivot or pivot-target parallel data BIBREF13, BIBREF14. Although the pivot-based methods can achieve not bad performance, it always falls into a computation-expensive and parameter-vast dilemma of quadratic growth in the number of source languages, and suffers from the error propagation problem BIBREF15.'],\n",
       "    'highlighted_evidence': ['We use traditional transfer learning, pivot-based method and multilingual NMT as our baselines.',\n",
       "     'Pivot-based Method is a common strategy to obtain a source$\\\\rightarrow $target model by introducing a pivot language. This approach is further divided into pivoting and pivot-synthetic. While the former firstly translates a source language into the pivot language which is later translated to the target language BIBREF4, BIBREF5, BIBREF12, the latter trains a source$\\\\rightarrow $target model with pseudo data generated from source-pivot or pivot-target parallel data BIBREF13, BIBREF14.']}],\n",
       "  'annotation_id': ['291a918eae943cf62b5d5ad4a9b6b24c4e3090f1',\n",
       "   '8ca8aaa07c7ee1d00c02322d47244d4489f151d1'],\n",
       "  'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58',\n",
       "   '258ee4069f740c400c0049a2580945a1cc7f044c']},\n",
       " {'answer': [{'unanswerable': False,\n",
       "    'extractive_spans': ['Europarl', 'MultiUN'],\n",
       "    'yes_no': None,\n",
       "    'free_form_answer': '',\n",
       "    'evidence': ['We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance. In all experiments, we use BLEU as the automatic metric for translation evaluation.'],\n",
       "    'highlighted_evidence': ['We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance.']},\n",
       "   {'unanswerable': False,\n",
       "    'extractive_spans': ['Europarl BIBREF31', 'MultiUN BIBREF32'],\n",
       "    'yes_no': None,\n",
       "    'free_form_answer': '',\n",
       "    'evidence': ['We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance. In all experiments, we use BLEU as the automatic metric for translation evaluation.'],\n",
       "    'highlighted_evidence': ['We evaluate our cross-lingual pre-training based transfer approach against several strong baselines on two public datatsets, Europarl BIBREF31 and MultiUN BIBREF32, which contain multi-parallel evaluation data to assess the zero-shot performance.']}],\n",
       "  'annotation_id': ['ce1cbd643169ab70cc3b807401798b400472868f',\n",
       "   'f5d30fb867823556668418ecbbafde77fa834f2f'],\n",
       "  'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58',\n",
       "   '258ee4069f740c400c0049a2580945a1cc7f044c']},\n",
       " {'answer': [{'unanswerable': False,\n",
       "    'extractive_spans': [],\n",
       "    'yes_no': None,\n",
       "    'free_form_answer': 'De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-En, En-Ru',\n",
       "    'evidence': ['For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation. We use 80K BPE splits as the vocabulary. Note that all sentences are tokenized by the tokenize.perl script, and we lowercase all data to avoid a large vocabulary for the MultiUN corpus.',\n",
       "     'The statistics of Europarl and MultiUN corpora are summarized in Table TABREF18. For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr$\\\\rightarrow $Es and De$\\\\rightarrow $Fr. For distant language pair Ro$\\\\rightarrow $De, we extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no official validation and test sets. For vocabulary, we use 60K sub-word tokens based on Byte Pair Encoding (BPE) BIBREF33.',\n",
       "     'FLOAT SELECTED: Table 1: Data Statistics.'],\n",
       "    'highlighted_evidence': ['For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation. ',\n",
       "     'The statistics of Europarl and MultiUN corpora are summarized in Table TABREF18. For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. ',\n",
       "     'FLOAT SELECTED: Table 1: Data Statistics.']},\n",
       "   {'unanswerable': False,\n",
       "    'extractive_spans': ['French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De)',\n",
       "     'Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation'],\n",
       "    'yes_no': None,\n",
       "    'free_form_answer': '',\n",
       "    'evidence': ['The statistics of Europarl and MultiUN corpora are summarized in Table TABREF18. For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr$\\\\rightarrow $Es and De$\\\\rightarrow $Fr. For distant language pair Ro$\\\\rightarrow $De, we extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no official validation and test sets. For vocabulary, we use 60K sub-word tokens based on Byte Pair Encoding (BPE) BIBREF33.',\n",
       "     'For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation. We use 80K BPE splits as the vocabulary. Note that all sentences are tokenized by the tokenize.perl script, and we lowercase all data to avoid a large vocabulary for the MultiUN corpus.'],\n",
       "    'highlighted_evidence': ['For Europarl corpus, we evaluate on French-English-Spanish (Fr-En-Es), German-English-French (De-En-Fr) and Romanian-English-German (Ro-En-De), where English acts as the pivot language, its left side is the source language, and its right side is the target language. We remove the multi-parallel sentences between different training corpora to ensure zero-shot settings. We use the devtest2006 as the validation set and the test2006 as the test set for Fr$\\\\rightarrow $Es and De$\\\\rightarrow $Fr. For distant language pair Ro$\\\\rightarrow $De, we extract 1,000 overlapping sentences from newstest2016 as the test set and the 2,000 overlapping sentences split from the training set as the validation set since there is no official validation and test sets.',\n",
       "     'For MultiUN corpus, we use four languages: English (En) is set as the pivot language, which has parallel data with other three languages which do not have parallel data between each other. The three languages are Arabic (Ar), Spanish (Es), and Russian (Ru), and mutual translation between themselves constitutes six zero-shot translation direction for evaluation.']}],\n",
       "  'annotation_id': ['36d46e79bc56de8706a4a7001d5e74fc25ecf15c',\n",
       "   '81063a32b1f4c7450eb6c0fbbcb649870b47e344'],\n",
       "  'worker_id': ['c1018a31c3272ce74964a3280069f62f314a1a58',\n",
       "   '258ee4069f740c400c0049a2580945a1cc7f044c']}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ds[0]['qas'][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3703aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions_free_form_answers(data):\n",
    "    # Initialize a list to store the extracted data\n",
    "    extracted_data = []\n",
    "\n",
    "    # Loop through the dataset\n",
    "    for item in data:\n",
    "        # Access the 'qas' key in the dataset\n",
    "        qas = item['qas']\n",
    "        for q in qas['question']:\n",
    "            # Initialize a dictionary to store question and its free-form answers\n",
    "            qa_dict = {'question': q, 'free_form_answers': []}\n",
    "            # Extract question ID to match with its answers\n",
    "            q_index = qas['question'].index(q)\n",
    "            # Extract answers for the current question\n",
    "            answers = qas['answers'][q_index]['answer']\n",
    "            for ans in answers:\n",
    "                # Check if the answer is not unanswerable\n",
    "                if not ans['unanswerable']:\n",
    "                    # Append free-form answer to the list in qa_dict\n",
    "                    qa_dict['free_form_answers'].append(ans['free_form_answer'])\n",
    "            # Append the dictionary to the extracted_data list\n",
    "            extracted_data.append(qa_dict)\n",
    "            \n",
    "\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ae131d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'which multilingual approaches do they compare with?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what are the pivot-based baselines?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'which datasets did they experiment with?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what language pairs are explored?',\n",
       "  'free_form_answers': ['De-En, En-Fr, Fr-En, En-Es, Ro-En, En-De, Ar-En, En-Ru',\n",
       "   '']},\n",
       " {'question': 'what ner models were evaluated?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what is the source of the news sentences?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'did they use a crowdsourcing platform for manual annotations?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what are the topics pulled from Reddit?',\n",
       "  'free_form_answers': ['',\n",
       "   'training data has posts from politics, business, science and other popular topics; the trained model is applied to millions of unannotated posts on all of Reddit']},\n",
       " {'question': 'What predictive model do they build?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What accuracy does the proposed system achieve?',\n",
       "  'free_form_answers': ['F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ',\n",
       "   'F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)']},\n",
       " {'question': 'What crowdsourcing platform is used?',\n",
       "  'free_form_answers': ['They did not use any platform, instead they hired undergraduate students to do the annotation.']},\n",
       " {'question': 'How do they match words before reordering them?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'On how many language pairs do they show that preordering assisting language sentences helps translation quality?',\n",
       "  'free_form_answers': ['5', '']},\n",
       " {'question': 'Which dataset(s) do they experiment with?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which information about text structure is included in the corpus?',\n",
       "  'free_form_answers': ['',\n",
       "   'paragraph, lines, textspan element (paragraph segmentation, line segmentation, Information on physical page segmentation(for PDF only))']},\n",
       " {'question': 'Which information about typography is included in the corpus?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'On which benchmarks they achieve the state of the art?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What they use in their propsoed framework?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What does KBQA abbreviate for', 'free_form_answers': ['', '']},\n",
       " {'question': 'What is te core component for KBQA?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What experiments are proposed to test that upper layers produce context-specific embeddings?',\n",
       "  'free_form_answers': ['They measure self-similarity, intra-sentence similarity and maximum explainable variance of the embeddings in the upper layers.',\n",
       "   'They plot the average cosine similarity between uniformly random words increases exponentially from layers 8 through 12.  \\nThey plot the average self-similarity of uniformly randomly sampled words in each layer of BERT, ELMo, and GPT-2 and shown that the higher layer produces more context-specific embeddings.\\nThey plot that word representations in a sentence become more context-specific in upper layers, they drift away from one another.']},\n",
       " {'question': 'How do they calculate a static embedding for each word?',\n",
       "  'free_form_answers': [\"They use the first principal component of a word's contextualized representation in a given layer as its static embedding.\",\n",
       "   '']},\n",
       " {'question': 'What is the performance of BERT on the task?',\n",
       "  'free_form_answers': ['F1 scores are:\\nHUBES-PHI: Detection(0.965), Classification relaxed (0.95), Classification strict (0.937)\\nMedoccan: Detection(0.972), Classification (0.967)',\n",
       "   '']},\n",
       " {'question': 'What are the other algorithms tested?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Does BERT reach the best performance among all the algorithms compared?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are the clinical datasets used in the paper?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'how is model compactness measured?',\n",
       "  'free_form_answers': ['Using file size on disk', '']},\n",
       " {'question': 'what was the baseline?', 'free_form_answers': ['', '']},\n",
       " {'question': 'what evaluation metrics were used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what datasets did they use?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What is the interannotator agreement for the human evaluation?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'Who were the human evaluators used?',\n",
       "  'free_form_answers': ['', \"20 annotatos from author's institution\"]},\n",
       " {'question': 'Is the template-based model realistic?  ',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Is the student reflection data very different from the newspaper data?  ',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the recent abstractive summarization method in this paper?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Why are prior knowledge distillation techniques models are ineffective in producing student models with vocabularies different from the original teacher models?  ',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What state-of-the-art compression techniques were used in the comparison?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What evaluations methods do they take?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'What is the size of the dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which methods are considered to find examples of biases and unwarranted inferences??',\n",
       "  'free_form_answers': ['',\n",
       "   'Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging']},\n",
       " {'question': 'What biases are found in the dataset?',\n",
       "  'free_form_answers': ['Ethnic bias', '']},\n",
       " {'question': 'What discourse relations does it work best/worst for?',\n",
       "  'free_form_answers': ['',\n",
       "   'Best: Expansion (Exp). Worst: Comparison (Comp).']},\n",
       " {'question': 'How much does this model improve state-of-the-art?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Where is a question generation model used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Were any of these tasks evaluated in any previous work?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do they build a model to automatically detect demographic, lingustic or psycological dimensons of people?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which demographic dimensions of people do they obtain?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How do they obtain psychological dimensions of people?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What is the baseline?', 'free_form_answers': ['', '']},\n",
       " {'question': 'Is the data de-identified?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What embeddings are used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What datasets did they use for evaluation?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'On top of BERT does the RNN layer work better or the transformer layer?',\n",
       "  'free_form_answers': ['', 'The transformer layer']},\n",
       " {'question': 'How was this data collected?',\n",
       "  'free_form_answers': ['',\n",
       "   'The crowdsourcing platform CrowdFlower was used to obtain natural dialog data that prompted the user to paraphrase, explain, and/or answer a question from a Simple questions BIBREF7 dataset. The CrowdFlower users were restricted to English-speaking countries to avoid dialogs  with poor English.']},\n",
       " {'question': 'What is the average length of dialog?',\n",
       "  'free_form_answers': ['4.49 turns',\n",
       "   '4.5 turns per dialog (8533 turns / 1900 dialogs)']},\n",
       " {'question': 'How are models evaluated in this human-machine communication game?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How many participants were trying this communication game?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What user variations have been tested?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What are the baselines used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'Do they use off-the-shelf NLP systems to build their assitant?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How does the IPA label data after interacting with users?',\n",
       "  'free_form_answers': ['It defined a sequence labeling task to extract custom entities from user input and label the next action (out of 13  custom actions defined).',\n",
       "   '']},\n",
       " {'question': 'What kind of repetitive and time-consuming activities does their assistant handle?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How was the audio data gathered?',\n",
       "  'free_form_answers': ['Through the All India Radio new channel where actors read news.',\n",
       "   '']},\n",
       " {'question': 'What is the GhostVLAD approach?',\n",
       "  'free_form_answers': ['',\n",
       "   'An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content.']},\n",
       " {'question': 'Which 7 Indian languages do they experiment with?',\n",
       "  'free_form_answers': ['Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam',\n",
       "   'Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)']},\n",
       " {'question': 'What datasets do they evaluate on?', 'free_form_answers': ['']},\n",
       " {'question': 'Do they evaluate only on English datasets?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What is the invertibility condition?',\n",
       "  'free_form_answers': ['The neural projector must be invertible.', '']},\n",
       " {'question': 'Do they show on which examples how conflict works better than attention?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which neural architecture do they use as a base for their attention conflict mechanisms?',\n",
       "  'free_form_answers': ['GRU-based encoder, interaction block, and classifier consisting of stacked fully-connected layers.',\n",
       "   '']},\n",
       " {'question': 'On which tasks do they test their conflict method?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do they use graphical models?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What are the sources of the datasets?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What metric is used for evaluation?',\n",
       "  'free_form_answers': ['F1, precision, recall, accuracy',\n",
       "   'Precision, recall, F1, accuracy']},\n",
       " {'question': 'Which eight NER tasks did they evaluate on?',\n",
       "  'free_form_answers': ['BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800',\n",
       "   'BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800']},\n",
       " {'question': 'What in-domain text did they use?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Does their framework automatically optimize for hyperparameters?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'Does their framework always generate purely attention-based models?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Do they test their framework performance on commonly used language pairs, such as English-to-German?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which languages do they test on for the under-resourced scenario?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Are the automatically constructed datasets subject to quality control?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do they focus on Reading Comprehension or multiple choice question answering?',\n",
       "  'free_form_answers': ['MULTIPLE CHOICE QUESTION ANSWERING', '']},\n",
       " {'question': 'After how many hops does accuracy decrease?',\n",
       "  'free_form_answers': ['', 'one additional hop']},\n",
       " {'question': 'How do they control for annotation artificats?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Is WordNet useful for taxonomic reasoning for this task?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How do they perform multilingual training?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What languages are evaluated?',\n",
       "  'free_form_answers': ['German, English, Spanish, Finnish, French, Russian,  Swedish.']},\n",
       " {'question': 'Does the model have attention?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What architecture does the decoder have?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What architecture does the encoder have?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is MSD prediction?',\n",
       "  'free_form_answers': ['The task of predicting MSD tags: V, PST, V.PCTP, PASS.',\n",
       "   '']},\n",
       " {'question': 'What type of inflections are considered?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'Do they use attention?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What other models do they compare to?',\n",
       "  'free_form_answers': ['SAN Baseline, BNA, DocQA, R.M-Reader, R.M-Reader+Verifier and DocQA+ELMo',\n",
       "   'BNA, DocQA, R.M-Reader, R.M-Reader + Verifier, DocQA + ELMo, R.M-Reader+Verifier+ELMo']},\n",
       " {'question': 'What is the architecture of the span detector?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What evaluation metric do they use?',\n",
       "  'free_form_answers': ['Accuracy', '']},\n",
       " {'question': 'What are the results from these proposed strategies?',\n",
       "  'free_form_answers': ['Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.',\n",
       "   '']},\n",
       " {'question': 'What are the baselines?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What are the two new strategies?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do they report results only on English data?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'How much better than the baseline is LiLi?',\n",
       "  'free_form_answers': ['In case of Freebase knowledge base, LiLi model had better F1 score than the single model by 0.20 , 0.01, 0.159 for kwn, unk, and all test Rel type.  The values for WordNet are 0.25, 0.1, 0.2. \\n']},\n",
       " {'question': 'What baseline is used in the experiments?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'In what way does LiLi imitate how humans acquire knowledge and perform inference during an interactive conversation?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What metrics are used to establish that this makes chatbots more knowledgeable and better at learning and conversation? ',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are the components of the general knowledge learning engine?',\n",
       "  'free_form_answers': ['Answer with content missing: (list)\\nLiLi should have the following capabilities:\\n1. to formulate an inference strategy for a given query that embeds processing and interactive actions.\\n2. to learn interaction behaviors (deciding what to ask and when to ask the user).\\n3. to leverage the acquired knowledge in the current and future inference process.\\n4. to perform 1, 2 and 3 in a lifelong manner for continuous knowledge learning.',\n",
       "   '']},\n",
       " {'question': 'How many labels do the datasets have?',\n",
       "  'free_form_answers': ['719313',\n",
       "   'Book, Electronics, Beauty and Music each have 6000, IMDB 84919, Yelp 231163, Cell Phone 194792 and Baby 160792 labeled data.']},\n",
       " {'question': 'What is the architecture of the model?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are the baseline methods?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What are the source and target domains?',\n",
       "  'free_form_answers': ['Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen',\n",
       "   '']},\n",
       " {'question': 'Did they use a crowdsourcing platform for annotations?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How do they deal with unknown distribution senses?',\n",
       "  'free_form_answers': ['The NÃ¤ive-Bayes classifier is corrected so it is not biased to most frequent classes',\n",
       "   '']},\n",
       " {'question': 'Do they report results only on English data?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What conclusions do the authors draw from their finding that the emotional appeal of ISIS and Catholic materials are similar?',\n",
       "  'free_form_answers': ['',\n",
       "   'By comparing scores for each word calculated using Depechemood dictionary and normalize emotional score for each article, they found Catholic and ISIS materials show similar scores']},\n",
       " {'question': 'How id Depechemood trained?',\n",
       "  'free_form_answers': ['By multiplying crowd-annotated document-emotion matrix with emotion-word matrix. ',\n",
       "   '']},\n",
       " {'question': 'How are similarities and differences between the texts from violent and non-violent religious groups analyzed?',\n",
       "  'free_form_answers': ['By using topic modeling and unsupervised emotion detection on ISIS materials and articles from Catholic women forum',\n",
       "   '']},\n",
       " {'question': 'How are prominent topics idenified in Dabiq and Rumiyah?',\n",
       "  'free_form_answers': ['',\n",
       "   'Using NMF based topic modeling and their coherence prominent topics are identified']},\n",
       " {'question': 'Are the images from a specific domain?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which datasets are used?',\n",
       "  'free_form_answers': ['Existential (OneShape, MultiShapes), Spacial (TwoShapes, Multishapes), Quantification (Count, Ratio) datasets are generated from ShapeWorldICE',\n",
       "   'ShapeWorldICE datasets: OneShape, MultiShapes, TwoShapes, MultiShapes, Count, and Ratio']},\n",
       " {'question': 'Which existing models are evaluated?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How is diversity measured?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What state-of-the-art deep neural network is used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What boundary assembling method is used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are previous state of the art results?',\n",
       "  'free_form_answers': ['Overall F1 score:\\n- He and Sun (2017) 58.23\\n- Peng and Dredze (2017) 58.99\\n- Xu et al. (2018) 59.11',\n",
       "   'For Named entity the maximum precision was 66.67%, and the average 62.58%, same values for Recall was 55.97% and 50.33%, and for F1 57.14% and 55.64%. Where for Nominal Mention had maximum recall of 74.48% and average of 73.67%, Recall had values of 54.55% and 53.7%,  and F1 had values of  62.97% and 62.12%. Finally the Overall F1 score had maximum value of 59.11% and average of 58.77%']},\n",
       " {'question': 'What is the model performance on target language reading comprehension?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What source-target language pairs were used in this work? ',\n",
       "  'free_form_answers': ['En-Fr, En-Zh, En-Jp, En-Kr, Zh-En, Zh-Fr, Zh-Jp, Zh-Kr to English, Chinese or Korean',\n",
       "   '',\n",
       "   '']},\n",
       " {'question': 'What model is used as a baseline?  ',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what does the model learn in zero-shot setting?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Do they inspect their model to see if their model learned to associate image parts with words related to entities?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Does their NER model learn NER from both text and images?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which types of named entities do they recognize?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Can named entities in SnapCaptions be discontigious?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How large is their MNER SnapCaptions dataset?',\n",
       "  'free_form_answers': ['', '10000']},\n",
       " {'question': 'What is masked document generation?',\n",
       "  'free_form_answers': ['A task for seq2seq model pra-training that recovers a masked document to its original form.',\n",
       "   '']},\n",
       " {'question': 'Which of the three pretraining tasks is the most helpful?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What useful information does attention capture?',\n",
       "  'free_form_answers': ['', 'Alignment points of the POS tags.']},\n",
       " {'question': 'What datasets are used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'In what cases is attention different from alignment?',\n",
       "  'free_form_answers': ['For certain POS tags, e.g. VERB, PRON.', '']},\n",
       " {'question': 'How do they calculate variance from the model outputs?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How much data samples do they start with before obtaining the initial model labels?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which model do they use for end-to-end speech recognition?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which dataset do they use?', 'free_form_answers': ['', '']},\n",
       " {'question': 'Which baselines did they compare against?',\n",
       "  'free_form_answers': ['Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks',\n",
       "   'Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \\nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018).']},\n",
       " {'question': 'What baselines did they consider?',\n",
       "  'free_form_answers': ['', 'Linear SVM, RBF SVM, and Random Forest']},\n",
       " {'question': 'What types of social media did they consider?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How was the dataset annotated?',\n",
       "  'free_form_answers': ['intents are annotated manually with guidance from queries collected using a scoping crowdsourcing task',\n",
       "   '']},\n",
       " {'question': 'Which classifiers are evaluated?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the size of this dataset?',\n",
       "  'free_form_answers': ['',\n",
       "   ' 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains and 1,200 out-of-scope queries.']},\n",
       " {'question': 'Where does the data come from?',\n",
       "  'free_form_answers': ['crowsourcing platform',\n",
       "   'For ins scope data collection:crowd workers which provide questions and commands related to topic domains and additional data the rephrase and scenario crowdsourcing tasks proposed by BIBREF2 is used. \\nFor out of scope data collection:  from workers mistakes-queries written for one of the 150 intents that did not actually match any of the intents and using scoping and scenario tasks with prompts based on topic areas found on Quora, Wikipedia, and elsewhere.']},\n",
       " {'question': 'What are method improvements of F1 for paraphrase identification?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': \"What are method's improvements of F1 for NER task for English and Chinese datasets?\",\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': \"What are method's improvements of F1 w.r.t. baseline BERT tagger for Chinese POS datasets?\",\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How are weights dynamically adjusted?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Ngrams of which length are aligned using PARENT?',\n",
       "  'free_form_answers': ['Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4']},\n",
       " {'question': 'How many people participated in their evaluation study of table-to-text models?',\n",
       "  'free_form_answers': ['about 500']},\n",
       " {'question': 'By how much more does PARENT correlate with human judgements in comparison to other text generation metrics?',\n",
       "  'free_form_answers': ['Best proposed metric has average correlation with human judgement of 0.913 and 0.846 compared to best compared metrics result of 0.758 and 0.829 on WikiBio and WebNLG challenge.',\n",
       "   'Their average correlation tops the best other model by 0.155 on WikiBio.']},\n",
       " {'question': 'Which stock market sector achieved the best performance?',\n",
       "  'free_form_answers': ['Energy with accuracy of 0.538', 'Energy']},\n",
       " {'question': 'What languages pairs are used in machine translation?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What sentiment classification dataset is used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What pooling function is used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'Do they report results only on English?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What neural network modules are included in NeuronBlocks?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How do the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques?',\n",
       "  'free_form_answers': ['By conducting a survey among engineers']},\n",
       " {'question': 'what datasets did they use?',\n",
       "  'free_form_answers': ['Dataset of total 3500 questions from the Internet and other sources such as books of general knowledge questions, history, etc.',\n",
       "   '3500 questions collected from the internet and books.']},\n",
       " {'question': 'what ml based approaches were compared?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Is pre-training effective in their evaluation?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What parallel corpus did they use?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How much does their model outperform existing models?',\n",
       "  'free_form_answers': ['Best proposed model result vs best previous result:\\nArxiv dataset: Rouge 1 (43.62 vs 42.81), Rouge L (29.30 vs 31.80), Meteor (21.78 vs 21.35)\\nPubmed dataset: Rouge 1 (44.85 vs 44.29), Rouge L (31.48 vs 35.21), Meteor (20.83 vs 20.56)',\n",
       "   'On arXiv dataset, the proposed model outperforms baselie model by (ROUGE-1,2,L)  0.67 0.72 0.77 respectively and by Meteor 0.31.\\n']},\n",
       " {'question': 'What do they mean by global and local context?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are the 18 propaganda techniques?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What dataset was used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What was the baseline for this task?',\n",
       "  'free_form_answers': ['The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.',\n",
       "   '']},\n",
       " {'question': 'What is a second order co-ocurrence matrix?',\n",
       "  'free_form_answers': ['',\n",
       "   'The matrix containing co-occurrences of the words which occur with the both words of every given pair of words.']},\n",
       " {'question': 'How many humans participated?', 'free_form_answers': ['16']},\n",
       " {'question': 'What embedding techniques are explored in the paper?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do the authors also try the model on other datasets?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What word level and character level model baselines are used?',\n",
       "  'free_form_answers': ['None',\n",
       "   'Word-level Memory Neural Networks (MemNNs) proposed in Bordes et al. (2015)']},\n",
       " {'question': 'By how much do they improve the efficacy of the attention mechanism?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'How were the human judgements assembled?',\n",
       "  'free_form_answers': ['50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.',\n",
       "   '']},\n",
       " {'question': 'Did they only experiment with one language pair?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which other approaches do they compare their model with?',\n",
       "  'free_form_answers': ['Akbik et al. (2018), Link et al. (2012)',\n",
       "   'They compare to Akbik et al. (2018) and Link et al. (2012).']},\n",
       " {'question': 'What results do they achieve using their proposed approach?',\n",
       "  'free_form_answers': ['F-1 score on the OntoNotes is 88%, and it is 53% on Wiki (gold).',\n",
       "   '']},\n",
       " {'question': 'How do they combine a deep learning model with a knowledge base?',\n",
       "  'free_form_answers': ['Entities from a deep learning model are linked to the related entities from a knowledge base by a lookup.',\n",
       "   '']},\n",
       " {'question': 'What are the models used for the baseline of the three NLP tasks?',\n",
       "  'free_form_answers': ['',\n",
       "   'For speech synthesis, they build a speech clustergen statistical speech synthesizer BIBREF9. For speech recognition, they use Kaldi BIBREF11. For Machine Translation, they use a Transformer architecture from BIBREF15.']},\n",
       " {'question': 'How is non-standard pronunciation identified?',\n",
       "  'free_form_answers': ['Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation.']},\n",
       " {'question': 'Is it valid to presume a bad medical wikipedia article should not contain much domain-specific jargon?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What novel PMI variants are introduced?',\n",
       "  'free_form_answers': ['clipped PMI; NNEGPMI', '']},\n",
       " {'question': 'What semantic and syntactic tasks are used as probes?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are the disadvantages to clipping negative PMI?',\n",
       "  'free_form_answers': ['It may lead to poor rare word representations and word analogies.']},\n",
       " {'question': 'Why are statistics from finite corpora unreliable?',\n",
       "  'free_form_answers': ['',\n",
       "   'A finite corpora may entirely omit rare word combinations']},\n",
       " {'question': 'what is the domain of the corpus?', 'free_form_answers': ['']},\n",
       " {'question': 'what challenges are identified?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what is the size of the speech corpus?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which two pairs of ERPs from the literature benefit from joint training?',\n",
       "  'free_form_answers': ['Answer with content missing: (Whole Method and Results sections) Self-paced reading times widely benefit ERP prediction, while eye-tracking data seems to have more limited benefit to just the ELAN, LAN, and PNP ERP components.\\nSelect:\\n- ELAN, LAN\\n- PNP ERP']},\n",
       " {'question': 'What datasets are used?',\n",
       "  'free_form_answers': ['Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\\nSelect:\\n- ERP data collected and computed by Frank et al. (2015)\\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)',\n",
       "   '']},\n",
       " {'question': 'which datasets did they experiment with?',\n",
       "  'free_form_answers': ['Universal Dependencies v1.2 treebanks for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German,\\nIndonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish',\n",
       "   '']},\n",
       " {'question': 'which languages are explored?', 'free_form_answers': ['', '']},\n",
       " {'question': 'Do they use number of votes as an indicator of preference?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What does a node in the network approach repesent?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'Which dataset do they use?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What kind of celebrities do they obtain tweets from?',\n",
       "  'free_form_answers': ['Amitabh Bachchan, Ariana Grande, Barack Obama, Bill Gates, Donald Trump,\\nEllen DeGeneres, J K Rowling, Jimmy Fallon, Justin Bieber, Kevin Durant, Kim Kardashian, Lady Gaga, LeBron James,Narendra Modi, Oprah Winfrey',\n",
       "   'Celebrities from varioius domains - Acting, Music, Politics, Business, TV, Author, Sports, Modeling. ']},\n",
       " {'question': 'How did they extend LAMA evaluation framework to focus on negation?',\n",
       "  'free_form_answers': ['',\n",
       "   'Create the negated LAMA dataset and  query the pretrained language models with both original LAMA and negated LAMA statements and compare their predictions.']},\n",
       " {'question': 'What summarization algorithms did the authors experiment with?',\n",
       "  'free_form_answers': ['LSA, TextRank, LexRank and ILP-based summary.',\n",
       "   'LSA, TextRank, LexRank']},\n",
       " {'question': 'What evaluation metrics were used for the summarization task?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What clustering algorithms were used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What evaluation metrics are looked at for classification tasks?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What methods were used for sentence classification?',\n",
       "  'free_form_answers': ['Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK and Pattern-based',\n",
       "   'Logistic Regression, Multinomial Naive Bayes, Random Forest, AdaBoost, Linear SVM, SVM with ADWSK, Pattern-based approach']},\n",
       " {'question': 'What is the average length of the sentences?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the size of the real-life dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are the language pairs explored in this paper?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do they experiment with the toolkits?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Have they made any attempt to correct MRC gold standards according to their findings? ',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What features are absent from MRC gold standards that can result in potential lexical ambiguity?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What modern MRC gold standards are analyzed?',\n",
       "  'free_form_answers': ['',\n",
       "   'MSMARCO,  HOTPOTQA, RECORD,  MULTIRC, NEWSQA, and DROP.']},\n",
       " {'question': 'How does proposed qualitative annotation schema looks like?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How many tweets were collected?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What language is explored in this paper?',\n",
       "  'free_form_answers': ['English language']},\n",
       " {'question': 'What are the baselines?', 'free_form_answers': ['', '', '']},\n",
       " {'question': 'What is the attention module pretrained on?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How long of dialog history is captured?',\n",
       "  'free_form_answers': ['two previous turns', '']},\n",
       " {'question': 'What evaluation metrics were used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What was the score of the proposed model?',\n",
       "  'free_form_answers': ['Best results authors obtain is EM 51.10 and F1 63.11',\n",
       "   'EM Score of 51.10']},\n",
       " {'question': 'What was the previous best model?', 'free_form_answers': ['']},\n",
       " {'question': 'Which datasets did they use for evaluation?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What hyperparameters are explored?',\n",
       "  'free_form_answers': ['Dimension size, window size, architecture, algorithm, epochs, hidden dimension size, learning rate, loss function, optimizer algorithm.',\n",
       "   'Hyperparameters explored were: dimension size, window size, architecture, algorithm and epochs.']},\n",
       " {'question': 'What Named Entity Recognition dataset is used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What sentiment analysis dataset is used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do they test both skipgram and c-bow?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the state-of-the-art model for the task?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What is the strong baseline?', 'free_form_answers': ['', '']},\n",
       " {'question': 'what aspects of conversation flow do they look at?',\n",
       "  'free_form_answers': ['The time devoted to self-coverage, opponent-coverage, and the number of adopted discussion points.',\n",
       "   '']},\n",
       " {'question': 'what debates dataset was used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'what is the state of the art?',\n",
       "  'free_form_answers': ['Babelfy, DBpedia Spotlight, Entityclassifier.eu, FOX, LingPipe MUC-7, NERD-ML, Stanford NER, TagMe 2']},\n",
       " {'question': 'what standard dataset were used?',\n",
       "  'free_form_answers': ['', '', '']},\n",
       " {'question': 'Do they perform error analysis?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How do their results compare to state-of-the-art?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'What is the Random Kitchen Sink approach?',\n",
       "  'free_form_answers': ['Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.',\n",
       "   '']},\n",
       " {'question': 'what are the baseline systems?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What word embeddings do they test?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How do they define similar equations?',\n",
       "  'free_form_answers': ['By using Euclidean distance computed between the context vector representations of the equations',\n",
       "   '']},\n",
       " {'question': 'What evaluation criteria and metrics were used to evaluate the generated text?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do they evaluate only on English datasets?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are the three steps to feature elimination?',\n",
       "  'free_form_answers': ['',\n",
       "   'reduced the dataset by eliminating features, apply feature selection to select highest ranked features to train and test the model and rank the performance of incrementally adding features.']},\n",
       " {'question': 'How is the dataset annotated?',\n",
       "  'free_form_answers': ['',\n",
       "   'The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression']},\n",
       " {'question': 'What dataset is used for this study?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what were their performance results?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'where did they obtain the annotated clinical notes from?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which architecture do they use for the encoder and decoder?',\n",
       "  'free_form_answers': ['',\n",
       "   'In encoder they use convolutional, NIN and bidirectional LSTM layers and in decoder they use unidirectional LSTM ']},\n",
       " {'question': 'How does their decoder generate text?',\n",
       "  'free_form_answers': ['',\n",
       "   'Decoder predicts the sequence of phoneme or grapheme at each time based on the previous output and context information with a beam search strategy']},\n",
       " {'question': 'Which dataset do they use?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What model is used to encode the images?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How is the sequential nature of the story captured?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Is the position in the sequence part of the input?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do the decoder LSTMs all have the same weights?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Is fine-tuning required to incorporate these embeddings into existing models?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How are meaningful chains in the graph selected?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do the authors also analyze transformer-based architectures?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do they remove seasonality from the time series?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What is the dimension of the embeddings?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What dataset is used to train the model?',\n",
       "  'free_form_answers': ['',\n",
       "   'Collected tweets and opening and closing stock prices of Microsoft.']},\n",
       " {'question': 'What is the previous state of the art?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which text embedding methodologies are used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which race and gender are given higher sentiment intensity predictions?',\n",
       "  'free_form_answers': ['Females are given higher sentiment intensity when predicting anger, joy or valence, but males are given higher sentiment intensity when predicting  fear.\\nAfrican American names are given higher score on the tasks of anger, fear, and sadness intensity prediction,  but European American names are given higher scores on joy and valence task.',\n",
       "   '']},\n",
       " {'question': 'What criteria are used to select the 8,640 English sentences?',\n",
       "  'free_form_answers': ['Sentences involving at least one race- or gender-associated word,  sentence  have to be short and grammatically simple,  sentence have to  include expressions of sentiment and emotion.',\n",
       "   '']},\n",
       " {'question': 'what were the baselines?',\n",
       "  'free_form_answers': ['LF-MMI Attention\\nSeq2Seq \\nRNN-T \\nChar E2E LF-MMI \\nPhone E2E LF-MMI \\nCTC + Gram-CTC']},\n",
       " {'question': 'what competitive results did they obtain?',\n",
       "  'free_form_answers': ['In case of read speech datasets,  their best model got the highest nov93 score of 16.1 and the highest nov92 score of 13.3.\\nIn case of Conversational Speech, their best model got the highest SWB of 8.3 and the highest CHM of 19.3. ',\n",
       "   \"On WSJ datasets author's best approach achieves 9.3 and 6.9 WER compared to best results of 7.5 and 4.1 on nov93 and nov92 subsets.\\nOn Hub5'00 datasets author's best approach achieves WER of 7.8 and 16.2 compared to best result of 7.3 and 14.2 on Switchboard (SWB) and Callhome (CHM) subsets.\"]},\n",
       " {'question': 'By how much is performance improved with multimodality?',\n",
       "  'free_form_answers': ['by 2.3-6.8 points in f1 score for intent recognition and 0.8-3.5 for slot filling',\n",
       "   'F1 score increased from 0.89 to 0.92']},\n",
       " {'question': 'Is collected multimodal in cabin dataset public?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What is the performance reported for the best models in the VLSP 2018 and VLSP 2019 challenges?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'Is the model tested against any baseline?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the language model combination technique used in the paper?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are the deep learning architectures used in the task?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How much is performance improved on NLI?',\n",
       "  'free_form_answers': ['',\n",
       "   'The average score improved by 1.4 points over the previous best result.']},\n",
       " {'question': 'Do they train their model starting from a checkpoint?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What BERT model do they test?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What downstream tasks are evaluated?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What is active learning?',\n",
       "  'free_form_answers': ['A process of training a model when selected unlabeled samples are annotated on each iteration.',\n",
       "   'Active learning is a process that selectively determines which unlabeled samples for a machine learning model should be annotated.']},\n",
       " {'question': 'what was the baseline?',\n",
       "  'free_form_answers': ['', 'M2M Transformer']},\n",
       " {'question': 'How is segmentation quality evaluated?',\n",
       "  'free_form_answers': ['Segmentation quality is evaluated by calculating the precision, recall, and F-score of the automatic segmentations in comparison to the segmentations made by expert annotators from the ANNODIS subcorpus.',\n",
       "   '']},\n",
       " {'question': 'How do they compare lexicons?',\n",
       "  'free_form_answers': ['Human evaluators were asked to evaluate on a scale from 1 to 5 the validity of the lexicon annotations made by the experts and crowd contributors.',\n",
       "   '']},\n",
       " {'question': 'Is it possible to convert a cloze-style questions to a naturally-looking questions?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'How larger are the training sets of these versions of ELMo compared to the previous ones?',\n",
       "  'free_form_answers': ['By 14 times.', 'up to 1.95 times larger']},\n",
       " {'question': 'What is the improvement in performance for Estonian in the NER task?',\n",
       "  'free_form_answers': ['5 percent points.', '0.05 F1']},\n",
       " {'question': 'what is the state of the art on WSJ?',\n",
       "  'free_form_answers': ['CNN-DNN-BLSTM-HMM', '']},\n",
       " {'question': 'How did they obtain the OSG dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How large is the Twitter dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what is the size of the augmented dataset?',\n",
       "  'free_form_answers': ['609']},\n",
       " {'question': 'How they utilize LDA and Gibbs sampling to evaluate ISWC and WWW publications?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What dataset do they use to evaluate their method?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': \"Why are current ELS's not sufficiently effective?\",\n",
       "  'free_form_answers': ['Linked entities may be ambiguous or too common', '']},\n",
       " {'question': 'What is the best model?', 'free_form_answers': ['']},\n",
       " {'question': 'How many sentences does the dataset contain?',\n",
       "  'free_form_answers': ['3606', '']},\n",
       " {'question': 'Do the authors train a Naive Bayes classifier on their dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the baseline?',\n",
       "  'free_form_answers': ['',\n",
       "   'Bam et al. SVM, Ma and Hovy w/glove, Lample et al. w/fastText, Lample et al. w/word2vec']},\n",
       " {'question': 'Which machine learning models do they explore?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the size of the dataset?',\n",
       "  'free_form_answers': ['Dataset contains 3606 total sentences and 79087 total entities.',\n",
       "   'ILPRL contains 548 sentences, OurNepali contains 3606 sentences']},\n",
       " {'question': 'What is the source of their dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do they try to use byte-pair encoding representations?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How many different types of entities exist in the dataset?',\n",
       "  'free_form_answers': ['OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities',\n",
       "   '']},\n",
       " {'question': 'How big is the new Nepali NER dataset?',\n",
       "  'free_form_answers': ['3606 sentences',\n",
       "   'Dataset contains 3606 total sentences and 79087 total entities.']},\n",
       " {'question': 'What is the performance improvement of the grapheme-level representation model over the character-level model?',\n",
       "  'free_form_answers': ['On OurNepali test dataset Grapheme-level representation model achieves average 0.16% improvement, on ILPRL test dataset it achieves maximum 1.62% improvement',\n",
       "   '']},\n",
       " {'question': 'Which models are used to solve NER for Nepali?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What language(s) is/are represented in the dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What baseline model is used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'Which variation provides the best results on this dataset?',\n",
       "  'free_form_answers': ['the model with multi-attention mechanism and a projected layer',\n",
       "   '']},\n",
       " {'question': 'What are the different variations of the attention-based approach which are examined?',\n",
       "  'free_form_answers': ['classic RNN model, avgRNN model, attentionRNN model and multiattention RNN model with and without a projected layer',\n",
       "   '']},\n",
       " {'question': 'What dataset is used for this work?',\n",
       "  'free_form_answers': ['Twitter dataset provided by the organizers',\n",
       "   'The dataset from the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference.']},\n",
       " {'question': 'What types of online harassment are studied?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What was the baseline?', 'free_form_answers': ['']},\n",
       " {'question': 'What were the datasets used in this paper?',\n",
       "  'free_form_answers': ['The dataset from the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference. ',\n",
       "   'Twitter dataset provided by organizers containing harassment and non-harassment tweets']},\n",
       " {'question': 'Is car-speak language collection of abstract features that classifier is later trained on?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Is order of \"words\" important in car speak language?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What are labels in car speak language dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How big is dataset of car-speak language?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the performance of classifiers?',\n",
       "  'free_form_answers': ['',\n",
       "   'Using F1 Micro measure, the KNN classifier perform 0.6762, the RF 0.6687, SVM 0.6712 and MLP 0.6778.']},\n",
       " {'question': 'What classifiers have been trained?',\n",
       "  'free_form_answers': ['KNN\\nRF\\nSVM\\nMLP', '']},\n",
       " {'question': \"How does car speak pertains to a car's physical attributes?\",\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What topic is covered in the Chinese Facebook data? ',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How many layers does the UTCNN model have?',\n",
       "  'free_form_answers': ['eight layers']},\n",
       " {'question': 'What topics are included in the debate data?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the size of the Chinese data?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Did they collected the two datasets?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are the baselines?',\n",
       "  'free_form_answers': ['',\n",
       "   'SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information']},\n",
       " {'question': 'What transfer learning tasks are evaluated?',\n",
       "  'free_form_answers': ['',\n",
       "   '',\n",
       "   'Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification.']},\n",
       " {'question': 'What metrics are used for the STS tasks?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How much time takes its training?', 'free_form_answers': ['']},\n",
       " {'question': 'How many GPUs are used for the training of SBERT?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'How are the siamese networks trained?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What other sentence embeddings methods are evaluated?',\n",
       "  'free_form_answers': ['GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent',\n",
       "   'Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder.']},\n",
       " {'question': 'What is the average length of the title text?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'Which pretrained word vectors did they use?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What evaluation metrics are used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which shallow approaches did they experiment with?',\n",
       "  'free_form_answers': ['',\n",
       "   'SVM with linear kernel using bag-of-words features']},\n",
       " {'question': 'Where do they obtain the news videos from?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the source of the news articles?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'which non-english language had the best performance?',\n",
       "  'free_form_answers': ['', 'Russsian']},\n",
       " {'question': 'which non-english language was the had the worst results?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'what datasets were used in evaluation?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what are the baselines?', 'free_form_answers': ['', '']},\n",
       " {'question': 'how did the authors translate the reviews to other languages?',\n",
       "  'free_form_answers': ['Using Google translation API.', '']},\n",
       " {'question': 'what dataset was used for training?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How do they demonstrate that this type of EEG has discriminative information about the intended articulatory movements responsible for speech?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What are the five different binary classification tasks?',\n",
       "  'free_form_answers': ['',\n",
       "   'presence/absence of consonants, presence/absence of phonemic nasal, presence/absence of bilabial, presence/absence of high-front vowels, and presence/absence of high-back vowels']},\n",
       " {'question': 'How was the spatial aspect of the EEG signal computed?',\n",
       "  'free_form_answers': ['',\n",
       "   'They use four-layered 2D CNN and two fully connected hidden layers on the channel covariance matrix to compute the spatial aspect.']},\n",
       " {'question': 'What data was presented to the subjects to elicit event-related responses?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How many electrodes were used on the subject in EEG sessions?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How many subjects does the EEG data come from?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do they report results only on English data?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What type of classifiers are used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which real-world datasets are used?',\n",
       "  'free_form_answers': ['Tweets related to CyberAttack and tweets related to PoliticianDeath',\n",
       "   '']},\n",
       " {'question': 'How are the interpretability merits of the approach demonstrated?',\n",
       "  'free_form_answers': [\"By involving humans for post-hoc evaluation of model's interpretability\",\n",
       "   '']},\n",
       " {'question': 'How are the accuracy merits of the approach demonstrated?',\n",
       "  'free_form_answers': ['',\n",
       "   'By evaluating the performance of the approach using accuracy and AUC']},\n",
       " {'question': 'How is the keyword specific expectation elicited from the crowd?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Does the paper provide any case studies to illustrate how one can use Macaw for CIS research?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What functionality does Macaw provide?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is a wizard of oz setup?',\n",
       "  'free_form_answers': ['',\n",
       "   \"a setup where the seeker interacts with a real conversational interface and the wizard, an intermediary, performs actions related to the seeker's message\"]},\n",
       " {'question': 'What interface does Macaw currently have?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What modalities are supported by Macaw?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What are the different modules in Macaw?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do they report results only on English data?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What baseline model is used?',\n",
       "  'free_form_answers': ['For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \\n\\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.',\n",
       "   '']},\n",
       " {'question': 'What news article sources are used?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How do they determine the exact section to use the input article?',\n",
       "  'free_form_answers': ['They use a multi-class classifier to determine the section it should be cited']},\n",
       " {'question': 'What features are used to represent the novelty of news articles to entity pages?',\n",
       "  'free_form_answers': ['KL-divergences of language models for the news article and the already added news references',\n",
       "   '']},\n",
       " {'question': 'What features are used to represent the salience and relative authority of entities?',\n",
       "  'free_form_answers': ['Salience features positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in.\\nThe relative authority of entity features:   comparative relevance of the news article to the different entities occurring in it.',\n",
       "   '']},\n",
       " {'question': 'Do they experiment with other tasks?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What baselines do they introduce?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How large is the corpus?', 'free_form_answers': ['', '']},\n",
       " {'question': 'How was annotation performed?',\n",
       "  'free_form_answers': ['Experienced medical doctors used a linguistic annotation tool to annotate entities.',\n",
       "   '']},\n",
       " {'question': 'How many documents are in the new corpus?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What baseline systems are proposed?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How did they obtain the dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What activation function do they use in their model?',\n",
       "  'free_form_answers': ['',\n",
       "   'Activation function is hyperparameter. Possible values: relu, selu, tanh.']},\n",
       " {'question': 'What baselines do they compare to?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How are chunks defined?',\n",
       "  'free_form_answers': ['Chunks is group of tweets from single account that  is consecutive in time - idea is that this group can show secret intention of malicious accounts.',\n",
       "   'sequence of $s$ tweets']},\n",
       " {'question': 'What features are extracted?', 'free_form_answers': ['', '']},\n",
       " {'question': 'How many layers does their model have?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'Was the approach used in this work to detect fake news fully supervised?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Based on this paper, what is the more predictive set of features to detect fake news?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How big is the dataset used in this work?',\n",
       "  'free_form_answers': ['Total dataset size: 171 account (522967 tweets)',\n",
       "   '212 accounts']},\n",
       " {'question': 'How is a \"chunk of posts\" defined in this work?',\n",
       "  'free_form_answers': ['', 'sequence of $s$ tweets']},\n",
       " {'question': 'What baselines were used in this work?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the performance of their method?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'Which evaluation methods are used?',\n",
       "  'free_form_answers': ['Quantitative evaluation methods using ROUGE, Recall, Precision and F1.',\n",
       "   '']},\n",
       " {'question': 'What dataset is used in this paper?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which other methods do they compare with?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How are sentences selected from the summary graph?',\n",
       "  'free_form_answers': ['',\n",
       "   ' Two methods: first is to simply pick initial few sentences,  second is to capture the relation between the two most important entities  (select the first sentence which contains both these entities).']},\n",
       " {'question': 'What models are used in the experiment?',\n",
       "  'free_form_answers': ['', '', '']},\n",
       " {'question': 'What are the differences between this dataset and pre-existing ones?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'In what language are the tweets?',\n",
       "  'free_form_answers': ['', '', '']},\n",
       " {'question': 'What is the size of the new dataset?',\n",
       "  'free_form_answers': ['14,100 tweets',\n",
       "   'Dataset contains total of 14100 annotations.']},\n",
       " {'question': 'What kinds of offensive content are explored?',\n",
       "  'free_form_answers': ['non-targeted profanity and swearing, targeted insults such as cyberbullying, offensive content related to ethnicity, gender or sexual orientation, political affiliation, religious belief, and anything belonging to hate speech',\n",
       "   '',\n",
       "   '']},\n",
       " {'question': 'What is the best performing model?', 'free_form_answers': ['']},\n",
       " {'question': 'How many annotators participated?', 'free_form_answers': ['']},\n",
       " {'question': 'What is the definition of offensive language?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What are the three layers of the annotation scheme?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How long is the dataset for each step of hierarchy?',\n",
       "  'free_form_answers': ['Level A: 14100 Tweets\\nLevel B: 4640 Tweets\\nLevel C: 4089 Tweets']},\n",
       " {'question': 'Do the authors report results only on English data?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'In the proposed metric, how is content relevance measured?',\n",
       "  'free_form_answers': ['The content relevance between the candidate summary and the human summary is evaluated using information retrieval - using the summaries as search queries and compare the overlaps of the retrieved results. ',\n",
       "   '']},\n",
       " {'question': 'What different correlations result when using different variants of ROUGE scores?',\n",
       "  'free_form_answers': ['',\n",
       "   'Using Pearson corelation measure,  for example, ROUGE-1-P is 0.257 and ROUGE-3-F 0.878.']},\n",
       " {'question': 'What manual Pyramid scores are used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': \"What is the common belief that this paper refutes? (c.f. 'contrary to the common belief, ROUGE is not much [sic] reliable'\",\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'which existing strategies are compared?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'what dataset was used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'what kinds of male and female words are looked at?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'how is mitigation of gender bias evaluated?',\n",
       "  'free_form_answers': ['Using INLINEFORM0 and INLINEFORM1']},\n",
       " {'question': 'what bias evaluation metrics are used?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What kind of questions are present in the dataset?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What baselines are presented?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What tasks were evaluated?',\n",
       "  'free_form_answers': ['',\n",
       "   'Detection of an aspect in a review, Prediction of the customer general satisfaction, Prediction of the global trend of an aspect in a given review, Prediction of whether the rating of a given aspect is above or under a given value, Prediction of the exact rating of an aspect in a review, Prediction of the list of all the positive/negative aspects mentioned in the review, Comparison between aspects, Prediction of the strengths and weaknesses in a review']},\n",
       " {'question': 'What language are the reviews in?', 'free_form_answers': ['']},\n",
       " {'question': 'Where are the hotel reviews from?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What was the baseline used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What are their results on both datasets?',\n",
       "  'free_form_answers': ['Combining  pattern based and Machine translation approaches gave the best overall F0.5 scores. It was 49.11 for FCE dataset  , 21.87 for the first annotation of  CoNLL-14, and 30.13 for the second annotation of CoNLL-14. ']},\n",
       " {'question': 'What textual patterns are extracted?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which annotated corpus did they use?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which languages are explored in this paper?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what language does this paper focus on?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what evaluation metrics did they use?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'by how much did their model improve?',\n",
       "  'free_form_answers': ['For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.',\n",
       "   '']},\n",
       " {'question': 'what state of the art methods did they compare with?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'what are the sizes of both datasets?',\n",
       "  'free_form_answers': ['',\n",
       "   'WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. ']},\n",
       " {'question': 'What are the distinctive characteristics of how Arabic speakers use offensive language?',\n",
       "  'free_form_answers': ['Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.',\n",
       "   '']},\n",
       " {'question': 'How did they analyze which topics, dialects and gender are most associated with tweets?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How many annotators tagged each tweet?',\n",
       "  'free_form_answers': ['One', 'One experienced annotator tagged all tweets']},\n",
       " {'question': 'How many tweets are in the dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'In what way is the offensive dataset not biased by topic, dialect or target?',\n",
       "  'free_form_answers': ['It does not use a seed list to gather tweets so the dataset does not skew to specific topics, dialect, targets.',\n",
       "   '']},\n",
       " {'question': 'What experiments are conducted?', 'free_form_answers': ['']},\n",
       " {'question': 'What is the combination of rewards for reinforcement learning?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are the difficulties in modelling the ironic pattern?',\n",
       "  'free_form_answers': ['',\n",
       "   'ironies are often obscure and hard to understand']},\n",
       " {'question': 'How did the authors find ironic data on twitter?',\n",
       "  'free_form_answers': ['They developed a classifier to find ironic sentences in twitter data',\n",
       "   'by crawling']},\n",
       " {'question': 'Who judged the irony accuracy, sentiment preservation and content preservation?',\n",
       "  'free_form_answers': ['Irony accuracy is judged only by human ; senriment preservation and content preservation are judged  both by human and using automatic metrics (ACC and BLEU).',\n",
       "   '']},\n",
       " {'question': 'How were the tweets annotated?',\n",
       "  'free_form_answers': ['tweets are annotated with only Favor or Against for two targets - Galatasaray and FenerbahÃ§e']},\n",
       " {'question': 'Which SVM approach resulted in the best performance?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What are hashtag features?',\n",
       "  'free_form_answers': ['hashtag features contain whether there is any hashtag in the tweet']},\n",
       " {'question': 'How many tweets did they collect?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which sports clubs are the targets?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Does this method help in sentiment classification task improvement?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'For how many probe tasks the shallow-syntax-aware contextual embedding perform better than ELMoâs embedding?',\n",
       "  'free_form_answers': ['', '3']},\n",
       " {'question': 'What are the black-box probes used?',\n",
       "  'free_form_answers': ['CCG Supertagging CCGBank , PTB part-of-speech tagging, EWT part-of-speech tagging,\\nChunking, Named Entity Recognition, Semantic Tagging, Grammar Error Detection, Preposition Supersense Role, Preposition Supersense Function, Event Factuality Detection',\n",
       "   '']},\n",
       " {'question': 'What are improvements for these two approaches relative to ELMo-only baselines?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which syntactic features are obtained automatically on downstream task data?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Do they report results only on English data?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'What baseline approaches does this approach out-perform?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What datasets are used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What alternative to Gibbs sampling is used?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How does this model overcome the assumption that all words in a document are generated from a single event?',\n",
       "  'free_form_answers': ['',\n",
       "   'by learning a projection function between the document-event distribution and four event related word distributions ']},\n",
       " {'question': 'How many users do they look at?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': \"What do they mean by a person's industry?\",\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What model did they use for their system?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What social media platform did they look at?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are the industry classes defined in this paper?',\n",
       "  'free_form_answers': ['technology, religion, fashion, publishing, sports or recreation, real estate, agriculture/environment, law, security/military, tourism, construction, museums or libraries, banking/investment banking, automotive',\n",
       "   'Technology, Religion, Fashion, Publishing, Sports coach, Real Estate, Law, Environment, Tourism, Construction, Museums, Banking, Security, Automotive.']},\n",
       " {'question': 'Do they report results only on English data?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What are the hyperparameters of the bi-GRU?',\n",
       "  'free_form_answers': ['They use the embedding layer with a size 35 and embedding dimension of 300. They use a dense layer with 70 units and a dropout layer with a rate of 50%.']},\n",
       " {'question': 'What baseline is used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What data is used in experiments?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What meaningful information does the GRU model capture, which traditional ML models do not?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the approach of previous work?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Is the lexicon the same for all languages?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How do they obtain the lexicon?', 'free_form_answers': ['']},\n",
       " {'question': 'What evaluation metric is used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which languages are similar to each other?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which datasets are employed for South African languages LID?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Does the paper report the performance of a baseline model on South African languages LID?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are the languages represented in the DSL datasets? ',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'Does the algorithm improve on the state-of-the-art methods?',\n",
       "  'free_form_answers': ['',\n",
       "   'From all reported results proposed method (NB+Lex) shows best accuracy on all 3 datasets - some models are not evaluated and not available in literature.']},\n",
       " {'question': 'What background knowledge do they leverage?',\n",
       "  'free_form_answers': ['',\n",
       "   'labelled features, which are words whose presence strongly indicates a specific class or topic']},\n",
       " {'question': 'What are the three regularization terms?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What NLP tasks do they consider?',\n",
       "  'free_form_answers': ['text classification for themes including sentiment, web-page, science, medical and healthcare']},\n",
       " {'question': 'How do they define robustness of a model?',\n",
       "  'free_form_answers': ['ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced',\n",
       "   'Low sensitivity to bias in prior knowledge']},\n",
       " {'question': 'Are the annotations automatic or manually created?',\n",
       "  'free_form_answers': ['Automatic', '']},\n",
       " {'question': 'Do the errors of the model reflect linguistic similarity between different L1s?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'Is the dataset balanced between speakers of different L1s?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How long are the essays on average?',\n",
       "  'free_form_answers': ['204 tokens', '']},\n",
       " {'question': 'How large are the textual descriptions of entities?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'What neural models are used to encode the text?',\n",
       "  'free_form_answers': ['NBOW, LSTM, attentive LSTM', '']},\n",
       " {'question': 'What baselines are used for comparison?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What datasets are used to evaluate this paper?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Which approach out of two proposed in the paper performed better in experiments?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What classification baselines are used for comparison?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What TIMIT datasets are used for testing?',\n",
       "  'free_form_answers': ['Once split into 8 subsets (A-H), the test set used are blocks D+H and blocks F+H',\n",
       "   '']},\n",
       " {'question': 'How does this approach compares to the state-of-the-art results on these tasks?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'What state-of-the-art results are achieved?',\n",
       "  'free_form_answers': ['F1 score of 92.19 on homographic pun detection, 80.19 on homographic pun location, 89.76 on heterographic pun detection.',\n",
       "   'for the homographic dataset F1 score of 92.19 and 80.19 on detection and location and for the heterographic dataset F1 score of 89.76 on detection']},\n",
       " {'question': 'What baselines do they compare with?',\n",
       "  'free_form_answers': ['They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF.']},\n",
       " {'question': 'What datasets are used in evaluation?',\n",
       "  'free_form_answers': ['',\n",
       "   'A homographic and heterographic benchmark datasets by BIBREF9.']},\n",
       " {'question': 'What is the tagging scheme employed?',\n",
       "  'free_form_answers': ['A new tagging scheme that tags the words before and after the pun as well as the pun words.',\n",
       "   '']},\n",
       " {'question': 'How they extract \"structured answer-relevant relation\"?',\n",
       "  'free_form_answers': ['Using the OpenIE toolbox and applying heuristic rules to select the most relevant relation.',\n",
       "   '']},\n",
       " {'question': 'How big are significant improvements?',\n",
       "  'free_form_answers': ['Metrics show better results on all metrics compared to baseline except Bleu1  on Zhou split (worse by 0.11 compared to baseline). Bleu1 score on DuSplit is 45.66 compared to best baseline 43.47, other metrics on average by 1']},\n",
       " {'question': 'What metrics do they use?', 'free_form_answers': ['', '']},\n",
       " {'question': 'On what datasets are experiments performed?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What was the baseline model?', 'free_form_answers': ['']},\n",
       " {'question': 'What dataset did they use?',\n",
       "  'free_form_answers': ['BioASQ  dataset',\n",
       "   'A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers.']},\n",
       " {'question': 'What was their highest recall score?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What was their highest MRR score?',\n",
       "  'free_form_answers': ['0.5115', '']},\n",
       " {'question': 'Does their model suffer exhibit performance drops when incorporating word importance?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How do they measure which words are under-translated by NMT models?',\n",
       "  'free_form_answers': ['They measured the under-translated words with low word importance score as calculated by Attribution.\\nmethod',\n",
       "   '']},\n",
       " {'question': 'How do their models decide how much improtance to give to the output words?',\n",
       "  'free_form_answers': ['',\n",
       "   'They compute the gradient of the output at each time step with respect to the input words to decide the importance.']},\n",
       " {'question': 'Which model architectures do they test their word importance approach on?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do they compare human-level performance to model performance for their dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are the weaknesses found by non-expert annotators of current state-of-the-art NLI models?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What data sources do they use for creating their dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do they use active learning to create their dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do the hashtag and SemEval datasets contain only English data?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What current state of the art method was used for comparison?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What set of approaches to hashtag segmentation are proposed?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How is the dataset of hashtags sourced?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How big is their created dataset?', 'free_form_answers': ['']},\n",
       " {'question': 'Which data do they use as a starting point for the dialogue dataset?',\n",
       "  'free_form_answers': ['A sample from nurse-initiated telephone conversations for congestive heart failure patients undergoing telepmonitoring, post-discharge from the Health Management Unit at Changi General Hospital',\n",
       "   '']},\n",
       " {'question': 'What labels do they create on their dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How do they select instances to their hold-out test set?',\n",
       "  'free_form_answers': ['1264 instances from simulated data, 1280 instances by adding two out-of-distribution symptoms and 944 instances manually delineated from the symptom checking portions of real-word dialogues',\n",
       "   '']},\n",
       " {'question': 'Which models/frameworks do they compare to?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which classification algorithm do they use for s2sL?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Up to how many samples do they experiment with?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do they use pretrained models?', 'free_form_answers': ['']},\n",
       " {'question': 'Do they report results only on English datasets?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How do the authors examine whether a model is robust to noise or not?',\n",
       "  'free_form_answers': ['By evaluating their model on adversarial sets containing misleading sentences',\n",
       "   '']},\n",
       " {'question': 'What type of model is KAR?', 'free_form_answers': ['']},\n",
       " {'question': \"Do the authors hypothesize that humans' robustness to noise is due to their general knowledge?\",\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What type of system does the baseline classification use?',\n",
       "  'free_form_answers': ['',\n",
       "   'Classification system use n-grams, bag-of-words, common words and hashtags as features and SVM, random forest, extra tree and NB classifiers.']},\n",
       " {'question': 'What experiments were carried out on the corpus?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How many annotators tagged each text?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Where did the texts in the corpus come from?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the previous state-of-the-art in summarization?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What dataset do they use?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What other models do they compare to?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What language model architectures are used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are the user-defined keywords?',\n",
       "  'free_form_answers': ['Words that a user wants them to appear in the generated output.',\n",
       "   \"terms common to hosts' descriptions of popular Airbnb properties, like 'subway', 'manhattan', or 'parking'\"]},\n",
       " {'question': 'Does the method achieve sota performance on this dataset?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What are the baselines used in the paper?',\n",
       "  'free_form_answers': ['GloVe vectors trained on Wikipedia Corpus with ensembling, and GloVe vectors trained on Airbnb Data without ensembling']},\n",
       " {'question': 'What is the size of the Airbnb?', 'free_form_answers': ['']},\n",
       " {'question': 'How better is performance compared to previous state-of-the-art models?',\n",
       "  'free_form_answers': ['F1 score of 97.5 on MSR and 95.7 on AS',\n",
       "   'MSR: 97.7 compared to 97.5 of baseline\\nAS: 95.7 compared to 95.6 of baseline']},\n",
       " {'question': 'How does Gaussian-masked directional multi-head attention works?',\n",
       "  'free_form_answers': ['pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters',\n",
       "   '']},\n",
       " {'question': 'What is meant by closed test setting?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are strong baselines model is compared to?',\n",
       "  'free_form_answers': ['Baseline models are:\\n- Chen et al., 2015a\\n- Chen et al., 2015b\\n- Liu et al., 2016\\n- Cai and Zhao, 2016\\n- Cai et al., 2017\\n- Zhou et al., 2017\\n- Ma et al., 2018\\n- Wang et al., 2019']},\n",
       " {'question': 'Does the dataset feature only English language data?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What additional features and context are proposed?',\n",
       "  'free_form_answers': ['using tweets that one has replied or quoted to as contextual information',\n",
       "   '']},\n",
       " {'question': 'What learning models are used on the dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What examples of the difficulties presented by the context-dependent nature of online aggression do they authors give?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Do they report results only on English data?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What evidence do the authors present that the model can capture some biases in data annotation and collection?',\n",
       "  'free_form_answers': ['The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate']},\n",
       " {'question': 'Which publicly available datasets are used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What baseline is used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What new fine-tuning methods are presented?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are the existing biases?',\n",
       "  'free_form_answers': ['',\n",
       "   'sampling tweets from specific keywords create systematic and substancial racial biases in datasets']},\n",
       " {'question': 'What biases does their model capture?',\n",
       "  'free_form_answers': ['Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters']},\n",
       " {'question': 'What existing approaches do they compare to?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the benchmark dataset?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What are the two neural embedding models?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'which neural embedding model works better?',\n",
       "  'free_form_answers': ['the CRX model', '']},\n",
       " {'question': 'What is the degree of dimension reduction of the efficient aggregation method?',\n",
       "  'free_form_answers': ['The number of dimensions can be reduced by up to 212 times.']},\n",
       " {'question': 'For which languages do they build word embeddings for?',\n",
       "  'free_form_answers': ['English']},\n",
       " {'question': 'How do they evaluate their resulting word embeddings?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What types of subwords do they incorporate in their model?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which matrix factorization methods do they use?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do they report results only on English data?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What experiments do they use to quantify the extent of interpretability?',\n",
       "  'free_form_answers': ['Human evaluation for interpretability using the word intrusion test and automated evaluation for interpretability using a semantic category-based approach based on the method and category dataset (SEMCAT).',\n",
       "   '']},\n",
       " {'question': 'Along which dimension do the semantically related words take larger values?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What is the additive modification to the objective function?',\n",
       "  'free_form_answers': [\"The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,\",\n",
       "   'An additive term added to the cost function for any one of the words of concept word-groups']},\n",
       " {'question': 'Which dataset do they use?', 'free_form_answers': ['']},\n",
       " {'question': 'Do they evaluate their learned representations on downstream tasks?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which representation learning architecture do they adopt?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'How do they encourage understanding of literature as part of their objective function?',\n",
       "  'free_form_answers': ['They group the existing works in terms of the objective function they optimize - within-tweet relationships, inter-tweet relationships, autoencoder, and weak supervision.']},\n",
       " {'question': 'What are the limitations of existing Vietnamese word segmentation systems?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Why challenges does word segmentation in Vietnamese pose?',\n",
       "  'free_form_answers': ['Acquire very large Vietnamese corpus and build a classifier with it, design a develop a big data warehouse and analytic framework, build a system to incrementally learn new corpora and interactively process feedback.',\n",
       "   '']},\n",
       " {'question': 'How successful are the approaches used to solve word segmentation in Vietnamese?',\n",
       "  'free_form_answers': ['Their accuracy in word segmentation is about 94%-97%.']},\n",
       " {'question': 'Which approaches have been applied to solve word segmentation in Vietnamese?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which two news domains are country-independent?',\n",
       "  'free_form_answers': ['mainstream news and disinformation', '']},\n",
       " {'question': 'How is the political bias of different sources included in the model?',\n",
       "  'free_form_answers': ['By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains',\n",
       "   '']},\n",
       " {'question': 'What are the two large-scale datasets used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are the global network features which quantify different aspects of the sharing process?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Which datasets are used for evaluation?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What previous methods is their model compared to?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Did they use a crowdsourcing platform?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How was the dataset collected?',\n",
       "  'free_form_answers': ['from 3rd to 9th grade science questions collected from 12 US states',\n",
       "   'Used from  science exam questions of the Aristo Reasoning Challenge (ARC) corpus.']},\n",
       " {'question': 'Which datasets do they use?', 'free_form_answers': ['', '']},\n",
       " {'question': 'How effective is their NCEL approach overall?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How do they verify generalization ability?',\n",
       "  'free_form_answers': ['By calculating Macro F1 metric at the document level.',\n",
       "   'by evaluating their model on five different benchmarks']},\n",
       " {'question': 'Do they only use adjacent entity mentions or use more than that in some cases (next to adjacent)?',\n",
       "  'free_form_answers': ['NCEL considers only adjacent mentions.',\n",
       "   'More than that in some cases (next to adjacent) ']},\n",
       " {'question': 'Do the authors mention any downside of lemmatizing input before training ELMo?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What other examples of morphologically-rich languages do the authors give?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Why is lemmatization not necessary in English?',\n",
       "  'free_form_answers': ['Advanced neural architectures and contextualized embedding models learn how to handle spelling and morphology variations.']},\n",
       " {'question': 'How big was the corpora they trained ELMo on?',\n",
       "  'free_form_answers': ['2174000000, 989000000',\n",
       "   '2174 million tokens for English and 989 million tokens for Russian']},\n",
       " {'question': 'What metrics are used for evaluation?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do they use pretrained embeddings?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What dataset is used?',\n",
       "  'free_form_answers': ['English WIKIBIO, French WIKIBIO , German WIKIBIO ',\n",
       "   '']},\n",
       " {'question': 'What is a bifocal attention mechanism?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What does the \"sensitivity\" quantity denote?',\n",
       "  'free_form_answers': ['',\n",
       "   'The expected number of unique outputs a word recognition system assigns to a set of adversarial perturbations ',\n",
       "   '']},\n",
       " {'question': 'What end tasks do they evaluate on?',\n",
       "  'free_form_answers': ['Sentiment analysis and paraphrase detection under adversarial attacks']},\n",
       " {'question': 'What is a semicharacter architecture?',\n",
       "  'free_form_answers': ['A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters',\n",
       "   '']},\n",
       " {'question': 'Do they experiment with offering multiple candidate corrections and voting on the model output, since this seems highly likely to outperform a one-best correction?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Why is the adversarial setting appropriate for misspelling recognition?',\n",
       "  'free_form_answers': ['Adversarial misspellings are a real-world problem']},\n",
       " {'question': 'Why do they experiment with RNNs instead of transformers for this task?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'How do the backoff strategies work?',\n",
       "  'free_form_answers': ['In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.',\n",
       "   'Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.',\n",
       "   'Backoff to \"a\" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK']},\n",
       " {'question': 'What baseline model is used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'Which additional latent variables are used in the model?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which parallel corpora are used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Overall, does having parallel data improve semantic role induction across multiple languages?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do they add one latent variable for each language pair in their Bayesian model?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What does an individual model consist of?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Do they improve on state-of-the-art semantic role induction?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'how many tags do they look at?', 'free_form_answers': ['']},\n",
       " {'question': 'which algorithm was the highest performer?',\n",
       "  'free_form_answers': ['A hybrid model consisting of best performing popularity-based approach with the best similarity-based approach']},\n",
       " {'question': 'how is diversity measured?', 'free_form_answers': ['', '']},\n",
       " {'question': 'how large is the vocabulary?', 'free_form_answers': ['', '']},\n",
       " {'question': 'what dataset was used?',\n",
       "  'free_form_answers': ['',\n",
       "   ' E-book annotation data: editor tags, Amazon search terms, and  Amazon review keywords.']},\n",
       " {'question': 'what algorithms did they use?', 'free_form_answers': ['']},\n",
       " {'question': 'How does their ensemble method work?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How large are the improvements of the Attention-Sum Reader model when using the BookTest dataset?',\n",
       "  'free_form_answers': ['',\n",
       "   'Answer with content missing: (Table 2) Accuracy of best AS reader results including ensembles are 78.4 and 83.7 when trained on BookTest compared to 71.0 and 68.9 when trained on CBT for Named endity and Common noun respectively.']},\n",
       " {'question': 'How do they show there is space for further improvement?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Do they report results only on English data?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What argument components do the ML methods aim to identify?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which machine learning methods are used in experiments?',\n",
       "  'free_form_answers': ['Structural Support Vector Machine', '']},\n",
       " {'question': 'How is the data in the new corpus come sourced?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What argumentation phenomena encounter in actual data are now accounted for by this work?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'What challenges do different registers and domains pose to this task?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'who transcribed the corpus?', 'free_form_answers': []},\n",
       " {'question': 'how was the speech collected?',\n",
       "  'free_form_answers': ['The speech was collected from respondents using an android application.',\n",
       "   '']},\n",
       " {'question': 'what accents are present in the corpus?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'what evaluation protocols are provided?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'what age range is in the data?', 'free_form_answers': []},\n",
       " {'question': 'what is the source of the data?', 'free_form_answers': ['']},\n",
       " {'question': 'what topics did they label?',\n",
       "  'free_form_answers': ['Demographics Age, DiagnosisHistory, MedicationHistory, ProcedureHistory, Symptoms/Signs, Vitals/Labs, Procedures/Results, Meds/Treatments, Movement, Other.',\n",
       "   'Demographics, Diagnosis History, Medication History, Procedure History, Symptoms, Labs, Procedures, Treatments, Hospital movements, and others']},\n",
       " {'question': 'did they compare with other extractive summarization methods?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'what datasets were used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'what levels of document preprocessing are looked at?',\n",
       "  'free_form_answers': ['', 'Level 1, Level 2 and Level 3.']},\n",
       " {'question': 'what keyphrase extraction models were reassessed?',\n",
       "  'free_form_answers': ['Answer with content missing: (LVL1, LVL2, LVL3) \\n- Stanford CoreNLP\\n- Optical Character Recognition (OCR) system, ParsCIT \\n- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion.']},\n",
       " {'question': 'how many articles are in the dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Is this dataset publicly available for commercial use?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How many different phenotypes are present in the dataset?',\n",
       "  'free_form_answers': ['',\n",
       "   'Thirteen different phenotypes are present in the dataset.']},\n",
       " {'question': 'What are 10 other phenotypes that are annotated?',\n",
       "  'free_form_answers': ['Adv. Heart Disease, Adv. Lung Disease, Alcohol Abuse, Chronic Neurologic Dystrophies, Dementia, Depression, Developmental Delay, Obesity, Psychiatric disorders and Substance Abuse']},\n",
       " {'question': 'What are the state of the art models?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which benchmark datasets are used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': \"What are the network's baseline features?\",\n",
       "  'free_form_answers': [' The features extracted from CNN.']},\n",
       " {'question': 'What tasks are used for evaluation?',\n",
       "  'free_form_answers': ['four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German',\n",
       "   '']},\n",
       " {'question': 'HOw does the method perform compared with baselines?',\n",
       "  'free_form_answers': ['On the datasets DE-EN, JA-EN, RO-EN, and EN-DE, the baseline achieves 29.79, 21.57, 32.70, and 26.02  BLEU score, respectively. The 1.5-entmax achieves  29.83, 22.13, 33.10, and 25.89 BLEU score, which is a difference of +0.04, +0.56, +0.40, and -0.13 BLEU score versus the baseline. The Î±-entmax achieves 29.90, 21.74, 32.89, and 26.93 BLEU score, which is a difference of +0.11, +0.17, +0.19, +0.91 BLEU score versus the baseline.']},\n",
       " {'question': 'How does their model improve interpretability compared to softmax transformers?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What baseline method is used?',\n",
       "  'free_form_answers': ['using word2vec to create features that are used as input to the SVM',\n",
       "   '']},\n",
       " {'question': 'What details are given about the Twitter dataset?',\n",
       "  'free_form_answers': ['',\n",
       "   'one of the Twitter datasets is about Turkish mobile network operators, there are positive, neutral and negative labels and provide the total amount plus the distribution of labels']},\n",
       " {'question': 'What details are given about the movie domain dataset?',\n",
       "  'free_form_answers': ['there are 20,244 reviews divided into positive and negative with an average 39 words per review, each one having a star-rating score',\n",
       "   '']},\n",
       " {'question': 'Which hand-crafted features are combined with word2vec?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What word-based and dictionary-based feature are used?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How are the supervised scores of the words calculated?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'what dataset was used?', 'free_form_answers': ['']},\n",
       " {'question': 'how many total combined features were there?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'what pretrained word embeddings were used?',\n",
       "  'free_form_answers': ['Pretrained word embeddings  were not used', '']},\n",
       " {'question': 'What evaluation metrics did look at?',\n",
       "  'free_form_answers': ['precision, recall, F1 and accuracy',\n",
       "   'Response time, resource consumption (memory, CPU, network bandwidth), precision, recall, F1, accuracy.']},\n",
       " {'question': 'What datasets are used?',\n",
       "  'free_form_answers': ['Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.',\n",
       "   'a self-collected financial intents dataset in Portuguese']},\n",
       " {'question': 'What is the state of the art described in the paper?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What GAN models were used as baselines to compare against?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How much improvement is gained from Adversarial Reward Augmented Maximum Likelihood (ARAML)?',\n",
       "  'free_form_answers': ['ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.',\n",
       "   'Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively.']},\n",
       " {'question': \"Is the discriminator's reward made available at each step to the generator?\",\n",
       "  'free_form_answers': []},\n",
       " {'question': 'What is the algorithm used to create word embeddings?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'What is the corpus used for the task?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How is evaluation performed?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What is a normal reading paradigm?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Did they experiment with this new dataset?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What kind of sentences were read?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'why are their techniques cheaper to implement?',\n",
       "  'free_form_answers': ['They use a slightly modified copy of the target to create the pseudo-text instead of full BT to make their technique cheaper',\n",
       "   'They do not require the availability of a backward translation engine.']},\n",
       " {'question': 'what data simulation techniques were introduced?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what is their explanation for the effectiveness of back-translation?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'what dataset is used?',\n",
       "  'free_form_answers': ['',\n",
       "   'Europarl tests from 2006, 2007, 2008; WMT newstest 2014.']},\n",
       " {'question': 'what language pairs are explored?',\n",
       "  'free_form_answers': ['English-German, English-French.',\n",
       "   'English-German, English-French']},\n",
       " {'question': 'what language is the data in?', 'free_form_answers': ['']},\n",
       " {'question': 'Does the experiments focus on a specific domain?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'how many training samples do you have for training?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'Do the answered questions measure for the usefulness of the answer?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What profile metadata is used for this analysis?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are the organic and inorganic ways to show political affiliation through profile changes?',\n",
       "  'free_form_answers': [\"Organic: mention of political parties names in the profile attributes, specific mentions of political handles in the profile attributes.\\nInorganic:  adding Chowkidar to the profile attributes, the effect of changing the profile attribute in accordance with Prime Minister's campaign, the addition of election campaign related keywords to the profile.\",\n",
       "   'Mentioning of political parties names and political twitter handles is the organic way to show political affiliation; adding Chowkidar or its variants to the profile is the inorganic way.']},\n",
       " {'question': 'How do profile changes vary for influential leads and their followers over the social movement?',\n",
       "  'free_form_answers': ['Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values.']},\n",
       " {'question': 'What evaluation metrics do they use?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the size of this dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How do they determine if tweets have been used by journalists?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'how small of a dataset did they train on?',\n",
       "  'free_form_answers': ['', '23085 hours of data']},\n",
       " {'question': 'what was their character error rate?',\n",
       "  'free_form_answers': ['2.49% for  layer-wise training, 2.63% for distillation, 6.26% for transfer learning.',\n",
       "   'Their best model achieved a 2.49% Character Error Rate.']},\n",
       " {'question': 'which lstm models did they compare with?',\n",
       "  'free_form_answers': ['Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers.']},\n",
       " {'question': 'Do they use datasets with transcribed text or do they determine text from the audio?',\n",
       "  'free_form_answers': ['They use text transcription.', 'both']},\n",
       " {'question': 'By how much does their model outperform the state of the art results?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How do they combine audio and text sequences in their RNN?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What was the baseline?', 'free_form_answers': ['', '']},\n",
       " {'question': 'By how much did they improve?',\n",
       "  'free_form_answers': ['They decrease MAE in 0.34']},\n",
       " {'question': 'What dataset did they use?',\n",
       "  'free_form_answers': [' high-quality datasets  from SemEval-2016 âSentiment Analysis in Twitterâ task',\n",
       "   '']},\n",
       " {'question': 'What is the reported agreement for the annotation?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'How many annotators participated?', 'free_form_answers': []},\n",
       " {'question': 'What features are used?', 'free_form_answers': []},\n",
       " {'question': 'What future possible improvements are listed?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which qualitative metric are used for evaluation?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is quantitative improvement of proposed method (the best variant) w.r.t. baseline (the best variant)?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How is \"propaganda\" defined for the purposes of this study?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What metrics are used in evaluation?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Which natural language(s) are studied in this paper?',\n",
       "  'free_form_answers': ['English']},\n",
       " {'question': 'Do they report results only on English data?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What objective function is used in the GAN?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Which datasets are used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What metrics are used for evaluation?',\n",
       "  'free_form_answers': ['Byte-Pair Encoding perplexity  (BPE PPL),\\nBLEU-1,\\nBLEU-4,\\nROUGE-L,\\npercentage of distinct unigram (D-1),\\npercentage of distinct bigrams(D-2),\\nuser matching accuracy(UMA),\\nMean Reciprocal Rank(MRR)\\nPairwise preference over baseline(PP)',\n",
       "   '',\n",
       "   ' Distinct-1/2, UMA = User Matching Accuracy, MRR\\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)']},\n",
       " {'question': 'What natural language(s) are the recipes written in?',\n",
       "  'free_form_answers': ['English', 'English']},\n",
       " {'question': 'What were their results on the new dataset?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What are the baseline models?', 'free_form_answers': ['']},\n",
       " {'question': 'How did they obtain the interactions?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Where do they get the recipes from?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What were the baselines?', 'free_form_answers': ['']},\n",
       " {'question': 'Does RoBERTa outperform BERT?', 'free_form_answers': ['']},\n",
       " {'question': 'Which multiple datasets did they train on during joint training?',\n",
       "  'free_form_answers': ['',\n",
       "   'BioScope Abstracts, SFU, and BioScope Full Papers']},\n",
       " {'question': 'What were the previously reported results?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What is the size of SFU Review corpus?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'What is the size of bioScope corpus?', 'free_form_answers': []},\n",
       " {'question': 'Do they study numerical properties of their obtained vectors (such as orthogonality)?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How do they score phrasal compositionality?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Which translation systems do they compare against?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what are their results on the constructed dataset?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'what evaluation metrics are reported?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what civil field is the dataset about?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what are the state-of-the-art models?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what is the size of the real-world civil case dataset?',\n",
       "  'free_form_answers': ['100 000 documents', '']},\n",
       " {'question': 'what datasets are used in the experiment?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Do they model semantics ', 'free_form_answers': ['', '']},\n",
       " {'question': 'How do they identify discussions of LGBTQ people in the New York Times?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Do they analyze specific derogatory words?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is novel about their document-level encoder?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What rouge score do they achieve?',\n",
       "  'free_form_answers': ['Best results on unigram:\\nCNN/Daily Mail: Rogue F1 43.85\\nNYT: Rogue Recall 49.02\\nXSum: Rogue F1 38.81',\n",
       "   'Highest scores for ROUGE-1, ROUGE-2 and ROUGE-L on CNN/DailyMail test set are 43.85, 20.34 and 39.90 respectively; on the XSum test set 38.81, 16.50 and 31.27 and on the NYT test set 49.02, 31.02 and 45.55']},\n",
       " {'question': 'What are the datasets used for evaluation?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What was their performance on emotion detection?',\n",
       "  'free_form_answers': [\"Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. \"]},\n",
       " {'question': 'Which existing benchmarks did they compare to?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which Facebook pages did they look at?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'LDA is an unsupervised method; is this paper introducing an unsupervised approach to spam detection?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the benchmark dataset and is its quality high?',\n",
       "  'free_form_answers': ['Social Honeypot dataset (public) and Weibo dataset (self-collected); yes',\n",
       "   'Social Honeypot, which is not of high quality']},\n",
       " {'question': 'How do they detect spammers?',\n",
       "  'free_form_answers': ['Extract features from the LDA model and use them in a binary classification task']},\n",
       " {'question': 'Do they use other evaluation metrics besides ROUGE?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is their ROUGE score?', 'free_form_answers': []},\n",
       " {'question': 'What are the baselines?',\n",
       "  'free_form_answers': ['Answer with content missing: (Experimental Setup missing subsections)\\nTo be selected: We compared REFRESH against a baseline which simply selects the first m leading sentences from each document (LEAD) and two neural models similar to ours (see left block in Figure 1), both trained with cross-entropy loss.\\nAnswer: LEAD']},\n",
       " {'question': 'What datasets do they use?',\n",
       "  'free_form_answers': ['', '1 IMDB dataset and 2 Yelp datasets']},\n",
       " {'question': 'What other factors affect the performance?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What are the benchmark attacking methods?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What domains are covered in the corpus?',\n",
       "  'free_form_answers': ['No specific domain is covered in the corpus.']},\n",
       " {'question': 'What is the architecture of their model?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How was the dataset collected?', 'free_form_answers': ['', '']},\n",
       " {'question': 'Which languages are part of the corpus?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How is the quality of the data empirically evaluated? ',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Is the data in CoVoST annotated for dialect?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Is Arabic one of the 11 languages in CoVost?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How big is Augmented LibriSpeech dataset?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'By how much does their best model outperform the state-of-the-art?',\n",
       "  'free_form_answers': ['0.8% F1 better than the best state-of-the-art',\n",
       "   'Best proposed model achieves F1 score of 84.9 compared to best previous result of 84.1.']},\n",
       " {'question': 'Which dataset do they train their models on?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How does their simple voting scheme work?',\n",
       "  'free_form_answers': ['',\n",
       "   'Among all the classes predicted by several models, for each test sentence, class with most votes are picked. In case of a tie, one of the most frequent classes are picked randomly.']},\n",
       " {'question': 'Which variant of the recurrent neural network do they use?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How do they obtain the new context represetation?',\n",
       "  'free_form_answers': ['They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation.']},\n",
       " {'question': 'Does the paper report the performance of the model for each individual language?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the performance of the baseline?',\n",
       "  'free_form_answers': ['M-Bert had 76.6 F1 macro score.',\n",
       "   '75.1% and 75.6% accuracy']},\n",
       " {'question': 'Did they pefrorm any cross-lingual vs single language evaluation?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What was the performance of multilingual BERT?',\n",
       "  'free_form_answers': ['BERT had 76.6 F1 macro score on x-stance dataset.']},\n",
       " {'question': 'What annotations are present in dataset?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What is an unordered text document, do these arise in real-world corpora?',\n",
       "  'free_form_answers': [\"A unordered text document is one where sentences in the document are disordered or jumbled. It doesn't appear that unordered text documents appear in corpora, but rather are introduced as part of processing pipeline.\"]},\n",
       " {'question': 'What kind of model do they use?', 'free_form_answers': ['']},\n",
       " {'question': 'Do they release a data set?', 'free_form_answers': ['', '']},\n",
       " {'question': 'Do they release code?', 'free_form_answers': ['', '']},\n",
       " {'question': 'Which languages do they evaluate on?', 'free_form_answers': []},\n",
       " {'question': 'Are the experts comparable to real-world users?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Are the answers double (and not triple) annotated?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Who were the experts used for annotation?',\n",
       "  'free_form_answers': ['Individuals with legal training', '']},\n",
       " {'question': 'What type of neural model was used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Were other baselines tested to compare with the neural baseline?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Does the paper clearly establish that the challenges listed here exist in this dataset and task?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Is this hashtag prediction task an established task, or something new?',\n",
       "  'free_form_answers': ['established task', '']},\n",
       " {'question': 'What is the word-level baseline?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What other tasks do they test their method on?',\n",
       "  'free_form_answers': ['None']},\n",
       " {'question': 'what is the word level baseline they compare to?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the state of the art system mentioned?',\n",
       "  'free_form_answers': ['Two knowledge-based systems,\\ntwo traditional word expert supervised systems, six recent neural-based systems, and one BERT feature-based system.']},\n",
       " {'question': 'Do they incoprorate WordNet into the model?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Is SemCor3.0 reflective of English language data in general?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Do they use large or small BERT?',\n",
       "  'free_form_answers': ['small BERT', 'small BERT']},\n",
       " {'question': 'How does the neural network architecture accomodate an unknown amount of senses per word?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Which fonts are the best indicators of high quality?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'What kind of model do they use?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Did they release their data set of academic papers?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Do the methods that work best on academic papers also work best on Wikipedia?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': \"What is their system's absolute accuracy?\",\n",
       "  'free_form_answers': ['59.4% on wikipedia dataset, 93.4% on peer-reviewed archive AI papers, 77.1%  on peer-reviewed archive Computation and Language papers, and 79.9% on peer-reviewed archive Machine Learning papers']},\n",
       " {'question': 'Which is more useful, visual or textual features?',\n",
       "  'free_form_answers': ['It depends on the dataset. Experimental results over two datasets reveal that textual and visual features are complementary. ']},\n",
       " {'question': 'Which languages do they use?',\n",
       "  'free_form_answers': ['', 'English']},\n",
       " {'question': 'How large is their data set?',\n",
       "  'free_form_answers': ['a sample of  29,794 wikipedia articles and 2,794 arXiv papers ']},\n",
       " {'question': 'Where do they get their ground truth quality judgments?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which models did they experiment with?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What were their best results on the benchmark datasets?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What were the baselines?', 'free_form_answers': ['']},\n",
       " {'question': 'Which datasets were used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'what datasets were used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'what are the previous state of the art?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what surface-level features are used?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'what linguistics features are used?',\n",
       "  'free_form_answers': ['POS, gender/number and stem POS']},\n",
       " {'question': 'what dataset statistics are provided?',\n",
       "  'free_form_answers': ['More than 2,100 texts were paired with 15 questions each, resulting in a total number of approx. 32,000 annotated questions. 13% of the questions are not answerable.  Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based).  The final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%).',\n",
       "   'Distribution of category labels, number of answerable-not answerable questions, number of text-based and script-based questions, average text, question, and answer length, number of questions per text']},\n",
       " {'question': 'what is the size of their dataset?', 'free_form_answers': ['']},\n",
       " {'question': 'what crowdsourcing platform was used?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'how was the data collected?',\n",
       "  'free_form_answers': ['The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation.']},\n",
       " {'question': \"What is best performing model among author's submissions, what performance it had?\",\n",
       "  'free_form_answers': ['For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively).']},\n",
       " {'question': 'What extracted features were most influencial on performance?',\n",
       "  'free_form_answers': ['Linguistic', '']},\n",
       " {'question': 'Did ensemble schemes help in boosting peformance, by how much?',\n",
       "  'free_form_answers': ['The best ensemble topped the best single model by 0.029 in F1 score on dev (external).',\n",
       "   'They increased F1 Score by 0.029 in Sentence Level Classification, and by 0.044 in Fragment-Level classification']},\n",
       " {'question': 'Which basic neural architecture perform best by itself?',\n",
       "  'free_form_answers': ['BERT']},\n",
       " {'question': 'What participating systems had better results than ones authors submitted?',\n",
       "  'free_form_answers': ['For SLC task : Ituorp, ProperGander and YMJA  teams had better results.\\nFor FLC task: newspeak and Antiganda teams had better results.']},\n",
       " {'question': 'What is specific to multi-granularity and multi-tasking neural arhiteture design?',\n",
       "  'free_form_answers': ['An output layer for each task',\n",
       "   'Multi-tasking is addressed by neural sequence tagger based on LSTM-CRF and linguistic features, while multi-granularity is addressed by ensemble of LSTM-CRF and BERT.']},\n",
       " {'question': 'Do they report results only on English data?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'What aspects of discussion are relevant to instructor intervention, according to the attention mechanism?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What was the previous state of the art for this task?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What type of latent context is used to predict instructor intervention?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Do they report results only on English dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What dataset does this approach achieve state of the art results on?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How much training data from the non-English language is used by the system?',\n",
       "  'free_form_answers': ['No data. Pretrained model is used.']},\n",
       " {'question': 'Is the system tested on low-resource languages?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What languages are the model transferred to?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How is the model transferred to other languages?',\n",
       "  'free_form_answers': ['Build a bilingual language model,   learn the target language specific parameters starting from a pretrained English LM , fine-tune both English and target model to obtain the bilingual LM.']},\n",
       " {'question': 'What metrics are used for evaluation?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What datasets are used for evaluation?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'what are the existing approaches?', 'free_form_answers': ['']},\n",
       " {'question': 'what dataset is used in this paper?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How is keyphrase diversity measured?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How was the StackExchange dataset collected?',\n",
       "  'free_form_answers': ['they obtained computer science related topics by looking at titles and user-assigned tags']},\n",
       " {'question': 'What does the TextWorld ACG dataset contain?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'What is the size of the StackExchange dataset?',\n",
       "  'free_form_answers': ['around 332k questions']},\n",
       " {'question': 'What were the baselines?',\n",
       "  'free_form_answers': ['CopyRNN (Meng et al., 2017), Multi-Task (Ye and Wang, 2018), and TG-Net (Chen et al., 2018b)',\n",
       "   '']},\n",
       " {'question': 'What two metrics are proposed?', 'free_form_answers': ['']},\n",
       " {'question': 'Can the findings of this paper be generalized to a general-purpose task?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Why does the proposed task a good proxy for the general-purpose sequence to sequence tasks?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What was the baseline?', 'free_form_answers': ['']},\n",
       " {'question': \"What was their system's performance?\",\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What other political events are included in the database?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What classifier did they use?', 'free_form_answers': ['']},\n",
       " {'question': 'What labels for antisocial events are available in datasets?',\n",
       "  'free_form_answers': ['The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: \"Don\\'t be rude or hostile to others users.\"']},\n",
       " {'question': 'What are two datasets model is applied to?',\n",
       "  'free_form_answers': ['',\n",
       "   \"An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. \"]},\n",
       " {'question': 'What is the CORD-19 dataset?', 'free_form_answers': ['', '']},\n",
       " {'question': 'How large is the collection of COVID-19 literature?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Which deep learning architecture do they use for sentence segmentation?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How do they utilize unlabeled data to improve model representations?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What is the McGurk effect?',\n",
       "  'free_form_answers': ['a perceptual illusion, where listening to a speech sound while watching a mouth pronounce a different sound changes how the audio is heard',\n",
       "   'When the perception of what we hear is influenced by what we see.']},\n",
       " {'question': 'Are humans and machine learning systems fooled by the same kinds of illusions?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'how many humans evaluated the results?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'what was the baseline?', 'free_form_answers': ['', '']},\n",
       " {'question': 'what phenomena do they mention is hard to capture?',\n",
       "  'free_form_answers': ['Four discourse phenomena - deixis, lexical cohesion, VP ellipsis, and ellipsis which affects NP inflection.']},\n",
       " {'question': 'by how much did the BLEU score improve?',\n",
       "  'free_form_answers': ['On average 0.64 ']},\n",
       " {'question': 'What is NER?',\n",
       "  'free_form_answers': ['',\n",
       "   'Named Entity Recognition, including entities such as proteins, genes, diseases, treatments, drugs, etc. in the biomedical domain']},\n",
       " {'question': 'Does the paper explore extraction from electronic health records?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Does jiant involve datasets for the 50 NLU tasks?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Is jiant compatible with models in any programming language?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What models are used for painting embedding and what for language style transfer?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What applicability of their approach is demonstrated by the authors?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'What limitations do the authors demnostrate of their model?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How does final model rate on Likert scale?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How big is English poem description of the painting dataset?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'What is best BLEU score of language style transfer authors got?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How better does new approach behave than existing solutions?',\n",
       "  'free_form_answers': ['',\n",
       "   'On Coin Collector, proposed model finds shorter path in fewer number of interactions with enironment.\\nOn Cooking World, proposed model uses smallest amount of steps and on average has bigger score and number of wins by significant margin.']},\n",
       " {'question': 'How is trajectory with how rewards extracted?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'On what Text-Based Games are experiments performed?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How do the authors show that their learned policy generalize better than existing solutions to unseen games?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How much is classification performance improved in experiments for low data regime and class-imbalance problems?',\n",
       "  'free_form_answers': ['Low data: SST-5, TREC, IMDB around 1-2 accuracy points better than baseline\\nImbalanced labels: the improvement over the base model increases as the data gets more imbalanced, ranging from around 6 accuracy points on 100:1000 to over 20 accuracy points on 20:1000']},\n",
       " {'question': 'What off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training is adapted?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What subtasks did they participate in?',\n",
       "  'free_form_answers': ['Answer with content missing: (Subscript 1: \"We did not participate in subtask 5 (E-c)\") Authors participated in EI-Reg, EI-Oc, V-Reg and V-Oc subtasks.']},\n",
       " {'question': 'What were the scores of their system?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How was the training data translated?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What dataset did they use?',\n",
       "  'free_form_answers': [' Selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment provided by organizers and  tweets translated form English to Spanish.',\n",
       "   '']},\n",
       " {'question': 'What other languages did they translate the data from?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What semi-supervised learning is applied?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How were the datasets annotated?', 'free_form_answers': ['']},\n",
       " {'question': 'What are the 12 languages covered?',\n",
       "  'free_form_answers': ['Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese',\n",
       "   'Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese']},\n",
       " {'question': 'Does the corpus contain only English documents?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What type of evaluation is proposed for this task?',\n",
       "  'free_form_answers': ['Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2']},\n",
       " {'question': 'What baseline system is proposed?',\n",
       "  'free_form_answers': ['Answer with content missing: (Baseline Method section) We implemented a simple approach inspired by previous work on concept map generation and keyphrase extraction.']},\n",
       " {'question': 'How were crowd workers instructed to identify important elements in large document collections?',\n",
       "  'free_form_answers': ['',\n",
       "   'They break down the task of importance annotation to the level of single propositions and obtain a score for each proposition indicating its importance in a document cluster, such that a ranking according to the score would reveal what is most important and should be included in a summary.']},\n",
       " {'question': 'Which collections of web documents are included in the corpus?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How do the authors define a concept map?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Is the LSTM baseline a sub-word model?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How is pseudo-perplexity defined?',\n",
       "  'free_form_answers': ['Answer with content missing: (formulas in selection): Pseudo-perplexity is perplexity where conditional joint probability is approximated.']},\n",
       " {'question': 'What is the model architecture used?',\n",
       "  'free_form_answers': ['LSTM to encode the question, VGG16 to extract visual features. The outputs of LSTM and VGG16 are multiplied element-wise and sent to a softmax layer.',\n",
       "   '']},\n",
       " {'question': 'How is the data used for training annotated?',\n",
       "  'free_form_answers': ['The number of redundant answers to collect from the crowd is predicted to efficiently capture the diversity of all answers from all visual questions.']},\n",
       " {'question': 'what quantitative analysis is done?',\n",
       "  'free_form_answers': ['Answer with content missing: (Evaluation section) Given that in CLIR the primary goal is to get a better ranked list of documents against a translated query, we only report Mean Average Precision (MAP).']},\n",
       " {'question': 'what are the baselines?', 'free_form_answers': ['', '']},\n",
       " {'question': 'Do they report results only on English data?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What machine learning and deep learning methods are used for RQE?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'by how much did nus outperform abus?',\n",
       "  'free_form_answers': ['Average success rate is higher by 2.6 percent points.']},\n",
       " {'question': 'what corpus is used to learn behavior?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which dataset has been used in this work?',\n",
       "  'free_form_answers': ['',\n",
       "   'The Reuters-8 dataset (with stop words removed)']},\n",
       " {'question': 'What can word subspace represent?',\n",
       "  'free_form_answers': ['Word vectors, usually in the context of others within the same class']},\n",
       " {'question': 'How big are improvements of small-scale unbalanced datasets when sentence representation is enhanced with topic information?',\n",
       "  'free_form_answers': ['',\n",
       "   'it has 0.024 improvement in accuracy comparing to ELMO Only and  0.006 improvement in F1 score  comparing to ELMO Only too']},\n",
       " {'question': 'To what baseline models is proposed model compared?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How big is dataset for testing?',\n",
       "  'free_form_answers': ['30 terms, each term-sanse pair has around 15 samples for testing']},\n",
       " {'question': 'What existing dataset is re-examined and corrected for training?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What are the qualitative experiments performed on benchmark datasets?',\n",
       "  'free_form_answers': ['Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.',\n",
       "   '']},\n",
       " {'question': 'How does this approach compare to other WSD approaches employing word embeddings?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What tasks did they use to evaluate performance for male and female speakers?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What is the goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which corpora does this paper analyse?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How many categories do authors define for speaker role?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How big is imbalance in analyzed corpora?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What are four major corpora of French broadcast?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What did the best systems use for their model?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'What were their results on the classification and regression tasks',\n",
       "  'free_form_answers': ['', 'F1 score result of 0.8099']},\n",
       " {'question': 'Do the authors conduct experiments on the tasks mentioned?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Did they collect their own datasets?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What data do they look at?', 'free_form_answers': ['']},\n",
       " {'question': 'What language do they explore?', 'free_form_answers': ['', '']},\n",
       " {'question': 'Do they report results only on English datasets?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which hyperparameters were varied in the experiments on the four tasks?',\n",
       "  'free_form_answers': ['number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding',\n",
       "   '']},\n",
       " {'question': 'Which other hyperparameters, other than number of clusters are typically evaluated in this type of research?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How were the cluster extracted? ',\n",
       "  'free_form_answers': ['Word clusters are extracted using k-means on word embeddings']},\n",
       " {'question': 'what were the evaluation metrics?',\n",
       "  'free_form_answers': ['',\n",
       "   'Unlabeled sentence-level F1, perplexity, grammatically judgment performance']},\n",
       " {'question': 'what are the state of the art methods?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'what english datasets were used?',\n",
       "  'free_form_answers': ['Answer with content missing: (Data section) Penn Treebank (PTB)']},\n",
       " {'question': 'which chinese datasets were used?',\n",
       "  'free_form_answers': ['Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)']},\n",
       " {'question': 'What were their distribution results?',\n",
       "  'free_form_answers': ['Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different']},\n",
       " {'question': 'How did they determine fake news tweets?',\n",
       "  'free_form_answers': ['an expert annotator determined if the tweet fell under a specific category',\n",
       "   '']},\n",
       " {'question': 'What is their definition of tweets going viral?',\n",
       "  'free_form_answers': ['Viral tweets are the ones that are retweeted more than 1000 times',\n",
       "   'those that contain a high number of retweets']},\n",
       " {'question': 'What are the characteristics of the accounts that spread fake news?',\n",
       "  'free_form_answers': ['Accounts that spread fake news are mostly unverified, recently created and have on average high friends/followers ratio',\n",
       "   '']},\n",
       " {'question': 'What is the threshold for determining that a tweet has gone viral?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How is the ground truth for fake news established?',\n",
       "  'free_form_answers': ['Ground truth is not established in the paper']},\n",
       " {'question': 'What was the baseline?', 'free_form_answers': []},\n",
       " {'question': 'Which three discriminative models did they use?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what NMT models did they compare with?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Where does the ancient Chinese dataset come from?',\n",
       "  'free_form_answers': ['',\n",
       "   'Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet ']},\n",
       " {'question': 'How many different characters were in dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': \"How does dataset model character's profiles?\",\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How big is the difference in performance between proposed model and baselines?',\n",
       "  'free_form_answers': ['Metric difference between Aloha and best baseline score:\\nHits@1/20: +0.061 (0.3642 vs 0.3032)\\nMRR: +0.0572(0.5114 vs 0.4542)\\nF1: -0.0484 (0.3901 vs 0.4385)\\nBLEU: +0.0474 (0.2867 vs 0.2393)']},\n",
       " {'question': 'What baseline models are used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'Was PolyReponse evaluated against some baseline?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What metric is used to evaluate PolyReponse system?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'How does PolyResponse architecture look like?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'In what 8 languages is PolyResponse engine used for restourant search and booking system?',\n",
       "  'free_form_answers': ['English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian',\n",
       "   '']},\n",
       " {'question': 'Why masking words in the decoder is helpful?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What is the ROUGE score of the highest performing model?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How are the different components of the model trained? Is it trained end-to-end?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'When is this paper published?', 'free_form_answers': []},\n",
       " {'question': 'Can their indexing-based method be applied to create other QA datasets in other domains, and not just Wikipedia?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How many question types do they find in the datasets analyzed?',\n",
       "  'free_form_answers': ['', '7']},\n",
       " {'question': 'How do they analyze contextual similaries across datasets?',\n",
       "  'free_form_answers': ['They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap.']},\n",
       " {'question': 'What were their performance results?',\n",
       "  'free_form_answers': ['best model achieves 0.94 F1 score for Wikipedia and Twitter datasets and 0.95 F1 on Formspring dataset']},\n",
       " {'question': 'What cyberbulling topics did they address?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Were any of the pipeline components based on deep learning models?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How is the effectiveness of this pipeline approach evaluated?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What is the size of the parallel corpus used to train the model constraints?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How does enforcing agreement between parse trees work across different languages?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What datasets are used to assess the performance of the system?',\n",
       "  'free_form_answers': ['',\n",
       "   'LORELEI datasets of Uzbek, Mandarin and Turkish']},\n",
       " {'question': 'How is the vocabulary of word-like or phoneme-like units automatically discovered?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'IS the graph representation supervised?',\n",
       "  'free_form_answers': ['The graph representation appears to be semi-supervised. It is included in the learning pipeline for the medical recommendation, where the attention model is learned. (There is some additional evidence that is unavailable in parsed text)']},\n",
       " {'question': 'Is the G-BERT model useful beyond the task considered?',\n",
       "  'free_form_answers': ['There is nothing specific about the approach that depends on medical recommendations. The approach combines graph data and text data into a single embedding.',\n",
       "   'It learns a representation of medical records. The learned representation (embeddings) can be used for other predictive tasks involving information from electronic health records.']},\n",
       " {'question': 'How well did the baseline perform?', 'free_form_answers': ['']},\n",
       " {'question': 'What is the baseline?', 'free_form_answers': ['', '']},\n",
       " {'question': 'what methods were used to reduce data sparsity effects?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'what was the baseline?', 'free_form_answers': ['', '']},\n",
       " {'question': 'did they collect their own data?', 'free_form_answers': ['']},\n",
       " {'question': 'what japanese-vietnamese dataset do they use?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How do they measure style transfer success?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'Do they introduce errors in the data or does the data already contain them?',\n",
       "  'free_form_answers': ['', 'Data already contain errors']},\n",
       " {'question': 'What error types is their model more reliable for?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How does their parallel data differ in terms of style?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'How do they split text to obtain sentence levels?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'Do they experiment with their proposed model on any other dataset other than MovieQA?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the difference of the proposed model with a standard RNN encoder-decoder?',\n",
       "  'free_form_answers': ['Introduce a \"Refinement Adjustment LSTM-based component\" to the decoder']},\n",
       " {'question': 'Does the model evaluated on NLG datasets or dialog datasets?',\n",
       "  'free_form_answers': ['NLG datasets', 'NLG datasets']},\n",
       " {'question': 'What tasks do they experiment with?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What is the meta knowledge specifically?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'Are there elements, other than pitch, that can potentially result in out of key converted singing?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'How is the quality of singing voice measured?',\n",
       "  'free_form_answers': ['',\n",
       "   'Automatic: Normalized cross correlation (NCC)\\nManual: Mean Opinion Score (MOS)']},\n",
       " {'question': 'what data did they use?', 'free_form_answers': ['', '']},\n",
       " {'question': 'what previous RNN models do they compare with?',\n",
       "  'free_form_answers': ['Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM']},\n",
       " {'question': 'What are examples of these artificats?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'What are the languages they use in their experiment?',\n",
       "  'free_form_answers': ['English\\nFrench\\nSpanish\\nGerman\\nGreek\\nBulgarian\\nRussian\\nTurkish\\nArabic\\nVietnamese\\nThai\\nChinese\\nHindi\\nSwahili\\nUrdu\\nFinnish',\n",
       "   '']},\n",
       " {'question': 'Does the professional translation or the machine translation introduce the artifacts?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Do they recommend translating the premise and hypothesis together?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Is the improvement over state-of-the-art statistically significant?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What are examples of these artifacts?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What languages do they use in their experiments?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How much higher quality is the resulting annotated data?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How do they match annotators to instances?',\n",
       "  'free_form_answers': ['Annotations from experts are used if they have already been collected.']},\n",
       " {'question': 'How much data is needed to train the task-specific encoder?',\n",
       "  'free_form_answers': ['57,505 sentences', '57,505 sentences']},\n",
       " {'question': 'What kind of out-of-domain data?', 'free_form_answers': []},\n",
       " {'question': 'Is an instance a sentence or an IE tuple?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Who are the crowdworkers?',\n",
       "  'free_form_answers': ['people in the US that use Amazon Mechanical Turk',\n",
       "   '']},\n",
       " {'question': 'Which toolkits do they use?', 'free_form_answers': ['', '']},\n",
       " {'question': 'Which sentiment class is the most accurately predicted by ELS systems?',\n",
       "  'free_form_answers': ['neutral sentiment']},\n",
       " {'question': 'Is datasets for sentiment analysis balanced?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What measures are used for evaluation?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'what were the baselines?',\n",
       "  'free_form_answers': ['BOW-LR, BOW-RF. TFIDF-RF, TextCNN, C-TextCNN', '']},\n",
       " {'question': 'what datasets were used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What BERT models are used?',\n",
       "  'free_form_answers': ['BERT-base, BERT-large, BERT-uncased, BERT-cased']},\n",
       " {'question': 'What are the sources of the datasets?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What labels does the dataset have?', 'free_form_answers': ['']},\n",
       " {'question': 'Do they evaluate on English only datasets?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What experiments are used to demonstrate the benefits of this approach?',\n",
       "  'free_form_answers': ['',\n",
       "   'Calculate test log-likelihood on the three considered datasets']},\n",
       " {'question': 'What hierarchical modelling approach is used?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'How do co-purchase patterns vary across seasons?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'Which words are used differently across ArXiv?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What is future work planed?', 'free_form_answers': ['']},\n",
       " {'question': 'What is this method improvement over the best performing state-of-the-art?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Which baselines are used for evaluation?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Did they used dataset from another domain for evaluation?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'How is sensationalism scorer trained?',\n",
       "  'free_form_answers': ['', '']},\n",
       " {'question': 'Which component is the least impactful?',\n",
       "  'free_form_answers': ['Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets.']},\n",
       " {'question': 'Which component has the greatest impact on performance?',\n",
       "  'free_form_answers': ['Increasing number of message passing iterations showed consistent improvement in performance - around 1 point improvement compared between 1 and 4 iterations',\n",
       "   '']},\n",
       " {'question': 'What is the state-of-the-art system?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'Which datasets are used?', 'free_form_answers': ['', '']},\n",
       " {'question': 'What is the message passing framework?',\n",
       "  'free_form_answers': ['It is a framework used to describe algorithms for neural networks represented as graphs. Main idea is that that representation of each vertex is updated based on messages from its neighbors.']},\n",
       " {'question': 'What other evaluation metrics are looked at?',\n",
       "  'free_form_answers': ['']},\n",
       " {'question': 'What is the best reported system?',\n",
       "  'free_form_answers': ['Gaze Sarcasm using Multi Instance Logistic Regression.',\n",
       "   '']},\n",
       " {'question': 'What kind of stylistic features are obtained?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'What traditional linguistics features did they use?',\n",
       "  'free_form_answers': []},\n",
       " {'question': 'What cognitive features are used?',\n",
       "  'free_form_answers': ['Readability (RED),  Number of Words (LEN), Avg. Fixation Duration (FDUR), Avg. Fixation Count (FC), Avg. Saccade Length (SL), Regression Count (REG), Skip count (SKIP), Count of regressions from second half\\nto first half of the sentence (RSF), Largest Regression Position (LREG),  Edge density of the saliency gaze\\ngraph (ED),  Fixation Duration at Left/Source\\n(F1H, F1S),  Fixation Duration at Right/Target\\n(F2H, F2S),  Forward Saccade Word Count of\\nSource (PSH, PSS),  Forward SaccadeWord Count of Destination\\n(PSDH, PSDS), Regressive Saccade Word Count of\\nSource (RSH, RSS),  Regressive Saccade Word Count of\\nDestination (RSDH, RSDS)']},\n",
       " ...]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_questions_free_form_answers(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    " j"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
