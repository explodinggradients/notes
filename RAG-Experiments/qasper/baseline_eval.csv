,question,rag_answer,rag_contexts,llm_answer,llm_contexts,ground_truth,answer_correctness_llm,answer_correctness_rag
0,What are some linguistic signals of dogmatic language?,"Pronouns, verb tense, sentiment (positive or negative), negative emotion, swearing, interrogative language, and negation are some linguistic signals of dogmatic language.","['From a behavioral standpoint,\ndogmatic people solve problems differently, spend-\ning less time framing a problem and expressing more\ncertainty in their solution (Lohman, 2010). Here we\nsimilarly examine how user behaviors on Reddit re-\nlate to a language model of dogmatism.\nErtel sought to capture dogmatism linguistically,\nthough a small lexicon of words that correspond\nwith high-level concepts like certainty and compro-\nmise (1985). McKenny then used this dictionary to\nrelate dogmatism to argument quality in student es-\nsays (2005). Our work expands on this approach,\napplying supervised models based on a broader set\nof linguistic categories to identify dogmatism in text.\nOther researchers have studied topics similar to\ndogmatism, such as signals of cognitive style in\nright-wing political thought (Van Hiel et al., 2010),\nthe language used by trolls on social media (Cheng\net al., 2015), or what makes for impartial language\non twitter (Zafar et al., 2016). A similar ﬂavor of\nwork has examined linguistic models that capture\npoliteness (Danescu-Niculescu-Mizil et al., 2013),\ndeception (Ott et al., 2011), and authority (Gilbert,\n2012). We took inspiration from these models when\nconstructing the feature sets in our work.\nFinally, while we examine what makes an opin-\nion dogmatic, other work has pushed further into the\nstructure of arguments, for example classifying their\njustiﬁcations (Hasan and Ng, 2014), or what makes\nan argument likely to win (Tan et al., 2016). Our'
 'ers, often in a strongly opinionated way (“you are a\nmoron” or “they are keeping us down”). Other pro-\nnoun types do not show signiﬁcant relationships.\nLike pronouns, verb tense can reveal subtle sig-\nnals in language use, such as the tendency of medi-\ncal inpatients to focus on the past (Wolf et al., 2007).\nOn social media, comments written in the present\ntense are more likely to be oriented towards a user’s\ncurrent interaction (“this isall so stupid”), creating\nopportunities to signal dogmatism. Alternatively,\ncomments in the past tense are more likely to re-\nfer to outside experiences (“it wasan awful party”),\nspeaking less to a user’s stance towards an ongoing\ndiscussion. We ﬁnd present tense is a positive sig-\nnal for dogmatism (1.11 odds) and past tense is a\nnegative signal (0.69 odds).\nDogmatic language can be either positively or\nnegatively charged in sentiment: for example, con-\nsider the positive statement “ Trump is the SAVIOR\nof this country!!! ” or the negative statement “ Are\nyou REALLY that stupid?? Education is the only\nway out of this horrible mess. It’s hard to imagine\nhow anyone could be so deluded. ” In diverse com-\nmunities, where people hold many different kinds\nof opinions, dogmatic opinions will often tend to\ncome into conﬂict with one another (McCluskey and\nHmielowski, 2012), producing a greater likelihood\nof negative sentiment. Perhaps for this reason, neg-\native emotion (2.09 odds) and swearing (3.80 odds)\nare useful positive signals of dogmatism, while pos-\nitive emotion shows no signiﬁcant relationship.\nFinally, we ﬁnd that interrogative language (1.12\nodds) and negation (1.35 odds) are two additional\npositive signals of dogmatism. While interrogative\nwords like “how” or “what” have many benign uses,\nthey disproportionately appear in our data in the\nform of rhetorical or emotionally charged questions,\nsuch as “how can anyone be that dumb?”\nMany of these linguistic signals are correlated\nwith each other, suggesting that dogmatism is the\ncumulative effect of many component relationships.\nFor example, consider the relatively non-dogmatic\nstatement: “I think the reviewers are wrong in this\ninstance.” Removing signals of insight , we have:\n“the reviewers are wrong in this instance,” which\nis slightly more dogmatic. Then removing relativ-\nity, we have: “the reviewers are wrong.” And ﬁ-\nnally, adding certainty , we have a dogmatic state-Classiﬁer In-domain Cross-domain\nBOW 0.853 0.776\nSENT 0.677 0.646\nLING 0.801 0.728\nBOW + SENT 0.860 0.783\nBOW + LING 0.881 0.791\nTable 2: The AUC scores for dogmatism classiﬁers within and\nacross domains. BOW (bag-of-words) and SENT (sentiment\nsignals) are baselines, and LING uses the linguistic features\nfrom Table 1. We compute in-domain accuracy using 15-fold\ncross-validation on the Reddit dataset, and cross-domain accu-\nracy by training on Reddit and evaluating on comments on arti-\ncles from the New York Times. Chance AUC is 0.5.\nment: “the reviewers are always wrong.”\n4 Predicting dogmatism\nWe now show how we can use the linguistic feature\nsets we have described to build a classiﬁer that pre-\ndicts dogmatism in comments. A predictive model\nfurther validates our feature sets, and also allows us\nto analyze dogmatism in millions of other Reddit\ncomments in a scalable way, with multiple uses in\nongoing, downstream analyses.\nPrediction task. Our goal is (1) to understand\nhow well we can use the strategies in Section 3\nto predict dogmatism, and (2) to test the domain-\nindependence of these strategies. First, we test the\nperformance of our model under cross-validation\nwithin the Reddit comment dataset. We then eval-\nuate the Reddit-based model on a held out corpus\nof New York Times comments annotated using the\ntechnique in Section 2. We did not refer to this sec-\nond dataset during feature construction.\nFor classiﬁcation, we consider two classes of\ncomments: dogmatic andnon-dogmatic .']","1. Absolutist Language: Dogmatic language often involves absolutist terms such as ""always"", ""never"", ""must"", ""cannot"", etc. These words leave no room for alternative perspectives or interpretations.

2. Lack of Qualifiers: Dogmatic language often lacks qualifiers such as ""maybe"", ""perhaps"", ""sometimes"", etc. These words soften statements and make them less absolute.

3. Dismissive Language: Dogmatic language often dismisses other viewpoints outright, using phrases like ""that's wrong"", ""that's nonsense"", etc.

4. Authoritative Language: Dogmatic language often appeals to authority, using phrases like ""it is known that"", ""it is a fact that"", etc. This can be a way of shutting down debate or discussion.

5. Negative Language: Dogmatic language often uses negative language to describe opposing viewpoints, using words like ""misguided"", ""foolish"", etc.

6. Lack of Evidence: Dogmatic language often makes strong claims without providing evidence or reasoning to back them up.

7. Repetitive Language: Dogmatic language often involves repetition of the same points or arguments, which can be a way of reinforcing a particular viewpoint. 

8. Emotional Language: Dogmatic language often uses emotional language to persuade or manipulate listeners, rather than relying on logical arguments or evidence. 

9. Binary Language: Dogmatic language often presents issues in binary terms, such as ""right/wrong"", ""good/bad"", ""us/them"", etc. This can be a way of simplifying complex issues and discouraging nuanced discussion. 

10. Inflexible Language: Dogmatic language is often inflexible, refusing to consider or incorporate new information or perspectives.",[''],"Present tense, past tense, negative emotion, swearing, interrogative language, and negation are some linguistic signals of dogmatic language.",0.4565912931773337,0.8221889232326328
1,How does the previous vowel affect the pronunciation of words in English?,"The previous vowel in English can be affected by a silent 'e', which alters the pronunciation of the vowel that precedes it.","['But despite\nbeing glottographic, in few writing systems do\ngraphemes correspond one-to-one with phonemes.\nThere are cases in which multiple graphemes rep-\nresent a single phoneme, as in the word thein En-\nglish:\nth e\nD @\nThere are cases in which a single grapheme rep-\nresents multiple phonemes, such as syllabaries, in\nwhich each symbol represents a syllable.\nIn many languages, there are silent letters, as in\nthe word hora in Spanish:\nh o r a\n-o R a\nThere are more complicated correspondences,\nsuch as the silent ein English that affects the pro-\nnunciation of the previous vowel, as seen in the\npair of words cape andcap.\nIt is possible for an orthographic system to have\nany or all of the above phenomena while remain-\ning unambiguous. However, some orthographic\n1The Chinese script, in which characters have both phono-\nlogical form and semantic meaning, is the best-known excep-\ntion.'
 'model’s ﬁrst predicted pronunciation is not cor-\nrect, it tends to assign more probability mass to\nguesses that are more similar to the correct one.\nApplying some sort of ﬁltering or reranking of the\nsystem output might therefore lead to better per-\nformance.\nPhoneme Closest phonemes\nb ph,B,F\n@ ˜ a,˘ e,W\ntht:,t,t ”\nx X,G,è\ny y:,Y,I\nô RG,r ”,R\nTable 8: Selected phonemes and the most similar\nphonemes, measured by the cosine similarity of\nthe embeddings learned by the LangID-All model\n7.4 Future Work\nBecause the language ID token is so beneﬁcial to\nperformance, it would be very interesting to ﬁnd\nways to extend a similar beneﬁt to unseen lan-\nguages. One possible way to do so is with tokens\nthat identify something other than the language,\nsuch as typological features about the language’s\nphonemic inventory. This could enable better\nsharing of resources among languages. Such typo-\nlogical knowledge is readily available in databases\nlike Phoible and WALS for a wide variety of lan-\nguages. It would be interesting to explore if any of\nthese features is a good predictor of a language’s\northographic rules.\nIt would also be interesting to apply the arti-\nﬁcial token approach to other problems besides\nmultilingual g2p. One closely related application\nis monolingual English g2p. Some of the ambi-\nguity of English spelling is due to the wide vari-\nety of loanwords in the language, many of which\nhave unassimilated spellings. Knowing the origins\nof these loanwords could provide a useful hint for\nﬁguring out their pronunciations. The etymology\nof a word could be tagged in an analogous way to\nhow language ID is tagged in multilingual g2p.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473 .\nMaximilian Bisani and Hermann Ney. 2008. Joint-\nsequence models for grapheme-to-phoneme conver-\nsion. Speech communication 50(5):434–451.Aliya Deri and Kevin Knight. 2016. Grapheme-to-\nphoneme models for (almost) any language. In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics . volume 1, pages\n399–408.\nMatthew S. Dryer and Martin Haspelmath, editors.\n2013. WALS Online . Max Planck Institute for Evo-\nlutionary Anthropology, Leipzig. http://wals.info/.\nBen Hixon, Eric Schneider, and Susan L Epstein. 2011.\nPhonemic similarity metrics to compare pronunci-\nation methods. In Twelfth Annual Conference of\nthe International Speech Communication Associa-\ntion (INTERSPEECH) . Florence, Italy, pages 825–\n828.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation\n9(8):1735–1780.\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Vi ´egas, Martin Wattenberg, Greg Corrado,\net al. 2016. Google’s multilingual neural machine\ntranslation system: Enabling zero-shot translation.\narXiv preprint arXiv:1611.04558 .\nPreethi Jyothi and Mark Hasegawa-Johnson. 2017.\nLow-resource grapheme-to-phoneme conversion us-\ning recurrent neural networks. In Proc. ICASSP .\nYoung-Bum Kim and Benjamin Snyder. 2012. Uni-\nversal grapheme-to-phoneme prediction over Latin\nalphabets. In Proceedings of the 2012 Joint Con-\nference on Empirical Methods in Natural Language\nProcessing and Computational Natural Language\nLearning . Association for Computational Linguis-\ntics, pages 332–343.\nG. Klein, Y . Kim, Y . Deng, J. Senellart, and A. M.\nRush. 2017. OpenNMT: Open-Source Toolkit\nfor Neural Machine Translation.']","The pronunciation of a word in English can be significantly affected by the previous vowel. This is due to a concept known as ""vowel harmony,"" where vowels in a word can influence each other's pronunciation. For example, in words with a silent 'e' at the end, the previous vowel is often pronounced as a long vowel. In the word ""rate,"" the 'a' is pronounced as a long vowel because of the silent 'e' at the end. Similarly, in words with two vowels together, the first vowel is usually pronounced as a long vowel, and the second vowel is silent. For example, in the word ""boat,"" the 'o' is a long vowel, and the 'a' is silent.",[''],The previous vowel affects the pronunciation of words in English through the phenomenon of vowel reduction.,0.4129590333265587,0.6071439017262947
2,What are the two natural language processing tasks that deal with the concept of simplified language?,Text simplification and readability assessment are the two natural language processing tasks that deal with the concept of simplified language.,"['As\npart of a rule-based approach, the operations car-\nried out typically include replacing complex lex-\nical and syntactic units by simpler ones. A sta-\ntistical approach generally conceptualizes the sim-\npliﬁcation task as one of converting a standard-\nlanguage into a simpliﬁed-language text using ma-\nchine translation. Nisioi et al. (2017) introduced\nneural machine translation to automatic text sim-\npliﬁcation. Research on automatic text simpliﬁ-\ncation is comparatively widespread for languages\nsuch as English, Swedish, Spanish, and Brazilian\nPortuguese. To the authors’ knowledge, no pro-\nductive system exists for German. Suter (2015),\nSuter et al. (2016) presented a prototype of a rule-\nbased system for German.\nMachine learning approaches to both readabil-\nity assessment and text simpliﬁcation rely on\ndata systematically prepared in the form of cor-'
 '2010. Challenging choices for text sim-\npliﬁcation. In Computational Processing of the Por-\ntuguese Language. Proceedings of the 9th Interna-\ntional Conference, PROPOR 2010 , pages 40–50,\nPorto Alegre, Brazil.\nKatarina Heimann M¨ uhlenbock. 2013. I see what\nyou mean: Assessing readability for speciﬁc target\ngroups . Ph.D. thesis, University of Gothenburg.\nWilliam Hwang, Hannaneh Hajishirzi, Mari Ostendorf,\nand Wei Wu. 2015. Aligning Sentences from Stan-\ndard Wikipedia to Simple Wikipedia. In Proceed-\nings of NAACL-HLT , pages 211–217.\nMarcel Just and Patricia Carpenter. 1980. A theory of\nreading. from eye ﬁxations to comprehension. Psy-\nchological review , 87(4):329–354.\nD. Kauchak, O. Mouradi, C. Pentoney, and G. Leroy.\n2014. Text Simpliﬁcation Tools: Using Machine\nLearning to Discover Features that Identify Difﬁcult\nText. In Proceedings of the 47th Hawaii Interna-\ntional Conference on System Sciences , pages 2616–\n2625.\nDavid Klaper, Sarah Ebling, and Martin V olk. 2013.\nBuilding a German/Simple German parallel corpus\nfor automatic text simpliﬁcation. In ACL Workshop\non Predicting and Improving Text Readability for\nTarget Reader Populations , pages 11–19, Soﬁa, Bul-\ngaria.\nSergiu Nisioi, Sanja ˇStajner, Simone Paolo Ponzetto,\nand Liviu P. Dinu. 2017. Exploring neural text sim-\npliﬁcation models. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics , pages 85–91, Vancouver, Canada, July.\nEmily Pitler and Ani Nenkova. 2008. Revisiting Read-\nability: A Uniﬁed Framework for Predicting Text\nQuality. In Proceedings of the Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP ’08) , pages 186–195, Honolulu, Hawaii.\nHelmut Schmid. 1995. Improvements in part-of-\nspeech tagging with an application to German. In\nProceedings of the EACL’95 SIGDAT Workshop ,\npages 47–50, Dublin, Ireland.']",Text simplification and Machine translation,[''],The two natural language processing tasks that deal with the concept of simplified language are automatic readability assessment and automatic text simplification.,0.21864069504668132,0.7408674932285024
3,What is the Cloze objective of MLM in cross-lingual pre-training?,The Cloze objective of MLM in cross-lingual pre-training is to predict the masked words that are randomly selected and replaced with the [MASK] token on monolingual corpora.,"['In the pretraining phase, we ﬁrst pretrain MLM on mono-\nlingual corpora of both source and pivot languages, and con-\ntinue to pretrain TLM or the proposed BRLM on the avail-\nable parallel data between source and pivot languages, in\norder to build a cross-lingual encoder shared by the source\nand pivot languages.\nIn the transfer phase, we train pivot →target NMT model\ninitialized by the cross-lingually pre-trained encoder, and\nﬁnally transfer the trained NMT model to source →target\ntranslation thanks to the shared encoder. Note that during\ntraining pivot→target NMT model, we freeze several layers\nof the cross-lingually pre-trained encoder to avoid the de-\ngeneracy issue.\nFor the more complicated scenario that either the source\nside or the target side has multiple languages, the encoder\nand the decoder are also shared across each side languages\nfor efﬁcient deployment of translation between multiple lan-\nguages.\nExperiments\nSetup\nWe evaluate our cross-lingual pre-training based transfer ap-\nproach against several strong baselines on two public datat-\nsets, Europarl (Koehn 2005) and MultiUN (Eisele and Chen\n2010), which contain multi-parallel evaluation data to assess\nthe zero-shot performance. In all experiments, we use BLEU\nas the automatic metric for translation evaluation.1\nDatasets. The statistics of Europarl and MultiUN cor-\npora are summarized in Table 1. For Europarl corpus, we\nevaluate on French-English-Spanish (Fr-En-Es), German-\nEnglish-French (De-En-Fr) and Romanian-English-German\n(Ro-En-De), where English acts as the pivot language, its\nleft side is the source language, and its right side is the target\nlanguage. We remove the multi-parallel sentences between\ndifferent training corpora to ensure zero-shot settings. We\nuse the devtest2006 as the validation set and the test2006 as\nthe test set for Fr→Es and De→Fr. For distant language pair\nRo→De, we extract 1,000 overlapping sentences from new-\nstest2016 as the test set and the 2,000 overlapping sentences\nsplit from the training set as the validation set since there is\nno ofﬁcial validation and test sets. For vocabulary, we use\n60K sub-word tokens based on Byte Pair Encoding (BPE)\n(Sennrich, Haddow, and Birch 2015).\nFor MultiUN corpus, we use four languages: English\n(En) is set as the pivot language, which has parallel data\n1We calculate BLEU scores with the multi-bleu.perl script.'
 'Figure 2: The overview of BRidge Language Modeling (BRLM). The BRLM extends MLM (Lample and Conneau 2019) to\npairs of parallel sentences and leverages explicit alignment information obtained by external aligner tool or additional attention\nlayer to encourage word representation alignment across different languages.\n•Directly translate source sentences into target sentences\nwith the parent model, which beneﬁts from the availabil-\nity of the universal encoder.\nThe key difﬁculty of this method is to ensure the intermedi-\nate representations of the universal encoder are language in-\nvariant. In the rest of this section, we ﬁrst present two exist-\ning methods yet to be explored in zero-shot translation, and\nthen propose a straightforward but effective cross-lingual\npre-training method. In the end, we present the whole train-\ning and inference protocol for transfer.\nMasked and Translation Language Model\nPretraining\nTwo existing cross-lingual pre-training methods, Masked\nLanguage Modeling (MLM) and Translation Language\nModeling (TLM), have shown their effectiveness on XNLI\ncross-lingual classiﬁcation task (Lample and Conneau 2019;\nHuang et al. 2019), but these methods have not been well\nstudied on cross-lingual generation tasks in zero-shot condi-\ntion. We attempt to take advantage of the cross-lingual abil-\nity of the two methods for zero-shot translation.\nSpeciﬁcally, MLM adopts the Cloze objective of\nBERT (Devlin et al. 2018) and predicts the masked words\nthat are randomly selected and replaced with [MASK] to-\nken on monolingual corpus. In practice, MLM takes dif-\nferent language monolingual corpora as input to ﬁnd fea-\ntures shared across different languages. With this method,\nword pieces shared in all languages have been mapped into\na shared space, which makes the sentence representationsacross different languages close (Pires, Schlinger, and Gar-\nrette 2019).\nSince MLM objective is unsupervised and only requires\nmonolingual data, TLM is designed to leverage parallel data\nwhen it is available. Actually, TLM is a simple extension of\nMLM, with the difference that TLM concatenates sentence\npair into a whole sentence, and then randomly masks words\nin both the source and target sentences. In this way, the\nmodel can either attend to surrounding words or to the trans-\nlation sentence, implicitly encouraging the model to align\nthe source and target language representations. Note that al-\nthough each sentence pair is formed into one sentence, the\npositions of the target sentence are reset to count form zero.\nBridge Language Model Pretraining\nAside from MLM and TLM, we propose BRidge Language\nModeling (BRLM) to further obtain word-level representa-\ntion alignment between different languages. This method is\ninspired by the assumption that if the feature spaces of dif-\nferent languages are aligned very well, the masked words\nin the corrupted sentence can also be guessed by the con-\ntext of the correspondingly aligned words on the other side.\nTo achieve this goal, BRLM is designed to strengthen the\nability to infer words across languages based on alignment\ninformation, instead of inferring words within monolingual\nsentence as in MLM or within the pseudo sentence formed\nby concatenating sentence pair as in TLM.\nAs illustrated in Figure 2, BRLM stacks shared encoder\nover both side sentences separately. In particular, we design\ntwo network structures for BRLM, which are divided into']","The Cloze objective of MLM (Masked Language Model) in cross-lingual pre-training is to predict the masked words in a sentence, which helps in understanding the context of different languages and improves the model's ability to understand and translate between multiple languages.",[''],The Cloze objective of MLM in cross-lingual pre-training is to predict the masked words that are randomly selected and replaced with [MASK] token on monolingual corpus.,0.6163231600467447,0.749504966355065
4,What is the main challenge that leads to the failure of zero-shot translation in transfer learning?,"The main challenge that leads to the failure of zero-shot translation in transfer learning is the language space mismatch problem, also known as the domain shift problem.","['tialize a low-resource source →target model ( child ) that is\nfurther optimized with a small amount of available paral-\nlel data. Although this approach has achieved success in\nsome low-resource language pairs, it still performs very\npoorly in extremely low-resource or zero-resource transla-\ntion scenario. Speciﬁcally, Kocmi and Bojar (2018) reports\nthat without any child model training data, the performance\nof the parent model on the child test set is miserable.\nIn this work, we argue that the language space mis-\nmatch problem, also named domain shift problem (Fu et al.\n2015), brings about the zero-shot translation failure in trans-\nfer learning. It is because transfer learning has no explicit\ntraining process to guarantee that the source and pivot lan-\nguages share the same feature distributions, causing that the\nchild model inherited from the parent model fails in such\na situation. For instance, as illustrated in the left of Fig-\nure 1, the points of the sentence pair with the same seman-\ntics are not overlapping in source space, resulting in that\nthe shared decoder will generate different translations de-\nnoted by different points in target space. Actually, transfer\nlearning for NMT can be viewed as a multi-domain problem\nwhere each source language forms a new domain. Minimiz-\ning the discrepancy between the feature distributions of dif-\nferent source languages, i.e., different domains, will ensure\nthe smooth transition between the parent and child models,arXiv:1912.01214v1  [cs.CL]  3 Dec 2019'
 'Cross-lingual Pre-training Based Transfer for Zero-shot Neural\nMachine Translation\nBaijun Ji‡, Zhirui Zhang§, Xiangyu Duan†‡∗, Min Zhang†‡, Boxing Chen§and Weihua Luo§\n†Institute of Artiﬁcial Intelligence, Soochow University, Suzhou, China\n‡School of Computer Science and Technology, Soochow University, Suzhou, China\n§Alibaba DAMO Academy, Hangzhou, China\n‡bjji@stu.suda.edu.cn†{xiangyuduan, minzhang}@suda.edu.cn\n§{zhirui.zzr,boxing.cbx,weihua.luowh}@alibaba-inc.com\nAbstract\nTransfer learning between different language pairs has shown\nits effectiveness for Neural Machine Translation (NMT) in\nlow-resource scenario. However, existing transfer methods\ninvolving a common target language are far from success in\nthe extreme scenario of zero-shot translation, due to the lan-\nguage space mismatch problem between transferor (the par-\nent model) and transferee (the child model) on the source\nside. To address this challenge, we propose an effective trans-\nfer learning approach based on cross-lingual pre-training. Our\nkey idea is to make all source languages share the same fea-\nture space and thus enable a smooth transition for zero-shot\ntranslation. To this end, we introduce one monolingual pre-\ntraining method and two bilingual pre-training methods to\nobtain a universal encoder for different languages. Once the\nuniversal encoder is constructed, the parent model built on\nsuch encoder is trained with large-scale annotated data and\nthen directly applied in zero-shot translation scenario. Exper-\niments on two public datasets show that our approach signif-\nicantly outperforms strong pivot-based baseline and various\nmultilingual NMT approaches.\nIntroduction\nAlthough Neural Machine Translation (NMT) has domi-\nnated recent research on translation tasks (Wu et al. 2016;\nVaswani et al. 2017; Hassan et al. 2018), NMT heavily relies\non large-scale parallel data, resulting in poor performance\non low-resource or zero-resource language pairs (Koehn\nand Knowles 2017). Translation between these low-resource\nlanguages (e.g., Arabic →Spanish) is usually accomplished\nwith pivoting through a rich-resource language (such as En-\nglish), i.e., Arabic (source) sentence is translated to En-\nglish (pivot) ﬁrst which is later translated to Spanish (tar-\nget) (Kauers et al. 2002; de Gispert and Mariño 2006).\nHowever, the pivot-based method requires doubled decoding\ntime and suffers from the propagation of translation errors.\nOne common alternative to avoid pivoting in NMT is\ntransfer learning (Zoph et al. 2016; Nguyen and Chiang\n2017; Kocmi and Bojar 2018; Kim et al. 2019) which lever-\nages a high-resource pivot →target model ( parent ) to ini-\n∗Corresponding Author.\nCopyright c⃝2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The circle and triangle dots represent source sen-\ntences in different language l1andl2, and the square dots\nmeans target sentences in language l3. A sample of transla-\ntion pairs is connected by the dashed line. We would like to\nforce each of the translation pairs has the same latent rep-\nresentation as the right part of the ﬁgure so as to transfer\nl1→l3model directly to l2→l3model.\ntialize a low-resource source →target model ( child ) that is\nfurther optimized with a small amount of available paral-\nlel data. Although this approach has achieved success in\nsome low-resource language pairs, it still performs very\npoorly in extremely low-resource or zero-resource transla-\ntion scenario. Speciﬁcally, Kocmi and Bojar (2018) reports\nthat without any child model training data, the performance\nof the parent model on the child test set is miserable.\nIn this work, we argue that the language space mis-\nmatch problem, also named domain shift problem (Fu et al.\n2015), brings about the zero-shot translation failure in trans-\nfer learning.']",The main challenge that leads to the failure of zero-shot translation in transfer learning is the lack of direct training data for the specific language pair. This makes it difficult for the model to accurately translate between two languages without having seen any examples of such translations during training.,[''],The main challenge that leads to the failure of zero-shot translation in transfer learning is the language space mismatch problem between the transferor (the parent model) and transferee (the child model) on the source side.,0.6124087348484565,0.6148418438928378
5,How does adversarial learning resolve the problem of large divergences between training and test examples in the context of neural networks?,"Adversarial learning resolves the problem of large divergences between training and test examples in the context of neural networks by adding an additional discriminator that classifies the type of source inputs. This discriminator helps in adapting the neural feature extraction to the specific setting by focusing on differentiating between the annotations provided by individual workers. By incorporating this discriminator, adversarial learning aims to minimize the impact of noise from non-expert annotations and ensure that the neural network can generalize well from training to testing data despite the divergences in input features.","['Worker Adversarial\nAdversarial learning has been an effective mechanism to re-\nsolve the problem of the input features between the training\nand test examples having large divergences (Goodfellow et\nal. 2014; Ganin et al. 2016). It has been successfully applied\non domain adaption (Gui et al. 2017), cross-lingual learn-\ning (Chen et al. 2016) and multi-task learning (Liu, Qiu, and\nHuang 2017). All settings involve feature shifting between\nthe training and testing.\nIn this paper, our setting is different. We are using the\nannotations from non-experts, which are noise and can in-\nﬂuence the ﬁnal performances if they are not properly pro-\ncessed. Directly learning based on the resulting corpus may\nadapt the neural feature extraction into the biased annota-\ntions. In this work, we assume that individual workers have\ntheir own guidelines in mind after short training. For exam-\nple, a perfect worker can annotate highly consistently with\nan expert, while common crowdsourcing workers may be\nconfused and have different understandings on certain con-\ntexts. Based on the assumption, we make an adaption for the\noriginal adversarial neural network to our setting.\nOur adaption is very simple. Brieﬂy speaking, the original\nadversarial learning adds an additional discriminator to clas-\nsify the type of source inputs, for example, the domain cate-\ngory in the domain adaption setting, while we add a discrim-\ninator to classify the annotation workers. Solely the features\nfrom the input sentence is not enough for worker classiﬁ-\ncation. The annotation result of the worker is also required.\nThus the inputs of our discriminator are different. Here we\nexploit both the source sentences and the crowd-annotated\nNE labels as basic inputs for the worker discrimination.\nIn the following, we describe the proposed adversarial\nlearning module, including both the submodels and the train-\ning method. As shown by the left part of Figure 1, the\nsubmodel consists of four parts: (1) a common Bi-LSTM\nover input characters; (2) an additional Bi-LSTM to en-\ncode crowd-annotated NE label sequence; (3) a convolu-\ntional neural network (CNN) to extract features for worker\ndiscriminator; (4) output and prediction.\nCommon Bi-LSTM over Characters\nTo build the adversarial part, ﬁrst we create a new bi-\ndirectional LSTM, named by the common Bi-LSTM:\nhcommon\n1hcommon\n2···hcommon\nn =Bi-LSTM (x1x2···xn).(5)\nAs shown in Figure 1, this Bi-LSTM is constructed over\nthe same input character representations of the private Bi-\nLSTM, in order to extract worker independent features.\nThe resulting features of the common Bi-LSTM are used\nfor both NER and the worker discriminator, different with\nthe features of private Bi-LSTM which are used for NER\nonly. As shown in Figure 1, we concatenate the outputs of\nthe common and private Bi-LSTMs together, and then feed\nthe results into the feed-forward combination layer of the\nNER part. Thus Formula 1 can be rewritten as:\nhner\nt=W(hcommon\nt⊕hprivate\nt) +b, (6)\nwhere Wis wider than the original combination because the\nnewly-added hcommon\nt .Noticeably, although the resulting common features are\nused for the worker discriminator, they actually have no ca-\npability to distinguish the workers. Because this part is ex-\nploited to maximize the loss of the worker discriminator, it\nwill be interpreted in the later training subsection. These fea-\ntures are invariant among different workers, thus they can\nhave less noises for NER. This is the goal of adversarial\nlearning, and we hope the NER being able to ﬁnd useful\nfeatures from these worker independent features.\nAdditional Bi-LSTM over Annotated NER Labels\nIn order to incorporate the annotated NE labels to predict the\nexact worker, we build another bi-directional LSTM (named\nby label Bi-LSTM) based on the crowd-annotated NE label\nsequence. This Bi-LSTM is used for worker discriminator\nonly. During the decoding of the testing phase, we will never\nhave this Bi-LSTM, because the worker discriminator is no\nlonger required.'
 'The prediction is to ﬁnd the worker which is responsible for\nthis annotation.\nAdversarial Training\nThe training objective with adversarial neural network is\ndifferent from the baseline model, as it includes the ex-\ntra worker discriminator. Thus the new objective includes\ntwo parts, one being the negative log-likelihood from NER\nwhich is the same as the baseline, and the other being the\nnegative the log-likelihood from the worker discriminator.\nIn order to obtain the negative log-likelihood of the\nworker discriminator, we use softmax to compute the prob-\nability of the actual worker ¯zas well, which is deﬁned by:\np(¯z|X,¯ y) =exp(oworker\n¯z)∑\nzexp(oworkerz), (10)\nwhere zshould enumerate all workers.\nBased on the above deﬁnition of probability, our new ob-\njective is deﬁned as follows:\nR(Θ,Θ′,X,¯ y,¯z) =loss(Θ,X,¯ y)−loss(Θ,Θ′,X)\n=−logp(¯ y|X) + log p(¯z|X,¯ y),(11)\nwhere Θis the set of all model parameters related to NER,\nandΘ′is the set of the remaining parameters which are only\nrelated to the worker discriminator, X,¯ yand¯zare the in-\nput sentence, the crowd-annotated NE labels and the cor-\nresponding annotator for this annotation, respectively. It is\nworth noting that the parameters of the common Bi-LSTM\nare included in the set of Θby deﬁnition.\nIn particular, our goal is not to simply minimize the new\nobjective. Actually, we aim for a saddle point, ﬁnding the\nparameters ΘandΘ′satisfying the following conditions:\nˆΘ = arg min\nΘR(Θ,Θ′,X,¯ y,¯z)\nˆΘ′= arg max\nΘ′R(ˆΘ,Θ′,X,¯ y,¯z)(12)\nwhere the ﬁrst equation aims to ﬁnd one Θthat minimizes\nour new objective R (·), and the second equation aims to ﬁnd\noneΘ′maximizing the same objective.\nIntuitively, the ﬁrst equation of Formula 12 tries to min-\nimize the NER loss, but at the same time maximize the\nworker discriminator loss by the shared parameters of the\ncommon Bi-LSTM. Thus the resulting features of common\nBi-LSTM actually attempt to hurt the worker discrimina-\ntor, which makes these features worker independent since\nthey are unable to distinguish different workers. The second\nequation tries to minimize the worker discriminator loss by\nits own parameter Θ′.\nWe use the standard back-propagation method to train the\nmodel parameters, the same as the baseline model. In order\nto incorporate the term of the argmax part of Formula 12 ,\nwe follow the previous work of adversarial training (Ganin\net al. 2016; Chen et al. 2016; Liu, Qiu, and Huang 2017),\nby introducing a gradient reverse layer between the com-\nmon Bi-LSTM and the CNN module, whose forward does\nnothing but the backward simply negates the gradients.#Sent AvgLen Kappa\nDL-PS 16,948 9.21 0.6033\nUC-MT 2,337 34.97 0.7437\nUC-UQ 2,300 7.69 0.7529\nTable 1: Statistics of labeled datasets.\nExperiments\nData Sets\nWith the purpose of obtaining evaluation datasets from\ncrowd annotators, we collect the sentences from two do-\nmains: Dialog and E-commerce domain. We hire undergrad-\nuate students to annotate the sentences. They are required to\nidentify the predeﬁned types of entities in the sentences. To-\ngether with the guideline document, the annotators are edu-\ncated some tips in ﬁfteen minutes and also provided with 20\nexemplifying sentences.\nLabeled Data: DL-PS. In Dialog domain (DL), we collect\nraw sentences from a chatbot application. And then we ran-\ndomly select 20K sentences as our pool and hire 43 students\nto annotate the sentences. We ask the annotators to label two\ntypes of entities: Person-Name and Song-Name. The anno-\ntators label the sentences independently.']","Adversarial learning resolves the problem of large divergences between training and test examples in the context of neural networks by generating synthetic training examples that are hard for the model to classify correctly. This is done by introducing a second ""adversary"" model that tries to generate examples to fool the first model. The first model then learns to improve its classification accuracy on these hard examples. This process helps to ensure that the model is not overfitting to the training data and can generalize well to unseen test data. It also helps to make the model more robust to noise and small perturbations in the input data.",[''],Adversarial learning resolves the problem of large divergences between training and test examples in the context of neural networks by adding an additional discriminator to classify the type of source inputs.,0.4244786737630293,0.6179219582677786
6,How much of the variance in a word's contextualized representations can be explained by a static embedding?,In no layer of any model can more than 5% of the variance in a word's contextualized representations be explained by a static embedding.,"['Figure 4: The maximum explainable variance (MEV) of a word is the proportion of variance in its contextualized\nrepresentations that can be explained by their ﬁrst principal component (see Deﬁnition 3). Above, we plot the\naverage MEV of uniformly randomly sampled words after adjusting for anisotropy. In no layer of any model can\nmore than 5% of the variance in a word’s contextualized representations be explained by a static embedding.\nStatic Embedding SimLex999 MEN WS353 RW Google MSR SemEval2012(2) BLESS AP\nGloVe 0.194 0.216 0.339 0.127 0.189 0.312 0.097 0.390 0.308\nFastText 0.239 0.239 0.432 0.176 0.203 0.289 0.104 0.375 0.291\nELMo, Layer 1 0.276 0.167 0.317 0.148 0.170 0.326 0.114 0.410 0.308\nELMo, Layer 2 0.215 0.151 0.272 0.133 0.130 0.268 0.132 0.395 0.318\nBERT, Layer 1 0.315 0.200 0.394 0.208 0.236 0.389 0.166 0.365 0.321\nBERT, Layer 2 0.320 0.166 0.383 0.188 0.230 0.385 0.149 0.365 0.321\nBERT, Layer 11 0.221 0.076 0.319 0.135 0.175 0.290 0.149 0.370 0.289\nBERT, Layer 12 0.233 0.082 0.325 0.144 0.184 0.307 0.144 0.360 0.294\nGPT-2, Layer 1 0.174 0.012 0.176 0.183 0.052 0.081 0.033 0.220 0.184\nGPT-2, Layer 2 0.135 0.036 0.171 0.180 0.045 0.062 0.021 0.245 0.184\nGPT-2, Layer 11 0.126 0.034 0.165 0.182 0.031 0.038 0.045 0.270 0.189\nGPT-2, Layer 12 0.140 -0.009 0.113 0.163 0.020 0.021 0.014 0.225 0.172\nTable 1: The performance of various static embeddings on word embedding benchmark tasks. The best result for\neach task is in bold. For the contextualizing models (ELMo, BERT, GPT-2), we use the ﬁrst principal component\nof a word’s contextualized representations in a given layer as its static embedding. The static embeddings created\nusing ELMo and BERT’s contextualized representations often outperform GloVe and FastText vectors.\nearlier, we can create static embeddings for each\nword by taking the ﬁrst principal component (PC)\nof its contextualized representations in a given\nlayer. In Table 1, we plot the performance of\nthese PC static embeddings on several benchmark\ntasks2. These tasks cover semantic similarity,\nanalogy solving, and concept categorization: Sim-\nLex999 (Hill et al., 2015), MEN (Bruni et al.,\n2014), WS353 (Finkelstein et al., 2002), RW (Lu-\nong et al., 2013), SemEval-2012 (Jurgens et al.,\n2012), Google analogy solving (Mikolov et al.,\n2013a) MSR analogy solving (Mikolov et al.,\n2013b), BLESS (Baroni and Lenci, 2011) and AP\n(Almuhareb and Poesio, 2004). We leave out lay-\ners 3 - 10 in Table 1 because their performance is\n2The Word Embeddings Benchmarks package was used\nfor evaluation.between those of Layers 2 and 11.'
 'what extent they can be replaced with static word\nembeddings, if at all. Our work in this paper is\nthus markedly different from most dissections of\ncontextualized representations. It is more similar\nto Mimno and Thompson (2017), which studied\nthe geometry of static word embedding spaces.\n3 Approach\n3.1 Contextualizing Models\nThe contextualizing models we study in this pa-\nper are ELMo, BERT, and GPT-21. We choose\nthe base cased version of BERT because it is most\ncomparable to GPT-2 with respect to number of\nlayers and dimensionality. The models we work\nwith are all pre-trained on their respective lan-\nguage modelling tasks. Although ELMo, BERT,\nand GPT-2 have 2, 12, and 12 hidden layers re-\nspectively, we also include the input layer of each\ncontextualizing model as its 0thlayer. This is be-\ncause the 0thlayer is not contextualized, making\nit a useful baseline against which to compare the\ncontextualization done by subsequent layers.\n3.2 Data\nTo analyze contextualized word representations,\nwe need input sentences to feed into our pre-\ntrained models. Our input data come from the\nSemEval Semantic Textual Similarity tasks from\nyears 2012 - 2016 (Agirre et al., 2012, 2013, 2014,\n2015). We use these datasets because they contain\nsentences in which the same words appear in dif-\nferent contexts. For example, the word ‘dog’ ap-\npears in “A panda dog is running on the road. ”\nand“A dog is trying to get bacon off his back. ”\nIf a model generated the same representation for\n‘dog’ in both these sentences, we could infer that\nthere was no contextualization; conversely, if the\ntwo representations were different, we could infer\nthat they were contextualized to some extent. Us-\ning these datasets, we map words to the list of sen-\ntences they appear in and their index within these\nsentences. We do not consider words that appear\nin less than 5 unique contexts in our analysis.\n3.3 Measures of Contextuality\nWe measure how contextual a word representation\nis using three different metrics: self-similarity ,\nintra-sentence similarity , and maximum explain-\nable variance .\n1We use the pretrained models provided in an earlier ver-\nsion of the PyTorch-Transformers library.Deﬁnition 1 Letwbe a word that appears in\nsentences{s1,...,sn}at indices{i1,...,in}respec-\ntively, such that w=s1[i1] =...=sn[in]. Let fℓ(s,i)\nbe a function that maps s[i]to its representation in\nlayerℓof model f. The self similarity ofwin layer\nℓis\nSelfSimℓ(w) =1\nn2−n∑\nj∑\nk̸=jcos(fℓ(sj,ij),fℓ(sk,ik))\n(1)\nwhere cos denotes the cosine similarity. In other\nwords, the self-similarity of a word win layer ℓis\nthe average cosine similarity between its contextu-\nalized representations across its nunique contexts.\nIf layer ℓdoes not contextualize the representa-\ntions at all, then SelfSimℓ(w) =1 (i.e., the repre-\nsentations are identical across all contexts). The\nmore contextualized the representations are for w,\nthe lower we would expect its self-similarity to be.\nDeﬁnition 2 Letsbe a sentence that is a se-\nquence⟨w1,...,wn⟩ofnwords. Let fℓ(s,i)be a\nfunction that maps s[i]to its representation in layer\nℓof model f. The intra-sentence similarity ofsin\nlayer ℓis\nIntraSim ℓ(s) =1\nn∑\nicos(⃗sℓ,fℓ(s,i))\nwhere ⃗sℓ=1\nn∑\nifℓ(s,i)(2)\nPut more simply, the intra-sentence similarity of a\nsentence is the average cosine similarity between\nits word representations and the sentence vector,\nwhich is just the mean of those word vectors. This\nmeasure captures how context-speciﬁcity mani-\nfests in the vector space.']","The amount of variance in a word's contextualized representations that can be explained by a static embedding can vary greatly depending on the specific context and the complexity of the language model used. However, generally, static embeddings are limited in their ability to capture the full variance as they do not account for the context in which a word is used.",[''],"In no layer of ELMo, BERT, or GPT-2 can more than 5% of the variance in a word's contextualized representations be explained by a static embedding, on average.",0.5975278006098115,0.739750115354584
7,How does pre-ordering the assisting language improve translation quality in a low-resource setting?,"Pre-ordering the assisting language improves translation quality in a low-resource setting by ensuring that the word order of the assisting language matches that of the source language. This alignment of word order helps in creating consistent contextual representations across the languages, which is crucial for successful multilingual neural machine translation.","['The\nnon pre-ordering model is able to forget the word-\norder of English and learn the word order of Indian\nlanguages. We attribute this behavior of the non\npre-ordered model to the phenomenon of catas-\ntrophic forgetting (McCloskey and Cohen, 1989;\nFrench, 1999) which enables the model to learn\nthe word-order of the source language when sufﬁ-\ncient child task parallel corpus is available.\nWe also compare the performance of the ﬁne-\ntuned model with the model trained only on the\navailable source-target parallel corpus with ran-\ndomly initialized weights (No Transfer Learning).\nTransfer learning, with and without pre-ordering,\nis better compared to training only on the small\nsource-target parallel corpus.\n6 Conclusion\nIn this paper, we show that handling word-order\ndivergence between the source and assisting lan-\nguages is crucial for the success of multilingual\nNMT in an extremely low-resource setting. We\nshow that pre-ordering the assisting language to\nmatch the word order of the source language sig-\nniﬁcantly improves translation quality in an ex-\ntremely low-resource setting. If pre-ordering is\nnot possible, ﬁne-tuning on a small source-target'
 'This could lead to the at-\ntention and the decoder layers generating different\ntranslations from the same (parallel) sentence in\nthe source or assisting language. This is undesir-\nable as we want the knowledge to be transferred\nfrom the parent model (assisting source →target)\nto the child model (source →target).\nIn this paper, we propose to pre-order English\nsentences (assisting language sentences) to match\nthe source language word-order and train the par-\nent model on the pre-ordered corpus. Table 1\nshows one of the pre-ordering rules (Ramanathan\net al., 2008) used along with an example sentence\nillustrating the effect of pre-ordering. This will en-\nsure that context of words in the parallel source\nand assisting language sentences are similar, lead-\ning to consistent contextual representations across\nthe source languages. Pre-ordering may also be\nbeneﬁcial for other word order divergence scenar-\nios (e.g., SOV to SVO), but we leave veriﬁcation\nof these additional scenarios for future work.\n4 Experimental Setup\nIn this section, we describe the languages exper-\nimented with, datasets used, the network hyper-']","Pre-ordering the assisting language improves translation quality in a low-resource setting by aligning the sentence structure of the assisting language closer to the target language. This reduces the complexity of the translation task and makes it easier for the translation model to learn and predict accurate translations. It also helps in handling long sentences and complex grammatical structures that are common in many languages. By pre-ordering the assisting language, the model can better understand the context and semantics of the sentence, leading to improved translation quality.",[''],"Pre-ordering the assisting language improves translation quality in a low-resource setting by ensuring that the contextual representations generated by the encoder for the source and assisting languages are consistent, leading to consistent translations.",0.42689108185629654,0.5437791613309404
8,What is a domain-specific attitude?,"A domain-specific attitude is considered to be oriented towards specific areas such as religion or politics, rather than being a deeper personality trait.","['The subreddit with the high-\nest average dogmatism level, cringepics , is a place\nto make fun of socially awkward messages, often\nfrom would-be romantic partners. Dogmatism here\ntends to take the form of “how could someone be\nthat stupid” and is directed at the subject of the post,\nas opposed to other members of the community.\nSimilarly, SubredditDrama is a community where\npeople come to talk about ﬁghts on the internet or\nsocial media. These ﬁghts are often then extended\nin discussion, for example: “If the best you can\ncome up with is that something you did was legal,\nit’s probably time to own up to being an ass. ” The\npresence of this subreddit in our analysis provides\na further sanity check that our model is capturing a\nrobust signal of dogmatism.\n5.2 How do dogmatic beliefs cluster? (R2)\nDogmatism is widely considered to be a domain-\nspeciﬁc attitude (for example, oriented towards re-\nligion or politics) as opposed to a deeper personality\ntrait (Rokeach, 1954). Here we use Reddit as a lens\nto examine this idea more closely. Are users who\nare dogmatic about one topic likely to be dogmatic\nabout others? Do clusters of dogmatism exist around\nparticular topics? To ﬁnd out, we examine the re-'
 '[Shearman and Levine2006] Sachiyo M Shearman and\nTimothy R Levine. 2006. Dogmatism updated: A\nscale revision and validation. Communication Quar-\nterly, 54(3):275–291.\n[Sheng et al.2008] Victor S Sheng, Foster Provost, and\nPanagiotis G Ipeirotis. 2008. Get another label? im-\nproving data quality and data mining using multiple,\nnoisy labelers. Proceedings of the 14th ACM SIGKDD\ninternational conference on Knowledge discovery and\ndata mining , pages 614–622.\n[Swink2011] Nathan Swink. 2011. Dogmatism and\nmoral conviction in individuals: Injustice for all.\n[Tan et al.2016] Chenhao Tan, Vlad Niculae, Cristian\nDanescu-Niculescu-Mizil, and Lillian Lee. 2016.\nWinning arguments: Interaction dynamics and persua-\nsion strategies in good-faith online discussions. In\nProceedings of WWW .\n[Van Hiel et al.2010] Alain Van Hiel, Emma Onraet, and\nSarah De Pauw. 2010. The relationship between\nsocial-cultural attitudes and behavioral measures of\ncognitive style: A meta-analytic integration of studies.\nJournal of personality , 78(6):1765–1800.\n[Wolf et al.2007] Markus Wolf, Jan Sedway, Cynthia M\nBulik, and Hans Kordy. 2007. Linguistic analyses of\nnatural written language: Unobtrusive assessment of\ncognitive style in eating disorders. International Jour-\nnal of Eating Disorders , 40(8):711–717.\n[Yoo and Gretzel2009] Kyung-Hyan Yoo and Ulrike\nGretzel. 2009. Comparison of deceptive and truthful\ntravel reviews. Information and communication\ntechnologies in tourism 2009 , pages 37–47.\n[Zafar et al.2016] Muhammad Bilal Zafar, Krishna P\nGummadi, and Cristian Danescu-Niculescu-Mizil.\n2016. Message impartiality in social media discus-\nsions. In Tenth International AAAI Conference on Web\nand Social Media .']",A domain-specific attitude refers to an individual's attitude or perspective that is specific to a particular area or subject matter. It is a specialized viewpoint that is not generalized but rather focused on a specific field or topic.,[''],"A domain-specific attitude is an attitude that is oriented towards a specific domain, such as religion or politics.",0.8387419337538076,0.6152859327792102
9,What is the process of Hard Alignment in BRLM-HA?,"In the process of Hard Alignment in BRLM-HA, an external aligner tool is first used on the source ↔ pivot parallel data to extract the alignment information of the sentence pair. During model training, some words in the source sentence are randomly masked, and the alignment information is leveraged to obtain the aligned words in the pivot sentence for the masked words. The Transformer encoder is then used to gain hidden states for both the source and pivot sentences. The training objective of BRLM-HA is to predict the masked words by considering not only the surrounding words in the source sentence but also the encoder outputs of the aligned words. This training process is symmetric, where some words in the pivot sentence are masked to obtain the aligned words in the source sentence as well.","['Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-\nSA) according to the way of generating the alignment infor-\nmation. These two structures actually extend MLM into a\nbilingual scenario, with the difference that BRLM leverages\nexternal aligner tool or additional attention layer to explic-\nitly introduce alignment information during model training.\n•Hard Alignment (BRLM-HA). We ﬁrst use exter-\nnal aligner tool on source ↔pivot parallel data to ex-\ntract the alignment information of sentence pair. Dur-\ning model training, given source ↔pivot sentence pair,\nBRLM-HA randomly masks some words in source sen-\ntence and leverages alignment information to obtain the\naligned words in pivot sentence for masked words. Based\non the processed input, BRLM-HA adopts the Trans-\nformer (Vaswani et al. 2017) encoder to gain the hid-\nden states for source and pivot sentences respectively.\nThen the training objective of BRLM-HA is to predict\nthe masked words by not only the surrounding words\nin source sentence but also the encoder outputs of the\naligned words. Note that this training process is also car-\nried out in a symmetric situation, in which we mask some\nwords in pivot sentence and obtain the aligned words in\nthe source sentence.\n•Soft Alignment (BRLM-SA). Instead of using external\naligner tool, BRLM-SA introduces an additional atten-\ntion layer to learn the alignment information together with\nmodel training. In this way, BRLM-SA avoids the effect\ncaused by external wrong alignment information and en-\nables many-to-one soft alignment during model training.\nSimilar with BRLM-HA, the training objective of BRLM-\nSA is to predict the masked words by not only the sur-\nrounding words in source sentence but also the outputs of\nattention layer. In our implementation, the attention layer\nis a multi-head attention layer adopted in Transformer,\nwhere the queries come from the masked source sentence,\nthe keys and values come from the pivot sentence.\nIn principle, MLM and TLM can learn some implicit align-\nment information during model training. However, the align-\nment process in MLM is inefﬁcient since the shared word\npieces only account for a small proportion of the whole cor-\npus, resulting in the difﬁculty of expanding the shared in-\nformation to align the whole corpus. TLM also lacks effort\nin alignment between the source and target sentences since\nTLM concatenates the sentence pair into one sequence, mak-\ning the explicit alignment between the source and target in-\nfeasible. BRLM fully utilizes the alignment information to\nobtain better word-level representation alignment between\ndifferent languages, which better relieves the burden of the\ndomain shift problem .\nTransfer Protocol\nWe consider the typical zero-shot translation scenario in\nwhich a high resource pivot language has parallel data with\nboth source and target languages, while source and target\nlanguages has no parallel data between themselves. Our pro-\nposed cross-lingual pretraining based transfer approach for\nsource→target zero-shot translation is mainly divided into\ntwo phrases: the pretraining phase and the transfer phase.Corpus Language Train Dev Test\nEuroparlDe-En,En-Fr 1M,1M 2,000 2,000\nFr-En,En-Es 1M,1M 2,000 2,000\nRo-En,En-De 0.6M,1.5M 2,000 1,000\nMultiUNAr-En,En-Es\nEn-Ru9.7M,11.3M\n11.6M4,000 4,000\nTable 1: Data Statistics.\nIn the pretraining phase, we ﬁrst pretrain MLM on mono-\nlingual corpora of both source and pivot languages, and con-\ntinue to pretrain TLM or the proposed BRLM on the avail-\nable parallel data between source and pivot languages, in\norder to build a cross-lingual encoder shared by the source\nand pivot languages.\nIn the transfer phase, we train pivot →target NMT model\ninitialized by the cross-lingually pre-trained encoder, and\nﬁnally transfer the trained NMT model to source →target\ntranslation thanks to the shared encoder. Note that during\ntraining pivot→target NMT model, we freeze several layers\nof the cross-lingually pre-trained encoder to avoid the de-\ngeneracy issue.'
 'Figure 2: The overview of BRidge Language Modeling (BRLM). The BRLM extends MLM (Lample and Conneau 2019) to\npairs of parallel sentences and leverages explicit alignment information obtained by external aligner tool or additional attention\nlayer to encourage word representation alignment across different languages.\n•Directly translate source sentences into target sentences\nwith the parent model, which beneﬁts from the availabil-\nity of the universal encoder.\nThe key difﬁculty of this method is to ensure the intermedi-\nate representations of the universal encoder are language in-\nvariant. In the rest of this section, we ﬁrst present two exist-\ning methods yet to be explored in zero-shot translation, and\nthen propose a straightforward but effective cross-lingual\npre-training method. In the end, we present the whole train-\ning and inference protocol for transfer.\nMasked and Translation Language Model\nPretraining\nTwo existing cross-lingual pre-training methods, Masked\nLanguage Modeling (MLM) and Translation Language\nModeling (TLM), have shown their effectiveness on XNLI\ncross-lingual classiﬁcation task (Lample and Conneau 2019;\nHuang et al. 2019), but these methods have not been well\nstudied on cross-lingual generation tasks in zero-shot condi-\ntion. We attempt to take advantage of the cross-lingual abil-\nity of the two methods for zero-shot translation.\nSpeciﬁcally, MLM adopts the Cloze objective of\nBERT (Devlin et al. 2018) and predicts the masked words\nthat are randomly selected and replaced with [MASK] to-\nken on monolingual corpus. In practice, MLM takes dif-\nferent language monolingual corpora as input to ﬁnd fea-\ntures shared across different languages. With this method,\nword pieces shared in all languages have been mapped into\na shared space, which makes the sentence representationsacross different languages close (Pires, Schlinger, and Gar-\nrette 2019).\nSince MLM objective is unsupervised and only requires\nmonolingual data, TLM is designed to leverage parallel data\nwhen it is available. Actually, TLM is a simple extension of\nMLM, with the difference that TLM concatenates sentence\npair into a whole sentence, and then randomly masks words\nin both the source and target sentences. In this way, the\nmodel can either attend to surrounding words or to the trans-\nlation sentence, implicitly encouraging the model to align\nthe source and target language representations. Note that al-\nthough each sentence pair is formed into one sentence, the\npositions of the target sentence are reset to count form zero.\nBridge Language Model Pretraining\nAside from MLM and TLM, we propose BRidge Language\nModeling (BRLM) to further obtain word-level representa-\ntion alignment between different languages. This method is\ninspired by the assumption that if the feature spaces of dif-\nferent languages are aligned very well, the masked words\nin the corrupted sentence can also be guessed by the con-\ntext of the correspondingly aligned words on the other side.\nTo achieve this goal, BRLM is designed to strengthen the\nability to infer words across languages based on alignment\ninformation, instead of inferring words within monolingual\nsentence as in MLM or within the pseudo sentence formed\nby concatenating sentence pair as in TLM.\nAs illustrated in Figure 2, BRLM stacks shared encoder\nover both side sentences separately. In particular, we design\ntwo network structures for BRLM, which are divided into']",Hard Alignment in BRLM-HA (Bidirectional Recurrent Language Model-Hard Alignment) refers to the process where each source word is aligned to exactly one target word in the sequence. This is in contrast to soft alignment where a source word can be aligned to multiple target words. The hard alignment process is typically used in machine translation models to improve the accuracy of translations.,[''],"BRLM-HA uses an external aligner tool on source ↔pivot parallel data to extract the alignment information of sentence pair. During model training, BRLM-HA randomly masks some words in the source sentence and leverages alignment information to obtain the aligned words in the pivot sentence for masked words. Based on the processed input, BRLM-HA adopts the Transformer encoder to gain the hidden states for source and pivot sentences respectively. The training objective of BRLM-HA is to predict the masked words by not only the surrounding words in the source sentence but also the encoder outputs of the aligned words.",0.5212906827875159,0.7818779503403315
10,What is the relationship between layer depth and context-specificity in contextualized word representations?,"The relationship between layer depth and context-specificity in contextualized word representations is that as the layers get deeper in models like ELMo, BERT, and GPT-2, the representations become more context-specific. In ELMo, words in the same sentence become more similar to each other as context-specificity increases in upper layers. In BERT, words become more dissimilar to each other in upper layers but are still more similar than randomly sampled words on average. However, in GPT-2, words in the same sentence are not more similar to each other than two randomly chosen words.","['tations is surprising.\n2. Occurrences of the same word in different\ncontexts have non-identical vector represen-\ntations. Where vector similarity is deﬁned\nas cosine similarity, these representations are\nmore dissimilar to each other in upper lay-\ners. This suggests that, much like how upper\nlayers of LSTMs produce more task-speciﬁc\nrepresentations (Liu et al., 2019a), upper lay-\ners of contextualizing models produce more\ncontext-speciﬁc representations.\n3. Context-speciﬁcity manifests very differently\nin ELMo, BERT, and GPT-2. In ELMo,\nrepresentations of words in the same sen-\ntence grow more similar to each other as\ncontext-speciﬁcity increases in upper layers;\nin BERT, they become more dissimilar to\neach other in upper layers but are still more\nsimilar than randomly sampled words are on\naverage; in GPT-2, however, words in the\nsame sentence are no more similar to each\nother than two randomly chosen words.\n4. After adjusting for the effect of anisotropy,\non average, less than 5% of the variance in a\nword’s contextualized representations can be\nexplained by their ﬁrst principal component.\nThis holds across all layers of all models.\nThis suggests that contextualized representa-\ntions do not correspond to a ﬁnite number\nof word-sense representations, and even in\nthe best possible scenario, static embeddings\nwould be a poor replacement for contextual-\nized ones. Still, static embeddings created\nby taking the ﬁrst principal component of\na word’s contextualized representations out-\nperform GloVe and FastText embeddings on\nmany word vector benchmarks.\nThese insights help justify why the use of contex-\ntualized representations has led to such signiﬁcant\nimprovements on many NLP tasks.\n2 Related Work\nStatic Word Embeddings Skip-gram with neg-\native sampling (SGNS) (Mikolov et al., 2013a)\nand GloVe (Pennington et al., 2014) are among\nthe best known models for generating static word\nembeddings. Though they learn embeddings itera-\ntively in practice, it has been proven that in theory,they both implicitly factorize a word-context ma-\ntrix containing a co-occurrence statistic (Levy and\nGoldberg, 2014a,b). Because they create a single\nrepresentation for each word, a notable problem\nwith static word embeddings is that all senses of a\npolysemous word must share a single vector.\nContextualized Word Representations Given\nthe limitations of static word embeddings, recent\nwork has tried to create context-sensitive word\nrepresentations. ELMo (Peters et al., 2018), BERT\n(Devlin et al., 2018), and GPT-2 (Radford et al.,\n2019) are deep neural language models that are\nﬁne-tuned to create models for a wide range of\ndownstream NLP tasks. Their internal representa-\ntions of words are called contextualized word rep-\nresentations because they are a function of the en-\ntire input sentence. The success of this approach\nsuggests that these representations capture highly\ntransferable and task-agnostic properties of lan-\nguage (Liu et al., 2019a).\nELMo creates contextualized representations of\neach token by concatenating the internal states of\na 2-layer biLSTM trained on a bidirectional lan-\nguage modelling task (Peters et al., 2018). In\ncontrast, BERT and GPT-2 are bi-directional and\nuni-directional transformer-based language mod-\nels respectively. Each transformer layer of 12-\nlayer BERT (base, cased) and 12-layer GPT-2 cre-\nates a contextualized representation of each token\nby attending to different parts of the input sentence\n(Devlin et al., 2018; Radford et al., 2019). BERT\n– and subsequent iterations on BERT (Liu et al.,\n2019b; Yang et al., 2019) – have achieved state-of-\nthe-art performance on various downstream NLP\ntasks, ranging from question-answering to senti-\nment analysis.\nProbing Tasks Prior analysis of contextualized\nword representations has largely been restricted\nto probing tasks (Tenney et al., 2019; Hewitt and\nManning, 2019).'
 'what extent they can be replaced with static word\nembeddings, if at all. Our work in this paper is\nthus markedly different from most dissections of\ncontextualized representations. It is more similar\nto Mimno and Thompson (2017), which studied\nthe geometry of static word embedding spaces.\n3 Approach\n3.1 Contextualizing Models\nThe contextualizing models we study in this pa-\nper are ELMo, BERT, and GPT-21. We choose\nthe base cased version of BERT because it is most\ncomparable to GPT-2 with respect to number of\nlayers and dimensionality. The models we work\nwith are all pre-trained on their respective lan-\nguage modelling tasks. Although ELMo, BERT,\nand GPT-2 have 2, 12, and 12 hidden layers re-\nspectively, we also include the input layer of each\ncontextualizing model as its 0thlayer. This is be-\ncause the 0thlayer is not contextualized, making\nit a useful baseline against which to compare the\ncontextualization done by subsequent layers.\n3.2 Data\nTo analyze contextualized word representations,\nwe need input sentences to feed into our pre-\ntrained models. Our input data come from the\nSemEval Semantic Textual Similarity tasks from\nyears 2012 - 2016 (Agirre et al., 2012, 2013, 2014,\n2015). We use these datasets because they contain\nsentences in which the same words appear in dif-\nferent contexts. For example, the word ‘dog’ ap-\npears in “A panda dog is running on the road. ”\nand“A dog is trying to get bacon off his back. ”\nIf a model generated the same representation for\n‘dog’ in both these sentences, we could infer that\nthere was no contextualization; conversely, if the\ntwo representations were different, we could infer\nthat they were contextualized to some extent. Us-\ning these datasets, we map words to the list of sen-\ntences they appear in and their index within these\nsentences. We do not consider words that appear\nin less than 5 unique contexts in our analysis.\n3.3 Measures of Contextuality\nWe measure how contextual a word representation\nis using three different metrics: self-similarity ,\nintra-sentence similarity , and maximum explain-\nable variance .\n1We use the pretrained models provided in an earlier ver-\nsion of the PyTorch-Transformers library.Deﬁnition 1 Letwbe a word that appears in\nsentences{s1,...,sn}at indices{i1,...,in}respec-\ntively, such that w=s1[i1] =...=sn[in]. Let fℓ(s,i)\nbe a function that maps s[i]to its representation in\nlayerℓof model f. The self similarity ofwin layer\nℓis\nSelfSimℓ(w) =1\nn2−n∑\nj∑\nk̸=jcos(fℓ(sj,ij),fℓ(sk,ik))\n(1)\nwhere cos denotes the cosine similarity. In other\nwords, the self-similarity of a word win layer ℓis\nthe average cosine similarity between its contextu-\nalized representations across its nunique contexts.\nIf layer ℓdoes not contextualize the representa-\ntions at all, then SelfSimℓ(w) =1 (i.e., the repre-\nsentations are identical across all contexts). The\nmore contextualized the representations are for w,\nthe lower we would expect its self-similarity to be.\nDeﬁnition 2 Letsbe a sentence that is a se-\nquence⟨w1,...,wn⟩ofnwords. Let fℓ(s,i)be a\nfunction that maps s[i]to its representation in layer\nℓof model f. The intra-sentence similarity ofsin\nlayer ℓis\nIntraSim ℓ(s) =1\nn∑\nicos(⃗sℓ,fℓ(s,i))\nwhere ⃗sℓ=1\nn∑\nifℓ(s,i)(2)\nPut more simply, the intra-sentence similarity of a\nsentence is the average cosine similarity between\nits word representations and the sentence vector,\nwhich is just the mean of those word vectors. This\nmeasure captures how context-speciﬁcity mani-\nfests in the vector space.']","The relationship between layer depth and context-specificity in contextualized word representations is that as the layer depth increases, the context-specificity of the word representations also increases. This means that deeper layers in the model are more sensitive to the specific context in which words are used, allowing for more accurate and nuanced representations of word meanings.",[''],"The higher the layer, the more context-specific the contextualized representations.",0.729635096077192,0.474206233710641
11,How does the pre-ordering of assisting language sentences help bridge the word order gap in low resource language translation?,"Pre-ordering the assisting language sentences to match the word order of the source language ensures that the context of words in the parallel sentences is similar. This leads to consistent contextual representations across the different source languages, helping to bridge the word order gap in low resource language translation.","['The\nnon pre-ordering model is able to forget the word-\norder of English and learn the word order of Indian\nlanguages. We attribute this behavior of the non\npre-ordered model to the phenomenon of catas-\ntrophic forgetting (McCloskey and Cohen, 1989;\nFrench, 1999) which enables the model to learn\nthe word-order of the source language when sufﬁ-\ncient child task parallel corpus is available.\nWe also compare the performance of the ﬁne-\ntuned model with the model trained only on the\navailable source-target parallel corpus with ran-\ndomly initialized weights (No Transfer Learning).\nTransfer learning, with and without pre-ordering,\nis better compared to training only on the small\nsource-target parallel corpus.\n6 Conclusion\nIn this paper, we show that handling word-order\ndivergence between the source and assisting lan-\nguages is crucial for the success of multilingual\nNMT in an extremely low-resource setting. We\nshow that pre-ordering the assisting language to\nmatch the word order of the source language sig-\nniﬁcantly improves translation quality in an ex-\ntremely low-resource setting. If pre-ordering is\nnot possible, ﬁne-tuning on a small source-target'
 'This could lead to the at-\ntention and the decoder layers generating different\ntranslations from the same (parallel) sentence in\nthe source or assisting language. This is undesir-\nable as we want the knowledge to be transferred\nfrom the parent model (assisting source →target)\nto the child model (source →target).\nIn this paper, we propose to pre-order English\nsentences (assisting language sentences) to match\nthe source language word-order and train the par-\nent model on the pre-ordered corpus. Table 1\nshows one of the pre-ordering rules (Ramanathan\net al., 2008) used along with an example sentence\nillustrating the effect of pre-ordering. This will en-\nsure that context of words in the parallel source\nand assisting language sentences are similar, lead-\ning to consistent contextual representations across\nthe source languages. Pre-ordering may also be\nbeneﬁcial for other word order divergence scenar-\nios (e.g., SOV to SVO), but we leave veriﬁcation\nof these additional scenarios for future work.\n4 Experimental Setup\nIn this section, we describe the languages exper-\nimented with, datasets used, the network hyper-']","Pre-ordering of assisting language sentences helps bridge the word order gap in low resource language translation by rearranging the words in the assisting language to match the word order of the target low-resource language before translation. This makes it easier for the translation model to learn the mapping between the two languages, as it reduces the complexity of the translation task. It also helps to preserve the meaning of the original sentence during translation. This approach is particularly useful when translating between languages with significantly different word orders.",[''],The pre-ordering of assisting language sentences helps bridge the word order gap in low resource language translation by matching the word order of the source language.,0.9099141235252903,0.8409492966540201
12,What approach was used for performing crowd annotation learning in Chinese Named Entity Recognition?,"The approach used for performing crowd annotation learning in Chinese Named Entity Recognition involved utilizing a common Bi-LSTM and a private Bi-LSTM to represent annotator-generic and -specific information, inspired by adversarial learning.","['Adversarial Learning for Chinese NER from Crowd Annotations∗\nYaosheng Yang1, Meishan Zhang4, Wenliang Chen1\nWei Zhang2, Haofen Wang3, Min Zhang1\n1School of Computer Science and Technology, Soochow University, China\n2Alibaba Group and3Shenzhen Gowild Robotics Co. Ltd\n4School of Computer Science and Technology, Heilongjiang University, China\n1ysyang@stu.suda.edu.cn, {wlchen, minzhang}@suda.edu.cn\n4mason.zms@gmail.com,2lantu.zw@alibaba-inc.com,3wang haofen@gowild.cn\nAbstract\nTo quickly obtain new labeled data, we can choose crowd-\nsourcing as an alternative way at lower cost in a short time.\nBut as an exchange, crowd annotations from non-experts may\nbe of lower quality than those from experts. In this paper, we\npropose an approach to performing crowd annotation learning\nfor Chinese Named Entity Recognition (NER) to make full\nuse of the noisy sequence labels from multiple annotators. In-\nspired by adversarial learning, our approach uses a common\nBi-LSTM and a private Bi-LSTM for representing annotator-\ngeneric and -speciﬁc information. The annotator-generic in-\nformation is the common knowledge for entities easily mas-\ntered by the crowd. Finally, we build our Chinese NE tagger\nbased on the LSTM-CRF model. In our experiments, we cre-\nate two data sets for Chinese NER tasks from two domains.\nThe experimental results show that our system achieves better\nscores than strong baseline systems.\nIntroduction\nThere has been signiﬁcant progress on Named Entity Recog-\nnition (NER) in recent years using models based on machine\nlearning algorithms (Zhao and Kit 2008; Collobert et al.\n2011; Lample et al. 2016). As with other Natural Language\nProcessing (NLP) tasks, building NER systems typically re-\nquires a massive amount of labeled training data which are\nannotated by experts. In real applications, we often need to\nconsider new types of entities in new domains where we do\nnot have existing annotated data. For such new types of en-\ntities, however, it is very hard to ﬁnd experts to annotate the\ndata within short time limits and hiring experts is costly and\nnon-scalable, both in terms of time and money.\nIn order to quickly obtain new training data, we can use\ncrowdsourcing as one alternative way at lower cost in a\nshort time. But as an exchange, crowd annotations from non-\nexperts may be of lower quality than those from experts. It\nis one biggest challenge to build a powerful NER system on\nsuch a low quality annotated data. Although we can obtain\nhigh quality annotations for each input sentence by majority\nvoting, it can be a waste of human labors to achieve such\na goal, especially for some ambiguous sentences which may\nrequire a number of annotations to reach an agreement. Thus\n∗The corresponding author is Wenliang Chen.\nCopyright c⃝2018, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.majority work directly build models on crowd annotations,\ntrying to model the differences among annotators, for exam-\nple, some of the annotators may be more trustful (Rodrigues,\nPereira, and Ribeiro 2014; Nguyen et al. 2017).\nHere we focus mainly on the Chinese NER, which is more\ndifﬁcult than NER for other languages such as English for\nthe lack of morphological variations such as capitalization\nand in particular the uncertainty in word segmentation. The\nChinese NE taggers trained on news domain often perform\npoor in other domains. Although we can alleviate the prob-\nlem by using character-level tagging to resolve the problem\nof poor word segmentation performances (Peng and Dredze\n2015), still there exists a large gap when the target domain\nchanges, especially for the texts of social media. Thus, in\norder to get a good tagger for new domains and also for the\nconditions of new entity types, we require large amounts of\nlabeled data. Therefore, crowdsourcing is a reasonable solu-\ntion for these situations.\nIn this paper, we propose an approach to training a Chi-\nnese NER system on the crowd-annotated data. Our goal is\nto extract additional annotator independent features by ad-\nversarial training, alleviating the annotation noises of non-\nexperts.'
 'In the ﬁrst case, none of the annotations get the cor-\nrect NER result, but our proposed model can capture it.\nThe result of LSTM-CRF is the same as majority-voting.\nIn the second example, the output of majority-voting is the\nworst, which can account for the reason why the same model\ntrained on the voted corpus performs so badly, as shown in\nTable 2. The model of LSTM-CRF fails to recognize the\nnamed entity “Xiexie” because of not trusting the second\nannotation, treating it as one noise annotation. Our proposed\nmodel is able to recognize it, because of its ability of extract-\ning worker independent features.\nConclusions\nIn this paper, we presented an approach to performing crowd\nannotation learning based on the idea of adversarial training\nfor Chinese Named Entity Recognition (NER). In our ap-\nproach, we use a common and private Bi-LSTMs for rep-\nresenting annotator-generic and -speciﬁc information, and\nlearn a label Bi-LSTM from the crowd-annotated NE label\nsequences. Finally, the proposed approach adopts a LSTM-\nCRF model to perform tagging. In our experiments, we cre-\nate two data sets for Chinese NER tasks in the dialog and e-\ncommerce domains. The experimental results show that the\nproposed approach outperforms strong baseline systems.']",The approach used for performing crowd annotation learning in Chinese Named Entity Recognition was the use of a probabilistic model. This model was designed to handle the uncertainty and variability in the annotations provided by the crowd. It also incorporated the reliability of the annotators into the learning process.,[''],The approach used for performing crowd annotation learning in Chinese Named Entity Recognition is adversarial training.,0.2308502672844375,0.7355835118285777
13,How many annotated sentences were generated based on the 20 February 2018 dump of Armenian Wikipedia?,7455,"['C. Generated data\nUsing the algorithm described above, we generated 7455\nannotated sentences with 163247 tokens based on 20 February\n2018 dump of Armenian Wikipedia.\nThe generated data is still signiﬁcantly smaller than the\nmanually annotated corpora from CoNLL 2002 and 2003.\nFor comparison, the train set of English CoNLL 2003 corpus\ncontains 203621 tokens and the German one 206931, while\nthe Spanish and Dutch corpora from CoNLL 2002 respectively\n273037 and 218737 lines. The smaller size of our generated\ndata can be attributed to the strict selection of candidate\nsentences as well as simply to the relatively small size of\nArmenian Wikipedia.\nThe accuracy of annotation in the generated corpus heavily\nrelies on the quality of links in Wikipedia articles. During gen-\neration, we assumed that ﬁrst mentions of all named entities\nhave an outgoing link to their article, however this was not a l-\nways the case in actual source data and as a result the train se t\ncontained sentences where not all named entities are labele d.\nAnnotation inaccuracies also stemmed from wrongly assigne d\nlink boundaries (for example, in Wikipedia article /Armayb/armre/armto/armvo/armvyun/armre\n/Armvo/armvyun/armyech/armlyun/armse/armlyun/armini /Armvev/armyech/armlyun/armini/armnu/armgim/armto/armvo/armnu (Arthur Wellesley) there is a link to the\nNapoleon article with the text "" /arme/Armnu/armayb/armpe/armvo/armlyun/armyech/armvo/armnu/armat "" (""Napoleon is""),\nwhen it should be "" /Armnu/armayb/armpe/armvo/armlyun/armyech/armvo/armnu/armat "" (""Napoleon"")). Another kind\nof common annotation errors occurred when a named entity\nappeared inside a link not targeting a LOC ,ORG , orPER\narticle (e.g. "" /Armayb/Armmen/Armnu /armnu/armayb/armkhe/armayb/armgim/armayb/armho/armayb/armken/armayb/armnu /armat/armnu/armtyun/armre/armvo/armvyun/armto/armhi/armvo/armvyun/armnu/armnu/armyech/armre/armvo/armvyun/armmen ""\n(""USA presidential elections"") is linked to the article /Armayb/Armmen/Armnu\n/armnu/armayb/armkhe/armayb/armgim/armayb/armho/armayb/armken/armayb/armnu /armat/armnu/armtyun/armre/armvo/armvyun/armto/armhi/armvo/armvyun/armnu/armnu/armyech/armre 2016 (United States presi-\ndential election, 2016) and as a result [ LOC/Armayb/Armmen/Armnu ] (USA) is\nlost).\nIII. T EST DATASET\nIn order to evaluate the models trained on generated data,\nwe manually annotated a named entities dataset comprising\n53453 tokens and 2566 sentences selected from over 250 news\ntexts from ilur.am2. This dataset is comparable in size with\nthe test sets of other languages (Table II). Included senten ces\nare from political, sports, local and world news (Figures 2,\n3), covering the period between August 2012 and July 2018.\nThe dataset provides annotations for 3 popular named entity\nclasses: people ( PER ), organizations ( ORG ), and locations\n(LOC ), and is released in CoNLL03 format with IOB tagging\nscheme. Tokens and sentences were segmented according to\nthe UD standards for the Armenian language [12].\n2http://ilur.am/news/newsline.html'
 'Fig. 1: Steps of automatic dataset extraction from Wikipedi a\nClassiﬁcation of Wikipedia articles into NE types\nLabelling common article aliases to increase coverage\nExtraction of text fragments with outgoing links\nLabelling links according to their target article’s type\nAdjustment of labeled entities’ boundaries\ncore differences between our approach and Nothman’s system\nis that we do not rely on manual classiﬁcation of articles and\ndo not use inter-language links to project article classiﬁc ations\nacross languages. Instead, our classiﬁcation algorithm us es\nonly an article’s Wikidata entry’s ﬁrst instance of label’s\nparent subclass of labels, which are, incidentally, language\nindependent and thus can be used for any language.\nThen, outgoing links in articles are assigned the article’s\ntype they are leading to. Sentences are included in the train ing\ncorpus only if they contain at least one named entity and\nall contained capitalized words have an outgoing link to an\narticle of known type. Since in Wikipedia articles only the ﬁ rst\nmention of each entity is linked, this approach becomes very\nrestrictive and in order to include more sentences, additio nal\nlinks are inferred. This is accomplished by compiling a list of\ncommon aliases for articles corresponding to named entitie s,\nand then ﬁnding text fragments matching those aliases to\nassign a named entity label. An article’s aliases include it s\ntitle, titles of disambiguation pages with the article, and texts\nof links leading to the article (e.g. /Armlyun/armyech/armnu/armini/armnu/armgim/armre/armayb/armda (Leningrad),\n/Armpe/armyech/armtyun/armre/armvo/armgim/armre/armayb/armda (Petrograd), /Armpe/armyech/armtyun/armyech/armre/armben/armvo/armvyun/armre/armgim (Peterburg) are aliases\nfor/Armse/armayb/armnu/armken/armtyun /Armpe/armyech/armtyun/armyech/armre/armben/armvo/armvyun/armre/armgim (Saint Petersburg)). The list of\naliases is compiled for all PER ,ORG ,LOC articles.\nAfter that, link boundaries are adjusted by removing the\nlabels for expressions in parentheses, the text after a comm a,\nand in some cases breaking into separate named entities if th e\nlinked text contains a comma. For example, [LOC/Armayb/armben/armvo/armvev/armhi/armayb/armnu\n/armparenleft/armke/armayb/armghat/armayb/armke/armparenright ](Abovyan (town)) is reworked into [LOC/Armayb/armben/armvo/armvev/armhi/armayb/armnu ]\n/armparenleft/armke/armayb/armghat/armayb/armke/armparenright .\nB. Using Wikidata to classify Wikipedia\nInstead of manually classifying Wikipedia articles as it wa s\ndone in Nothman et al., we developed a rule-based classiﬁer\nthat used an article’s Wikidata instance of andsubclass of\nattributes to ﬁnd the corresponding named entity type.The classiﬁcation could be done using solely instance\noflabels, but these labels are unnecessarily speciﬁc for the\ntask and building a mapping on it would require a more\ntime-consuming and meticulous work. Therefore, we classiﬁ ed\narticles based on their ﬁrst instance of attribute’s subclass of\nvalues. Table I displays the mapping between these values an d\nnamed entity types. Using higher-level subclass of values was\nnot an option as their values often were too general, making\nit impossible to derive the correct named entity category.\nC. Generated data\nUsing the algorithm described above, we generated 7455\nannotated sentences with 163247 tokens based on 20 February\n2018 dump of Armenian Wikipedia.\nThe generated data is still signiﬁcantly smaller than the\nmanually annotated corpora from CoNLL 2002 and 2003.\nFor comparison, the train set of English CoNLL 2003 corpus\ncontains 203621 tokens and the German one 206931, while\nthe Spanish and Dutch corpora from CoNLL 2002 respectively\n273037 and 218737 lines. The smaller size of our generated\ndata can be attributed to the strict selection of candidate\nsentences as well as simply to the relatively small size of\nArmenian Wikipedia.\nThe accuracy of annotation in the generated corpus heavily\nrelies on the quality of links in Wikipedia articles.']",The text does not provide information on the number of annotated sentences generated based on the 20 February 2018 dump of Armenian Wikipedia.,[''],7455 annotated sentences were generated based on the 20 February 2018 dump of Armenian Wikipedia.,0.22731461276895684,0.9573100818360805
14,What is the loss function used in the baseline CRF model?,The loss function used in the baseline CRF model is the negative log-likelihood objective.,"['The\nother part is a transition matrix Twhich deﬁnes the scores\nof two successive labels. Tis also a model parameter. Based\nononer\ntandT, we use the Viterbi algorithm to ﬁnd the best-\nscoring label sequence.\nWe can formalize the CRF tagging process as follows:\noner\nt=Wnerhner\nt, t∈[1, n]\nscore (X,y) =n∑\nt=1(ot,yt+Tyt−1,yt)\nyner= arg max\ny(\nscore (X,y)))\n,(2)\nwhere score (·)is the scoring function for a given output la-\nbel sequence y=y1y2···ynbased on input X,yneris the\nresulting label sequence, Wneris a model parameter.\nTraining\nTo train model parameters, we exploit a negative log-\nlikelihood objective as the loss function. We apply softmax\nover all candidate output label sequences, thus the probabil-\nity of the crowd-annotated label sequence is computed by:\np(¯ y|X) =exp(\nscore (X,¯ y))\n∑\ny∈YXexp(\nscore (X,y)), (3)\nwhere ¯ yis the crowd-annotated label sequences and YXis\nall candidate label sequence of input X.\nBased on the above formula, the loss function of our base-\nline model is:\nloss(Θ,X,¯ y) =−logp(¯ y|X), (4)\nwhere Θis the set of all model parameters. We use standard\nback-propagation method to minimize the loss function of\nthe baseline CRF model.'
 'x1 x2 ...... xn−1 xnhprivate\n1 hprivate\n2...... hprivate\nn−1hprivate\nnhcommon\nn hcommon\nn−1...... hcommon\n2 hcommon\n1 hlabel\nn hlabel\nn−1...... hlabel\n2 hlabel\n1\nx′1 x′2...... x′n−1x′n...... hner\n2 hner\n1 hner\nn−1hner\nn\n⨁...... oner\n1 oner\n2 oner\nn−1oner\nn...... L-PER B-PER O O\nw1 w2 ...... wn−1 wn ¯y1 ¯y2 ...... ¯yn−1 ¯yn...... hworker\n2 hworker\n1 hworker\nn−1hworker\nn\n⨁hworkeroworkerworker\nBi-LSTMBi-LSTMBi-LSTMCNNBaseline Worker-AdversarialFigure 1: The framework of the proposed model, which consists of two parts.\ntor representation of each sentential character by: xt=\nlook-up (ct,EW), t∈[1, n].\nFeature Extraction\nBased on the vector sequence x1x2···xn, we extract\nhigher-level features hner\n1hner\n2···hner\nnby using a bidirec-\ntional LSTM module and a simple feed-forward neural layer,\nwhich are then used for CRF tagging at the next step.\nLSTM is a type of recurrent neural network (RNN),\nwhich is designed for solving the exploding and dimin-\nishing gradients of basic RNNs (Graves and Schmidhu-\nber 2005). It has been widely used in a number of NLP\ntasks, including POS-tagging (Huang, Xu, and Yu 2015;\nMa and Hovy 2016), parsing (Dyer et al. 2015) and machine\ntranslation (Wu et al. 2016), because of its strong capabili-\nties of modeling natural language sentences.\nBy traversing x1x2···xnby order and reversely, we ob-\ntain the output features hprivate\n1hprivate\n2···hprivate\nn of the bi-\nLSTM, where hprivate\nt =− →ht⊕← −ht. Here we refer this Bi-\nLSTM as private in order to differentiate it with the com-\nmon Bi-LSTM over the same character inputs which will be\nintroduced in the next section.\nFurther we make an integration of the output vectors of\nbi-directional LSTM by a linear feed-forward neural layer,\nresulting in the features hner\n1hner\n2···hner\nnby equation:\nhner\nt=Whprivate\nt +b, (1)\nwhere Wandbare both model parameters.\nCRF Tagging\nFinally we feed the resulting features hner\nt, t∈[1, n]into a\nCRF layer directly for NER decoding. CRF tagging is one\nglobally normalized model, aiming to ﬁnd the best output\nsequence considering the dependencies between successive\nlabels. In the sequence labeling setting for NER, the output\nlabel of one position has a strong dependency on the label of\nthe previous position. For example, the label before “I-XX”must be either “B-XX” or “I-XX”, where “XX” should be\nexactly the same.\nCRF involves two parts for prediction. First we should\ncompute the scores for each label based hner\nt, resulting in\noner\nt, whose dimension is the number of output labels. The\nother part is a transition matrix Twhich deﬁnes the scores\nof two successive labels. Tis also a model parameter. Based\nononer\ntandT, we use the Viterbi algorithm to ﬁnd the best-\nscoring label sequence.\nWe can formalize the CRF tagging process as follows:\noner\nt=Wnerhner\nt, t∈[1, n]\nscore (X,y) =n∑\nt=1(ot,yt+Tyt−1,yt)\nyner= arg max\ny(\nscore (X,y)))\n,(2)\nwhere score (·)is the scoring function for a given output la-\nbel sequence y=y1y2···ynbased on input X,yneris the\nresulting label sequence, Wneris a model parameter.\nTraining\nTo train model parameters, we exploit a negative log-\nlikelihood objective as the loss function.']",The loss function used in the baseline CRF model is the negative log-likelihood.,[''],The loss function used in the baseline CRF model is -logp(¯ y|X).,0.7360396828161958,0.7350770986584575
15,What are some positive signals of dogmatism in language use?,"Positive signals of dogmatism in language use include present tense verb usage, negative emotion, swearing, interrogative language, and negation.","['ers, often in a strongly opinionated way (“you are a\nmoron” or “they are keeping us down”). Other pro-\nnoun types do not show signiﬁcant relationships.\nLike pronouns, verb tense can reveal subtle sig-\nnals in language use, such as the tendency of medi-\ncal inpatients to focus on the past (Wolf et al., 2007).\nOn social media, comments written in the present\ntense are more likely to be oriented towards a user’s\ncurrent interaction (“this isall so stupid”), creating\nopportunities to signal dogmatism. Alternatively,\ncomments in the past tense are more likely to re-\nfer to outside experiences (“it wasan awful party”),\nspeaking less to a user’s stance towards an ongoing\ndiscussion. We ﬁnd present tense is a positive sig-\nnal for dogmatism (1.11 odds) and past tense is a\nnegative signal (0.69 odds).\nDogmatic language can be either positively or\nnegatively charged in sentiment: for example, con-\nsider the positive statement “ Trump is the SAVIOR\nof this country!!! ” or the negative statement “ Are\nyou REALLY that stupid?? Education is the only\nway out of this horrible mess. It’s hard to imagine\nhow anyone could be so deluded. ” In diverse com-\nmunities, where people hold many different kinds\nof opinions, dogmatic opinions will often tend to\ncome into conﬂict with one another (McCluskey and\nHmielowski, 2012), producing a greater likelihood\nof negative sentiment. Perhaps for this reason, neg-\native emotion (2.09 odds) and swearing (3.80 odds)\nare useful positive signals of dogmatism, while pos-\nitive emotion shows no signiﬁcant relationship.\nFinally, we ﬁnd that interrogative language (1.12\nodds) and negation (1.35 odds) are two additional\npositive signals of dogmatism. While interrogative\nwords like “how” or “what” have many benign uses,\nthey disproportionately appear in our data in the\nform of rhetorical or emotionally charged questions,\nsuch as “how can anyone be that dumb?”\nMany of these linguistic signals are correlated\nwith each other, suggesting that dogmatism is the\ncumulative effect of many component relationships.\nFor example, consider the relatively non-dogmatic\nstatement: “I think the reviewers are wrong in this\ninstance.” Removing signals of insight , we have:\n“the reviewers are wrong in this instance,” which\nis slightly more dogmatic. Then removing relativ-\nity, we have: “the reviewers are wrong.” And ﬁ-\nnally, adding certainty , we have a dogmatic state-Classiﬁer In-domain Cross-domain\nBOW 0.853 0.776\nSENT 0.677 0.646\nLING 0.801 0.728\nBOW + SENT 0.860 0.783\nBOW + LING 0.881 0.791\nTable 2: The AUC scores for dogmatism classiﬁers within and\nacross domains. BOW (bag-of-words) and SENT (sentiment\nsignals) are baselines, and LING uses the linguistic features\nfrom Table 1. We compute in-domain accuracy using 15-fold\ncross-validation on the Reddit dataset, and cross-domain accu-\nracy by training on Reddit and evaluating on comments on arti-\ncles from the New York Times. Chance AUC is 0.5.\nment: “the reviewers are always wrong.”\n4 Predicting dogmatism\nWe now show how we can use the linguistic feature\nsets we have described to build a classiﬁer that pre-\ndicts dogmatism in comments. A predictive model\nfurther validates our feature sets, and also allows us\nto analyze dogmatism in millions of other Reddit\ncomments in a scalable way, with multiple uses in\nongoing, downstream analyses.\nPrediction task. Our goal is (1) to understand\nhow well we can use the strategies in Section 3\nto predict dogmatism, and (2) to test the domain-\nindependence of these strategies. First, we test the\nperformance of our model under cross-validation\nwithin the Reddit comment dataset. We then eval-\nuate the Reddit-based model on a held out corpus\nof New York Times comments annotated using the\ntechnique in Section 2. We did not refer to this sec-\nond dataset during feature construction.\nFor classiﬁcation, we consider two classes of\ncomments: dogmatic andnon-dogmatic .'
 'From a behavioral standpoint,\ndogmatic people solve problems differently, spend-\ning less time framing a problem and expressing more\ncertainty in their solution (Lohman, 2010). Here we\nsimilarly examine how user behaviors on Reddit re-\nlate to a language model of dogmatism.\nErtel sought to capture dogmatism linguistically,\nthough a small lexicon of words that correspond\nwith high-level concepts like certainty and compro-\nmise (1985). McKenny then used this dictionary to\nrelate dogmatism to argument quality in student es-\nsays (2005). Our work expands on this approach,\napplying supervised models based on a broader set\nof linguistic categories to identify dogmatism in text.\nOther researchers have studied topics similar to\ndogmatism, such as signals of cognitive style in\nright-wing political thought (Van Hiel et al., 2010),\nthe language used by trolls on social media (Cheng\net al., 2015), or what makes for impartial language\non twitter (Zafar et al., 2016). A similar ﬂavor of\nwork has examined linguistic models that capture\npoliteness (Danescu-Niculescu-Mizil et al., 2013),\ndeception (Ott et al., 2011), and authority (Gilbert,\n2012). We took inspiration from these models when\nconstructing the feature sets in our work.\nFinally, while we examine what makes an opin-\nion dogmatic, other work has pushed further into the\nstructure of arguments, for example classifying their\njustiﬁcations (Hasan and Ng, 2014), or what makes\nan argument likely to win (Tan et al., 2016). Our']","1. Certainty in Statements: Dogmatic individuals often express their beliefs with absolute certainty. They use definitive language such as ""always"", ""never"", ""must"", ""should"", etc.

2. Lack of Openness to Other Perspectives: Dogmatic people often dismiss or ignore other viewpoints. They may use language that dismisses other opinions, such as ""that's wrong"" or ""that's not possible"".

3. Use of Authoritative Sources: Dogmatists often reference authoritative sources to back up their beliefs, without considering other interpretations or evidence.

4. Use of Stereotypes and Generalizations: Dogmatic language often includes broad generalizations and stereotypes, which oversimplify complex issues.

5. Emotional Language: Dogmatists often use emotional language to persuade others to their point of view. This can include language that appeals to fear, anger, or other strong emotions.

6. Lack of Questions: Dogmatic individuals often do not ask questions or seek to understand other perspectives. Their language is often declarative rather than interrogative.

7. Resistance to Change: Dogmatists often use language that resists change or new ideas. They may use phrases like ""we've always done it this way"" or ""this is the way it should be"". 

8. Use of Moral Absolutes: Dogmatic individuals often use language that implies moral absolutes, such as ""right"" and ""wrong"", ""good"" and ""evil"", without acknowledging the complexity of moral issues.",[''],"Positive signals of dogmatism in language use include pronouns used in a strongly opinionated way, verb tense focused on the past, present tense indicating dogmatism, negative sentiment, swearing, interrogative language, and negation.",0.7501323892012586,0.7131809205023116
16,How does the matching between different levels of relation/question representations pose a difficulty?,"The matching between different levels of relation/question representations poses a difficulty due to the fact that either layer of question representations could potentially correspond to either level of relation representations. This ambiguity raises challenges in effectively aligning and comparing the varying levels of information between relations and questions, making it complex to establish meaningful connections between them.","['As a result, we hope the question representa-\ntions could also comprise vectors that summa-\nrize various lengths of phrase information (differ-\nent levels of abstraction), in order to match rela-\ntion representations of different granularity. We\ndeal with this problem by applying deep BiL-\nSTMs on questions. The ﬁrst-layer of BiLSTM\nworks on the word embeddings of question words\nq={q1,···,qN}and gets hidden representations\nΓ(1)\n1:N= [γ(1)\n1;···;γ(1)\nN]. The second-layer BiL-\nSTM works on Γ(1)\n1:Nto get the second set of hid-\nden representations Γ(2)\n1:N. Since the second BiL-\nSTM starts with the hidden vectors from the ﬁrst\nlayer, intuitively it could learn more general and\nabstract information compared to the ﬁrst layer.\nNote that the ﬁrst(second)-layer of question rep-\nresentations does not necessarily correspond to the\nword(relation)-level relation representations, in-\nstead either layer of question representations could\npotentially match to either level of relation repre-\nsentations. This raises the difﬁculty of matching\nbetween different levels of relation/question rep-\nresentations; the following section gives our pro-\nposal to deal with such problem.'
 'This makes KB entity typing\nitself a difﬁcult problem so no previous used en-\ntity information in the relation detection model.3\n3 Background: Different Granularity in\nKB Relations\nPrevious research (Yih et al., 2015; Yin et al.,\n2016) formulates KB relation detection as a se-\nquence matching problem. However, while the\nquestions are natural word sequences, how to rep-\nresent relations as sequences remains a challeng-\ning problem. Here we give an overview of two\ntypes of relation sequence representations com-\nmonly used in previous work.\n(1) Relation Name as a Single Token (relation-\nlevel). In this case, each relation name is treated\nas a unique token. The problem with this ap-\nproach is that it suffers from the low relation cov-\nerage due to limited amount of training data, thus\ncannot generalize well to large number of open-\ndomain relations. For example, in Figure 1, when\ntreating relation names as single tokens, it will be\ndifﬁcult to match the questions to relation names\n“episodes written ” and “ starring roles ” if these\nnames do not appear in training data – their rela-\ntion embeddings hrs will be random vectors thus\nare not comparable to question embeddings hqs.\n(2) Relation as Word Sequence (word-level ). In\nthis case, the relation is treated as a sequence of\nwords from the tokenized relation name. It has\nbetter generalization, but suffers from the lack\nof global information from the original relation\nnames. For example in Figure 1(b), when doing\nonly word-level matching, it is difﬁcult to rank the\ntarget relation “ starring roles ” higher compared\nto the incorrect relation “ plays produced ”. This\nis because the incorrect relation contains word\n“plays ”, which is more similar to the question\n3Such entity information has been used in KBQA systems\nas features for the ﬁnal answer re-rankers.']","The matching between different levels of relation/question representations poses a difficulty because it requires understanding and aligning complex semantic structures. This process often involves dealing with ambiguity, variability in language expression, and the need for contextual understanding. It can be challenging to design algorithms or models that can accurately and effectively handle these complexities. Additionally, it can be difficult to gather and annotate the necessary training data for machine learning models to learn these representations.",[''],The matching between different levels of relation/question representations poses a difficulty because the first (second) layer of question representations does not necessarily correspond to the word (relation)-level relation representations.,0.4011321836032465,0.49156631760023445
17,What is the dataset used for training a neural g2p system?,"In order to train a neural g2p system, one needs a large quantity of pronunciation data.","['systems contain ambiguities. English is well-\nknown for its spelling ambiguities. Abjads, used\nfor Arabic and Hebrew, do not give full represen-\ntation to vowels.\nConsequently, g2p is harder than simply replac-\ning each grapheme symbol with a corresponding\nphoneme symbol. It is the problem of replacing a\ngrapheme sequence\nG=g1,g2,...,g m\nwith a phoneme sequence\nΦ =φ1,φ2,...,φ n\nwhere the sequences are not necessarily of the\nsame length. Data-driven g2p is therefore the\nproblem of ﬁnding the phoneme sequence that\nmaximizes the likelihood of the grapheme se-\nquence:\nˆΦ = arg max\nΦ′Pr(Φ′|G)\nData-driven approaches are especially useful\nfor problems in which the rules that govern them\nare complex and difﬁcult to engineer by hand.\ng2p for languages with ambiguous orthographies\nis such a problem. Multilingual g2p, in which the\nvarious languages have similar but different and\npossibly contradictory spelling rules, can be seen\nas an extreme case of that. Therefore, a data-\ndriven sequence-to-sequence model is a natural\nchoice.\n4 Methods\n4.1 Encoder–Decoder Models\nIn order to ﬁnd the best phoneme sequence, we\nuse a neural encoder–decoder model with atten-\ntion (Bahdanau et al., 2014). The model consists\nof two main parts: the encoder compresses each\nsource grapheme sequence Ginto a ﬁxed-length\nvector. The decoder , conditioned on this ﬁxed-\nlength vector, generates the output phoneme se-\nquence Φ.\nThe encoder and decoder are both implemented\nas recurrent neural networks, which have the ad-\nvantage of being able to process sequences of ar-\nbitrary length and use long histories efﬁciently.\nThey are trained jointly to minimize cross-entropy\non the training data. We had our best results\nwhen using a bidirectional encoder, which consists\nof two separate encoders which process the inputEnc. & dec. model type LSTM\nAttention General\nEnc. & dec. layers 2\nHidden layer size 150\nSource embedding size 150\nTarget embedding size 150\nBatch size 64\nOptimizer SGD\nLearning rate 1.0\nTraining epochs 13\nTable 1: Hyperparameters for multilingual g2p\nmodels\nin forward and reverse directions. We used long\nshort-term memory units (Hochreiter and Schmid-\nhuber, 1997) for both the encoder and decoder.\nFor the attention mechanism, we used the general\nglobal attention architecture described by Luong\net al. (2015).\nWe implemented2all models with OpenNMT\n(Klein et al., 2017). Our hyperparameters, which\nwe determined by experimentation, are listed in\nTable 1.\n4.2 Training Multilingual Models\nPresenting pronunciation data in several languages\nto the network might create problems because dif-\nferent languages have different pronunciation pat-\nterns. For example, the string ‘real’ is pronounced\ndifferently in English, German, Spanish, and Por-\ntuguese. We solve this problem by prepending\neach grapheme sequence with an artiﬁcial token\nconsisting of the language’s ISO 639-3 code en-\nclosed in angle brackets. The English word ‘real’,\nfor example, would be presented to the system as\n<eng>r e a l\nThe artiﬁcial token is treated simply as an element\nof the grapheme sequence. This is similar to the\napproach taken by Johnson et al. (2016) in their\nzero-shot NMT system. However, their source-\nside artiﬁcial tokens identify the target language,\nwhereas ours identify the source language. An\nalternative approach, used by ¨Ostling and Tiede-\nmann (2017), would be to concatenate a language\nembedding to the input at each time step. They\ndo not evaluate their approach on grapheme-to-\nphoneme conversion.\n5 Data\nIn order to train a neural g2p system, one needs a\nlarge quantity of pronunciation data. A standard\n2https://github.com/bpopeters/mg2p'
 'Massively Multilingual Neural Grapheme-to-Phoneme Conversion\nBen Peters\nSaarland University\nSaarbr ¨ucken, Germany\nbenzurdopeters@gmail.comJon Dehdari andJosef van Genabith\nDFKI & Saarland University\nSaarbr ¨ucken, Germany\nfirstname.lastname@dfki.de\nAbstract\nGrapheme-to-phoneme conversion (g2p)\nis necessary for text-to-speech and auto-\nmatic speech recognition systems. Most\ng2p systems are monolingual: they require\nlanguage-speciﬁc data or handcrafting of\nrules. Such systems are difﬁcult to ex-\ntend to low resource languages, for which\ndata and handcrafted rules are not avail-\nable. As an alternative, we present a neu-\nral sequence-to-sequence approach to g2p\nwhich is trained on spelling–pronunciation\npairs in hundreds of languages. The sys-\ntem shares a single encoder and decoder\nacross all languages, allowing it to utilize\nthe intrinsic similarities between different\nwriting systems. We show an 11% im-\nprovement in phoneme error rate over an\napproach based on adapting high-resource\nmonolingual g2p models to low-resource\nlanguages. Our model is also much more\ncompact relative to previous approaches.\n1 Introduction\nAccurate grapheme-to-phoneme conversion (g2p)\nis important for any application that depends on\nthe sometimes inconsistent relationship between\nspoken and written language. Most prominently,\nthis includes text-to-speech and automatic speech\nrecognition. Most work on g2p has focused on\na few languages for which extensive pronuncia-\ntion data is available (Bisani and Ney, 2008; No-\nvak et al., 2016; Rao et al., 2015; Yao and Zweig,\n2015, inter alia) . Most languages lack these re-\nsources. However, a low resource language’s writ-\ning system is likely to be similar to the writing sys-\ntems of languages that do have sufﬁcient pronun-\nciation data. Therefore g2p may be possible for\nlow resource languages if this high resource datacan be properly utilized.\nWe attempt to leverage high resource data by\ntreating g2p as a multisource neural machine\ntranslation (NMT) problem. The source sequences\nfor our system are words in the standard orthogra-\nphy in any language. The target sequences are the\ncorresponding representation in the International\nPhonetic Alphabet (IPA). Our results show that the\nparameters learned by the shared encoder–decoder\nare able to exploit the orthographic and phonemic\nsimilarities between the various languages in our\ndata.\n2 Related Work\n2.1 Low Resource g2p\nOur approach is similar in goal to Deri and Knight\n(2016)’s model for adapting high resource g2p\nmodels for low resource languages. They trained\nweighted ﬁnite state transducer (wFST) models on\na variety of high resource languages, then trans-\nferred those models to low resource languages, us-\ning a language distance metric to choose which\nhigh resource models to use and a phoneme dis-\ntance metric to map the high resource language’s\nphonemes to the low resource language’s phoneme\ninventory. These distance metrics are computed\nbased on data from Phoible (Moran et al., 2014)\nand URIEL (Littell et al., 2017).\nOther low resource g2p systems have used a\nstrategy of combining multiple models. Schlippe\net al. (2014) trained several data-driven g2p sys-\ntems on varying quantities of monolingual data\nand combined their outputs with a phoneme-level\nvoting scheme. This led to improvements over the\nbest-performing single system for small quantities\nof data in some languages. Jyothi and Hasegawa-\nJohnson (2017) trained recurrent neural networks\nfor small data sets and found that a version of their\nsystem that combined the neural network outputarXiv:1708.01464v1  [cs.CL]  4 Aug 2017']","The dataset used for training a neural g2p (grapheme-to-phoneme) system typically consists of a large number of word-phoneme pairs. This could be from a variety of sources, such as pronunciation dictionaries or transcribed speech data.",[''],The multilingual pronunciation corpus collected by Deri and Knight (2016) is used for training the neural g2p system.,0.8247608350851882,0.7276740955138978
18,What is the formula used to compute the probability of the crowd-annotated label sequence in the baseline model?,"The formula used to compute the probability of the crowd-annotated label sequence in the baseline model is: 

p(¯ y|X) =exp(score(X,¯ y)) / ∑y∈YX exp(score(X,y))","['The\nother part is a transition matrix Twhich deﬁnes the scores\nof two successive labels. Tis also a model parameter. Based\nononer\ntandT, we use the Viterbi algorithm to ﬁnd the best-\nscoring label sequence.\nWe can formalize the CRF tagging process as follows:\noner\nt=Wnerhner\nt, t∈[1, n]\nscore (X,y) =n∑\nt=1(ot,yt+Tyt−1,yt)\nyner= arg max\ny(\nscore (X,y)))\n,(2)\nwhere score (·)is the scoring function for a given output la-\nbel sequence y=y1y2···ynbased on input X,yneris the\nresulting label sequence, Wneris a model parameter.\nTraining\nTo train model parameters, we exploit a negative log-\nlikelihood objective as the loss function. We apply softmax\nover all candidate output label sequences, thus the probabil-\nity of the crowd-annotated label sequence is computed by:\np(¯ y|X) =exp(\nscore (X,¯ y))\n∑\ny∈YXexp(\nscore (X,y)), (3)\nwhere ¯ yis the crowd-annotated label sequences and YXis\nall candidate label sequence of input X.\nBased on the above formula, the loss function of our base-\nline model is:\nloss(Θ,X,¯ y) =−logp(¯ y|X), (4)\nwhere Θis the set of all model parameters. We use standard\nback-propagation method to minimize the loss function of\nthe baseline CRF model.'
 'In recent\nyears, a series of work have focused on how to use crowd-\nsourcing data efﬁciently in tasks such as classiﬁcation (Felt\net al. 2015; Bi et al. 2014), and compare quality of crowd\nand expert labels (Dumitrache, Aroyo, and Welty 2017).\nIn sequence labeling tasks, Dredze, Talukdar, and Cram-\nmer (2009) viewed this task as a multi-label problem while\nRodrigues, Pereira, and Ribeiro (2014) took workers iden-\ntities into account by assuming that each sentential word\nwas tagged correctly by one of the crowdsourcing workers\nand proposed a CRF-based model with multiple annotators.\nNguyen et al. (2017) introduced a crowd representation in\nwhich the crowd vectors were added into the LSTM-CRF\nmodel at train time, but ignored them at test time. In this\npaper, we apply adversarial training on crowd annotations\non Chinese NER in new domains, and achieve better perfor-\nmances than previous studies on crowdsourcing learning.\nBaseline: LSTM-CRF\nWe use a neural CRF model as the baseline system (Ratinov\nand Roth 2009), treating NER as a sequence labeling prob-\nlem over Chinese characters, which has achieved state-of-\nthe-art performances (Peng and Dredze 2015). To this end,\nwe explore the BIEO schema to convert NER into sequence\nlabeling, following Lample et al. (2016), where sentential\ncharacter is assigned with one unique tag. Concretely, we tag\nthe non-entity character by label “O”, the beginning charac-\nter of an entity by “B-XX”, the ending character of an entity\nby “E-XX” and the other character of an entity by “I-XX”,\nwhere “XX” denotes the entity type.\nWe build high-level neural features from the input char-\nacter sequence by a bi-directional LSTM (Lample et al.\n2016). The resulting features are combined and then are\nfed into an output CRF layer for decoding. In summary, the\nbaseline model has three main components. First, we make\nvector representations for sentential characters x1x2···xn,\ntransforming the discrete inputs into low-dimensional neu-\nral inputs. Second, feature extraction is performed to obtain\nhigh-level features hner\n1hner\n2···hner\nn, by using a bi-directional\nLSTM (Bi-LSTM) structure together with a linear trans-\nformation over x1x2···xn. Third, we apply a CRF tag-\nging module over hner\n1hner\n2···hner\nn, obtaining the ﬁnal output\nNE labels. The overall framework of the baseline model is\nshown by the right part of Figure 1.\nVector Representation of Characters\nTo represent Chinese characters, we simply exploit a neu-\nral embedding layer to map discrete characters into the low-\ndimensional vector representations. The goal is achieved\nby a looking-up table EW, which is a model parameter\nand will be ﬁne-tuned during training. The looking-up ta-\nble can be initialized either by random or by using a pre-\ntrained embeddings from large scale raw corpus. For a given\nChinese character sequence c1c2···cn, we obtain the vec-']",The formula used to compute the probability of the crowd-annotated label sequence in the baseline model is not specified in the question. The formula can vary depending on the specific model and method used.,[''],"p(¯ y|X) =exp(
score (X,¯ y))
∑
y∈YXexp(
score (X,y)), (3)",0.1915045920576247,0.7261001802230316
19,What linguistic features can be used to predict dogmatism in comments?,"Linguistic features such as verb tense, emotion (positive and negative), swearing, interrogative language, and negation can be used to predict dogmatism in comments. These features provide signals that indicate the presence of dogmatic language in social media interactions.","['ers, often in a strongly opinionated way (“you are a\nmoron” or “they are keeping us down”). Other pro-\nnoun types do not show signiﬁcant relationships.\nLike pronouns, verb tense can reveal subtle sig-\nnals in language use, such as the tendency of medi-\ncal inpatients to focus on the past (Wolf et al., 2007).\nOn social media, comments written in the present\ntense are more likely to be oriented towards a user’s\ncurrent interaction (“this isall so stupid”), creating\nopportunities to signal dogmatism. Alternatively,\ncomments in the past tense are more likely to re-\nfer to outside experiences (“it wasan awful party”),\nspeaking less to a user’s stance towards an ongoing\ndiscussion. We ﬁnd present tense is a positive sig-\nnal for dogmatism (1.11 odds) and past tense is a\nnegative signal (0.69 odds).\nDogmatic language can be either positively or\nnegatively charged in sentiment: for example, con-\nsider the positive statement “ Trump is the SAVIOR\nof this country!!! ” or the negative statement “ Are\nyou REALLY that stupid?? Education is the only\nway out of this horrible mess. It’s hard to imagine\nhow anyone could be so deluded. ” In diverse com-\nmunities, where people hold many different kinds\nof opinions, dogmatic opinions will often tend to\ncome into conﬂict with one another (McCluskey and\nHmielowski, 2012), producing a greater likelihood\nof negative sentiment. Perhaps for this reason, neg-\native emotion (2.09 odds) and swearing (3.80 odds)\nare useful positive signals of dogmatism, while pos-\nitive emotion shows no signiﬁcant relationship.\nFinally, we ﬁnd that interrogative language (1.12\nodds) and negation (1.35 odds) are two additional\npositive signals of dogmatism. While interrogative\nwords like “how” or “what” have many benign uses,\nthey disproportionately appear in our data in the\nform of rhetorical or emotionally charged questions,\nsuch as “how can anyone be that dumb?”\nMany of these linguistic signals are correlated\nwith each other, suggesting that dogmatism is the\ncumulative effect of many component relationships.\nFor example, consider the relatively non-dogmatic\nstatement: “I think the reviewers are wrong in this\ninstance.” Removing signals of insight , we have:\n“the reviewers are wrong in this instance,” which\nis slightly more dogmatic. Then removing relativ-\nity, we have: “the reviewers are wrong.” And ﬁ-\nnally, adding certainty , we have a dogmatic state-Classiﬁer In-domain Cross-domain\nBOW 0.853 0.776\nSENT 0.677 0.646\nLING 0.801 0.728\nBOW + SENT 0.860 0.783\nBOW + LING 0.881 0.791\nTable 2: The AUC scores for dogmatism classiﬁers within and\nacross domains. BOW (bag-of-words) and SENT (sentiment\nsignals) are baselines, and LING uses the linguistic features\nfrom Table 1. We compute in-domain accuracy using 15-fold\ncross-validation on the Reddit dataset, and cross-domain accu-\nracy by training on Reddit and evaluating on comments on arti-\ncles from the New York Times. Chance AUC is 0.5.\nment: “the reviewers are always wrong.”\n4 Predicting dogmatism\nWe now show how we can use the linguistic feature\nsets we have described to build a classiﬁer that pre-\ndicts dogmatism in comments. A predictive model\nfurther validates our feature sets, and also allows us\nto analyze dogmatism in millions of other Reddit\ncomments in a scalable way, with multiple uses in\nongoing, downstream analyses.\nPrediction task. Our goal is (1) to understand\nhow well we can use the strategies in Section 3\nto predict dogmatism, and (2) to test the domain-\nindependence of these strategies. First, we test the\nperformance of our model under cross-validation\nwithin the Reddit comment dataset. We then eval-\nuate the Reddit-based model on a held out corpus\nof New York Times comments annotated using the\ntechnique in Section 2. We did not refer to this sec-\nond dataset during feature construction.\nFor classiﬁcation, we consider two classes of\ncomments: dogmatic andnon-dogmatic .'
 'Identifying Dogmatism in Social Media: Signals and Models\nEthan Fast and Eric Horvitz\nethaen@stanford.edu, horvitz@microsoft.com\nAbstract\nWe explore linguistic and behavioral features\nof dogmatism in social media and construct\nstatistical models that can identify dogmatic\ncomments. Our model is based on a corpus of\nReddit posts, collected across a diverse set of\nconversational topics and annotated via paid\ncrowdsourcing. We operationalize key aspects\nof dogmatism described by existing psychol-\nogy theories (such as over-conﬁdence), ﬁnd-\ning they have predictive power. We also ﬁnd\nevidence for new signals of dogmatism, such\nas the tendency of dogmatic posts to refrain\nfrom signaling cognitive processes. When we\nuse our predictive model to analyze millions\nof other Reddit posts, we ﬁnd evidence that\nsuggests dogmatism is a deeper personality\ntrait, present for dogmatic users across many\ndifferent domains, and that users who engage\non dogmatic comments tend to show increases\nin dogmatic posts themselves.\n1 Introduction\n“I’m supposed to trust the opinion of a MS min-\nion? The people that produced Windows ME, Vista\nand 8? They don’t even understand people, yet they\nthink they can predict the behavior of new, self-\nguiding AI?” – anonymous\n“I think an AI would make it easier for Patients to\nconﬁde their information because by nature, a robot\ncannot judge them. Win-win? :D”’ – anonymous\nDogmatism describes the tendency to lay down\nopinions as incontrovertibly true, without respect for\nconﬂicting evidence or the opinions of others (Ox-\nford Dictionary, 2016). Which user is more dog-\nmatic in the examples above? This question is sim-\nple for humans. Phrases like “they think” and “theydon’t even understand,” suggest an intractability of\nopinion, while “I think” and “win-win?” suggest\nthe opposite. Can we train computers to draw sim-\nilar distinctions? Work in psychology has called\nout many aspects of dogmatism that can be modeled\ncomputationally via natural language, such as over-\nconﬁdence and strong emotions (Rokeach, 1954).\nWe present a statistical model of dogmatism that\naddresses two complementary goals. First, we val-\nidate psychological theories by examining the pre-\ndictive power of feature sets that guide the model’s\npredictions. For example, do linguistic signals of\ncertainty help to predict a post is dogmatic, as the-\nory would suggest? Second, we apply our model to\nanswer four questions:\nR1: What kinds of topics (e.g., guns, LGBT) at-\ntract the highest levels of dogmatism?\nR2: How do dogmatic beliefs cluster?\nR3: How does dogmatism inﬂuence a conversa-\ntion on social media?\nR4: How do other user behaviors (e.g., frequency\nand breadth of posts) relate to dogmatism?\nWe train a predictive model to classify dogmatic\nposts from Reddit, one of the most popular discus-\nsion communities on the web.1Posts on Reddit cap-\nture discussion and debate across a diverse set of do-\nmains and topics – users talk about everything from\nclimate change and abortion, to world news and re-\nlationship advice, to the future of artiﬁcial intelli-\ngence. As a prerequisite to training our model, we\nhave created a corpus of 5,000 Reddit posts anno-\ntated with levels of dogmatism, which we are releas-\ning to share with other researchers.\n1http://www.reddit.comarXiv:1609.00425v1  [cs.CL]  1 Sep 2016']","1. Use of Absolute Language: Dogmatic individuals often use absolute language, such as ""always,"" ""never,"" ""must,"" ""should,"" etc. They tend to see things in black and white, with no room for gray areas.

2. Lack of Qualifiers: Dogmatic comments often lack qualifiers like ""maybe,"" ""possibly,"" ""could,"" etc. They present their opinions as facts.

3. Use of Negative Language: Dogmatic individuals often use negative language to dismiss or invalidate opposing views.

4. Use of Authoritative Language: Dogmatic comments often contain authoritative language, suggesting that the speaker believes they have the final say on the matter.

5. Lack of Open-ended Questions: Dogmatic individuals rarely ask open-ended questions, as they are not typically open to other perspectives.

6. Use of Personal Pronouns: Frequent use of personal pronouns like ""I,"" ""me,"" ""my,"" etc., can indicate a self-centered perspective, which is often associated with dogmatism.

7. Use of Emotional Language: Dogmatic individuals often use emotional language to manipulate or persuade others to agree with their views.

8. Lack of Acknowledgement of Other Perspectives: Dogmatic comments often lack acknowledgement of other perspectives, indicating a closed-minded viewpoint. 

9. Repetition: Dogmatic individuals often repeat their beliefs or opinions, reinforcing their rigid stance.

10. Use of Derogatory Language: Dogmatic individuals may use derogatory language to belittle or dismiss those who disagree with them.",[''],"The linguistic features that can be used to predict dogmatism in comments include pronoun types, verb tense, sentiment (positive or negative), swearing, interrogative language, and negation.",0.5821545813697062,0.5424864955012949
20,What is relation extraction and how does it relate to information extraction?,"Relation extraction is a sub-field of information extraction that focuses on identifying relationships between entities in a text. In relation extraction, the goal is to determine whether a text indicates any types of connections or associations between two target entities. This process is typically formulated as a classification task, where the text is analyzed to identify specific relations between the entities. Traditional methods in relation extraction often rely on hand-crafted features to perform this task effectively.","['2 Related Work\nRelation Extraction Relation extraction (RE) is\nan important sub-ﬁeld of information extraction.\nGeneral research in this ﬁeld usually works on a\n(small) pre-deﬁned relation set, where given a text\nparagraph and two target entities, the goal is to\ndetermine whether the text indicates any types of\nrelations between the entities or not. As a result\nRE is usually formulated as a classiﬁcation task .\nTraditional RE methods rely on large amount of\nhand-crafted features (Zhou et al., 2005; Rink and\nHarabagiu, 2010; Sun et al., 2011). Recent re-\nsearch beneﬁts a lot from the advancement of deep\nlearning: from word embeddings (Nguyen and Gr-\nishman, 2014; Gormley et al., 2015) to deep net-\nworks like CNNs and LSTMs (Zeng et al., 2014;\ndos Santos et al., 2015; Vu et al., 2016) and atten-\ntion models (Zhou et al., 2016; Wang et al., 2016).\nThe above research assumes there is a ﬁxed\n(closed) set of relation types, thus no zero-shot\nlearning capability is required. The number\nof relations is usually not large: The widely\nused ACE2005 has 11/32 coarse/ﬁne-grained rela-\ntions; SemEval2010 Task8 has 19 relations; TAC-'
 'Question: what episode was mike kelleythe writer of\nKnowledgeBaseMike\tKelley(American\ttelevision\twriter/producer)Mike\tKelley(American\tbaseball\tplayer)…Entity Linking \nLove\tWill\tFind\ta\tWayUSA…First\tbaseman…episodes_written\nposition_playedRelation Detection(a)(b)Question: what tvshow did grantshowplay onin2008Mike\tKelley?episodes_writtenEntity Linking RelationDetectionGrantShow?starring_rolesseries(date)from2008ConstraintDetectionGrantShow\t(American\tactor)SwingTown\nBigLoveepisodesScoundrelsseries2011from20102008Figure 1: KBQA examples and its three key components. (a) A single relation example. We ﬁrst identify the topic entity with\nentity linking and then detect the relation asked by the question with relation detection (from all relations connecting the topic\nentity). Based on the detected entity and relation, we form a query to search the KB for the correct answer “ Love Will Find a\nWay”. (b) A more complex question containing two entities. By using “ Grant Show ” as the topic entity, we could detect a chain\nof relations “ starring roles-series ” pointing to the answer. An additional constraint detection takes the other entity “ 2008 ” as\na constraint, to ﬁlter the correct answer “ SwingTown ” from all candidates found by the topic entity and relation.\nthat original relation names can sometimes help\nto match longer question contexts, we propose\nto build both relation-level and word-level rela-\ntion representations. Third, we use deep bidirec-\ntional LSTMs ( BiLSTM s) to learn different levels\nof question representations in order to match the\ndifferent levels of relation information. Finally, we\npropose a residual learning method for sequence\nmatching, which makes the model training easier\nand results in more abstract (deeper) question rep-\nresentations, thus improves hierarchical matching.\nIn order to assess how the proposed improved\nrelation detection could beneﬁt the KBQA end\ntask, we also propose a simple KBQA implemen-\ntation composed of two-step relation detection .\nGiven an input question and a set of candidate enti-\nties retrieved by an entity linker based on the ques-\ntion, our proposed relation detection model plays a\nkey role in the KBQA process: (1) Re-ranking the\nentity candidates according to whether they con-\nnect to high conﬁdent relations detected from the\nraw question text by the relation detection model.\nThis step is important to deal with the ambigui-\nties normally present in entity linking results. (2)\nFinding the core relation (chains) for each topic\nentity2selection from a much smaller candidate\nentity set after re-ranking. The above steps are\nfollowed by an optional constraint detection step,\nwhen the question cannot be answered by single\nrelations (e.g., multiple entities in the question).\nFinally the highest scored query from the above\n2Following Yih et al. (2015), here topic entity refers to\nthe root of the (directed) query tree; and core-chain is the\ndirected path of relation from root to the answer node.steps is used to query the KB for answers.\nOur main contributions include: (i) An im-\nproved relation detection model by hierarchical\nmatching between questions and relations with\nresidual learning; (ii) We demonstrate that the im-\nproved relation detector enables our simple KBQA\nsystem to achieve state-of-the-art results on both\nsingle-relation and multi-relation KBQA tasks.\n2 Related Work\nRelation Extraction Relation extraction (RE) is\nan important sub-ﬁeld of information extraction.\nGeneral research in this ﬁeld usually works on a\n(small) pre-deﬁned relation set, where given a text\nparagraph and two target entities, the goal is to\ndetermine whether the text indicates any types of\nrelations between the entities or not. As a result\nRE is usually formulated as a classiﬁcation task .\nTraditional RE methods rely on large amount of\nhand-crafted features (Zhou et al., 2005; Rink and\nHarabagiu, 2010; Sun et al., 2011).']","Relation extraction is a subtask of information extraction that involves identifying and classifying semantic relationships between entities in a text. These relationships could be anything from 'is a part of', 'is an instance of', 'is a cause of', etc. Information extraction, on the other hand, is a broader task that involves extracting structured information from unstructured data sources like text. So, relation extraction is a part of the overall process of information extraction.",[''],Relation extraction is a sub-field of information extraction that involves determining whether a text indicates any types of relations between entities. It is related to information extraction as it focuses on extracting specific relationships between entities mentioned in a text.,0.7417587164580192,0.8875693610956551
21,How does the Linguistic Inquiry and Word Count (LIWC) system categorize words in relation to dogmatism?,"The LIWC system categorizes words based on high-level psychological categories such as certainty or tentativeness in relation to dogmatism. It identifies words that express certainty positively and tentativeness negatively. For instance, words like ""always"" are associated with certainty, while words like ""possibly"" are linked to tentativeness. Additionally, LIWC captures terms related to cognition that add nuance to statements, such as ""think,"" ""know,"" or ""believe,"" which are often characteristic of non-dogmatic language.","['(Doroudi et al., 2016). For instance, here is an ex-\nample of a highly dogmatic (5)comment:\nI won’t be happy until I see the executive\nsuite of BofA, Wells, and all the others, frog-\nmarched into waiting squad cars. It’s AL-\nREADY BEEN ESTABLISHED that...\nAnd a minimally dogmatic (1)comment:\nI agree. I would like to compile a playlist for\nus trance yogi’s, even if you just would like to\nexperiment with it. Is there any preference on\nwhich platform to use?\nEach comment has been annotated by three indepen-\ndent workers on AMT, which is enough to produce\nreliable results in most labeling tasks (Sheng et al.,\n2008). To compute an aggregate measure of dogma-\ntism for each comment, we summed the scores of all\nthree workers. We show the resulting distribution of\nannotations in Figure 1.\nInter-annotator agreement. To evaluate the reli-\nability of annotations we compute Krippendorff’s α,\na measure of agreement designed for variable levels\nof measurement such as a Likert scale (Hayes and\nKrippendorff, 2007). An αof0indicates agreement\nindistinguishable from chance, while an αof 1 indi-\ncates perfect agreement. Across all annotations we\nﬁndα= 0.44. While workers agree much more\nthan chance, clearly dogmatism is also subjective.\nIn fact, when we examine only the middle two quar-\ntiles of the dogmatism annotations, we ﬁnd agree-\nment is no better than chance. Alternatively, when\nwe measure agreement only among the top and bot-\ntom quartiles of annotations, we ﬁnd agreement of\nα= 0.69. This suggests comments with scores that\nare only slightly dogmatic are unreliable and often\nsubject to human disagreement. For this reason, we\nuse only the top and bottom quartiles of comments\nwhen training our model.\n3 Approaches to Identifying Dogmatism\nWe now consider strategies for identifying dog-\nmatism based on prior work in psychology. We\nstart with the Linguistic Inquiry and Word Count\n(LIWC), a lexicon popular in the social sciences\n(Pennebaker et al., 2001). LIWC provides human\nvalidated lists of words that correspond to high-\nlevel psychological categories such as certainty or\nperception . In other studies, LIWC has uncoveredlinguistic signals relating to politeness (Danescu-\nNiculescu-Mizil et al., 2013), deception (Yoo and\nGretzel, 2009), or authority in texts (Gilbert, 2012).\nHere, we examine how dogmatism relates to 17 of\nLIWC’s categories (Table 1).\nTo compute the relationships between LIWC cat-\negories and dogmatism, we ﬁrst count the relevant\ncategory terms that appear in each annotated Reddit\ncomment, normalized by its word count. We then\ncalculate odds ratios on the aggregate counts of each\nLIWC category over the top and bottom quartiles of\ndogmatic comments. As we have discussed, using\nthe top and bottom quartiles of comments provides\na more reliable signal of dogmatism. We check for\nsigniﬁcant differences in categories between dog-\nmatic and non-dogmatic comments using the Mann-\nWhitney U test and apply Holmes method for cor-\nrection. All odds we report in this section are signif-\nicant after correction.\nDogmatic statements tend to express a high de-\ngree of certainty (Rokeach, 1954). Here we consider\nLIWC categories that express certainty both posi-\ntively ( certainty ) and negatively ( tentativeness ). For\nexample, the word “always” is certain, while “possi-\nbly” is tentative. Conforming to existing theory, cer-\ntainty is more associated with dogmatic comments\n(1.52 odds), while tentativeness is more associated\nwith the absence of dogmatism (0.88 odds).\nTerms used to verbalize cognition can act as a\nhedge that often characterizes non-dogmatic lan-\nguage. LIWC’s insight category captures this effect\nthrough words such as “think,” “know,” or “believe.”\nThese words add nuance to a statement (Pennebaker\nand Francis, 1996), signaling it is the product of\nsomeone’s mind (“ I think you should give this paper\na good review”) and not meant to be interpreted as\nan objective truth.'
 'From a behavioral standpoint,\ndogmatic people solve problems differently, spend-\ning less time framing a problem and expressing more\ncertainty in their solution (Lohman, 2010). Here we\nsimilarly examine how user behaviors on Reddit re-\nlate to a language model of dogmatism.\nErtel sought to capture dogmatism linguistically,\nthough a small lexicon of words that correspond\nwith high-level concepts like certainty and compro-\nmise (1985). McKenny then used this dictionary to\nrelate dogmatism to argument quality in student es-\nsays (2005). Our work expands on this approach,\napplying supervised models based on a broader set\nof linguistic categories to identify dogmatism in text.\nOther researchers have studied topics similar to\ndogmatism, such as signals of cognitive style in\nright-wing political thought (Van Hiel et al., 2010),\nthe language used by trolls on social media (Cheng\net al., 2015), or what makes for impartial language\non twitter (Zafar et al., 2016). A similar ﬂavor of\nwork has examined linguistic models that capture\npoliteness (Danescu-Niculescu-Mizil et al., 2013),\ndeception (Ott et al., 2011), and authority (Gilbert,\n2012). We took inspiration from these models when\nconstructing the feature sets in our work.\nFinally, while we examine what makes an opin-\nion dogmatic, other work has pushed further into the\nstructure of arguments, for example classifying their\njustiﬁcations (Hasan and Ng, 2014), or what makes\nan argument likely to win (Tan et al., 2016). Our']","The LIWC system does not specifically categorize words in relation to dogmatism. LIWC is a text analysis software that counts words in psychologically meaningful categories. While it can analyze categories such as emotional tone, social concerns, cognitive processes, and personal concerns, it does not have a specific category for dogmatism. However, researchers could potentially use the software to study dogmatism by identifying and analyzing words and phrases commonly associated with dogmatic language.",[''],The LIWC system categorizes words in relation to dogmatism based on high-level psychological categories such as certainty and tentativeness.,0.834898614431824,0.9332629451741664
22,What type of embeddings does ELMo's input layer produce?,The input layer of ELMo produces static word embeddings.,"['what extent they can be replaced with static word\nembeddings, if at all. Our work in this paper is\nthus markedly different from most dissections of\ncontextualized representations. It is more similar\nto Mimno and Thompson (2017), which studied\nthe geometry of static word embedding spaces.\n3 Approach\n3.1 Contextualizing Models\nThe contextualizing models we study in this pa-\nper are ELMo, BERT, and GPT-21. We choose\nthe base cased version of BERT because it is most\ncomparable to GPT-2 with respect to number of\nlayers and dimensionality. The models we work\nwith are all pre-trained on their respective lan-\nguage modelling tasks. Although ELMo, BERT,\nand GPT-2 have 2, 12, and 12 hidden layers re-\nspectively, we also include the input layer of each\ncontextualizing model as its 0thlayer. This is be-\ncause the 0thlayer is not contextualized, making\nit a useful baseline against which to compare the\ncontextualization done by subsequent layers.\n3.2 Data\nTo analyze contextualized word representations,\nwe need input sentences to feed into our pre-\ntrained models. Our input data come from the\nSemEval Semantic Textual Similarity tasks from\nyears 2012 - 2016 (Agirre et al., 2012, 2013, 2014,\n2015). We use these datasets because they contain\nsentences in which the same words appear in dif-\nferent contexts. For example, the word ‘dog’ ap-\npears in “A panda dog is running on the road. ”\nand“A dog is trying to get bacon off his back. ”\nIf a model generated the same representation for\n‘dog’ in both these sentences, we could infer that\nthere was no contextualization; conversely, if the\ntwo representations were different, we could infer\nthat they were contextualized to some extent. Us-\ning these datasets, we map words to the list of sen-\ntences they appear in and their index within these\nsentences. We do not consider words that appear\nin less than 5 unique contexts in our analysis.\n3.3 Measures of Contextuality\nWe measure how contextual a word representation\nis using three different metrics: self-similarity ,\nintra-sentence similarity , and maximum explain-\nable variance .\n1We use the pretrained models provided in an earlier ver-\nsion of the PyTorch-Transformers library.Deﬁnition 1 Letwbe a word that appears in\nsentences{s1,...,sn}at indices{i1,...,in}respec-\ntively, such that w=s1[i1] =...=sn[in]. Let fℓ(s,i)\nbe a function that maps s[i]to its representation in\nlayerℓof model f. The self similarity ofwin layer\nℓis\nSelfSimℓ(w) =1\nn2−n∑\nj∑\nk̸=jcos(fℓ(sj,ij),fℓ(sk,ik))\n(1)\nwhere cos denotes the cosine similarity. In other\nwords, the self-similarity of a word win layer ℓis\nthe average cosine similarity between its contextu-\nalized representations across its nunique contexts.\nIf layer ℓdoes not contextualize the representa-\ntions at all, then SelfSimℓ(w) =1 (i.e., the repre-\nsentations are identical across all contexts). The\nmore contextualized the representations are for w,\nthe lower we would expect its self-similarity to be.\nDeﬁnition 2 Letsbe a sentence that is a se-\nquence⟨w1,...,wn⟩ofnwords. Let fℓ(s,i)be a\nfunction that maps s[i]to its representation in layer\nℓof model f. The intra-sentence similarity ofsin\nlayer ℓis\nIntraSim ℓ(s) =1\nn∑\nicos(⃗sℓ,fℓ(s,i))\nwhere ⃗sℓ=1\nn∑\nifℓ(s,i)(2)\nPut more simply, the intra-sentence similarity of a\nsentence is the average cosine similarity between\nits word representations and the sentence vector,\nwhich is just the mean of those word vectors. This\nmeasure captures how context-speciﬁcity mani-\nfests in the vector space.'
 'These tasks cover semantic similarity,\nanalogy solving, and concept categorization: Sim-\nLex999 (Hill et al., 2015), MEN (Bruni et al.,\n2014), WS353 (Finkelstein et al., 2002), RW (Lu-\nong et al., 2013), SemEval-2012 (Jurgens et al.,\n2012), Google analogy solving (Mikolov et al.,\n2013a) MSR analogy solving (Mikolov et al.,\n2013b), BLESS (Baroni and Lenci, 2011) and AP\n(Almuhareb and Poesio, 2004). We leave out lay-\ners 3 - 10 in Table 1 because their performance is\n2The Word Embeddings Benchmarks package was used\nfor evaluation.between those of Layers 2 and 11.\nThe best-performing PC static embeddings be-\nlong to the ﬁrst layer of BERT, although those\nfrom the other layers of BERT and ELMo also out-\nperform GloVe and FastText on most benchmarks.\nFor all three contextualizing models, PC static em-\nbeddings created from lower layers are more effec-\ntive those created from upper layers. Those cre-\nated using GPT-2 also perform markedly worse\nthan their counterparts from ELMo and BERT.\nGiven that upper layers are much more context-\nspeciﬁc than lower layers, and given that GPT-\n2’s representations are more context-speciﬁc than\nELMo and BERT’s (see Figure 2), this suggests\nthat the PCs of highly context-speciﬁc representa-\ntions are less effective on traditional benchmarks.\nThose derived from less context-speciﬁc represen-']",ELMo's input layer produces character-based word embeddings.,[''],ELMo's input layer produces static character-level embeddings without using contextual or even positional information.,0.7315314717625631,0.7322785893651262
23,What is the concept of maximum explainable variance in the context of word embeddings?,"The concept of maximum explainable variance in the context of word embeddings refers to the proportion of variance in a word's contextualized representations that can be explained by their first principal component. It provides an upper limit on how well a static embedding could replace a word's contextualized representations. If the maximum explainable variance is closer to 0, it indicates that a static embedding would be a poor replacement, while a value of 1 suggests that a static embedding would perfectly replace the contextualized representations.","['Figure 4: The maximum explainable variance (MEV) of a word is the proportion of variance in its contextualized\nrepresentations that can be explained by their ﬁrst principal component (see Deﬁnition 3). Above, we plot the\naverage MEV of uniformly randomly sampled words after adjusting for anisotropy. In no layer of any model can\nmore than 5% of the variance in a word’s contextualized representations be explained by a static embedding.\nStatic Embedding SimLex999 MEN WS353 RW Google MSR SemEval2012(2) BLESS AP\nGloVe 0.194 0.216 0.339 0.127 0.189 0.312 0.097 0.390 0.308\nFastText 0.239 0.239 0.432 0.176 0.203 0.289 0.104 0.375 0.291\nELMo, Layer 1 0.276 0.167 0.317 0.148 0.170 0.326 0.114 0.410 0.308\nELMo, Layer 2 0.215 0.151 0.272 0.133 0.130 0.268 0.132 0.395 0.318\nBERT, Layer 1 0.315 0.200 0.394 0.208 0.236 0.389 0.166 0.365 0.321\nBERT, Layer 2 0.320 0.166 0.383 0.188 0.230 0.385 0.149 0.365 0.321\nBERT, Layer 11 0.221 0.076 0.319 0.135 0.175 0.290 0.149 0.370 0.289\nBERT, Layer 12 0.233 0.082 0.325 0.144 0.184 0.307 0.144 0.360 0.294\nGPT-2, Layer 1 0.174 0.012 0.176 0.183 0.052 0.081 0.033 0.220 0.184\nGPT-2, Layer 2 0.135 0.036 0.171 0.180 0.045 0.062 0.021 0.245 0.184\nGPT-2, Layer 11 0.126 0.034 0.165 0.182 0.031 0.038 0.045 0.270 0.189\nGPT-2, Layer 12 0.140 -0.009 0.113 0.163 0.020 0.021 0.014 0.225 0.172\nTable 1: The performance of various static embeddings on word embedding benchmark tasks. The best result for\neach task is in bold. For the contextualizing models (ELMo, BERT, GPT-2), we use the ﬁrst principal component\nof a word’s contextualized representations in a given layer as its static embedding. The static embeddings created\nusing ELMo and BERT’s contextualized representations often outperform GloVe and FastText vectors.\nearlier, we can create static embeddings for each\nword by taking the ﬁrst principal component (PC)\nof its contextualized representations in a given\nlayer. In Table 1, we plot the performance of\nthese PC static embeddings on several benchmark\ntasks2. These tasks cover semantic similarity,\nanalogy solving, and concept categorization: Sim-\nLex999 (Hill et al., 2015), MEN (Bruni et al.,\n2014), WS353 (Finkelstein et al., 2002), RW (Lu-\nong et al., 2013), SemEval-2012 (Jurgens et al.,\n2012), Google analogy solving (Mikolov et al.,\n2013a) MSR analogy solving (Mikolov et al.,\n2013b), BLESS (Baroni and Lenci, 2011) and AP\n(Almuhareb and Poesio, 2004). We leave out lay-\ners 3 - 10 in Table 1 because their performance is\n2The Word Embeddings Benchmarks package was used\nfor evaluation.between those of Layers 2 and 11.'
 'ﬁrstmsingular values of this matrix, the maximum\nexplainable variance is\nMEV ℓ(w) =σ2\n1\n∑iσ2\ni(3)\nMEV ℓ(w)is the proportion of variance in w’s con-\ntextualized representations for a given layer that\ncan be explained by their ﬁrst principal compo-\nnent. It gives us an upper bound on how well a\nstatic embedding could replace a word’s contex-\ntualized representations. The closer MEV ℓ(w)is\nto 0, the poorer a replacement a static embedding\nwould be; if MEV ℓ(w) =1, then a static embed-\nding would be a perfect replacement for the con-\ntextualized representations.\n3.4 Adjusting for Anisotropy\nIt is important to consider isotropy (or the lack\nthereof) when discussing contextuality. For ex-\nample, if word vectors were perfectly isotropic\n(i.e., directionally uniform), then SelfSimℓ(w) =\n0.95 would suggest that w’s representations were\npoorly contextualized. However, consider the sce-\nnario where word vectors are so anisotropic that\nany two words have on average a cosine similar-\nity of 0.99. Then SelfSimℓ(w) =0.95 would actu-\nally suggest the opposite – that w’s representations\nwere well contextualized. This is because repre-\nsentations of win different contexts would on av-\nerage be more dissimilar to each other than two\nrandomly chosen words.\nTo adjust for the effect of anisotropy, we use\nthree anisotropic baselines , one for each of our\ncontextuality measures. For self-similarity and\nintra-sentence similarity, the baseline is the aver-\nage cosine similarity between the representations\nof uniformly randomly sampled words from dif-\nferent contexts. The more anisotropic the word\nrepresentations are in a given layer, the closer this\nbaseline is to 1. For maximum explainable vari-\nance (MEV), the baseline is the proportion of vari-\nance in uniformly randomly sampled word repre-\nsentations that is explained by their ﬁrst principal\ncomponent. The more anisotropic the representa-\ntions in a given layer, the closer this baseline is\nto 1: even for a random assortment of words, the\nprincipal component would be able to explain a\nlarge proportion of the variance.\nSince contextuality measures are calculated for\neach layer of a contextualizing model, we cal-\nculate separate baselines for each layer as well.We then subtract from each measure its respective\nbaseline to get the anisotropy-adjusted contexual-\nity measure . For example, the anisotropy-adjusted\nself-similarity is\nBaseline (fℓ) =Ex,y∼U(O)[cos(fℓ(x),fℓ(y))]\nSelfSim∗\nℓ(w) =SelfSimℓ(w)−Baseline (fℓ)(4)\nwhereOis the set of all word occurrences and\nfℓ(·)maps a word occurrence to its representation\nin layer ℓof model f. Unless otherwise stated, ref-\nerences to contextuality measures in the rest of the\npaper refer to the anisotropy-adjusted measures,\nwhere both the raw measure and baseline are esti-\nmated with 1K uniformly randomly sampled word\nrepresentations.\n4 Findings\n4.1 (An)Isotropy\nContextualized representations are anisotropic\nin all non-input layers. If word representations\nfrom a particular layer were isotropic (i.e., direc-\ntionally uniform), then the average cosine similar-\nity between uniformly randomly sampled words\nwould be 0 (Arora et al., 2017). The closer this\naverage is to 1, the more anisotropic the represen-\ntations. The geometric interpretation of anisotropy\nis that the word representations all occupy a nar-\nrow cone in the vector space rather than being uni-\nform in all directions; the greater the anisotropy,\nthe narrower this cone (Mimno and Thompson,\n2017). As seen in Figure 1, this implies that in\nalmost all layers of BERT, ELMo and GPT-2, the\nrepresentations of all words occupy a narrow cone\nin the vector space. The only exception is ELMo’s\ninput layer, which produces static character-level\nembeddings without using contextual or even po-\nsitional information (Peters et al., 2018).']","The concept of maximum explainable variance in the context of word embeddings refers to the highest amount of variance that can be explained by the factors or dimensions in the word embedding model. In other words, it is the maximum amount of information from the original high-dimensional data that can be captured or represented in the lower-dimensional word embedding space. This concept is important in understanding the effectiveness and accuracy of the word embedding model in capturing the semantic and syntactic relationships between words.",[''],The concept of maximum explainable variance in the context of word embeddings refers to the proportion of variance in a word's contextualized representations that can be explained by their first principal component.,0.8844055510877276,0.6193194136119937
24,What is the role of the HR-BiLSTM model in the entity linking process in the KBQA pipeline system?,"The HR-BiLSTM model is utilized in the entity re-ranking step of the entity linking process in the KBQA pipeline system. It scores all relations in the knowledge base associated with the entity candidates, and these scores are used to re-rank the entity candidates. The model helps in improving the accuracy of entity linking by considering relation information in addition to entity names.","['Remark: Another way of hierarchical matching\nconsists in relying on attention mechanism , e.g.\n(Parikh et al., 2016), to ﬁnd the correspondence\nbetween different levels of representations. This\nperforms below the HR-BiLSTM (see Table 2).\n5 KBQA Enhanced by Relation\nDetection\nThis section describes our KBQA pipeline system.\nWe make minimal efforts beyond the training of\nthe relation detection model, making the whole\nsystem easy to build.\nFollowing previous work (Yih et al., 2015; Xu\net al., 2016), our KBQA system takes an existing\nentity linker to produce the top- Klinked entities,\nELK(q), for a question q(“initial entity linking ”).\nThen we generate the KB queries for qfollowing\nthe four steps illustrated in Algorithm 1.\nAlgorithm 1: KBQA with two-step relation detection\nInput : Question q, Knowledge Base KB, the initial\ntop-Kentity candidates ELK(q)\nOutput: Top query tuple (ˆe,ˆr,{(c, rc)})\n1Entity Re-Ranking (ﬁrst-step relation detection ): Use\ntheraw question text as input for a relation detector to\nscore all relations in the KB that are associated to the\nentities in ELK(q); use the relation scores to re-rank\nELK(q)and generate a shorter list EL′\nK′(q)\ncontaining the top- K′entity candidates (Section 5.1)\n2Relation Detection : Detect relation(s) using the\nreformatted question text in which the topic entity is\nreplaced by a special token <e>(Section 5.2)\n3Query Generation : Combine the scores from step 1\nand 2, and select the top pair (ˆe,ˆr)(Section 5.3)\n4Constraint Detection (optional): Compute similarity\nbetween qand any neighbor entity cof the entities\nalong ˆr(connecting by a relation rc) , add the high\nscoring candrcto the query (Section 5.4).\nCompared to previous approaches, the main dif-\nference is that we have an additional entity re-\nranking step after the initial entity linking . We\nhave this step because we have observed that entity\nlinking sometimes becomes a bottleneck in KBQA\nsystems. For example, on SimpleQuestions the\nbest reported linker could only get 72.7% top-1\naccuracy on identifying topic entities. This is usu-\nally due to the ambiguities of entity names, e.g. in\nFig 1(a), there are TV writer andbaseball player\n“Mike Kelley ”, which is impossible to distinguish\nwith only entity name matching.\nHaving observed that different entity candidates\nusually connect to different relations, here we pro-\npose to help entity disambiguation in the initial en-\ntity linking with relations detected in questions.Sections 5.1 and 5.2 elaborate how our relation\ndetection help to re-rank entities in the initial en-\ntity linking, and then those re-ranked entities en-\nable more accurate relation detection. The KBQA\nend task, as a result, beneﬁts from this process.\n5.1 Entity Re-Ranking\nIn this step, we use the raw question text as input\nfor a relation detector to score all relations in the\nKB with connections to at least one of the entity\ncandidates in ELK(q). We call this step relation\ndetection on entity set since it does not work on\na single topic entity as the usual settings. We use\nthe HR-BiLSTM as described in Sec. 4. For each\nquestionq, after generating a score srel(r;q)for\neach relation using HR-BiLSTM, we use the top\nlbest scoring relations ( Rl\nq) to re-rank the origi-\nnal entity candidates. Concretely, for each entity\neand its associated relations Re, given the origi-\nnal entity linker score slinker , and the score of the\nmost conﬁdent relation r∈Rl\nq∩Re, we sum these\ntwo scores to re-rank the entities:\nsrerank (e;q) =α·slinker(e;q)\n+(1−α)·max\nr∈Rlq∩Resrel(r;q).\nFinally, we select top K′<K entities according to\nscoresrerank to form the re-ranked list EL′\nK′(q).\nWe use the same example in Fig 1(a) to illustrate\nthe idea.'
 '58.0%) for the\nconstraint detection module to improve.\nFinally, like STAGG, which uses multiple rela-\ntion detectors (see Yih et al. (2015) for the three\nmodels used), we also try to use the top-3 rela-\ntion detectors from Section 6.2. As shown on the\nlast row of Table 3, this gives a signiﬁcant perfor-\nmance boost, resulting in a new state-of-the-art re-\nsult on SimpleQuestions and a result comparable\nto the state-of-the-art on WebQSP.\n7 Conclusion\nKB relation detection is a key step in KBQA and\nis signiﬁcantly different from general relation ex-\ntraction tasks. We propose a novel KB relation\ndetection model, HR-BiLSTM, that performs hier-\narchical matching between questions and KB rela-\ntions. Our model outperforms the previous meth-\nods on KB relation detection tasks and allows our\nKBQA system to achieve state-of-the-arts. For fu-\nture work, we will investigate the integration of\nour HR-BiLSTM into end-to-end systems. For ex-\nample, our model could be integrated into the de-\ncoder in (Liang et al., 2016), to provide better se-\nquence prediction. We will also investigate new\nemerging datasets like GraphQuestions (Su et al.,\n2016) and ComplexQuestions (Bao et al., 2016) to\nhandle more characteristics of general QA.\n12Note that another reason is that we are evaluating on ac-\ncuracy here. When evaluating on F1 the gap will be smaller.']","The HR-BiLSTM model in the entity linking process in the KBQA (Knowledge-Based Question Answering) pipeline system is responsible for recognizing and linking entities in the input question to the corresponding entities in the knowledge base. It uses a hybrid representation to encode the input question and applies a bidirectional long short-term memory (BiLSTM) network to capture the context information of the question. This helps in accurately identifying and linking the entities, which is crucial for understanding the question and providing the correct answer.",[''],The HR-BiLSTM model is used in the entity re-ranking step of the entity linking process in the KBQA pipeline system.,0.5372544025946459,0.5441063008790155
25,"What metrics measure word representations in ELMo, BERT, and GPT-2 models?","The metrics that measure word representations in ELMo, BERT, and GPT-2 models are self-similarity, intra-sentence similarity, and maximum explainable variance.","['what extent they can be replaced with static word\nembeddings, if at all. Our work in this paper is\nthus markedly different from most dissections of\ncontextualized representations. It is more similar\nto Mimno and Thompson (2017), which studied\nthe geometry of static word embedding spaces.\n3 Approach\n3.1 Contextualizing Models\nThe contextualizing models we study in this pa-\nper are ELMo, BERT, and GPT-21. We choose\nthe base cased version of BERT because it is most\ncomparable to GPT-2 with respect to number of\nlayers and dimensionality. The models we work\nwith are all pre-trained on their respective lan-\nguage modelling tasks. Although ELMo, BERT,\nand GPT-2 have 2, 12, and 12 hidden layers re-\nspectively, we also include the input layer of each\ncontextualizing model as its 0thlayer. This is be-\ncause the 0thlayer is not contextualized, making\nit a useful baseline against which to compare the\ncontextualization done by subsequent layers.\n3.2 Data\nTo analyze contextualized word representations,\nwe need input sentences to feed into our pre-\ntrained models. Our input data come from the\nSemEval Semantic Textual Similarity tasks from\nyears 2012 - 2016 (Agirre et al., 2012, 2013, 2014,\n2015). We use these datasets because they contain\nsentences in which the same words appear in dif-\nferent contexts. For example, the word ‘dog’ ap-\npears in “A panda dog is running on the road. ”\nand“A dog is trying to get bacon off his back. ”\nIf a model generated the same representation for\n‘dog’ in both these sentences, we could infer that\nthere was no contextualization; conversely, if the\ntwo representations were different, we could infer\nthat they were contextualized to some extent. Us-\ning these datasets, we map words to the list of sen-\ntences they appear in and their index within these\nsentences. We do not consider words that appear\nin less than 5 unique contexts in our analysis.\n3.3 Measures of Contextuality\nWe measure how contextual a word representation\nis using three different metrics: self-similarity ,\nintra-sentence similarity , and maximum explain-\nable variance .\n1We use the pretrained models provided in an earlier ver-\nsion of the PyTorch-Transformers library.Deﬁnition 1 Letwbe a word that appears in\nsentences{s1,...,sn}at indices{i1,...,in}respec-\ntively, such that w=s1[i1] =...=sn[in]. Let fℓ(s,i)\nbe a function that maps s[i]to its representation in\nlayerℓof model f. The self similarity ofwin layer\nℓis\nSelfSimℓ(w) =1\nn2−n∑\nj∑\nk̸=jcos(fℓ(sj,ij),fℓ(sk,ik))\n(1)\nwhere cos denotes the cosine similarity. In other\nwords, the self-similarity of a word win layer ℓis\nthe average cosine similarity between its contextu-\nalized representations across its nunique contexts.\nIf layer ℓdoes not contextualize the representa-\ntions at all, then SelfSimℓ(w) =1 (i.e., the repre-\nsentations are identical across all contexts). The\nmore contextualized the representations are for w,\nthe lower we would expect its self-similarity to be.\nDeﬁnition 2 Letsbe a sentence that is a se-\nquence⟨w1,...,wn⟩ofnwords. Let fℓ(s,i)be a\nfunction that maps s[i]to its representation in layer\nℓof model f. The intra-sentence similarity ofsin\nlayer ℓis\nIntraSim ℓ(s) =1\nn∑\nicos(⃗sℓ,fℓ(s,i))\nwhere ⃗sℓ=1\nn∑\nifℓ(s,i)(2)\nPut more simply, the intra-sentence similarity of a\nsentence is the average cosine similarity between\nits word representations and the sentence vector,\nwhich is just the mean of those word vectors. This\nmeasure captures how context-speciﬁcity mani-\nfests in the vector space.'
 'How Contextual are Contextualized Word Representations?\nComparing the Geometry of BERT, ELMo, and GPT-2 Embeddings\nKawin Ethayarajh∗\nStanford University\nkawin@stanford.edu\nAbstract\nReplacing static word embeddings with con-\ntextualized word representations has yielded\nsigniﬁcant improvements on many NLP tasks.\nHowever, just how contextual are the contex-\ntualized representations produced by models\nsuch as ELMo and BERT? Are there inﬁnitely\nmany context-speciﬁc representations for each\nword, or are words essentially assigned one of\na ﬁnite number of word-sense representations?\nFor one, we ﬁnd that the contextualized rep-\nresentations of all words are not isotropic in\nany layer of the contextualizing model. While\nrepresentations of the same word in differ-\nent contexts still have a greater cosine simi-\nlarity than those of two different words, this\nself-similarity is much lower in upper layers.\nThis suggests that upper layers of contextu-\nalizing models produce more context-speciﬁc\nrepresentations, much like how upper layers\nof LSTMs produce more task-speciﬁc repre-\nsentations. In all layers of ELMo, BERT, and\nGPT-2, on average, less than 5% of the vari-\nance in a word’s contextualized representa-\ntions can be explained by a static embedding\nfor that word, providing some justiﬁcation for\nthe success of contextualized representations.\n1 Introduction\nThe application of deep learning methods to NLP\nis made possible by representing words as vec-\ntors in a low-dimensional continuous space. Tradi-\ntionally, these word embeddings were static : each\nword had a single vector, regardless of context\n(Mikolov et al., 2013a; Pennington et al., 2014).\nThis posed several problems, most notably that\nall senses of a polysemous word had to share the\nsame representation. More recent work, namely\ndeep neural language models such as ELMo (Pe-\nters et al., 2018) and BERT (Devlin et al., 2018),\n∗Work partly done at the University of Toronto.have successfully created contextualized word rep-\nresentations , word vectors that are sensitive to\nthe context in which they appear. Replacing\nstatic embeddings with contextualized representa-\ntions has yielded signiﬁcant improvements on a di-\nverse array of NLP tasks, ranging from question-\nanswering to coreference resolution.\nThe success of contextualized word represen-\ntations suggests that despite being trained with\nonly a language modelling task, they learn highly\ntransferable and task-agnostic properties of lan-\nguage. In fact, linear probing models trained on\nfrozen contextualized representations can predict\nlinguistic properties of words (e.g., part-of-speech\ntags) almost as well as state-of-the-art models (Liu\net al., 2019a; Hewitt and Manning, 2019). Still,\nthese representations remain poorly understood.\nFor one, just how contextual are these contextu-\nalized word representations? Are there inﬁnitely\nmany context-speciﬁc representations that BERT\nand ELMo can assign to each word, or are words\nessentially assigned one of a ﬁnite number of\nword-sense representations?\nWe answer this question by studying the geom-\netry of the representation space for each layer of\nELMo, BERT, and GPT-2. Our analysis yields\nsome surprising ﬁndings:\n1. In all layers of all three models, the con-\ntextualized word representations of all words\nare not isotropic: they are not uniformly dis-\ntributed with respect to direction. Instead,\nthey are anisotropic , occupying a narrow\ncone in the vector space. The anisotropy in\nGPT-2’s last layer is so extreme that two ran-\ndom words will on average have almost per-\nfect cosine similarity! Given that isotropy\nhas both theoretical and empirical beneﬁts for\nstatic embeddings (Mu et al., 2018), the ex-\ntent of anisotropy in contextualized represen-arXiv:1909.00512v1  [cs.CL]  2 Sep 2019']","The metrics that measure word representations in ELMo, BERT, and GPT-2 models are typically Perplexity, F1 Score, and GLUE benchmark. These models are also evaluated based on their performance on specific tasks such as Named Entity Recognition (NER), Sentiment Analysis, Question Answering, and others.",[''],"We measure how contextual a word representation is using three different metrics: self-similarity, intra-sentence similarity, and maximum explainable variance.",0.21452721929320906,0.7338022147877458
26,How does MLM and BRLM-SA impact cross-lingual sentence representation and zero-shot translation?,"MLM and BRLM-SA impact cross-lingual sentence representation and zero-shot translation by enhancing the stability and similarity of cross-lingual representations of sentence pairs. BRLM-SA, in particular, demonstrates the most stable and similar cross-lingual representations of sentence pairs on all layers, leading to improved performance in zero-shot translation. This indicates that better cross-lingual representations can benefit the process of transfer learning, ultimately aiding in achieving better word-level representation alignment between source and pivot languages and alleviating the domain shift problem.","['SA with back translation also achieves better performance\nthan the original supervised Transformer.\nAnalysis\nSentence Representation. We ﬁrst evaluate the represen-\ntational invariance across languages for all cross-lingual pre-\ntraining methods. Following Arivazhagan et al. (2018), we\nadopt max-pooling operation to collect the sentence rep-\nresentation of each encoder layer for all source-pivot sen-\ntence pairs in the Europarl validation sets. Then we calcu-\nlate the cosine similarity for each sentence pair and aver-\nage all cosine scores. As shown in Figure 3, we can ob-\nserve that, MLM+BRLM-SA has the most stable and similar\ncross-lingual representations of sentence pairs on all layers,\nwhile it achieves the best performance in zero-shot transla-\ntion. This demonstrates that better cross-lingual representa-\ntions can beneﬁt for the process of transfer learning. Besides,\nMLM+BRLM-HA is not as superior as MLM+BRLM-\nSA and even worse than MLM+TLM on Fr-En, since\nMLM+BRLM-HA may suffer from the wrong alignment\nknowledge from an external aligner tool. We also ﬁnd an in-\nteresting phenomenon that as the number of layers increases,\nthe cosine similarity decreases.\nContextualized Word Representation. We further sam-\nple an English-Russian sentence pair from the MultiUN\nvalidation sets and visualize the cosine similarity between\nhidden states of the top encoder layer to further investi-\ngate the difference of all cross-lingual pre-training meth-\nods. As shown in Figure 4, the hidden states generated by\nMLM+BRLM-SA have higher similarity for two aligned\nwords. It indicates that MLM+BRLM-SA can gain bet-\nter word-level representation alignment between source and\npivot languages, which better relieves the burden of the do-\nmain shift problem .\nThe Effect of Freezing Parameters. To freeze parame-\nters is a common strategy to avoid catastrophic forgetting in\ntransfer learning (Howard and Ruder 2018). Table 4 shows\nthe performance of transfer learning with freezing different\nlayers on MultiUN test set, in which En →Ru denotes the\nparent model, Ar→Ru and Es→Ru are two child models,\nand all models are based on MLM+BRLM-SA. We can ﬁnd\nthat updating all parameters during training will cause a no-\ntable drop on the zero-shot direction due to the catastrophic\nforgetting. On the contrary, freezing all the parameters leads\nto the decline on supervised direction because the language\nfeatures extracted during pre-training is not sufﬁcient for\nMT task. Freezing the ﬁrst four layers of the transformer\nshows the best performance and keeps the balance between\npre-training and ﬁne-tuning.\nConclusion\nIn this paper, we propose a cross-lingual pretraining based\ntransfer approach for the challenging zero-shot translation\ntask, in which source and target languages have no parallel\ndata, while they both have parallel data with a high resource\n(a) MLM\n (b) MLM+TLM\n(c) MLM+BRLM-HA\n (d) MLM+BRLM-SA\nFigure 4: Cosine similarity visualization at word level given\nan English-Russian sentence pair from the MultiUN valida-\ntion sets. Brighter indicates higher similarity.\nFreezing Layers En→Ru Ar→Ru Es→Ru\nNone 37.80 16.09 19.80\n2 37.79 21.47 28.35\n4 37.55 25.49 30.47\n6 35.31 22.90 28.22\nTable 4: BLEU score of freezing different layers. The num-\nber in Freezing Layers column denotes that the number of\nencoder layers will not be updated.\npivot language. With the aim of building the language in-\nvariant representation between source and pivot languages\nfor smooth transfer of the parent model of pivot →target di-\nrection to the child model of source →target direction, we in-\ntroduce one monolingual pretraining method and two bilin-\ngual pretraining methods to construct an universal encoder\nfor the source and pivot languages. Experiments on public\ndatasets show that our approaches signiﬁcantly outperforms\nseveral strong baseline systems, and manifest the language\ninvariance characteristics in both sentence level and word\nlevel neural representations.\nAcknowledgments\nWe would like to thank the anonymous reviewers for the\nhelpful comments. This work was supported by National\nKey R&D Program of China (Grant No. 2016YFE0132100),\nNational Natural Science Foundation of China (Grant No.'
 'Figure 2: The overview of BRidge Language Modeling (BRLM). The BRLM extends MLM (Lample and Conneau 2019) to\npairs of parallel sentences and leverages explicit alignment information obtained by external aligner tool or additional attention\nlayer to encourage word representation alignment across different languages.\n•Directly translate source sentences into target sentences\nwith the parent model, which beneﬁts from the availabil-\nity of the universal encoder.\nThe key difﬁculty of this method is to ensure the intermedi-\nate representations of the universal encoder are language in-\nvariant. In the rest of this section, we ﬁrst present two exist-\ning methods yet to be explored in zero-shot translation, and\nthen propose a straightforward but effective cross-lingual\npre-training method. In the end, we present the whole train-\ning and inference protocol for transfer.\nMasked and Translation Language Model\nPretraining\nTwo existing cross-lingual pre-training methods, Masked\nLanguage Modeling (MLM) and Translation Language\nModeling (TLM), have shown their effectiveness on XNLI\ncross-lingual classiﬁcation task (Lample and Conneau 2019;\nHuang et al. 2019), but these methods have not been well\nstudied on cross-lingual generation tasks in zero-shot condi-\ntion. We attempt to take advantage of the cross-lingual abil-\nity of the two methods for zero-shot translation.\nSpeciﬁcally, MLM adopts the Cloze objective of\nBERT (Devlin et al. 2018) and predicts the masked words\nthat are randomly selected and replaced with [MASK] to-\nken on monolingual corpus. In practice, MLM takes dif-\nferent language monolingual corpora as input to ﬁnd fea-\ntures shared across different languages. With this method,\nword pieces shared in all languages have been mapped into\na shared space, which makes the sentence representationsacross different languages close (Pires, Schlinger, and Gar-\nrette 2019).\nSince MLM objective is unsupervised and only requires\nmonolingual data, TLM is designed to leverage parallel data\nwhen it is available. Actually, TLM is a simple extension of\nMLM, with the difference that TLM concatenates sentence\npair into a whole sentence, and then randomly masks words\nin both the source and target sentences. In this way, the\nmodel can either attend to surrounding words or to the trans-\nlation sentence, implicitly encouraging the model to align\nthe source and target language representations. Note that al-\nthough each sentence pair is formed into one sentence, the\npositions of the target sentence are reset to count form zero.\nBridge Language Model Pretraining\nAside from MLM and TLM, we propose BRidge Language\nModeling (BRLM) to further obtain word-level representa-\ntion alignment between different languages. This method is\ninspired by the assumption that if the feature spaces of dif-\nferent languages are aligned very well, the masked words\nin the corrupted sentence can also be guessed by the con-\ntext of the correspondingly aligned words on the other side.\nTo achieve this goal, BRLM is designed to strengthen the\nability to infer words across languages based on alignment\ninformation, instead of inferring words within monolingual\nsentence as in MLM or within the pseudo sentence formed\nby concatenating sentence pair as in TLM.\nAs illustrated in Figure 2, BRLM stacks shared encoder\nover both side sentences separately. In particular, we design\ntwo network structures for BRLM, which are divided into']","MLM (Masked Language Model) and BRLM-SA (Bidirectional Language Model with Self-Attention) significantly impact cross-lingual sentence representation and zero-shot translation. 

MLM is a pre-training method that helps in learning the contextual relations between words in a sentence. It improves the quality of sentence representation by understanding the semantics and syntax of the sentence. This is particularly useful in cross-lingual sentence representation as it aids in capturing the contextual meaning of sentences across different languages.

BRLM-SA, on the other hand, uses a self-attention mechanism to capture dependencies between all words in a sentence, regardless of their positions. This allows the model to better understand the structure and meaning of the sentence, which is crucial for accurate translation.

In terms of zero-shot translation, both MLM and BRLM-SA can be beneficial. Zero-shot translation refers to the ability to translate between a pair of languages without having seen any direct translation data between the two. MLM and BRLM-SA can help in this by learning a shared representation for all languages during pre-training, which can then be fine-tuned for specific translation tasks. This shared representation can help the model to generalize from seen language pairs to unseen ones, enabling zero-shot translation.",[''],"MLM and BRLM-SA have a positive impact on cross-lingual sentence representation and zero-shot translation. MLM alone performs better than cross-lingual transfer and achieves comparable results to pivoting. When MLM is combined with BRLM-SA, the performance is further improved. MLM+BRLM-SA performs the best among all proposed cross-lingual pretraining based transfer approaches.",0.8892519375529047,0.8413919030496367
27,How does BERT perform compared to other systems on the MEDDOCAN 2019 shared task dataset and the NUB ES-PHI dataset?,"BERT outperforms other systems on both the MEDDOCAN 2019 shared task dataset and the NUB ES-PHI dataset. In the experiments conducted, BERT demonstrates superior performance without needing any adaptation or domain-specific feature engineering. Particularly, BERT achieves higher recall compared to the other systems, which is crucial for sensitive data anonymization tasks.","['0 50 1000.40.60.81\ntrain data (%)precision\n0 50 1000.40.60.81\ntrain data (%)recall\n0 50 1000.40.60.81\ntrain data (%)F1-score\nCRF spaCy BERT\nFigure 2: Performance decay with decreasing amounts of training data on the sensitive information detection task in the\nNUB ES-PHI corpus\nDetection Classiﬁcation\nPrec Rec F1 Prec Rec F1\nCRF (Perez et al., 2019) 0.977 0.943 0.960 0.971 0.937 0.954\nspaCy (Perez et al., 2019) 0.967 0.953 0.965 0.965 0.947 0.956\nNLND S2 (Lange et al., 2019) 0.976 0.973 0.974 0.971 0.968 0.970\nNLND S3 (Lange et al., 2019) 0.975 0.975 0.975 0.970 0.969 0.970\nBERT + CRF (Mao and Liu, 2019) 0.968 0.919 0.943 0.965 0.912 0.937\nBERT 0.973 0.972 0.972 0.968 0.967 0.967\nTable 8: Results of Experiment B: MEDDOCAN\na larger extent from recall decline, which is not that marked\nin the case of BERT. Overall, these results indicate that the\ntransfer-learning achieved through the BERT multilingual\npre-trained model not only helps obtain better results, but\nalso lowers the need of manually labelled data for this ap-\nplication domain.\n4.2. Experiment B: MEDDOCAN\nThe results of the two MEDDOCAN scenarios –detection\nand classiﬁcation– are shown in Table 8. These results fol-\nlow the same pattern as in the previous experiments, with\nthe CRF classiﬁer being the most precise of all, and BERT\noutperforming both the CRF and spaCy classiﬁers thanks to\nits greater recall. We also show the results of Mao and Liu\n(2019) who, despite of having used a BERT-based system,\nachieve lower scores than our models. The reason why it\nshould be so remain unclear.\nWith regard to the winner of the MEDDOCAN shared\ntask, the BERT-based model has not improved the scores\nobtained by neither the domain-dependent (S3) nor the\ndomain-independent (S2) NLNDE model. However, at-\ntending to the obtained results, BERT remains only 0.3 F1-\nscore points behind, and would have achieved the second\nposition among all the MEDDOCAN shared task competi-\ntors. Taking into account that only 3% of the gold labels\nremain incorrectly annotated, the task can be considered al-\nmost solved, and it is not clear if the differences among the\nsystems are actually signiﬁcant, or whether they stem from\nminor variations in initialisation or a long-tail of minor la-belling inconsistencies.\n5. Conclusions and Future Work\nIn this work we have brieﬂy introduced the problems re-\nlated to data privacy protection in clinical domain. We have\nalso described some of the groundbreaking advances on the\nNatural Language Processing ﬁeld due to the appearance of\nTransformers-based deep-learning architectures and trans-\nfer learning from very large general-domain multilingual\ncorpora, focusing our attention in one of its most represen-\ntative examples, Google’s BERT model.\nIn order to assess the performance of BERT for Span-\nish clinical data anonymisation, we have conducted sev-\neral experiments with a BERT-based sequence labelling\napproach using the pre-trained multilingual BERT model\nshared by Google as the starting point for the model train-\ning. We have compared this BERT-based sequence la-\nbelling against other methods and systems. One of the ex-\nperiments uses the MEDDOCAN 2019 shared task dataset,\nwhile the other uses a novel Spanish clinical reports dataset\ncalled NUB ES-PHI.\nThe results of the experiments show that, in NUB ES-PHI,\nthe BERT-based model outperforms the other systems with-\nout requiring any adaptation or domain-speciﬁc feature en-\ngineering, just by being trained on the provided labelled\ndata.'
 'In order to assess the performance of BERT for Span-\nish clinical data anonymisation, we have conducted sev-\neral experiments with a BERT-based sequence labelling\napproach using the pre-trained multilingual BERT model\nshared by Google as the starting point for the model train-\ning. We have compared this BERT-based sequence la-\nbelling against other methods and systems. One of the ex-\nperiments uses the MEDDOCAN 2019 shared task dataset,\nwhile the other uses a novel Spanish clinical reports dataset\ncalled NUB ES-PHI.\nThe results of the experiments show that, in NUB ES-PHI,\nthe BERT-based model outperforms the other systems with-\nout requiring any adaptation or domain-speciﬁc feature en-\ngineering, just by being trained on the provided labelled\ndata. Interestingly, the BERT-based model obtains a re-\nmarkably higher recall than the other systems. High recall\nis a desirable outcome because, when anonymising sensible\ndocuments, the accidental leak of sensible data is likely to']",The text does not provide information on how BERT performs compared to other systems on the MEDDOCAN 2019 shared task dataset and the NUB ES-PHI dataset.,[''],The BERT-based model outperforms other systems on the MEDDOCAN 2019 shared task dataset and the NUB ES-PHI dataset.,0.7229998880663904,0.8769977708567686
28,What are the key components of KBQA examples and how do they contribute to the relation detection model?,"The key components of KBQA examples are entity linking and relation detection. Entity linking links n-grams in questions to KB entities, while relation detection identifies the KB relation(s) a question refers to. These components contribute to the relation detection model by enhancing each other. The relation detection model proposed in the context uses a hierarchical recurrent neural network with residual learning to detect KB relations based on input questions. By integrating entity linking and the relation detector, the two components work together to improve the overall performance of the KBQA system, leading to state-of-the-art accuracy for both single-relation and multi-relation question answering benchmarks.","['58.0%) for the\nconstraint detection module to improve.\nFinally, like STAGG, which uses multiple rela-\ntion detectors (see Yih et al. (2015) for the three\nmodels used), we also try to use the top-3 rela-\ntion detectors from Section 6.2. As shown on the\nlast row of Table 3, this gives a signiﬁcant perfor-\nmance boost, resulting in a new state-of-the-art re-\nsult on SimpleQuestions and a result comparable\nto the state-of-the-art on WebQSP.\n7 Conclusion\nKB relation detection is a key step in KBQA and\nis signiﬁcantly different from general relation ex-\ntraction tasks. We propose a novel KB relation\ndetection model, HR-BiLSTM, that performs hier-\narchical matching between questions and KB rela-\ntions. Our model outperforms the previous meth-\nods on KB relation detection tasks and allows our\nKBQA system to achieve state-of-the-arts. For fu-\nture work, we will investigate the integration of\nour HR-BiLSTM into end-to-end systems. For ex-\nample, our model could be integrated into the de-\ncoder in (Liang et al., 2016), to provide better se-\nquence prediction. We will also investigate new\nemerging datasets like GraphQuestions (Su et al.,\n2016) and ComplexQuestions (Bao et al., 2016) to\nhandle more characteristics of general QA.\n12Note that another reason is that we are evaluating on ac-\ncuracy here. When evaluating on F1 the gap will be smaller.'
 'Improved Neural Relation Detection for Knowledge Base Question\nAnswering\nMo Yu†Wenpeng Yin⋆Kazi Saidul Hasan‡Cicero dos Santos†\nBing Xiang‡Bowen Zhou†\n†AI Foundations, IBM Research, USA\n⋆Center for Information and Language Processing, LMU Munich\n‡IBM Watson, USA\n{yum,kshasan,cicerons,bingxia,zhou }@us.ibm.com, wenpeng@cis.lmu.de\nAbstract\nRelation detection is a core component of\nmany NLP applications including Knowl-\nedge Base Question Answering (KBQA).\nIn this paper, we propose a hierarchi-\ncal recurrent neural network enhanced by\nresidual learning which detects KB re-\nlations given an input question. Our\nmethod uses deep residual bidirectional\nLSTMs to compare questions and rela-\ntion names via different levels of abstrac-\ntion. Additionally, we propose a sim-\nple KBQA system that integrates entity\nlinking and our proposed relation detec-\ntor to make the two components enhance\neach other. Our experimental results show\nthat our approach not only achieves out-\nstanding relation detection performance,\nbut more importantly, it helps our KBQA\nsystem achieve state-of-the-art accuracy\nfor both single-relation (SimpleQuestions)\nand multi-relation (WebQSP) QA bench-\nmarks.\n1 Introduction\nKnowledge Base Question Answering (KBQA)\nsystems answer questions by obtaining informa-\ntion from KB tuples (Berant et al., 2013; Yao et al.,\n2014; Bordes et al., 2015; Bast and Haussmann,\n2015; Yih et al., 2015; Xu et al., 2016). For an\ninput question, these systems typically generate a\nKB query, which can be executed to retrieve the\nanswers from a KB. Figure 1 illustrates the process\nused to parse two sample questions in a KBQA\nsystem: (a) a single-relation question, which can\nbe answered with a single <head-entity, relation,\ntail-entity>KB tuple (Fader et al., 2013; Yih et al.,\n2014; Bordes et al., 2015); and (b) a more complex\ncase, where some constraints need to be handledfor multiple entities in the question. The KBQA\nsystem in the ﬁgure performs two key tasks: (1)\nentity linking , which links n-grams in questions\nto KB entities, and (2) relation detection , which\nidentiﬁes the KB relation(s) a question refers to.\nThe main focus of this work is to improve the\nrelation detection subtask and further explore how\nit can contribute to the KBQA system. Although\ngeneral relation detection1methods are well stud-\nied in the NLP community, such studies usually\ndo not take the end task of KBQA into considera-\ntion. As a result, there is a signiﬁcant gap between\ngeneral relation detection studies and KB-speciﬁc\nrelation detection. First, in most general relation\ndetection tasks, the number of target relations is\nlimited, normally smaller than 100. In contrast, in\nKBQA even a small KB, like Freebase2M (Bor-\ndes et al., 2015), contains more than 6,000 relation\ntypes. Second, relation detection for KBQA often\nbecomes a zero-shot learning task, since some test\ninstances may have unseen relations in the training\ndata. For example, the SimpleQuestions (Bordes\net al., 2015) data set has 14% of the golden test\nrelations not observed in golden training tuples.\nThird, as shown in Figure 1(b), for some KBQA\ntasks like WebQuestions (Berant et al., 2013), we\nneed to predict a chain of relations instead of a\nsingle relation. This increases the number of tar-\nget relation types and the sizes of candidate rela-\ntion pools, further increasing the difﬁculty of KB\nrelation detection. Owing to these reasons, KB re-\nlation detection is signiﬁcantly more challenging\ncompared to general relation detection tasks.\nThis paper improves KB relation detection to\ncope with the problems mentioned above. First, in\norder to deal with the unseen relations, we propose\nto break the relation names into word sequences\nfor question-relation matching. Second, noticing\n1In the information extraction ﬁeld such tasks are usually\ncalled relation extraction orrelation classiﬁcation .arXiv:1704.06194v2  [cs.CL]  27 May 2017']","KBQA examples, or Knowledge-Based Question Answering examples, typically consist of three key components: the question, the entity, and the relation. 

1. The Question: This is the user's input or query. It is the problem that the system needs to solve. The question is processed and analyzed to extract the necessary information for the answer.

2. The Entity: This is the subject of the question. It is the specific item or concept that the question is about. The entity is identified and used to search the knowledge base for relevant information.

3. The Relation: This is the connection between the entity and the answer. It is the specific type of information that the question is asking for about the entity. The relation is used to narrow down the search in the knowledge base and find the exact answer.

These components contribute to the relation detection model by providing the necessary information to understand the question and find the correct answer. The question provides the context, the entity provides the focus, and the relation provides the direction. Together, they allow the model to accurately and efficiently search the knowledge base and generate the correct answer.",[''],"The key components of KBQA examples are entity linking, relation detection, and constraint detection. They contribute to the relation detection model by providing input for scoring relations and filtering candidate entities.",0.7690625239502518,0.817166211899801
29,What approach is proposed in Cross-lingual Pre-training research to address language space mismatch and enable zero-shot translation?,The proposed approach in Cross-lingual Pre-training research to address language space mismatch and enable zero-shot translation involves leveraging cross-lingual pre-training methods to ensure that all source languages share the same feature space.,"['Cross-lingual Pre-training Based Transfer for Zero-shot Neural\nMachine Translation\nBaijun Ji‡, Zhirui Zhang§, Xiangyu Duan†‡∗, Min Zhang†‡, Boxing Chen§and Weihua Luo§\n†Institute of Artiﬁcial Intelligence, Soochow University, Suzhou, China\n‡School of Computer Science and Technology, Soochow University, Suzhou, China\n§Alibaba DAMO Academy, Hangzhou, China\n‡bjji@stu.suda.edu.cn†{xiangyuduan, minzhang}@suda.edu.cn\n§{zhirui.zzr,boxing.cbx,weihua.luowh}@alibaba-inc.com\nAbstract\nTransfer learning between different language pairs has shown\nits effectiveness for Neural Machine Translation (NMT) in\nlow-resource scenario. However, existing transfer methods\ninvolving a common target language are far from success in\nthe extreme scenario of zero-shot translation, due to the lan-\nguage space mismatch problem between transferor (the par-\nent model) and transferee (the child model) on the source\nside. To address this challenge, we propose an effective trans-\nfer learning approach based on cross-lingual pre-training. Our\nkey idea is to make all source languages share the same fea-\nture space and thus enable a smooth transition for zero-shot\ntranslation. To this end, we introduce one monolingual pre-\ntraining method and two bilingual pre-training methods to\nobtain a universal encoder for different languages. Once the\nuniversal encoder is constructed, the parent model built on\nsuch encoder is trained with large-scale annotated data and\nthen directly applied in zero-shot translation scenario. Exper-\niments on two public datasets show that our approach signif-\nicantly outperforms strong pivot-based baseline and various\nmultilingual NMT approaches.\nIntroduction\nAlthough Neural Machine Translation (NMT) has domi-\nnated recent research on translation tasks (Wu et al. 2016;\nVaswani et al. 2017; Hassan et al. 2018), NMT heavily relies\non large-scale parallel data, resulting in poor performance\non low-resource or zero-resource language pairs (Koehn\nand Knowles 2017). Translation between these low-resource\nlanguages (e.g., Arabic →Spanish) is usually accomplished\nwith pivoting through a rich-resource language (such as En-\nglish), i.e., Arabic (source) sentence is translated to En-\nglish (pivot) ﬁrst which is later translated to Spanish (tar-\nget) (Kauers et al. 2002; de Gispert and Mariño 2006).\nHowever, the pivot-based method requires doubled decoding\ntime and suffers from the propagation of translation errors.\nOne common alternative to avoid pivoting in NMT is\ntransfer learning (Zoph et al. 2016; Nguyen and Chiang\n2017; Kocmi and Bojar 2018; Kim et al. 2019) which lever-\nages a high-resource pivot →target model ( parent ) to ini-\n∗Corresponding Author.\nCopyright c⃝2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The circle and triangle dots represent source sen-\ntences in different language l1andl2, and the square dots\nmeans target sentences in language l3. A sample of transla-\ntion pairs is connected by the dashed line. We would like to\nforce each of the translation pairs has the same latent rep-\nresentation as the right part of the ﬁgure so as to transfer\nl1→l3model directly to l2→l3model.\ntialize a low-resource source →target model ( child ) that is\nfurther optimized with a small amount of available paral-\nlel data. Although this approach has achieved success in\nsome low-resource language pairs, it still performs very\npoorly in extremely low-resource or zero-resource transla-\ntion scenario. Speciﬁcally, Kocmi and Bojar (2018) reports\nthat without any child model training data, the performance\nof the parent model on the child test set is miserable.\nIn this work, we argue that the language space mis-\nmatch problem, also named domain shift problem (Fu et al.\n2015), brings about the zero-shot translation failure in trans-\nfer learning.'
 'Although\nthese methods are successful in the low-resource scene,\nthey have limited effects in zero-shot translation.\n•Multilingual NMT (MNMT) enables training a single\nmodel that supports translation from multiple source lan-\nguages into multiple target languages, even those unseen\nlanguage pairs (Firat, Cho, and Bengio 2016; Firat et al.\n2016; Johnson et al. 2016; Al-Shedivat and Parikh 2019;\nAharoni, Johnson, and Firat 2019). Aside from sim-\npler deployment, MNMT beneﬁts from transfer learning\nwhere low-resource language pairs are trained together\nwith high-resource ones. However, Gu et al. (2019) point\nout that MNMT for zero-shot translation easily fails, and\nis sensitive to the hyper-parameter setting. Also, MNMT\nusually performs worse than the pivot-based method in\nzero-shot translation setting (Arivazhagan et al. 2018).\n•Unsupervised NMT (UNMT) considers a harder setting,\nin which only large-scale monolingual corpora are avail-\nable for training. Recently, many methods have been pro-\nposed to improve the performance of UNMT, including\nusing denoising auto-encoder, statistic machine transla-\ntion (SMT) and unsupervised pre-training (Artetxe et\nal. 2017; Lample et al. 2018; Ren et al. 2019; Lample\nand Conneau 2019). Since UNMT performs well between\nsimilar languages (e.g., English-German translation), its\nperformance between distant languages is still far from\nexpectation.\nOur proposed method belongs to the transfer learning,\nbut it is different from traditional transfer methods which\ntrain a parent model as starting point. Before training a par-\nent model, our approach fully leverages cross-lingual pre-\ntraining methods to make all source languages share the\nsame feature space and thus enables a smooth transition for\nzero-shot translation.\nApproach\nIn this section, we will present a cross-lingual pre-\ntraining based transfer approach. This method is designed\nfor a common zero-shot scenario where there are a lot\nof source↔pivot and pivot↔target bilingual data but no\nsource↔target parallel data, and the whole training process\ncan be summarized as follows step by step:\n•Pre-train a universal encoder with source/pivot monolin-\ngual or source↔pivot bilingual data.\n•Train a pivot→target parent model built on the pre-trained\nuniversal encoder with the available parallel data. Dur-\ning the training process, we freeze several layers of the\npre-trained universal encoder to avoid the degeneracy is-\nsue (Howard and Ruder 2018).']","The approach proposed in Cross-lingual Pre-training research to address language space mismatch and enable zero-shot translation is the use of a shared multilingual pre-training model. This model is trained on multiple languages and can generate representations that are shared across languages. This allows the model to leverage the knowledge learned from one language to another, enabling zero-shot translation.",[''],The proposed approach in Cross-lingual Pre-training research to address language space mismatch and enable zero-shot translation is to use cross-lingual language model pre-training.,0.8858839608330311,0.6195877855846365
30,What is the role of improved relation detection in a KBQA system and how does it contribute to achieving state-of-the-art results in KBQA tasks?,"Improved relation detection in a KBQA system plays a crucial role in enhancing the system's performance by accurately identifying the KB relations referenced in a given question. This enhancement allows the KBQA system to generate more precise KB queries, leading to improved accuracy in retrieving answers from the knowledge base. By integrating entity linking with the proposed relation detector, the two components complement each other, resulting in a synergistic effect that boosts the overall performance of the KBQA system. The refined relation detection method not only achieves outstanding results in relation detection but also significantly contributes to the system achieving state-of-the-art accuracy in both single-relation and multi-relation KBQA benchmarks.","['Improved Neural Relation Detection for Knowledge Base Question\nAnswering\nMo Yu†Wenpeng Yin⋆Kazi Saidul Hasan‡Cicero dos Santos†\nBing Xiang‡Bowen Zhou†\n†AI Foundations, IBM Research, USA\n⋆Center for Information and Language Processing, LMU Munich\n‡IBM Watson, USA\n{yum,kshasan,cicerons,bingxia,zhou }@us.ibm.com, wenpeng@cis.lmu.de\nAbstract\nRelation detection is a core component of\nmany NLP applications including Knowl-\nedge Base Question Answering (KBQA).\nIn this paper, we propose a hierarchi-\ncal recurrent neural network enhanced by\nresidual learning which detects KB re-\nlations given an input question. Our\nmethod uses deep residual bidirectional\nLSTMs to compare questions and rela-\ntion names via different levels of abstrac-\ntion. Additionally, we propose a sim-\nple KBQA system that integrates entity\nlinking and our proposed relation detec-\ntor to make the two components enhance\neach other. Our experimental results show\nthat our approach not only achieves out-\nstanding relation detection performance,\nbut more importantly, it helps our KBQA\nsystem achieve state-of-the-art accuracy\nfor both single-relation (SimpleQuestions)\nand multi-relation (WebQSP) QA bench-\nmarks.\n1 Introduction\nKnowledge Base Question Answering (KBQA)\nsystems answer questions by obtaining informa-\ntion from KB tuples (Berant et al., 2013; Yao et al.,\n2014; Bordes et al., 2015; Bast and Haussmann,\n2015; Yih et al., 2015; Xu et al., 2016). For an\ninput question, these systems typically generate a\nKB query, which can be executed to retrieve the\nanswers from a KB. Figure 1 illustrates the process\nused to parse two sample questions in a KBQA\nsystem: (a) a single-relation question, which can\nbe answered with a single <head-entity, relation,\ntail-entity>KB tuple (Fader et al., 2013; Yih et al.,\n2014; Bordes et al., 2015); and (b) a more complex\ncase, where some constraints need to be handledfor multiple entities in the question. The KBQA\nsystem in the ﬁgure performs two key tasks: (1)\nentity linking , which links n-grams in questions\nto KB entities, and (2) relation detection , which\nidentiﬁes the KB relation(s) a question refers to.\nThe main focus of this work is to improve the\nrelation detection subtask and further explore how\nit can contribute to the KBQA system. Although\ngeneral relation detection1methods are well stud-\nied in the NLP community, such studies usually\ndo not take the end task of KBQA into considera-\ntion. As a result, there is a signiﬁcant gap between\ngeneral relation detection studies and KB-speciﬁc\nrelation detection. First, in most general relation\ndetection tasks, the number of target relations is\nlimited, normally smaller than 100. In contrast, in\nKBQA even a small KB, like Freebase2M (Bor-\ndes et al., 2015), contains more than 6,000 relation\ntypes. Second, relation detection for KBQA often\nbecomes a zero-shot learning task, since some test\ninstances may have unseen relations in the training\ndata. For example, the SimpleQuestions (Bordes\net al., 2015) data set has 14% of the golden test\nrelations not observed in golden training tuples.\nThird, as shown in Figure 1(b), for some KBQA\ntasks like WebQuestions (Berant et al., 2013), we\nneed to predict a chain of relations instead of a\nsingle relation. This increases the number of tar-\nget relation types and the sizes of candidate rela-\ntion pools, further increasing the difﬁculty of KB\nrelation detection. Owing to these reasons, KB re-\nlation detection is signiﬁcantly more challenging\ncompared to general relation detection tasks.\nThis paper improves KB relation detection to\ncope with the problems mentioned above. First, in\norder to deal with the unseen relations, we propose\nto break the relation names into word sequences\nfor question-relation matching. Second, noticing\n1In the information extraction ﬁeld such tasks are usually\ncalled relation extraction orrelation classiﬁcation .arXiv:1704.06194v2  [cs.CL]  27 May 2017'
 '58.0%) for the\nconstraint detection module to improve.\nFinally, like STAGG, which uses multiple rela-\ntion detectors (see Yih et al. (2015) for the three\nmodels used), we also try to use the top-3 rela-\ntion detectors from Section 6.2. As shown on the\nlast row of Table 3, this gives a signiﬁcant perfor-\nmance boost, resulting in a new state-of-the-art re-\nsult on SimpleQuestions and a result comparable\nto the state-of-the-art on WebQSP.\n7 Conclusion\nKB relation detection is a key step in KBQA and\nis signiﬁcantly different from general relation ex-\ntraction tasks. We propose a novel KB relation\ndetection model, HR-BiLSTM, that performs hier-\narchical matching between questions and KB rela-\ntions. Our model outperforms the previous meth-\nods on KB relation detection tasks and allows our\nKBQA system to achieve state-of-the-arts. For fu-\nture work, we will investigate the integration of\nour HR-BiLSTM into end-to-end systems. For ex-\nample, our model could be integrated into the de-\ncoder in (Liang et al., 2016), to provide better se-\nquence prediction. We will also investigate new\nemerging datasets like GraphQuestions (Su et al.,\n2016) and ComplexQuestions (Bao et al., 2016) to\nhandle more characteristics of general QA.\n12Note that another reason is that we are evaluating on ac-\ncuracy here. When evaluating on F1 the gap will be smaller.']","Improved relation detection in a Knowledge Base Question Answering (KBQA) system plays a crucial role in understanding the semantic relationships between entities in a given question. It helps in accurately mapping the natural language questions to the corresponding entities and relations in the knowledge base. 

The contribution of improved relation detection to achieving state-of-the-art results in KBQA tasks is significant. It enhances the precision of the system by reducing the ambiguity in understanding the question's intent. It also helps in better handling of complex questions that involve multiple entities and relations. 

Moreover, improved relation detection can help in dealing with unseen or rare relations that are not frequently present in the training data, thereby improving the system's generalization capability. It also aids in better handling of synonyms or paraphrases of relations, which can further improve the system's robustness and accuracy. 

In summary, improved relation detection is a key factor in enhancing the performance of a KBQA system, enabling it to deliver more accurate and reliable answers.",[''],"The improved relation detection model in a KBQA system plays a key role in re-ranking entity candidates based on high confident relations detected from the raw question text. This helps to deal with ambiguities in entity linking results. Additionally, the improved relation detector enables the KBQA system to achieve state-of-the-art results in both single-relation and multi-relation KBQA tasks.",0.9887569177439062,0.8932094172148669
31,"""How does adversarial learning in worker adversarial settings address large divergences between training and test examples, and what modifications are made to the original adversarial neural network in this context?""","Adversarial learning in worker adversarial settings addresses large divergences between training and test examples by adding a discriminator to classify the annotation workers. This modification to the original adversarial neural network involves using both the source sentences and the crowd-annotated NE labels as basic inputs for worker discrimination, in addition to the features from the input sentence.","['Worker Adversarial\nAdversarial learning has been an effective mechanism to re-\nsolve the problem of the input features between the training\nand test examples having large divergences (Goodfellow et\nal. 2014; Ganin et al. 2016). It has been successfully applied\non domain adaption (Gui et al. 2017), cross-lingual learn-\ning (Chen et al. 2016) and multi-task learning (Liu, Qiu, and\nHuang 2017). All settings involve feature shifting between\nthe training and testing.\nIn this paper, our setting is different. We are using the\nannotations from non-experts, which are noise and can in-\nﬂuence the ﬁnal performances if they are not properly pro-\ncessed. Directly learning based on the resulting corpus may\nadapt the neural feature extraction into the biased annota-\ntions. In this work, we assume that individual workers have\ntheir own guidelines in mind after short training. For exam-\nple, a perfect worker can annotate highly consistently with\nan expert, while common crowdsourcing workers may be\nconfused and have different understandings on certain con-\ntexts. Based on the assumption, we make an adaption for the\noriginal adversarial neural network to our setting.\nOur adaption is very simple. Brieﬂy speaking, the original\nadversarial learning adds an additional discriminator to clas-\nsify the type of source inputs, for example, the domain cate-\ngory in the domain adaption setting, while we add a discrim-\ninator to classify the annotation workers. Solely the features\nfrom the input sentence is not enough for worker classiﬁ-\ncation. The annotation result of the worker is also required.\nThus the inputs of our discriminator are different. Here we\nexploit both the source sentences and the crowd-annotated\nNE labels as basic inputs for the worker discrimination.\nIn the following, we describe the proposed adversarial\nlearning module, including both the submodels and the train-\ning method. As shown by the left part of Figure 1, the\nsubmodel consists of four parts: (1) a common Bi-LSTM\nover input characters; (2) an additional Bi-LSTM to en-\ncode crowd-annotated NE label sequence; (3) a convolu-\ntional neural network (CNN) to extract features for worker\ndiscriminator; (4) output and prediction.\nCommon Bi-LSTM over Characters\nTo build the adversarial part, ﬁrst we create a new bi-\ndirectional LSTM, named by the common Bi-LSTM:\nhcommon\n1hcommon\n2···hcommon\nn =Bi-LSTM (x1x2···xn).(5)\nAs shown in Figure 1, this Bi-LSTM is constructed over\nthe same input character representations of the private Bi-\nLSTM, in order to extract worker independent features.\nThe resulting features of the common Bi-LSTM are used\nfor both NER and the worker discriminator, different with\nthe features of private Bi-LSTM which are used for NER\nonly. As shown in Figure 1, we concatenate the outputs of\nthe common and private Bi-LSTMs together, and then feed\nthe results into the feed-forward combination layer of the\nNER part. Thus Formula 1 can be rewritten as:\nhner\nt=W(hcommon\nt⊕hprivate\nt) +b, (6)\nwhere Wis wider than the original combination because the\nnewly-added hcommon\nt .Noticeably, although the resulting common features are\nused for the worker discriminator, they actually have no ca-\npability to distinguish the workers. Because this part is ex-\nploited to maximize the loss of the worker discriminator, it\nwill be interpreted in the later training subsection. These fea-\ntures are invariant among different workers, thus they can\nhave less noises for NER. This is the goal of adversarial\nlearning, and we hope the NER being able to ﬁnd useful\nfeatures from these worker independent features.\nAdditional Bi-LSTM over Annotated NER Labels\nIn order to incorporate the annotated NE labels to predict the\nexact worker, we build another bi-directional LSTM (named\nby label Bi-LSTM) based on the crowd-annotated NE label\nsequence. This Bi-LSTM is used for worker discriminator\nonly. During the decoding of the testing phase, we will never\nhave this Bi-LSTM, because the worker discriminator is no\nlonger required.'
 'The prediction is to ﬁnd the worker which is responsible for\nthis annotation.\nAdversarial Training\nThe training objective with adversarial neural network is\ndifferent from the baseline model, as it includes the ex-\ntra worker discriminator. Thus the new objective includes\ntwo parts, one being the negative log-likelihood from NER\nwhich is the same as the baseline, and the other being the\nnegative the log-likelihood from the worker discriminator.\nIn order to obtain the negative log-likelihood of the\nworker discriminator, we use softmax to compute the prob-\nability of the actual worker ¯zas well, which is deﬁned by:\np(¯z|X,¯ y) =exp(oworker\n¯z)∑\nzexp(oworkerz), (10)\nwhere zshould enumerate all workers.\nBased on the above deﬁnition of probability, our new ob-\njective is deﬁned as follows:\nR(Θ,Θ′,X,¯ y,¯z) =loss(Θ,X,¯ y)−loss(Θ,Θ′,X)\n=−logp(¯ y|X) + log p(¯z|X,¯ y),(11)\nwhere Θis the set of all model parameters related to NER,\nandΘ′is the set of the remaining parameters which are only\nrelated to the worker discriminator, X,¯ yand¯zare the in-\nput sentence, the crowd-annotated NE labels and the cor-\nresponding annotator for this annotation, respectively. It is\nworth noting that the parameters of the common Bi-LSTM\nare included in the set of Θby deﬁnition.\nIn particular, our goal is not to simply minimize the new\nobjective. Actually, we aim for a saddle point, ﬁnding the\nparameters ΘandΘ′satisfying the following conditions:\nˆΘ = arg min\nΘR(Θ,Θ′,X,¯ y,¯z)\nˆΘ′= arg max\nΘ′R(ˆΘ,Θ′,X,¯ y,¯z)(12)\nwhere the ﬁrst equation aims to ﬁnd one Θthat minimizes\nour new objective R (·), and the second equation aims to ﬁnd\noneΘ′maximizing the same objective.\nIntuitively, the ﬁrst equation of Formula 12 tries to min-\nimize the NER loss, but at the same time maximize the\nworker discriminator loss by the shared parameters of the\ncommon Bi-LSTM. Thus the resulting features of common\nBi-LSTM actually attempt to hurt the worker discrimina-\ntor, which makes these features worker independent since\nthey are unable to distinguish different workers. The second\nequation tries to minimize the worker discriminator loss by\nits own parameter Θ′.\nWe use the standard back-propagation method to train the\nmodel parameters, the same as the baseline model. In order\nto incorporate the term of the argmax part of Formula 12 ,\nwe follow the previous work of adversarial training (Ganin\net al. 2016; Chen et al. 2016; Liu, Qiu, and Huang 2017),\nby introducing a gradient reverse layer between the com-\nmon Bi-LSTM and the CNN module, whose forward does\nnothing but the backward simply negates the gradients.#Sent AvgLen Kappa\nDL-PS 16,948 9.21 0.6033\nUC-MT 2,337 34.97 0.7437\nUC-UQ 2,300 7.69 0.7529\nTable 1: Statistics of labeled datasets.\nExperiments\nData Sets\nWith the purpose of obtaining evaluation datasets from\ncrowd annotators, we collect the sentences from two do-\nmains: Dialog and E-commerce domain. We hire undergrad-\nuate students to annotate the sentences. They are required to\nidentify the predeﬁned types of entities in the sentences. To-\ngether with the guideline document, the annotators are edu-\ncated some tips in ﬁfteen minutes and also provided with 20\nexemplifying sentences.\nLabeled Data: DL-PS. In Dialog domain (DL), we collect\nraw sentences from a chatbot application. And then we ran-\ndomly select 20K sentences as our pool and hire 43 students\nto annotate the sentences. We ask the annotators to label two\ntypes of entities: Person-Name and Song-Name. The anno-\ntators label the sentences independently.']","Adversarial learning in worker adversarial settings addresses large divergences between training and test examples by creating a robust model that can handle the discrepancies between the training and testing data. This is achieved by introducing adversarial examples during the training process, which are designed to mislead the model, thereby forcing it to learn more generalized features and become more robust to unseen data.

In terms of modifications to the original adversarial neural network, the main change is the introduction of a second neural network, known as the adversary. This adversary network is trained to generate adversarial examples that maximize the error of the main network. The main network, in turn, is trained to minimize its error on both the original and adversarial examples. This creates a dynamic where the main network is constantly adapting to more challenging examples, thereby improving its ability to handle large divergences between training and test examples. 

Additionally, the loss function may be modified to include a term that penalizes large differences between the predictions on the original and adversarial examples. This encourages the network to learn a more robust representation that is less sensitive to small perturbations in the input data.",[''],Adversarial learning in worker adversarial settings addresses large divergences between training and test examples by adding a discriminator to classify the annotation workers. Modifications made to the original adversarial neural network in this context include using both the source sentences and the crowd-annotated NE labels as inputs for the worker discriminator.,0.8894954374746196,0.6244295890063573
32,"""What were the results of the experiments comparing BERT-based models with other systems in the NUB ES-PHI and MEDDOCAN datasets?""","In the NUB ES-PHI dataset, the results of the experiments showed that the BERT-based model outperformed the other systems, achieving a higher recall without needing any adaptation or domain-specific feature engineering. In the MEDDOCAN dataset, the BERT-based model also showed superior performance compared to other systems, particularly excelling in the detection scenario by achieving a higher recall despite having a lower precision than the other systems.","['In order to assess the performance of BERT for Span-\nish clinical data anonymisation, we have conducted sev-\neral experiments with a BERT-based sequence labelling\napproach using the pre-trained multilingual BERT model\nshared by Google as the starting point for the model train-\ning. We have compared this BERT-based sequence la-\nbelling against other methods and systems. One of the ex-\nperiments uses the MEDDOCAN 2019 shared task dataset,\nwhile the other uses a novel Spanish clinical reports dataset\ncalled NUB ES-PHI.\nThe results of the experiments show that, in NUB ES-PHI,\nthe BERT-based model outperforms the other systems with-\nout requiring any adaptation or domain-speciﬁc feature en-\ngineering, just by being trained on the provided labelled\ndata. Interestingly, the BERT-based model obtains a re-\nmarkably higher recall than the other systems. High recall\nis a desirable outcome because, when anonymising sensible\ndocuments, the accidental leak of sensible data is likely to'
 'Speciﬁcally, we in-\nclude the results of a domain-independent NLNDE model\n(S2), and the results of a model enriched with domain-\nspeciﬁc embeddings (S3). Finally, we include the results\nobtained by Mao and Liu (2019) with a CRF output layer\non top of BERT embeddings. MEDDOCAN consists of\ntwo scenarios:\nDetection - This evaluation measures how good a system\nis at detecting sensitive text spans, regardless of the\ncategory assigned to them.\nClassiﬁcation - In this scenario, systems are required to\nmatch exactly not only the boundaries of each sensi-\ntive span, but also the category assigned.\nThe systems are evaluated in terms of micro-averaged pre-\ncision, recall and F-1 score. Note that, in contrast to the\nevaluation in Experiment A, MEDDOCAN measurements\nare entity-based instead of tokenwise. An exhaustive expla-\nnation of the MEDDOCAN evaluation procedure is avail-\nable online10, as well as the ofﬁcial evaluation script11,\nwhich we used to obtain the reported results.\n4. Results\nThis section describes the results obtained in the two sets\nof experiments: NUB ES-PHI and MEDDOCAN.\n4.1. Experiment A: NUB ES-PHI\nTable 5 shows the results of the conducted experiments in\nNUB ES-PHI for all the compared systems. The included\nbaseline serves to give a quick insight about how chal-\nlenging the data is. With simple regular expressions and\ngazetteers a precision of 0.853 is obtained. On the other\nhand, the recall, which directly depends on the coverage\nprovided by the rules and resources, drops to 0.469. Hence,\nthis task is unlikely to be solved without the generalisa-\ntion capabilities provided by machine-learning and deep-\nlearning models.\nRegarding the detection scenario –that is, the scenario con-\ncerned with a binary classiﬁcation to determine whether\neach individual token conveys sensitive information or not–\n, it can be observed that BERT outperforms its competitors.\nA fact worth highlighting is that, according to these results,\nBERT achieves a precision lower than the rest of the sys-\ntems (i.e., it makes more false positive predictions); in ex-\nchange, it obtains a remarkably higher recall. Noticeably, it\n10http://temu.bsc.es/meddocan/index.php/evaluation/\n11https://github.com/PlanTL-SANIDAD/MEDDOCAN-\nEvaluation-Script']",The text does not provide information on the results of the experiments comparing BERT-based models with other systems in the NUB ES-PHI and MEDDOCAN datasets.,[''],"The BERT-based model outperformed other systems in the NUB ES-PHI dataset and achieved comparable results in the MEDDOCAN dataset. In the NUB ES-PHI dataset, the BERT-based model achieved an F1-score of 0.972, while the CRF and spaCy models achieved F1-scores of 0.960 and 0.965, respectively. In the MEDDOCAN dataset, the BERT-based model achieved an F1-score of 0.967 for detection and 0.965 for classification, outperforming the CRF and spaCy models. However, the BERT-based model did not surpass the scores achieved by the NLNDE models in the MEDDOCAN dataset.",0.5917053345674992,0.5384158941138394
33,"How does context-specificity differ in ELMo, BERT, and GPT-2 and how does it relate to word similarity in the same sentence?","Context-specificity differs in ELMo, BERT, and GPT-2 in the following ways: In ELMo, as context-specificity increases in upper layers, representations of words in the same sentence become more similar to each other. In BERT, as context-specificity increases in upper layers, representations of words in the same sentence become more dissimilar to each other but are still more similar than randomly sampled words on average. In GPT-2, however, words in the same sentence are not more similar to each other than two randomly chosen words. This variation in context-specificity across the models impacts the similarity of words in the same sentence, with ELMo showing increased similarity, BERT showing increased dissimilarity, and GPT-2 showing no significant similarity between words in the same sentence.","['what extent they can be replaced with static word\nembeddings, if at all. Our work in this paper is\nthus markedly different from most dissections of\ncontextualized representations. It is more similar\nto Mimno and Thompson (2017), which studied\nthe geometry of static word embedding spaces.\n3 Approach\n3.1 Contextualizing Models\nThe contextualizing models we study in this pa-\nper are ELMo, BERT, and GPT-21. We choose\nthe base cased version of BERT because it is most\ncomparable to GPT-2 with respect to number of\nlayers and dimensionality. The models we work\nwith are all pre-trained on their respective lan-\nguage modelling tasks. Although ELMo, BERT,\nand GPT-2 have 2, 12, and 12 hidden layers re-\nspectively, we also include the input layer of each\ncontextualizing model as its 0thlayer. This is be-\ncause the 0thlayer is not contextualized, making\nit a useful baseline against which to compare the\ncontextualization done by subsequent layers.\n3.2 Data\nTo analyze contextualized word representations,\nwe need input sentences to feed into our pre-\ntrained models. Our input data come from the\nSemEval Semantic Textual Similarity tasks from\nyears 2012 - 2016 (Agirre et al., 2012, 2013, 2014,\n2015). We use these datasets because they contain\nsentences in which the same words appear in dif-\nferent contexts. For example, the word ‘dog’ ap-\npears in “A panda dog is running on the road. ”\nand“A dog is trying to get bacon off his back. ”\nIf a model generated the same representation for\n‘dog’ in both these sentences, we could infer that\nthere was no contextualization; conversely, if the\ntwo representations were different, we could infer\nthat they were contextualized to some extent. Us-\ning these datasets, we map words to the list of sen-\ntences they appear in and their index within these\nsentences. We do not consider words that appear\nin less than 5 unique contexts in our analysis.\n3.3 Measures of Contextuality\nWe measure how contextual a word representation\nis using three different metrics: self-similarity ,\nintra-sentence similarity , and maximum explain-\nable variance .\n1We use the pretrained models provided in an earlier ver-\nsion of the PyTorch-Transformers library.Deﬁnition 1 Letwbe a word that appears in\nsentences{s1,...,sn}at indices{i1,...,in}respec-\ntively, such that w=s1[i1] =...=sn[in]. Let fℓ(s,i)\nbe a function that maps s[i]to its representation in\nlayerℓof model f. The self similarity ofwin layer\nℓis\nSelfSimℓ(w) =1\nn2−n∑\nj∑\nk̸=jcos(fℓ(sj,ij),fℓ(sk,ik))\n(1)\nwhere cos denotes the cosine similarity. In other\nwords, the self-similarity of a word win layer ℓis\nthe average cosine similarity between its contextu-\nalized representations across its nunique contexts.\nIf layer ℓdoes not contextualize the representa-\ntions at all, then SelfSimℓ(w) =1 (i.e., the repre-\nsentations are identical across all contexts). The\nmore contextualized the representations are for w,\nthe lower we would expect its self-similarity to be.\nDeﬁnition 2 Letsbe a sentence that is a se-\nquence⟨w1,...,wn⟩ofnwords. Let fℓ(s,i)be a\nfunction that maps s[i]to its representation in layer\nℓof model f. The intra-sentence similarity ofsin\nlayer ℓis\nIntraSim ℓ(s) =1\nn∑\nicos(⃗sℓ,fℓ(s,i))\nwhere ⃗sℓ=1\nn∑\nifℓ(s,i)(2)\nPut more simply, the intra-sentence similarity of a\nsentence is the average cosine similarity between\nits word representations and the sentence vector,\nwhich is just the mean of those word vectors. This\nmeasure captures how context-speciﬁcity mani-\nfests in the vector space.'
 'tations is surprising.\n2. Occurrences of the same word in different\ncontexts have non-identical vector represen-\ntations. Where vector similarity is deﬁned\nas cosine similarity, these representations are\nmore dissimilar to each other in upper lay-\ners. This suggests that, much like how upper\nlayers of LSTMs produce more task-speciﬁc\nrepresentations (Liu et al., 2019a), upper lay-\ners of contextualizing models produce more\ncontext-speciﬁc representations.\n3. Context-speciﬁcity manifests very differently\nin ELMo, BERT, and GPT-2. In ELMo,\nrepresentations of words in the same sen-\ntence grow more similar to each other as\ncontext-speciﬁcity increases in upper layers;\nin BERT, they become more dissimilar to\neach other in upper layers but are still more\nsimilar than randomly sampled words are on\naverage; in GPT-2, however, words in the\nsame sentence are no more similar to each\nother than two randomly chosen words.\n4. After adjusting for the effect of anisotropy,\non average, less than 5% of the variance in a\nword’s contextualized representations can be\nexplained by their ﬁrst principal component.\nThis holds across all layers of all models.\nThis suggests that contextualized representa-\ntions do not correspond to a ﬁnite number\nof word-sense representations, and even in\nthe best possible scenario, static embeddings\nwould be a poor replacement for contextual-\nized ones. Still, static embeddings created\nby taking the ﬁrst principal component of\na word’s contextualized representations out-\nperform GloVe and FastText embeddings on\nmany word vector benchmarks.\nThese insights help justify why the use of contex-\ntualized representations has led to such signiﬁcant\nimprovements on many NLP tasks.\n2 Related Work\nStatic Word Embeddings Skip-gram with neg-\native sampling (SGNS) (Mikolov et al., 2013a)\nand GloVe (Pennington et al., 2014) are among\nthe best known models for generating static word\nembeddings. Though they learn embeddings itera-\ntively in practice, it has been proven that in theory,they both implicitly factorize a word-context ma-\ntrix containing a co-occurrence statistic (Levy and\nGoldberg, 2014a,b). Because they create a single\nrepresentation for each word, a notable problem\nwith static word embeddings is that all senses of a\npolysemous word must share a single vector.\nContextualized Word Representations Given\nthe limitations of static word embeddings, recent\nwork has tried to create context-sensitive word\nrepresentations. ELMo (Peters et al., 2018), BERT\n(Devlin et al., 2018), and GPT-2 (Radford et al.,\n2019) are deep neural language models that are\nﬁne-tuned to create models for a wide range of\ndownstream NLP tasks. Their internal representa-\ntions of words are called contextualized word rep-\nresentations because they are a function of the en-\ntire input sentence. The success of this approach\nsuggests that these representations capture highly\ntransferable and task-agnostic properties of lan-\nguage (Liu et al., 2019a).\nELMo creates contextualized representations of\neach token by concatenating the internal states of\na 2-layer biLSTM trained on a bidirectional lan-\nguage modelling task (Peters et al., 2018). In\ncontrast, BERT and GPT-2 are bi-directional and\nuni-directional transformer-based language mod-\nels respectively. Each transformer layer of 12-\nlayer BERT (base, cased) and 12-layer GPT-2 cre-\nates a contextualized representation of each token\nby attending to different parts of the input sentence\n(Devlin et al., 2018; Radford et al., 2019). BERT\n– and subsequent iterations on BERT (Liu et al.,\n2019b; Yang et al., 2019) – have achieved state-of-\nthe-art performance on various downstream NLP\ntasks, ranging from question-answering to senti-\nment analysis.\nProbing Tasks Prior analysis of contextualized\nword representations has largely been restricted\nto probing tasks (Tenney et al., 2019; Hewitt and\nManning, 2019).']","ELMo, BERT, and GPT-2 are all language models that use context to understand the meaning of words, but they differ in how they handle context-specificity.

ELMo uses a bi-directional LSTM to generate word embeddings. It considers both the previous and next words in a sentence to understand the context of a word. However, it generates the same word representation regardless of the word's position in the sentence.

BERT, on the other hand, uses a transformer-based model and considers the context from both directions. It generates different embeddings for a word depending on its position and context in a sentence. This makes BERT more context-specific than ELMo.

GPT-2, like BERT, uses a transformer-based model but it is unidirectional, meaning it only considers the context to the left of the word. This makes it less context-specific than BERT but more so than ELMo.

In terms of word similarity, the more context-specific a model is, the better it can understand the nuanced meanings of words in the same sentence. For example, BERT can differentiate between the different meanings of the word ""bank"" in the sentences ""I went to the bank"" and ""I sat on the bank of the river"", while ELMo and GPT-2 might struggle with this.",[''],"Context-specificity differs in ELMo, BERT, and GPT-2. In ELMo, words in the same sentence are more similar to one another in upper layers. In BERT, words in the same sentence are more dissimilar to one another in upper layers. In GPT-2, word representations in the same sentence are no more similar to each other than randomly sampled words.",0.8198881756014516,0.41066261269823223
34,What is the role of the improved relation detection model in the KBQA process and how does it contribute to achieving state-of-the-art results?,"The improved relation detection model plays a crucial role in the KBQA process by enhancing the identification of KB relations based on input questions. This model utilizes deep residual bidirectional LSTMs to compare questions and relation names at different levels of abstraction, enabling more accurate detection of relations within the knowledge base. By integrating this enhanced relation detector into the KBQA system, it helps improve the accuracy of the system for both single-relation and multi-relation question answering tasks, ultimately contributing to achieving state-of-the-art results in KBQA benchmarks.","['58.0%) for the\nconstraint detection module to improve.\nFinally, like STAGG, which uses multiple rela-\ntion detectors (see Yih et al. (2015) for the three\nmodels used), we also try to use the top-3 rela-\ntion detectors from Section 6.2. As shown on the\nlast row of Table 3, this gives a signiﬁcant perfor-\nmance boost, resulting in a new state-of-the-art re-\nsult on SimpleQuestions and a result comparable\nto the state-of-the-art on WebQSP.\n7 Conclusion\nKB relation detection is a key step in KBQA and\nis signiﬁcantly different from general relation ex-\ntraction tasks. We propose a novel KB relation\ndetection model, HR-BiLSTM, that performs hier-\narchical matching between questions and KB rela-\ntions. Our model outperforms the previous meth-\nods on KB relation detection tasks and allows our\nKBQA system to achieve state-of-the-arts. For fu-\nture work, we will investigate the integration of\nour HR-BiLSTM into end-to-end systems. For ex-\nample, our model could be integrated into the de-\ncoder in (Liang et al., 2016), to provide better se-\nquence prediction. We will also investigate new\nemerging datasets like GraphQuestions (Su et al.,\n2016) and ComplexQuestions (Bao et al., 2016) to\nhandle more characteristics of general QA.\n12Note that another reason is that we are evaluating on ac-\ncuracy here. When evaluating on F1 the gap will be smaller.'
 'Improved Neural Relation Detection for Knowledge Base Question\nAnswering\nMo Yu†Wenpeng Yin⋆Kazi Saidul Hasan‡Cicero dos Santos†\nBing Xiang‡Bowen Zhou†\n†AI Foundations, IBM Research, USA\n⋆Center for Information and Language Processing, LMU Munich\n‡IBM Watson, USA\n{yum,kshasan,cicerons,bingxia,zhou }@us.ibm.com, wenpeng@cis.lmu.de\nAbstract\nRelation detection is a core component of\nmany NLP applications including Knowl-\nedge Base Question Answering (KBQA).\nIn this paper, we propose a hierarchi-\ncal recurrent neural network enhanced by\nresidual learning which detects KB re-\nlations given an input question. Our\nmethod uses deep residual bidirectional\nLSTMs to compare questions and rela-\ntion names via different levels of abstrac-\ntion. Additionally, we propose a sim-\nple KBQA system that integrates entity\nlinking and our proposed relation detec-\ntor to make the two components enhance\neach other. Our experimental results show\nthat our approach not only achieves out-\nstanding relation detection performance,\nbut more importantly, it helps our KBQA\nsystem achieve state-of-the-art accuracy\nfor both single-relation (SimpleQuestions)\nand multi-relation (WebQSP) QA bench-\nmarks.\n1 Introduction\nKnowledge Base Question Answering (KBQA)\nsystems answer questions by obtaining informa-\ntion from KB tuples (Berant et al., 2013; Yao et al.,\n2014; Bordes et al., 2015; Bast and Haussmann,\n2015; Yih et al., 2015; Xu et al., 2016). For an\ninput question, these systems typically generate a\nKB query, which can be executed to retrieve the\nanswers from a KB. Figure 1 illustrates the process\nused to parse two sample questions in a KBQA\nsystem: (a) a single-relation question, which can\nbe answered with a single <head-entity, relation,\ntail-entity>KB tuple (Fader et al., 2013; Yih et al.,\n2014; Bordes et al., 2015); and (b) a more complex\ncase, where some constraints need to be handledfor multiple entities in the question. The KBQA\nsystem in the ﬁgure performs two key tasks: (1)\nentity linking , which links n-grams in questions\nto KB entities, and (2) relation detection , which\nidentiﬁes the KB relation(s) a question refers to.\nThe main focus of this work is to improve the\nrelation detection subtask and further explore how\nit can contribute to the KBQA system. Although\ngeneral relation detection1methods are well stud-\nied in the NLP community, such studies usually\ndo not take the end task of KBQA into considera-\ntion. As a result, there is a signiﬁcant gap between\ngeneral relation detection studies and KB-speciﬁc\nrelation detection. First, in most general relation\ndetection tasks, the number of target relations is\nlimited, normally smaller than 100. In contrast, in\nKBQA even a small KB, like Freebase2M (Bor-\ndes et al., 2015), contains more than 6,000 relation\ntypes. Second, relation detection for KBQA often\nbecomes a zero-shot learning task, since some test\ninstances may have unseen relations in the training\ndata. For example, the SimpleQuestions (Bordes\net al., 2015) data set has 14% of the golden test\nrelations not observed in golden training tuples.\nThird, as shown in Figure 1(b), for some KBQA\ntasks like WebQuestions (Berant et al., 2013), we\nneed to predict a chain of relations instead of a\nsingle relation. This increases the number of tar-\nget relation types and the sizes of candidate rela-\ntion pools, further increasing the difﬁculty of KB\nrelation detection. Owing to these reasons, KB re-\nlation detection is signiﬁcantly more challenging\ncompared to general relation detection tasks.\nThis paper improves KB relation detection to\ncope with the problems mentioned above. First, in\norder to deal with the unseen relations, we propose\nto break the relation names into word sequences\nfor question-relation matching. Second, noticing\n1In the information extraction ﬁeld such tasks are usually\ncalled relation extraction orrelation classiﬁcation .arXiv:1704.06194v2  [cs.CL]  27 May 2017']","The improved relation detection model plays a crucial role in the Knowledge-Based Question Answering (KBQA) process. It is responsible for accurately identifying and extracting the relations between entities in a given question. This is a critical step in the KBQA process as it helps in understanding the context of the question and retrieving the correct answer from the knowledge base.

The improved relation detection model contributes to achieving state-of-the-art results in several ways. Firstly, it uses advanced techniques like deep learning and natural language processing to better understand the semantics of the question, leading to more accurate relation detection. Secondly, it can handle complex questions with multiple entities and relations, which traditional models may struggle with. Lastly, it can also deal with ambiguous questions by considering multiple possible interpretations and choosing the most likely one based on the context.

By improving the accuracy and robustness of relation detection, the improved model significantly enhances the overall performance of the KBQA system, leading to state-of-the-art results.",[''],"The improved relation detection model plays a key role in the KBQA process by re-ranking the entity candidates based on high confident relations detected from the raw question text. It helps deal with ambiguities in entity linking results. Additionally, the model finds the core relation for each topic entity, which is selected from a smaller candidate entity set after re-ranking. The improved relation detection model contributes to achieving state-of-the-art results in both single-relation and multi-relation KBQA tasks.",0.8255040826740905,0.5837359493653285
35,"""What model is used for sensitive information detection and classification in Spanish clinical text?",BERT's multilingual model is used for sensitive information detection and classification in Spanish clinical text.,"['(2016), or Khin et al. (2018)–, other languages are also\nattracting growing interest. Some examples are Mamede\net al. (2016) for Portuguese and Tveit et al. (2004) for\nNorwegian. With respect to the anonymisation of text writ-\nten in Spanish, recent studies include Medina and Turmo\n(2018), Hassan et al. (2018) and Garc ´ıa-Sardi ˜na (2018).\nMost notably, in 2019 the ﬁrst community challenge about\nanonymisation of medical documents in Spanish, MED-\nDOCAN1(Marimon et al., 2019), was held as part of\nthe IberLEF initiative. The winners of the challenge –the\nNeither-Language-nor-Domain-Experts (NLNDE) (Lange\net al., 2019)– achieved F1-scores as high as 0.975 in the\ntask of sensitive information detection and categorisation\nby using recurrent neural networks with Conditional Ran-\ndom Field (CRF) output layers.\nAt the same challenge, Mao and Liu (2019) occupied the\n8thposition among 18 participants using BERT. According\nto the description of the system, the authors used BERT-\nBase Multilingual Cased and an output CRF layer. How-\never, their system is ∼3 F1-score points below our imple-\nmentation without the CRF layer.\n3. Materials and Methods\nThe aim of this paper is to evaluate BERT’s multilingual\nmodel and compare it to other established machine-learning\nalgorithms in a speciﬁc task: sensitive data detection and\nclassiﬁcation in Spanish clinical free text. This section de-\nscribes the data involved in the experiments and the systems\nevaluated. Finally, we introduce the experimental setup.\n3.1. Data\nTwo datasets are exploited in this article. Both datasets\nconsist of plain text containing clinical narrative written in\nSpanish, and their respective manual annotations of sensi-\ntive information in BRAT (Stenetorp et al., 2012) standoff\nformat2. In order to feed the data to the different algorithms\npresented in Section 3.2., these datasets were transformed\nto comply with the commonly used BIO sequence repre-\nsentation scheme (Ramshaw and Marcus, 1999).\n3.1.1. NUB ES-PHI\nNUB ES(Lima et al., 2019) is a corpus of around 7,000 real\nmedical reports written in Spanish and annotated with nega-\ntion and uncertainty information. Before being published,\nsensitive information had to be manually annotated and re-\nplaced for the corpus to be safely shared. In this article,\n1http://temu.bsc.es/meddocan/\n2https://brat.nlplab.org/standoff.html'
 'In order to assess the performance of BERT for Span-\nish clinical data anonymisation, we have conducted sev-\neral experiments with a BERT-based sequence labelling\napproach using the pre-trained multilingual BERT model\nshared by Google as the starting point for the model train-\ning. We have compared this BERT-based sequence la-\nbelling against other methods and systems. One of the ex-\nperiments uses the MEDDOCAN 2019 shared task dataset,\nwhile the other uses a novel Spanish clinical reports dataset\ncalled NUB ES-PHI.\nThe results of the experiments show that, in NUB ES-PHI,\nthe BERT-based model outperforms the other systems with-\nout requiring any adaptation or domain-speciﬁc feature en-\ngineering, just by being trained on the provided labelled\ndata. Interestingly, the BERT-based model obtains a re-\nmarkably higher recall than the other systems. High recall\nis a desirable outcome because, when anonymising sensible\ndocuments, the accidental leak of sensible data is likely to']","The model used for sensitive information detection and classification in Spanish clinical text is not specifically mentioned. However, various Natural Language Processing (NLP) models can be used for this purpose, such as Named Entity Recognition (NER) models.",[''],The model used for sensitive information detection and classification in Spanish clinical text is BERT (Bidirectional Encoder Representations from Transformers). ,0.23293054354245274,0.6124824312521855
36,"""What is the proposed solution for word-order divergence in multilingual NMT in a low-resource setting, and how does it involve pre-ordering English sentences?""","The proposed solution for addressing word-order divergence in multilingual NMT in a low-resource setting involves pre-ordering English sentences to match the word order of the source language. This approach aims to ensure that the contextual representations generated by the encoder for the two languages are consistent. By pre-ordering English sentences to align with the source language word order and training the parent model on this pre-ordered corpus, the goal is to bridge the gap in word order between the assisting language and the source language, leading to more consistent contextual representations across the source languages.","['2 Related Work\nTo the best of our knowledge, no work has ad-\ndressed word order divergence in transfer learn-\ning for multilingual NMT. However, some work\nexists for other NLP tasks in a multilingual set-\nting. For Named Entity Recognition (NER), Xie\net al. (2018) use a self-attention layer after the\nBi-LSTM layer to address word-order divergence\nfor Named Entity Recognition (NER) task. The\napproach does not show any signiﬁcant improve-\nments, possibly because the divergence has to be\naddressed before/during construction of the con-\ntextual embeddings in the Bi-LSTM layer. Joty\net al. (2017) use adversarial training for cross-\nlingual question-question similarity ranking. The\nadversarial training tries to force the sentence rep-\nresentation generated by the encoder of similar\nsentences from different input languages to have\nsimilar representations.\nPre-ordering the source language sentences to\nmatch the target language word order has been\nfound useful in addressing word-order divergence\nfor Phrase-Based SMT (Collins et al., 2005; Ra-\nmanathan et al., 2008; Navratil et al., 2012; Chat-\nterjee et al., 2014). For NMT, Ponti et al. (2018)\nand Kawara et al. (2018) have explored pre-\nordering. Ponti et al. (2018) demonstrated that\nby reducing the syntactic divergence between the\nsource and the target languages, consistent im-\nprovements in NMT performance can be obtained.\nOn the contrary, Kawara et al. (2018) reported\ndrop in NMT performance due to pre-ordering.\nNote that these works address source-target diver-\ngence, not divergence between source languages\nin multilingual NMT scenario.\n3 Proposed Solution\nConsider the task of translating for an extremely\nlow-resource language pair. The parallel corpus\nbetween the two languages, if available may be\ntoo small to train an NMT model. Similar to Zoph\net al. (2016), we use transfer learning to over-\ncome data sparsity between the source and the\ntarget languages. We choose English as the as-\nsisting language in all our experiments. In our\nresource-scarce scenario, we have no parallel cor-\npus for training the child model. Hence, at test\ntime, the source language sentence is translated\nusing the parent model after performing a word-\nby-word translation from source to the assisting\nlanguage using a bilingual dictionary.Before Reordering After Reordering\nS\nNP0 VP\nV NP 1S\nNP0 VP\nNP1V\nS\nNP\nNNP\nAnuragVP\nMD\nwillVP\nVB\nmeetNP\nNNP\nThakurS\nNP\nNNP\nAnuragVP\nNP\nNNP\nThakurVP\nMD\nwillVP\nVB\nmeet\nTable 1: Example showing transitive verb before and\nafter reordering (Adapted from Chatterjee et al. (2014))\nSince the source language and the assisting lan-\nguage (English) have different word order, we hy-\npothesize that it leads to inconsistencies in the\ncontextual representations generated by the en-\ncoder for the two languages. Speciﬁcally, given an\nEnglish sentence (SVO word order) and its transla-\ntion in the source language (SOV word order), the\nencoder representations for words in the two sen-\ntences will be different due to different contexts\nof synonymous words. This could lead to the at-\ntention and the decoder layers generating different\ntranslations from the same (parallel) sentence in\nthe source or assisting language. This is undesir-\nable as we want the knowledge to be transferred\nfrom the parent model (assisting source →target)\nto the child model (source →target).\nIn this paper, we propose to pre-order English\nsentences (assisting language sentences) to match\nthe source language word-order and train the par-\nent model on the pre-ordered corpus. Table 1\nshows one of the pre-ordering rules (Ramanathan\net al., 2008) used along with an example sentence\nillustrating the effect of pre-ordering. This will en-\nsure that context of words in the parallel source\nand assisting language sentences are similar, lead-\ning to consistent contextual representations across\nthe source languages.'
 'Addressing Word-order Divergence in Multilingual Neural Machine\nTranslation for extremely Low Resource Languages\nRudra Murthy V†, Anoop Kunchukuttan‡, Pushpak Bhattacharyya†\n†Center for Indian Language Technology (CFILT)\nDepartment of Computer Science and Engineering\nIIT Bombay, India.\n‡Microsoft AI & Research, Hyderabad, India.\n{rudra,pb }@cse.iitb.ac.in, ankunchu@microsoft.com\nAbstract\nTransfer learning approaches for Neural Ma-\nchine Translation (NMT) trains a NMT model\non an assisting language-target language pair\n(parent model) which is later ﬁne-tuned for\nthe source language-target language pair of in-\nterest (child model), with the target language\nbeing the same. In many cases, the assist-\ning language has a different word order from\nthe source language. We show that divergent\nword order adversely limits the beneﬁts from\ntransfer learning when little to no parallel cor-\npus between the source and target language is\navailable. To bridge this divergence, we pro-\npose to pre-order the assisting language sen-\ntences to match the word order of the source\nlanguage and train the parent model. Our ex-\nperiments on many language pairs show that\nbridging the word order gap leads to major\nimprovements in the translation quality in ex-\ntremely low-resource scenarios.\n1 Introduction\nTransfer learning for multilingual Neural Machine\nTranslation (NMT) (Zoph et al., 2016; Dabre et al.,\n2017; Nguyen and Chiang, 2017) attempts to im-\nprove the NMT performance on the source to\ntarget language pair (child task) using an assist-\ning source language (assisting to target language\ntranslation is the parent task). Here, the parent\nmodel is trained on the assisting and target lan-\nguage parallel corpus and the trained weights are\nused to initialize the child model. If source-target\nlanguage pair parallel corpus is available, the child\nmodel can further be ﬁne-tuned. The weight ini-\ntialization reduces the requirement on the training\ndata for the source-target language pair by trans-\nferring knowledge from the parent task, thereby\nimproving the performance on the child task.\nHowever, the divergence between the source\nand the assisting language can adversely impactthe beneﬁts obtained from transfer learning. Mul-\ntiple studies have shown that transfer learning\nworks best when the languages are related (Zoph\net al., 2016; Nguyen and Chiang, 2017; Dabre\net al., 2017). Zoph et al. (2016) studied the in-\nﬂuence of language divergence between languages\nchosen for training the parent and the child model,\nand showed that choosing similar languages for\ntraining the parent and the child model leads to\nbetter improvements from transfer learning.\nSeveral studies have tried to address the lex-\nical divergence between the source and the tar-\nget languages either by using Byte Pair Encoding\n(BPE) as basic input representation units (Nguyen\nand Chiang, 2017) or character-level NMT sys-\ntem (Lee et al., 2017) or bilingual embeddings\n(Gu et al., 2018). However, the effect of word\norder divergence and its mitigation has not been\nexplored. In a practical setting, it is not uncom-\nmon to have source and assisting languages with\ndifferent word order. For instance, it is possible to\nﬁnd parallel corpora between English (SVO word\norder) and some Indian (SOV word order) lan-\nguages, but very little parallel corpora between In-\ndian languages. Hence, it is natural to use English\nas an assisting language for inter-Indian language\ntranslation.\nTo address the word order divergence, we pro-\npose to pre-order the assisting language sentences\n(SVO) to match the word order of the source lan-\nguage (SOV). We consider an extremely resource-\nconstrained scenario, where there is no parallel\ncorpus for the child task. From our experiments,\nwe show that there is a signiﬁcant increase in the\ntranslation accuracy for the unseen source-target\nlanguage pair.arXiv:1811.00383v2  [cs.CL]  10 Apr 2019']","The proposed solution for word-order divergence in multilingual Neural Machine Translation (NMT) in a low-resource setting is to use a pre-ordering approach. This involves rearranging the words in English sentences into the target language word order before translation. The pre-ordering model is trained to predict the word order of the target language based on the source language. This method helps to reduce the complexity of the translation task for the NMT model, making it more effective in low-resource settings.",[''],"The proposed solution for word-order divergence in multilingual NMT in a low-resource setting involves pre-ordering English sentences to match the source language word-order. This ensures that the context of words in the parallel source and assisting language sentences are similar, leading to consistent contextual representations across the source languages.",0.8420648618714162,0.6748160839035593
37,What structures extend MLM into a bilingual scenario and introduce alignment information during model training?,Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-SA) are the structures that extend MLM into a bilingual scenario and introduce alignment information during model training.,"['Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-\nSA) according to the way of generating the alignment infor-\nmation. These two structures actually extend MLM into a\nbilingual scenario, with the difference that BRLM leverages\nexternal aligner tool or additional attention layer to explic-\nitly introduce alignment information during model training.\n•Hard Alignment (BRLM-HA). We ﬁrst use exter-\nnal aligner tool on source ↔pivot parallel data to ex-\ntract the alignment information of sentence pair. Dur-\ning model training, given source ↔pivot sentence pair,\nBRLM-HA randomly masks some words in source sen-\ntence and leverages alignment information to obtain the\naligned words in pivot sentence for masked words. Based\non the processed input, BRLM-HA adopts the Trans-\nformer (Vaswani et al. 2017) encoder to gain the hid-\nden states for source and pivot sentences respectively.\nThen the training objective of BRLM-HA is to predict\nthe masked words by not only the surrounding words\nin source sentence but also the encoder outputs of the\naligned words. Note that this training process is also car-\nried out in a symmetric situation, in which we mask some\nwords in pivot sentence and obtain the aligned words in\nthe source sentence.\n•Soft Alignment (BRLM-SA). Instead of using external\naligner tool, BRLM-SA introduces an additional atten-\ntion layer to learn the alignment information together with\nmodel training. In this way, BRLM-SA avoids the effect\ncaused by external wrong alignment information and en-\nables many-to-one soft alignment during model training.\nSimilar with BRLM-HA, the training objective of BRLM-\nSA is to predict the masked words by not only the sur-\nrounding words in source sentence but also the outputs of\nattention layer. In our implementation, the attention layer\nis a multi-head attention layer adopted in Transformer,\nwhere the queries come from the masked source sentence,\nthe keys and values come from the pivot sentence.\nIn principle, MLM and TLM can learn some implicit align-\nment information during model training. However, the align-\nment process in MLM is inefﬁcient since the shared word\npieces only account for a small proportion of the whole cor-\npus, resulting in the difﬁculty of expanding the shared in-\nformation to align the whole corpus. TLM also lacks effort\nin alignment between the source and target sentences since\nTLM concatenates the sentence pair into one sequence, mak-\ning the explicit alignment between the source and target in-\nfeasible. BRLM fully utilizes the alignment information to\nobtain better word-level representation alignment between\ndifferent languages, which better relieves the burden of the\ndomain shift problem .\nTransfer Protocol\nWe consider the typical zero-shot translation scenario in\nwhich a high resource pivot language has parallel data with\nboth source and target languages, while source and target\nlanguages has no parallel data between themselves. Our pro-\nposed cross-lingual pretraining based transfer approach for\nsource→target zero-shot translation is mainly divided into\ntwo phrases: the pretraining phase and the transfer phase.Corpus Language Train Dev Test\nEuroparlDe-En,En-Fr 1M,1M 2,000 2,000\nFr-En,En-Es 1M,1M 2,000 2,000\nRo-En,En-De 0.6M,1.5M 2,000 1,000\nMultiUNAr-En,En-Es\nEn-Ru9.7M,11.3M\n11.6M4,000 4,000\nTable 1: Data Statistics.\nIn the pretraining phase, we ﬁrst pretrain MLM on mono-\nlingual corpora of both source and pivot languages, and con-\ntinue to pretrain TLM or the proposed BRLM on the avail-\nable parallel data between source and pivot languages, in\norder to build a cross-lingual encoder shared by the source\nand pivot languages.\nIn the transfer phase, we train pivot →target NMT model\ninitialized by the cross-lingually pre-trained encoder, and\nﬁnally transfer the trained NMT model to source →target\ntranslation thanks to the shared encoder. Note that during\ntraining pivot→target NMT model, we freeze several layers\nof the cross-lingually pre-trained encoder to avoid the de-\ngeneracy issue.'
 'Figure 2: The overview of BRidge Language Modeling (BRLM). The BRLM extends MLM (Lample and Conneau 2019) to\npairs of parallel sentences and leverages explicit alignment information obtained by external aligner tool or additional attention\nlayer to encourage word representation alignment across different languages.\n•Directly translate source sentences into target sentences\nwith the parent model, which beneﬁts from the availabil-\nity of the universal encoder.\nThe key difﬁculty of this method is to ensure the intermedi-\nate representations of the universal encoder are language in-\nvariant. In the rest of this section, we ﬁrst present two exist-\ning methods yet to be explored in zero-shot translation, and\nthen propose a straightforward but effective cross-lingual\npre-training method. In the end, we present the whole train-\ning and inference protocol for transfer.\nMasked and Translation Language Model\nPretraining\nTwo existing cross-lingual pre-training methods, Masked\nLanguage Modeling (MLM) and Translation Language\nModeling (TLM), have shown their effectiveness on XNLI\ncross-lingual classiﬁcation task (Lample and Conneau 2019;\nHuang et al. 2019), but these methods have not been well\nstudied on cross-lingual generation tasks in zero-shot condi-\ntion. We attempt to take advantage of the cross-lingual abil-\nity of the two methods for zero-shot translation.\nSpeciﬁcally, MLM adopts the Cloze objective of\nBERT (Devlin et al. 2018) and predicts the masked words\nthat are randomly selected and replaced with [MASK] to-\nken on monolingual corpus. In practice, MLM takes dif-\nferent language monolingual corpora as input to ﬁnd fea-\ntures shared across different languages. With this method,\nword pieces shared in all languages have been mapped into\na shared space, which makes the sentence representationsacross different languages close (Pires, Schlinger, and Gar-\nrette 2019).\nSince MLM objective is unsupervised and only requires\nmonolingual data, TLM is designed to leverage parallel data\nwhen it is available. Actually, TLM is a simple extension of\nMLM, with the difference that TLM concatenates sentence\npair into a whole sentence, and then randomly masks words\nin both the source and target sentences. In this way, the\nmodel can either attend to surrounding words or to the trans-\nlation sentence, implicitly encouraging the model to align\nthe source and target language representations. Note that al-\nthough each sentence pair is formed into one sentence, the\npositions of the target sentence are reset to count form zero.\nBridge Language Model Pretraining\nAside from MLM and TLM, we propose BRidge Language\nModeling (BRLM) to further obtain word-level representa-\ntion alignment between different languages. This method is\ninspired by the assumption that if the feature spaces of dif-\nferent languages are aligned very well, the masked words\nin the corrupted sentence can also be guessed by the con-\ntext of the correspondingly aligned words on the other side.\nTo achieve this goal, BRLM is designed to strengthen the\nability to infer words across languages based on alignment\ninformation, instead of inferring words within monolingual\nsentence as in MLM or within the pseudo sentence formed\nby concatenating sentence pair as in TLM.\nAs illustrated in Figure 2, BRLM stacks shared encoder\nover both side sentences separately. In particular, we design\ntwo network structures for BRLM, which are divided into']",The structures that extend Masked Language Model (MLM) into a bilingual scenario and introduce alignment information during model training are Cross-lingual Language Model Pretraining (XLM) and Translation Language Model (TLM).,[''],Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-SA) are the two structures that extend MLM into a bilingual scenario and introduce alignment information during model training.,0.7234982935511918,0.7495842950464436
38,"What is the context-speciﬁcity of stopwords in ELMo, BERT, and GPT-2?","The context-specificity of stopwords in ELMo, BERT, and GPT-2 varies. In ELMo, representations of stopwords in the same sentence become more similar to each other as context-specificity increases in upper layers. In BERT, stopwords become more dissimilar to each other in upper layers but are still more similar than randomly sampled words on average. However, in GPT-2, stopwords in the same sentence are not more similar to each other than two randomly chosen words.","['How Contextual are Contextualized Word Representations?\nComparing the Geometry of BERT, ELMo, and GPT-2 Embeddings\nKawin Ethayarajh∗\nStanford University\nkawin@stanford.edu\nAbstract\nReplacing static word embeddings with con-\ntextualized word representations has yielded\nsigniﬁcant improvements on many NLP tasks.\nHowever, just how contextual are the contex-\ntualized representations produced by models\nsuch as ELMo and BERT? Are there inﬁnitely\nmany context-speciﬁc representations for each\nword, or are words essentially assigned one of\na ﬁnite number of word-sense representations?\nFor one, we ﬁnd that the contextualized rep-\nresentations of all words are not isotropic in\nany layer of the contextualizing model. While\nrepresentations of the same word in differ-\nent contexts still have a greater cosine simi-\nlarity than those of two different words, this\nself-similarity is much lower in upper layers.\nThis suggests that upper layers of contextu-\nalizing models produce more context-speciﬁc\nrepresentations, much like how upper layers\nof LSTMs produce more task-speciﬁc repre-\nsentations. In all layers of ELMo, BERT, and\nGPT-2, on average, less than 5% of the vari-\nance in a word’s contextualized representa-\ntions can be explained by a static embedding\nfor that word, providing some justiﬁcation for\nthe success of contextualized representations.\n1 Introduction\nThe application of deep learning methods to NLP\nis made possible by representing words as vec-\ntors in a low-dimensional continuous space. Tradi-\ntionally, these word embeddings were static : each\nword had a single vector, regardless of context\n(Mikolov et al., 2013a; Pennington et al., 2014).\nThis posed several problems, most notably that\nall senses of a polysemous word had to share the\nsame representation. More recent work, namely\ndeep neural language models such as ELMo (Pe-\nters et al., 2018) and BERT (Devlin et al., 2018),\n∗Work partly done at the University of Toronto.have successfully created contextualized word rep-\nresentations , word vectors that are sensitive to\nthe context in which they appear. Replacing\nstatic embeddings with contextualized representa-\ntions has yielded signiﬁcant improvements on a di-\nverse array of NLP tasks, ranging from question-\nanswering to coreference resolution.\nThe success of contextualized word represen-\ntations suggests that despite being trained with\nonly a language modelling task, they learn highly\ntransferable and task-agnostic properties of lan-\nguage. In fact, linear probing models trained on\nfrozen contextualized representations can predict\nlinguistic properties of words (e.g., part-of-speech\ntags) almost as well as state-of-the-art models (Liu\net al., 2019a; Hewitt and Manning, 2019). Still,\nthese representations remain poorly understood.\nFor one, just how contextual are these contextu-\nalized word representations? Are there inﬁnitely\nmany context-speciﬁc representations that BERT\nand ELMo can assign to each word, or are words\nessentially assigned one of a ﬁnite number of\nword-sense representations?\nWe answer this question by studying the geom-\netry of the representation space for each layer of\nELMo, BERT, and GPT-2. Our analysis yields\nsome surprising ﬁndings:\n1. In all layers of all three models, the con-\ntextualized word representations of all words\nare not isotropic: they are not uniformly dis-\ntributed with respect to direction. Instead,\nthey are anisotropic , occupying a narrow\ncone in the vector space. The anisotropy in\nGPT-2’s last layer is so extreme that two ran-\ndom words will on average have almost per-\nfect cosine similarity! Given that isotropy\nhas both theoretical and empirical beneﬁts for\nstatic embeddings (Mu et al., 2018), the ex-\ntent of anisotropy in contextualized represen-arXiv:1909.00512v1  [cs.CL]  2 Sep 2019'
 'tations is surprising.\n2. Occurrences of the same word in different\ncontexts have non-identical vector represen-\ntations. Where vector similarity is deﬁned\nas cosine similarity, these representations are\nmore dissimilar to each other in upper lay-\ners. This suggests that, much like how upper\nlayers of LSTMs produce more task-speciﬁc\nrepresentations (Liu et al., 2019a), upper lay-\ners of contextualizing models produce more\ncontext-speciﬁc representations.\n3. Context-speciﬁcity manifests very differently\nin ELMo, BERT, and GPT-2. In ELMo,\nrepresentations of words in the same sen-\ntence grow more similar to each other as\ncontext-speciﬁcity increases in upper layers;\nin BERT, they become more dissimilar to\neach other in upper layers but are still more\nsimilar than randomly sampled words are on\naverage; in GPT-2, however, words in the\nsame sentence are no more similar to each\nother than two randomly chosen words.\n4. After adjusting for the effect of anisotropy,\non average, less than 5% of the variance in a\nword’s contextualized representations can be\nexplained by their ﬁrst principal component.\nThis holds across all layers of all models.\nThis suggests that contextualized representa-\ntions do not correspond to a ﬁnite number\nof word-sense representations, and even in\nthe best possible scenario, static embeddings\nwould be a poor replacement for contextual-\nized ones. Still, static embeddings created\nby taking the ﬁrst principal component of\na word’s contextualized representations out-\nperform GloVe and FastText embeddings on\nmany word vector benchmarks.\nThese insights help justify why the use of contex-\ntualized representations has led to such signiﬁcant\nimprovements on many NLP tasks.\n2 Related Work\nStatic Word Embeddings Skip-gram with neg-\native sampling (SGNS) (Mikolov et al., 2013a)\nand GloVe (Pennington et al., 2014) are among\nthe best known models for generating static word\nembeddings. Though they learn embeddings itera-\ntively in practice, it has been proven that in theory,they both implicitly factorize a word-context ma-\ntrix containing a co-occurrence statistic (Levy and\nGoldberg, 2014a,b). Because they create a single\nrepresentation for each word, a notable problem\nwith static word embeddings is that all senses of a\npolysemous word must share a single vector.\nContextualized Word Representations Given\nthe limitations of static word embeddings, recent\nwork has tried to create context-sensitive word\nrepresentations. ELMo (Peters et al., 2018), BERT\n(Devlin et al., 2018), and GPT-2 (Radford et al.,\n2019) are deep neural language models that are\nﬁne-tuned to create models for a wide range of\ndownstream NLP tasks. Their internal representa-\ntions of words are called contextualized word rep-\nresentations because they are a function of the en-\ntire input sentence. The success of this approach\nsuggests that these representations capture highly\ntransferable and task-agnostic properties of lan-\nguage (Liu et al., 2019a).\nELMo creates contextualized representations of\neach token by concatenating the internal states of\na 2-layer biLSTM trained on a bidirectional lan-\nguage modelling task (Peters et al., 2018). In\ncontrast, BERT and GPT-2 are bi-directional and\nuni-directional transformer-based language mod-\nels respectively. Each transformer layer of 12-\nlayer BERT (base, cased) and 12-layer GPT-2 cre-\nates a contextualized representation of each token\nby attending to different parts of the input sentence\n(Devlin et al., 2018; Radford et al., 2019). BERT\n– and subsequent iterations on BERT (Liu et al.,\n2019b; Yang et al., 2019) – have achieved state-of-\nthe-art performance on various downstream NLP\ntasks, ranging from question-answering to senti-\nment analysis.\nProbing Tasks Prior analysis of contextualized\nword representations has largely been restricted\nto probing tasks (Tenney et al., 2019; Hewitt and\nManning, 2019).']","ELMo, BERT, and GPT-2 are all language models that use context to understand the meaning of words. However, their handling of stopwords (commonly used words like 'the', 'is', 'in', etc.) varies.

ELMo uses a bi-directional LSTM model to understand the context of words, including stopwords. It generates word embeddings by considering the entire input sentence. Therefore, the context-specificity of stopwords in ELMo is high.

BERT, like ELMo, also uses context for word embeddings. It uses a transformer model and considers the context from both directions (left and right of the word). Therefore, the context-specificity of stopwords in BERT is also high.

GPT-2, on the other hand, uses a transformer model like BERT but it is unidirectional (left-to-right). It does consider context, but only from the left of the word. Therefore, the context-specificity of stopwords in GPT-2 is lower compared to ELMo and BERT.",[''],"Stopwords have among the most context-specific representations in ELMo, BERT, and GPT-2.",0.9269458514308111,0.897407759100139
39,How is sequence labeling used in NLP tasks and what are some related research areas?,"Sequence labeling is commonly used in NLP tasks, particularly in Named Entity Recognition (NER), where each word in a sentence is assigned a label indicating the type of entity it represents. Related research areas include adversarial training, which focuses on learning invariant features across different domains or tasks, and crowdsourcing, which involves obtaining labeled data from non-experts to train NER classifiers. Adversarial training has been applied in NLP for tasks like domain adaptation, cross-lingual learning, and multi-task learning, aiming to find invariant features across different data distributions. Crowdsourcing is utilized to collect labeled data at a lower cost, although with potentially lower quality compared to expert annotations, and has been explored in various NLP tasks to efficiently leverage non-expert annotations.","['medical records), the involved techniques are related\nto Natural Language Processing (NLP). When using NLP\napproaches, it is common to pose the problem of documentanonymisation as a sequence labelling problem, i.e. clas-\nsifying each token within a sequence as being sensitive in-\nformation or not. Further, depending on the objective of\nthe anonymisation task, it is also important to determine\nthe type of sensitive information (names of individuals, ad-\ndresses, age, sex, etc.).\nThe anonymisation systems based on NLP techniques per-\nform reasonably well, but are far from perfect. Depend-\ning on the difﬁculty posed by each dataset or the amount\nof available data for training machine learning models,\nthe performance achieved by these methods is not enough\nto fully rely on them in certain situations (Abouelmehdi\net al., 2018). However, in the last two years, the NLP\ncommunity has reached an important milestone thanks to\nthe appearance of the so-called Transformers neural net-\nwork architectures (Wolf et al., 2019). In this paper, we\nconduct several experiments in sensitive information de-\ntection and classiﬁcation on Spanish clinical text using\nBERT (from ‘Bidirectional Encoder Representations from\nTransformers’) (Devlin et al., 2019) as the base for a se-\nquence labelling approach. The experiments are carried\nout on two datasets: the MEDDOCAN: Medical Document\nAnonymization shared task dataset (Marimon et al., 2019),\nand NUB ES(Lima et al., 2019), a corpus of real medical\nreports in Spanish. In these experiments, we compare the\nperformance of BERT with other machine-learning-based\nsystems, some of which use language-speciﬁc features. Our\naim is to evaluate how good a BERT-based model performs\nwithout language nor domain specialisation apart from the\ntraining data labelled for the task at hand.\nThe rest of the paper is structured as follows: the next sec-\ntion describes related work about data anonymisation in\ngeneral and clinical data anonymisation in particular; it also\nprovides a more detailed explanation and background about\nthe Transformers architecture and BERT. Section 3. de-\nscribes the data involved in the experiments and the systemsarXiv:2003.03106v2  [cs.CL]  17 Mar 2020'
 'ing the NER loss. Thus the resulting features of the common\nBi-LSTM are worker invariant and NER sensitive.\nFor evaluation, we create two Chinese NER datasets in\ntwo domains: dialog and e-commerce. We require the crowd\nannotators to label the types of entities, including person,\nsong, brand, product, and so on. Identifying these entities\nis useful for chatbot and e-commerce platforms (Kl ¨uwer\n2011). Then we conduct experiments on the newly created\ndatasets to verify the effectiveness of the proposed adversar-\nial neural network model. The results show that our system\noutperforms very strong baseline systems. In summary, we\nmake the following contributions:\n•We propose a crowd-annotation learning model based on\nadversarial neural networks. The model uses labeled data\ncreated by non-experts to train a NER classiﬁer and simul-\ntaneously learns the common and private features among\nthe non-expert annotators.\n•We create two data sets in dialog and e-commerce do-\nmains by crowd annotations. The experimental results\nshow that the proposed approach performs the best among\nall the comparison systems.\nRelated Work\nOur work is related to three lines of research: Sequence la-\nbeling, Adversarial training, and Crowdsourcing.\nSequence labeling. NER is widely treated as a sequence la-\nbeling problem, by assigning a unique label over each sen-\ntential word (Ratinov and Roth 2009). Early studies on se-\nquence labeling often use the models of HMM, MEMM,\nand CRF (Lafferty et al. 2001) based on manually-crafted\ndiscrete features, which can suffer the feature sparsity prob-\nlem and require heavy feature engineering. Recently, neural\nnetwork models have been successfully applied to sequence\nlabeling (Collobert et al. 2011; Huang, Xu, and Yu 2015;\nLample et al. 2016). Among these work, the model which\nuses Bi-LSTM for feature extraction and CRF for decoding\nhas achieved state-of-the-art performances (Huang, Xu, and\nYu 2015; Lample et al. 2016), which is exploited as the base-\nline model in our work.\nAdversarial Training. Adversarial Networks have achieved\ngreat success in computer vision such as image genera-\ntion (Denton et al. 2015; Ganin et al. 2016). In the NLP\ncommunity, the method is mainly exploited under the set-\ntings of domain adaption (Zhang, Barzilay, and Jaakkola\n2017; Gui et al. 2017), cross-lingual (Chen et al. 2016;\nKim et al. 2017) and multi-task learning (Chen et al. 2017;\nLiu, Qiu, and Huang 2017). All these settings involve the\nfeature divergences between the training and test examples,\nand aim to learn invariant features across the divergences by\nan additional adversarial discriminator, such as domain dis-\ncriminator. Our work is similar to these work but is applies\non crowdsourcing learning, aiming to ﬁnd invariant features\namong different crowdsourcing workers.\nCrowdsourcing. Most NLP tasks require a massive amount\nof labeled training data which are annotated by experts.\nHowever, hiring experts is costly and non-scalable, both in\nterms of time and money. Instead, crowdsourcing is anothersolution to obtain labeled data at a lower cost but with rela-\ntive lower quality than those from experts. Snow et al. (2008)\ncollected labeled results for several NLP tasks from Amazon\nMechanical Turk and demonstrated that non-experts annota-\ntions were quite useful for training new systems. In recent\nyears, a series of work have focused on how to use crowd-\nsourcing data efﬁciently in tasks such as classiﬁcation (Felt\net al. 2015; Bi et al. 2014), and compare quality of crowd\nand expert labels (Dumitrache, Aroyo, and Welty 2017).\nIn sequence labeling tasks, Dredze, Talukdar, and Cram-\nmer (2009) viewed this task as a multi-label problem while\nRodrigues, Pereira, and Ribeiro (2014) took workers iden-\ntities into account by assuming that each sentential word\nwas tagged correctly by one of the crowdsourcing workers\nand proposed a CRF-based model with multiple annotators.\nNguyen et al.']","Sequence labeling is used in Natural Language Processing (NLP) tasks to assign a categorical label to each member of a sequence of observed values. A common example of this is part-of-speech tagging, where each word in a sentence is labeled with its appropriate part of speech. Other examples include named entity recognition (where each word is labeled as being a part of a named entity or not) and chunking (where phrases in a sentence are identified and labeled).

Related research areas include machine learning, deep learning, and artificial intelligence, as these technologies are often used to develop and improve sequence labeling algorithms. Other related areas include information extraction, text mining, and computational linguistics, which often use sequence labeling techniques to analyze and interpret text data.",[''],"Sequence labeling is widely used in NLP tasks, where it involves assigning a unique label to each word in a sentence. It has been applied to tasks such as named entity recognition (NER). Early studies used models like HMM, MEMM, and CRF with manually-crafted features. More recently, neural network models, particularly those using Bi-LSTM for feature extraction and CRF for decoding, have achieved state-of-the-art performance. Adversarial training has also been applied in NLP, particularly in domain adaptation, cross-lingual learning, and multi-task learning. Crowdsourcing is another approach used to obtain labeled training data at a lower cost, although the quality may be lower than that of expert annotations. Several studies have focused on using crowdsourcing data efficiently, including in sequence labeling tasks.",0.7045969787588808,0.5243535435779351
40,"What potential benefits can be derived from constructing a corpus of social media posts annotated with dogmatism scores, and how can this corpus be utilized to encourage pro-social behavior in online communities?","The construction of a corpus of social media posts annotated with dogmatism scores can provide insights into linguistic features of dogmatism and enable the building of predictive models to analyze new content. This corpus can be utilized to identify behavioral predictors of dogmatism and discover topical patterns in the comments of dogmatic users. By applying computational models to this corpus, it may be possible to help users shed their dogmatic beliefs. Ultimately, this work opens up new avenues for encouraging pro-social behavior in online communities by leveraging the understanding gained from analyzing dogmatism in social media posts.","['model may allow future researchers to probe these\nquestions more deeply.\n7 Conclusion\nWe have constructed the ﬁrst corpus of social me-\ndia posts annotated with dogmatism scores, allowing\nus to explore linguistic features of dogmatism and\nbuild a predictive model that analyzes new content.\nWe apply this model to Reddit, where we discover\nbehavioral predictors of dogmatism and topical pat-\nterns in the comments of dogmatic users.\nCould we use this computational model to help\nusers shed their dogmatic beliefs? Looking forward,\nour work makes possible new avenues for encourag-\ning pro-social behavior in online communities.\nReferences\n[Cheng et al.2015] Justin Cheng, Cristian Danescu-\nNiculescu-Mizil, and Jure Leskovec. 2015. Antisocial\nbehavior in online discussion communities. arXiv\npreprint arXiv:1504.00680 .\n[Church and Hanks1990] Kenneth Ward Church and\nPatrick Hanks. 1990. Word association norms,\nmutual information, and lexicography. Computational\nlinguistics , 16(1):22–29.\n[Crowson2009] H Michael Crowson. 2009. Does the\ndog scale measure dogmatism? another look at con-\nstruct validity. The Journal of social psychology ,\n149(3):365–383.\n[Danescu-Niculescu-Mizil et al.2013] Cristian Danescu-\nNiculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure\nLeskovec, and Christopher Potts. 2013. A computa-\ntional approach to politeness with application to social\nfactors. arXiv preprint arXiv:1306.6078 .\n[Doroudi et al.2016] Shayan Doroudi, Ece Kamar, Emma\nBrunskill, and Eric Horvitz. 2016. Toward a learning\nscience for complex crowdsourcing tasks. In Proceed-\nings of the 2016 CHI Conference on Human Factors in\nComputing Systems , pages 2623–2634. ACM.\n[El-Nawawy and Powers2010] Mohammed El-Nawawy\nand Shawn Powers. 2010. Al-jazeera english a con-\nciliatory medium in a conﬂict-driven environment?\nGlobal Media and Communication , 6(1):61–84.\n[Ertel1985] S Ertel. 1985. Content analysis: An alter-\nnative approach to open and closed minds. The High\nSchool Journal , 68(4):229–240.\n[Gilbert2012] Eric Gilbert. 2012. Phrases that signal\nworkplace hierarchy. In Proceedings of the ACM 2012\nconference on Computer Supported Cooperative Work ,\npages 1037–1046. ACM.[Gurney et al.2013] Daniel J Gurney, Shelley McKeown,\nJamie Churchyard, and Neil Howlett. 2013. Believe it\nor not: Exploring the relationship between dogmatism\nand openness within non-religious samples. Personal-\nity and Individual Differences , 55(8):936–940.\n[Hasan and Ng2014] Kazi Saidul Hasan and Vincent Ng.\n2014. Why are you taking this stance? identifying and\nclassifying reasons in ideological debates. In EMNLP ,\npages 751–762.\n[Hayes and Krippendorff2007] Andrew F Hayes and\nKlaus Krippendorff. 2007. Answering the call\nfor a standard reliability measure for coding data.\nCommunication methods and measures , 1(1):77–89.\n[Lohman2010] Margaret C Lohman. 2010. An unex-\namined triumvirate: dogmatism, problem solving, and\nhrd. Human Resource Development Review .\n[Martin et al.2011] Matthew M Martin, Sydney M Stag-\ngers, and Carolyn M Anderson. 2011. The relation-\nships between cognitive ﬂexibility with dogmatism,\nintellectual ﬂexibility, preference for consistency, and\nself-compassion. Communication Research Reports ,\n28(3):275–280.\n[McCluskey and Hmielowski2012] Michael McCluskey\nand Jay Hmielowski. 2012. Opinion expression dur-\ning social conﬂict: Comparing online reader com-\nments and letters to the editor. Journalism , 13(3):303–\n319.'
 'Identifying Dogmatism in Social Media: Signals and Models\nEthan Fast and Eric Horvitz\nethaen@stanford.edu, horvitz@microsoft.com\nAbstract\nWe explore linguistic and behavioral features\nof dogmatism in social media and construct\nstatistical models that can identify dogmatic\ncomments. Our model is based on a corpus of\nReddit posts, collected across a diverse set of\nconversational topics and annotated via paid\ncrowdsourcing. We operationalize key aspects\nof dogmatism described by existing psychol-\nogy theories (such as over-conﬁdence), ﬁnd-\ning they have predictive power. We also ﬁnd\nevidence for new signals of dogmatism, such\nas the tendency of dogmatic posts to refrain\nfrom signaling cognitive processes. When we\nuse our predictive model to analyze millions\nof other Reddit posts, we ﬁnd evidence that\nsuggests dogmatism is a deeper personality\ntrait, present for dogmatic users across many\ndifferent domains, and that users who engage\non dogmatic comments tend to show increases\nin dogmatic posts themselves.\n1 Introduction\n“I’m supposed to trust the opinion of a MS min-\nion? The people that produced Windows ME, Vista\nand 8? They don’t even understand people, yet they\nthink they can predict the behavior of new, self-\nguiding AI?” – anonymous\n“I think an AI would make it easier for Patients to\nconﬁde their information because by nature, a robot\ncannot judge them. Win-win? :D”’ – anonymous\nDogmatism describes the tendency to lay down\nopinions as incontrovertibly true, without respect for\nconﬂicting evidence or the opinions of others (Ox-\nford Dictionary, 2016). Which user is more dog-\nmatic in the examples above? This question is sim-\nple for humans. Phrases like “they think” and “theydon’t even understand,” suggest an intractability of\nopinion, while “I think” and “win-win?” suggest\nthe opposite. Can we train computers to draw sim-\nilar distinctions? Work in psychology has called\nout many aspects of dogmatism that can be modeled\ncomputationally via natural language, such as over-\nconﬁdence and strong emotions (Rokeach, 1954).\nWe present a statistical model of dogmatism that\naddresses two complementary goals. First, we val-\nidate psychological theories by examining the pre-\ndictive power of feature sets that guide the model’s\npredictions. For example, do linguistic signals of\ncertainty help to predict a post is dogmatic, as the-\nory would suggest? Second, we apply our model to\nanswer four questions:\nR1: What kinds of topics (e.g., guns, LGBT) at-\ntract the highest levels of dogmatism?\nR2: How do dogmatic beliefs cluster?\nR3: How does dogmatism inﬂuence a conversa-\ntion on social media?\nR4: How do other user behaviors (e.g., frequency\nand breadth of posts) relate to dogmatism?\nWe train a predictive model to classify dogmatic\nposts from Reddit, one of the most popular discus-\nsion communities on the web.1Posts on Reddit cap-\nture discussion and debate across a diverse set of do-\nmains and topics – users talk about everything from\nclimate change and abortion, to world news and re-\nlationship advice, to the future of artiﬁcial intelli-\ngence. As a prerequisite to training our model, we\nhave created a corpus of 5,000 Reddit posts anno-\ntated with levels of dogmatism, which we are releas-\ning to share with other researchers.\n1http://www.reddit.comarXiv:1609.00425v1  [cs.CL]  1 Sep 2016']","Constructing a corpus of social media posts annotated with dogmatism scores can have several potential benefits. Firstly, it can provide valuable insights into the prevalence and patterns of dogmatic behavior in online communities. This can help researchers understand how dogmatism influences online discourse and social dynamics. Secondly, it can be used to develop and train machine learning algorithms to automatically detect and measure dogmatism in social media content. This can facilitate real-time monitoring and analysis of dogmatism on a large scale.

The corpus can be utilized to encourage pro-social behavior in online communities in several ways. It can be used to educate users about the negative effects of dogmatism and promote more open-minded and respectful communication. It can also be used to develop tools and interventions to mitigate dogmatism. For example, social media platforms can use the corpus to design algorithms that identify and flag dogmatic content, or suggest alternative viewpoints to users exhibiting dogmatic behavior. This can help to foster a more balanced and constructive dialogue, and promote tolerance and understanding among users.",[''],"The potential benefits of constructing a corpus of social media posts annotated with dogmatism scores include exploring linguistic features of dogmatism and building a predictive model to analyze new content. This model can be utilized to identify behavioral predictors of dogmatism and topical patterns in the comments of dogmatic users on platforms like Reddit. Looking forward, this work makes possible new avenues for encouraging pro-social behavior in online communities.",0.8524475178873927,0.79074856156264
41,"According to the context, which cross-lingual pre-training method demonstrates the most stable and similar cross-lingual representations of sentence pairs on all layers, and achieves the best performance in zero-shot translation?","MLM+BRLM-SA has the most stable and similar cross-lingual representations of sentence pairs on all layers, and achieves the best performance in zero-shot translation.","['Cross-lingual Pre-training Based Transfer for Zero-shot Neural\nMachine Translation\nBaijun Ji‡, Zhirui Zhang§, Xiangyu Duan†‡∗, Min Zhang†‡, Boxing Chen§and Weihua Luo§\n†Institute of Artiﬁcial Intelligence, Soochow University, Suzhou, China\n‡School of Computer Science and Technology, Soochow University, Suzhou, China\n§Alibaba DAMO Academy, Hangzhou, China\n‡bjji@stu.suda.edu.cn†{xiangyuduan, minzhang}@suda.edu.cn\n§{zhirui.zzr,boxing.cbx,weihua.luowh}@alibaba-inc.com\nAbstract\nTransfer learning between different language pairs has shown\nits effectiveness for Neural Machine Translation (NMT) in\nlow-resource scenario. However, existing transfer methods\ninvolving a common target language are far from success in\nthe extreme scenario of zero-shot translation, due to the lan-\nguage space mismatch problem between transferor (the par-\nent model) and transferee (the child model) on the source\nside. To address this challenge, we propose an effective trans-\nfer learning approach based on cross-lingual pre-training. Our\nkey idea is to make all source languages share the same fea-\nture space and thus enable a smooth transition for zero-shot\ntranslation. To this end, we introduce one monolingual pre-\ntraining method and two bilingual pre-training methods to\nobtain a universal encoder for different languages. Once the\nuniversal encoder is constructed, the parent model built on\nsuch encoder is trained with large-scale annotated data and\nthen directly applied in zero-shot translation scenario. Exper-\niments on two public datasets show that our approach signif-\nicantly outperforms strong pivot-based baseline and various\nmultilingual NMT approaches.\nIntroduction\nAlthough Neural Machine Translation (NMT) has domi-\nnated recent research on translation tasks (Wu et al. 2016;\nVaswani et al. 2017; Hassan et al. 2018), NMT heavily relies\non large-scale parallel data, resulting in poor performance\non low-resource or zero-resource language pairs (Koehn\nand Knowles 2017). Translation between these low-resource\nlanguages (e.g., Arabic →Spanish) is usually accomplished\nwith pivoting through a rich-resource language (such as En-\nglish), i.e., Arabic (source) sentence is translated to En-\nglish (pivot) ﬁrst which is later translated to Spanish (tar-\nget) (Kauers et al. 2002; de Gispert and Mariño 2006).\nHowever, the pivot-based method requires doubled decoding\ntime and suffers from the propagation of translation errors.\nOne common alternative to avoid pivoting in NMT is\ntransfer learning (Zoph et al. 2016; Nguyen and Chiang\n2017; Kocmi and Bojar 2018; Kim et al. 2019) which lever-\nages a high-resource pivot →target model ( parent ) to ini-\n∗Corresponding Author.\nCopyright c⃝2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The circle and triangle dots represent source sen-\ntences in different language l1andl2, and the square dots\nmeans target sentences in language l3. A sample of transla-\ntion pairs is connected by the dashed line. We would like to\nforce each of the translation pairs has the same latent rep-\nresentation as the right part of the ﬁgure so as to transfer\nl1→l3model directly to l2→l3model.\ntialize a low-resource source →target model ( child ) that is\nfurther optimized with a small amount of available paral-\nlel data. Although this approach has achieved success in\nsome low-resource language pairs, it still performs very\npoorly in extremely low-resource or zero-resource transla-\ntion scenario. Speciﬁcally, Kocmi and Bojar (2018) reports\nthat without any child model training data, the performance\nof the parent model on the child test set is miserable.\nIn this work, we argue that the language space mis-\nmatch problem, also named domain shift problem (Fu et al.\n2015), brings about the zero-shot translation failure in trans-\nfer learning.'
 'SA with back translation also achieves better performance\nthan the original supervised Transformer.\nAnalysis\nSentence Representation. We ﬁrst evaluate the represen-\ntational invariance across languages for all cross-lingual pre-\ntraining methods. Following Arivazhagan et al. (2018), we\nadopt max-pooling operation to collect the sentence rep-\nresentation of each encoder layer for all source-pivot sen-\ntence pairs in the Europarl validation sets. Then we calcu-\nlate the cosine similarity for each sentence pair and aver-\nage all cosine scores. As shown in Figure 3, we can ob-\nserve that, MLM+BRLM-SA has the most stable and similar\ncross-lingual representations of sentence pairs on all layers,\nwhile it achieves the best performance in zero-shot transla-\ntion. This demonstrates that better cross-lingual representa-\ntions can beneﬁt for the process of transfer learning. Besides,\nMLM+BRLM-HA is not as superior as MLM+BRLM-\nSA and even worse than MLM+TLM on Fr-En, since\nMLM+BRLM-HA may suffer from the wrong alignment\nknowledge from an external aligner tool. We also ﬁnd an in-\nteresting phenomenon that as the number of layers increases,\nthe cosine similarity decreases.\nContextualized Word Representation. We further sam-\nple an English-Russian sentence pair from the MultiUN\nvalidation sets and visualize the cosine similarity between\nhidden states of the top encoder layer to further investi-\ngate the difference of all cross-lingual pre-training meth-\nods. As shown in Figure 4, the hidden states generated by\nMLM+BRLM-SA have higher similarity for two aligned\nwords. It indicates that MLM+BRLM-SA can gain bet-\nter word-level representation alignment between source and\npivot languages, which better relieves the burden of the do-\nmain shift problem .\nThe Effect of Freezing Parameters. To freeze parame-\nters is a common strategy to avoid catastrophic forgetting in\ntransfer learning (Howard and Ruder 2018). Table 4 shows\nthe performance of transfer learning with freezing different\nlayers on MultiUN test set, in which En →Ru denotes the\nparent model, Ar→Ru and Es→Ru are two child models,\nand all models are based on MLM+BRLM-SA. We can ﬁnd\nthat updating all parameters during training will cause a no-\ntable drop on the zero-shot direction due to the catastrophic\nforgetting. On the contrary, freezing all the parameters leads\nto the decline on supervised direction because the language\nfeatures extracted during pre-training is not sufﬁcient for\nMT task. Freezing the ﬁrst four layers of the transformer\nshows the best performance and keeps the balance between\npre-training and ﬁne-tuning.\nConclusion\nIn this paper, we propose a cross-lingual pretraining based\ntransfer approach for the challenging zero-shot translation\ntask, in which source and target languages have no parallel\ndata, while they both have parallel data with a high resource\n(a) MLM\n (b) MLM+TLM\n(c) MLM+BRLM-HA\n (d) MLM+BRLM-SA\nFigure 4: Cosine similarity visualization at word level given\nan English-Russian sentence pair from the MultiUN valida-\ntion sets. Brighter indicates higher similarity.\nFreezing Layers En→Ru Ar→Ru Es→Ru\nNone 37.80 16.09 19.80\n2 37.79 21.47 28.35\n4 37.55 25.49 30.47\n6 35.31 22.90 28.22\nTable 4: BLEU score of freezing different layers. The num-\nber in Freezing Layers column denotes that the number of\nencoder layers will not be updated.\npivot language. With the aim of building the language in-\nvariant representation between source and pivot languages\nfor smooth transfer of the parent model of pivot →target di-\nrection to the child model of source →target direction, we in-\ntroduce one monolingual pretraining method and two bilin-\ngual pretraining methods to construct an universal encoder\nfor the source and pivot languages. Experiments on public\ndatasets show that our approaches signiﬁcantly outperforms\nseveral strong baseline systems, and manifest the language\ninvariance characteristics in both sentence level and word\nlevel neural representations.\nAcknowledgments\nWe would like to thank the anonymous reviewers for the\nhelpful comments. This work was supported by National\nKey R&D Program of China (Grant No. 2016YFE0132100),\nNational Natural Science Foundation of China (Grant No.']","The text does not provide information on which cross-lingual pre-training method demonstrates the most stable and similar cross-lingual representations of sentence pairs on all layers, and achieves the best performance in zero-shot translation.",[''],MLM+BRLM-SA,0.17915494047551175,0.9649598510392301
42,"What regularities are observed in the phoneme embeddings of the multilingual model, and how do these embeddings relate to the graphemes used in different languages?","The regularities observed in the phoneme embeddings of the multilingual model show that similar phonemes tend to have embeddings that are close together in the embedding space. These embeddings reflect the relationships between phonemes across different languages, capturing similarities and differences in pronunciation patterns. This allows the model to understand and predict how different graphemes in various languages correspond to specific phonemes based on their phonetic similarities encoded in the embeddings.","['with the output of the wFST-based Phonetisaurus\nsystem (Novak et al., 2016) did better than either\nsystem alone.\nA different approach came from Kim and Sny-\nder (2012), who used supervised learning with\nan undirected graphical model to induce the\ngrapheme–phoneme mappings for languages writ-\nten in the Latin alphabet. Given a short text in\na language, the model predicts the language’s or-\nthographic rules. To create phonemic context fea-\ntures from the short text, the model na ¨ıvely maps\ngraphemes to IPA symbols written with the same\ncharacter, and uses the features of these symbols\nto learn an approximation of the phonotactic con-\nstraints of the language. In their experiments,\nthese phonotactic features proved to be more valu-\nable than geographical and genetic features drawn\nfrom WALS (Dryer and Haspelmath, 2013).\n2.2 Multilingual Neural NLP\nIn recent years, neural networks have emerged as\na common way to use data from several languages\nin a single system. Google’s zero-shot neural ma-\nchine translation system (Johnson et al., 2016)\nshares an encoder and decoder across all language\npairs. In order to facilitate this multi-way transla-\ntion, they prepend an artiﬁcial token to the begin-\nning of each source sentence at both training and\ntranslation time. The token identiﬁes what lan-\nguage the sentence should be translated to. This\napproach has three beneﬁts: it is far more efﬁcient\nthan building a separate model for each language\npair; it allows for translation between languages\nthat share no parallel data; and it improves re-\nsults on low-resource languages by allowing them\nto implicitly share parameters with high-resource\nlanguages. Our g2p system is inspired by this ap-\nproach, although it differs in that there is only one\ntarget “language”, IPA, and the artiﬁcial tokens\nidentify the language of the source instead of the\nlanguage of the target.\nOther work has also made use of multilingually-\ntrained neural networks. Phoneme-level polyglot\nlanguage models (Tsvetkov et al., 2016) train a\nsingle model on multiple languages and addition-\nally condition on externally constructed typolog-\nical data about the language. ¨Ostling and Tiede-\nmann (2017) used a similar approach, in which\na character-level neural language model is trained\non a massively multilingual corpus. A language\nembedding vector is concatenated to the input ateach time step. The language embeddings their\nsystem learned correlate closely to the genetic re-\nlationships between languages. However, neither\nof these models was applied to g2p.\n3 Grapheme-to-Phoneme\ng2p is the problem of converting the orthographic\nrepresentation of a word into a phonemic repre-\nsentation. A phoneme is an abstract unit of sound\nwhich may have different realizations in different\ncontexts. For example, the English phoneme /p/\nhas two phonetic realizations (or allophones):\n•[ph], as in the word ‘pain’ [pheI n]\n•[p], as in the word ‘Spain’ [s p eI n]\nEnglish speakers without linguistic training of-\nten struggle to perceive any difference between\nthese sounds. Writing systems usually do not dis-\ntinguish between allophones: [ph]and[p]are both\nwritten as⟨p⟩in English. The sounds are written\ndifferently in languages where they contrast, such\nas Hindi and Eastern Armenian.\nMost writing systems in use today are glot-\ntographic, meaning that their symbols encode\nsolely phonological information1. But despite\nbeing glottographic, in few writing systems do\ngraphemes correspond one-to-one with phonemes.\nThere are cases in which multiple graphemes rep-\nresent a single phoneme, as in the word thein En-\nglish:\nth e\nD @\nThere are cases in which a single grapheme rep-\nresents multiple phonemes, such as syllabaries, in\nwhich each symbol represents a syllable.\nIn many languages, there are silent letters, as in\nthe word hora in Spanish:\nh o r a\n-o R a\nThere are more complicated correspondences,\nsuch as the silent ein English that affects the pro-\nnunciation of the previous vowel, as seen in the\npair of words cape andcap.\nIt is possible for an orthographic system to have\nany or all of the above phenomena while remain-\ning unambiguous.'
 'model’s ﬁrst predicted pronunciation is not cor-\nrect, it tends to assign more probability mass to\nguesses that are more similar to the correct one.\nApplying some sort of ﬁltering or reranking of the\nsystem output might therefore lead to better per-\nformance.\nPhoneme Closest phonemes\nb ph,B,F\n@ ˜ a,˘ e,W\ntht:,t,t ”\nx X,G,è\ny y:,Y,I\nô RG,r ”,R\nTable 8: Selected phonemes and the most similar\nphonemes, measured by the cosine similarity of\nthe embeddings learned by the LangID-All model\n7.4 Future Work\nBecause the language ID token is so beneﬁcial to\nperformance, it would be very interesting to ﬁnd\nways to extend a similar beneﬁt to unseen lan-\nguages. One possible way to do so is with tokens\nthat identify something other than the language,\nsuch as typological features about the language’s\nphonemic inventory. This could enable better\nsharing of resources among languages. Such typo-\nlogical knowledge is readily available in databases\nlike Phoible and WALS for a wide variety of lan-\nguages. It would be interesting to explore if any of\nthese features is a good predictor of a language’s\northographic rules.\nIt would also be interesting to apply the arti-\nﬁcial token approach to other problems besides\nmultilingual g2p. One closely related application\nis monolingual English g2p. Some of the ambi-\nguity of English spelling is due to the wide vari-\nety of loanwords in the language, many of which\nhave unassimilated spellings. Knowing the origins\nof these loanwords could provide a useful hint for\nﬁguring out their pronunciations. The etymology\nof a word could be tagged in an analogous way to\nhow language ID is tagged in multilingual g2p.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473 .\nMaximilian Bisani and Hermann Ney. 2008. Joint-\nsequence models for grapheme-to-phoneme conver-\nsion. Speech communication 50(5):434–451.Aliya Deri and Kevin Knight. 2016. Grapheme-to-\nphoneme models for (almost) any language. In Pro-\nceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics . volume 1, pages\n399–408.\nMatthew S. Dryer and Martin Haspelmath, editors.\n2013. WALS Online . Max Planck Institute for Evo-\nlutionary Anthropology, Leipzig. http://wals.info/.\nBen Hixon, Eric Schneider, and Susan L Epstein. 2011.\nPhonemic similarity metrics to compare pronunci-\nation methods. In Twelfth Annual Conference of\nthe International Speech Communication Associa-\ntion (INTERSPEECH) . Florence, Italy, pages 825–\n828.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation\n9(8):1735–1780.\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim\nKrikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat,\nFernanda Vi ´egas, Martin Wattenberg, Greg Corrado,\net al. 2016. Google’s multilingual neural machine\ntranslation system: Enabling zero-shot translation.\narXiv preprint arXiv:1611.04558 .\nPreethi Jyothi and Mark Hasegawa-Johnson. 2017.\nLow-resource grapheme-to-phoneme conversion us-\ning recurrent neural networks. In Proc. ICASSP .\nYoung-Bum Kim and Benjamin Snyder. 2012. Uni-\nversal grapheme-to-phoneme prediction over Latin\nalphabets. In Proceedings of the 2012 Joint Con-\nference on Empirical Methods in Natural Language\nProcessing and Computational Natural Language\nLearning . Association for Computational Linguis-\ntics, pages 332–343.\nG. Klein, Y . Kim, Y . Deng, J. Senellart, and A. M.\nRush. 2017. OpenNMT: Open-Source Toolkit\nfor Neural Machine Translation.']","The regularities observed in the phoneme embeddings of the multilingual model often reflect the phonetic similarities between different languages. This means that phonemes with similar sounds across different languages tend to have similar embeddings. 

These embeddings relate to the graphemes used in different languages in that they represent the sounds that these graphemes make. For example, the grapheme 'a' in English and 'а' in Russian represent similar sounds, and thus their phoneme embeddings would be similar. This allows the model to understand the phonetic relationships between different languages and can help in tasks such as automatic speech recognition and text-to-speech synthesis.",[''],The phoneme embeddings of the multilingual model show regularities and tend to be phonetically similar to each other.,0.8769195790307345,0.8809697440391336
43,How does the context-specificity of contextualized word representations change across the layers of neural language models?,"The context-specificity of contextualized word representations tends to increase across the layers of neural language models. In upper layers of these models, the representations of the same word in different contexts become more dissimilar to each other. This indicates that as the layers progress, the representations become more context-specific, with upper layers producing more context-specific representations compared to lower layers.","['what extent they can be replaced with static word\nembeddings, if at all. Our work in this paper is\nthus markedly different from most dissections of\ncontextualized representations. It is more similar\nto Mimno and Thompson (2017), which studied\nthe geometry of static word embedding spaces.\n3 Approach\n3.1 Contextualizing Models\nThe contextualizing models we study in this pa-\nper are ELMo, BERT, and GPT-21. We choose\nthe base cased version of BERT because it is most\ncomparable to GPT-2 with respect to number of\nlayers and dimensionality. The models we work\nwith are all pre-trained on their respective lan-\nguage modelling tasks. Although ELMo, BERT,\nand GPT-2 have 2, 12, and 12 hidden layers re-\nspectively, we also include the input layer of each\ncontextualizing model as its 0thlayer. This is be-\ncause the 0thlayer is not contextualized, making\nit a useful baseline against which to compare the\ncontextualization done by subsequent layers.\n3.2 Data\nTo analyze contextualized word representations,\nwe need input sentences to feed into our pre-\ntrained models. Our input data come from the\nSemEval Semantic Textual Similarity tasks from\nyears 2012 - 2016 (Agirre et al., 2012, 2013, 2014,\n2015). We use these datasets because they contain\nsentences in which the same words appear in dif-\nferent contexts. For example, the word ‘dog’ ap-\npears in “A panda dog is running on the road. ”\nand“A dog is trying to get bacon off his back. ”\nIf a model generated the same representation for\n‘dog’ in both these sentences, we could infer that\nthere was no contextualization; conversely, if the\ntwo representations were different, we could infer\nthat they were contextualized to some extent. Us-\ning these datasets, we map words to the list of sen-\ntences they appear in and their index within these\nsentences. We do not consider words that appear\nin less than 5 unique contexts in our analysis.\n3.3 Measures of Contextuality\nWe measure how contextual a word representation\nis using three different metrics: self-similarity ,\nintra-sentence similarity , and maximum explain-\nable variance .\n1We use the pretrained models provided in an earlier ver-\nsion of the PyTorch-Transformers library.Deﬁnition 1 Letwbe a word that appears in\nsentences{s1,...,sn}at indices{i1,...,in}respec-\ntively, such that w=s1[i1] =...=sn[in]. Let fℓ(s,i)\nbe a function that maps s[i]to its representation in\nlayerℓof model f. The self similarity ofwin layer\nℓis\nSelfSimℓ(w) =1\nn2−n∑\nj∑\nk̸=jcos(fℓ(sj,ij),fℓ(sk,ik))\n(1)\nwhere cos denotes the cosine similarity. In other\nwords, the self-similarity of a word win layer ℓis\nthe average cosine similarity between its contextu-\nalized representations across its nunique contexts.\nIf layer ℓdoes not contextualize the representa-\ntions at all, then SelfSimℓ(w) =1 (i.e., the repre-\nsentations are identical across all contexts). The\nmore contextualized the representations are for w,\nthe lower we would expect its self-similarity to be.\nDeﬁnition 2 Letsbe a sentence that is a se-\nquence⟨w1,...,wn⟩ofnwords. Let fℓ(s,i)be a\nfunction that maps s[i]to its representation in layer\nℓof model f. The intra-sentence similarity ofsin\nlayer ℓis\nIntraSim ℓ(s) =1\nn∑\nicos(⃗sℓ,fℓ(s,i))\nwhere ⃗sℓ=1\nn∑\nifℓ(s,i)(2)\nPut more simply, the intra-sentence similarity of a\nsentence is the average cosine similarity between\nits word representations and the sentence vector,\nwhich is just the mean of those word vectors. This\nmeasure captures how context-speciﬁcity mani-\nfests in the vector space.'
 'tations is surprising.\n2. Occurrences of the same word in different\ncontexts have non-identical vector represen-\ntations. Where vector similarity is deﬁned\nas cosine similarity, these representations are\nmore dissimilar to each other in upper lay-\ners. This suggests that, much like how upper\nlayers of LSTMs produce more task-speciﬁc\nrepresentations (Liu et al., 2019a), upper lay-\ners of contextualizing models produce more\ncontext-speciﬁc representations.\n3. Context-speciﬁcity manifests very differently\nin ELMo, BERT, and GPT-2. In ELMo,\nrepresentations of words in the same sen-\ntence grow more similar to each other as\ncontext-speciﬁcity increases in upper layers;\nin BERT, they become more dissimilar to\neach other in upper layers but are still more\nsimilar than randomly sampled words are on\naverage; in GPT-2, however, words in the\nsame sentence are no more similar to each\nother than two randomly chosen words.\n4. After adjusting for the effect of anisotropy,\non average, less than 5% of the variance in a\nword’s contextualized representations can be\nexplained by their ﬁrst principal component.\nThis holds across all layers of all models.\nThis suggests that contextualized representa-\ntions do not correspond to a ﬁnite number\nof word-sense representations, and even in\nthe best possible scenario, static embeddings\nwould be a poor replacement for contextual-\nized ones. Still, static embeddings created\nby taking the ﬁrst principal component of\na word’s contextualized representations out-\nperform GloVe and FastText embeddings on\nmany word vector benchmarks.\nThese insights help justify why the use of contex-\ntualized representations has led to such signiﬁcant\nimprovements on many NLP tasks.\n2 Related Work\nStatic Word Embeddings Skip-gram with neg-\native sampling (SGNS) (Mikolov et al., 2013a)\nand GloVe (Pennington et al., 2014) are among\nthe best known models for generating static word\nembeddings. Though they learn embeddings itera-\ntively in practice, it has been proven that in theory,they both implicitly factorize a word-context ma-\ntrix containing a co-occurrence statistic (Levy and\nGoldberg, 2014a,b). Because they create a single\nrepresentation for each word, a notable problem\nwith static word embeddings is that all senses of a\npolysemous word must share a single vector.\nContextualized Word Representations Given\nthe limitations of static word embeddings, recent\nwork has tried to create context-sensitive word\nrepresentations. ELMo (Peters et al., 2018), BERT\n(Devlin et al., 2018), and GPT-2 (Radford et al.,\n2019) are deep neural language models that are\nﬁne-tuned to create models for a wide range of\ndownstream NLP tasks. Their internal representa-\ntions of words are called contextualized word rep-\nresentations because they are a function of the en-\ntire input sentence. The success of this approach\nsuggests that these representations capture highly\ntransferable and task-agnostic properties of lan-\nguage (Liu et al., 2019a).\nELMo creates contextualized representations of\neach token by concatenating the internal states of\na 2-layer biLSTM trained on a bidirectional lan-\nguage modelling task (Peters et al., 2018). In\ncontrast, BERT and GPT-2 are bi-directional and\nuni-directional transformer-based language mod-\nels respectively. Each transformer layer of 12-\nlayer BERT (base, cased) and 12-layer GPT-2 cre-\nates a contextualized representation of each token\nby attending to different parts of the input sentence\n(Devlin et al., 2018; Radford et al., 2019). BERT\n– and subsequent iterations on BERT (Liu et al.,\n2019b; Yang et al., 2019) – have achieved state-of-\nthe-art performance on various downstream NLP\ntasks, ranging from question-answering to senti-\nment analysis.\nProbing Tasks Prior analysis of contextualized\nword representations has largely been restricted\nto probing tasks (Tenney et al., 2019; Hewitt and\nManning, 2019).']","In neural language models, the context-specificity of contextualized word representations tends to increase as we move up the layers. Lower layers are more focused on the syntax and individual word meanings, while higher layers capture more of the semantic context and abstract concepts. This means that the same word can have different representations in different contexts at higher layers, reflecting the model's understanding of the word's role within its specific context.",[''],The context-specificity of contextualized word representations decreases as the layers of neural language models increase.,0.47957478918995566,0.8752326848792763
44,"According to the context, what is the performance of the BERT-based model in the MEDDOCAN 2019 shared task compared to other systems?","The BERT-based model in the MEDDOCAN 2019 shared task outperforms both the CRF and spaCy classifiers in terms of recall, while having a precision lower than the rest of the systems.","['0 50 1000.40.60.81\ntrain data (%)precision\n0 50 1000.40.60.81\ntrain data (%)recall\n0 50 1000.40.60.81\ntrain data (%)F1-score\nCRF spaCy BERT\nFigure 2: Performance decay with decreasing amounts of training data on the sensitive information detection task in the\nNUB ES-PHI corpus\nDetection Classiﬁcation\nPrec Rec F1 Prec Rec F1\nCRF (Perez et al., 2019) 0.977 0.943 0.960 0.971 0.937 0.954\nspaCy (Perez et al., 2019) 0.967 0.953 0.965 0.965 0.947 0.956\nNLND S2 (Lange et al., 2019) 0.976 0.973 0.974 0.971 0.968 0.970\nNLND S3 (Lange et al., 2019) 0.975 0.975 0.975 0.970 0.969 0.970\nBERT + CRF (Mao and Liu, 2019) 0.968 0.919 0.943 0.965 0.912 0.937\nBERT 0.973 0.972 0.972 0.968 0.967 0.967\nTable 8: Results of Experiment B: MEDDOCAN\na larger extent from recall decline, which is not that marked\nin the case of BERT. Overall, these results indicate that the\ntransfer-learning achieved through the BERT multilingual\npre-trained model not only helps obtain better results, but\nalso lowers the need of manually labelled data for this ap-\nplication domain.\n4.2. Experiment B: MEDDOCAN\nThe results of the two MEDDOCAN scenarios –detection\nand classiﬁcation– are shown in Table 8. These results fol-\nlow the same pattern as in the previous experiments, with\nthe CRF classiﬁer being the most precise of all, and BERT\noutperforming both the CRF and spaCy classiﬁers thanks to\nits greater recall. We also show the results of Mao and Liu\n(2019) who, despite of having used a BERT-based system,\nachieve lower scores than our models. The reason why it\nshould be so remain unclear.\nWith regard to the winner of the MEDDOCAN shared\ntask, the BERT-based model has not improved the scores\nobtained by neither the domain-dependent (S3) nor the\ndomain-independent (S2) NLNDE model. However, at-\ntending to the obtained results, BERT remains only 0.3 F1-\nscore points behind, and would have achieved the second\nposition among all the MEDDOCAN shared task competi-\ntors. Taking into account that only 3% of the gold labels\nremain incorrectly annotated, the task can be considered al-\nmost solved, and it is not clear if the differences among the\nsystems are actually signiﬁcant, or whether they stem from\nminor variations in initialisation or a long-tail of minor la-belling inconsistencies.\n5. Conclusions and Future Work\nIn this work we have brieﬂy introduced the problems re-\nlated to data privacy protection in clinical domain. We have\nalso described some of the groundbreaking advances on the\nNatural Language Processing ﬁeld due to the appearance of\nTransformers-based deep-learning architectures and trans-\nfer learning from very large general-domain multilingual\ncorpora, focusing our attention in one of its most represen-\ntative examples, Google’s BERT model.\nIn order to assess the performance of BERT for Span-\nish clinical data anonymisation, we have conducted sev-\neral experiments with a BERT-based sequence labelling\napproach using the pre-trained multilingual BERT model\nshared by Google as the starting point for the model train-\ning. We have compared this BERT-based sequence la-\nbelling against other methods and systems. One of the ex-\nperiments uses the MEDDOCAN 2019 shared task dataset,\nwhile the other uses a novel Spanish clinical reports dataset\ncalled NUB ES-PHI.\nThe results of the experiments show that, in NUB ES-PHI,\nthe BERT-based model outperforms the other systems with-\nout requiring any adaptation or domain-speciﬁc feature en-\ngineering, just by being trained on the provided labelled\ndata.'
 'Speciﬁcally, we in-\nclude the results of a domain-independent NLNDE model\n(S2), and the results of a model enriched with domain-\nspeciﬁc embeddings (S3). Finally, we include the results\nobtained by Mao and Liu (2019) with a CRF output layer\non top of BERT embeddings. MEDDOCAN consists of\ntwo scenarios:\nDetection - This evaluation measures how good a system\nis at detecting sensitive text spans, regardless of the\ncategory assigned to them.\nClassiﬁcation - In this scenario, systems are required to\nmatch exactly not only the boundaries of each sensi-\ntive span, but also the category assigned.\nThe systems are evaluated in terms of micro-averaged pre-\ncision, recall and F-1 score. Note that, in contrast to the\nevaluation in Experiment A, MEDDOCAN measurements\nare entity-based instead of tokenwise. An exhaustive expla-\nnation of the MEDDOCAN evaluation procedure is avail-\nable online10, as well as the ofﬁcial evaluation script11,\nwhich we used to obtain the reported results.\n4. Results\nThis section describes the results obtained in the two sets\nof experiments: NUB ES-PHI and MEDDOCAN.\n4.1. Experiment A: NUB ES-PHI\nTable 5 shows the results of the conducted experiments in\nNUB ES-PHI for all the compared systems. The included\nbaseline serves to give a quick insight about how chal-\nlenging the data is. With simple regular expressions and\ngazetteers a precision of 0.853 is obtained. On the other\nhand, the recall, which directly depends on the coverage\nprovided by the rules and resources, drops to 0.469. Hence,\nthis task is unlikely to be solved without the generalisa-\ntion capabilities provided by machine-learning and deep-\nlearning models.\nRegarding the detection scenario –that is, the scenario con-\ncerned with a binary classiﬁcation to determine whether\neach individual token conveys sensitive information or not–\n, it can be observed that BERT outperforms its competitors.\nA fact worth highlighting is that, according to these results,\nBERT achieves a precision lower than the rest of the sys-\ntems (i.e., it makes more false positive predictions); in ex-\nchange, it obtains a remarkably higher recall. Noticeably, it\n10http://temu.bsc.es/meddocan/index.php/evaluation/\n11https://github.com/PlanTL-SANIDAD/MEDDOCAN-\nEvaluation-Script']",The text does not provide information on the performance of the BERT-based model in the MEDDOCAN 2019 shared task compared to other systems.,[''],The BERT-based model falls 0.3 F1-score points behind the shared task winning system in the MEDDOCAN 2019 shared task.,0.22457392527421327,0.6052181107784196
45,"What is the hypothesis regarding the difficulty of training deep BiLSTMs without shortcut connections in relation to KB relation detection, based on the experiments conducted?","The hypothesis regarding the difficulty of training deep BiLSTMs without shortcut connections in relation to KB relation detection, based on the experiments conducted, is that training deep BiLSTMs is more difficult without shortcut connections.","['single-layer BiLSTM. Under our setting the two-\nlayer model captures the single-layer model as a\nspecial case (so it could potentially better ﬁt the\ntraining data), this result suggests that the deep\nBiLSTM without shortcut connections might suf-\nfers more from training difﬁculty.\nFinally, we hypothesize that HR-BiLSTM is\nmore than combination of two BiLSTMs with\nresidual connections, because it encourages the\nhierarchical architecture to learn different levels\nof abstraction . To verify this, we replace the deep\nBiLSTM question encoder with two single-layer\nBiLSTMs (both on words) with shortcut connec-\ntions between their hidden states. This decreases\ntest accuracy to 76.11%. It gives similar training\naccuracy compared to HR-BiLSTM, indicating a\nmore serious over-ﬁtting problem. This proves\nthat the residual and deep structures both con-\ntribute to the good performance of HR-BiLSTM.\n6.3 KBQA End-Task Results\nTable 3 compares our system with two published\nbaselines (1) STAGG (Yih et al., 2015), the state-\nof-the-art on WebQSP11and (2) AMPCNN (Yin\net al., 2016), the state-of-the-art on SimpleQues-\ntions. Since these two baselines are specially de-\nsigned/tuned for one particular dataset, they do not\ngeneralize well when applied to the other dataset.\nIn order to highlight the effect of different rela-\ntion detection models on the KBQA end-task, we\nalso implemented another baseline that uses our\nKBQA system but replaces HR-BiLSTM with our\nimplementation of AMPCNN (for SimpleQues-\ntions) or the char-3-gram BiCNN (for WebQSP)\nrelation detectors (second block in Table 3).\nCompared to the baseline relation detector (3rd\nrow of results), our method, which includes an im-\nproved relation detector (HR-BiLSTM), improves\nthe KBQA end task by 2-3% (4th row). Note that\nin contrast to previous KBQA systems, our sys-\ntem does not use joint-inference or feature-based\nre-ranking step, nevertheless it still achieves better\nor comparable results to the state-of-the-art.\nThe third block of the table details two ablation\ntests for the proposed components in our KBQA\nsystems: (1) Removing the entity re-ranking step\nsigniﬁcantly decreases the scores. Since the re-\nranking step relies on the relation detection mod-\nels, this shows that our HR-BiLSTM model con-\ntributes to the good performance in multiple ways.\n11The STAGG score on SQ is from (Bao et al., 2016).Accuracy\nSystem SQ WQ\nSTAGG 72.8 63.9\nAMPCNN (Yin et al., 2016) 76.4 -\nBaseline: Our Method w/75.1 60.0baseline relation detector\nOur Method 77.0 63.0\nw/o entity re-ranking 74.9 60.6\nw/o constraints - 58.0\nOur Method (multi-detectors) 78.7 63.9\nTable 3: KBQA results on SimpleQuestions (SQ)\nand WebQSP (WQ) test sets. The numbers in\ngreen color are directly comparable to our results\nsince we start with the same entity linking results.\nAppendix C gives the detailed performance of the\nre-ranking step. (2) In contrast to the conclusion\nin (Yih et al., 2015), constraint detection is crucial\nfor our system12. This is probably because our\njoint performance on topic entity and core-chain\ndetection is more accurate (77.5% top-1 accuracy),\nleaving a huge potential (77.5% vs. 58.0%) for the\nconstraint detection module to improve.\nFinally, like STAGG, which uses multiple rela-\ntion detectors (see Yih et al. (2015) for the three\nmodels used), we also try to use the top-3 rela-\ntion detectors from Section 6.2. As shown on the\nlast row of Table 3, this gives a signiﬁcant perfor-\nmance boost, resulting in a new state-of-the-art re-\nsult on SimpleQuestions and a result comparable\nto the state-of-the-art on WebQSP.\n7 Conclusion\nKB relation detection is a key step in KBQA and\nis signiﬁcantly different from general relation ex-\ntraction tasks.'
 'On SimpleQuestions, even remov-\ning the deep layers only causes a small drop in per-\nformance. WebQSP beneﬁts more from residual\nand deeper architecture, possibly because in this\ndataset it is more important to handle larger scope\nof context matching.\nFinally, on WebQSP, replacing BiLSTM with\nCNN in our hierarchical matching framework re-\nsults in a large performance drop. Yet on Sim-\npleQuestions the gap is much smaller. We believe\nthis is because the LSTM relation encoder can bet-\nter learn the composition of chains of relations in\nWebQSP, as it is better at dealing with longer de-\npendencies.\nAnalysis Next, we present empirical evidences,\nwhich show why our HR-BiLSTM model achieves\nthe best scores. We use WebQSP for the analy-\nsis purposes. First, we have the hypothesis that\ntraining of the weighted-sum model usually falls\nto local optima, since deep BiLSTMs do not guar-\nantee that the two-levels of question hidden rep-\nresentations are comparable . This is evidenced\nby that during training one layer usually gets a\nweight close to 0 thus is ignored. For exam-\nple, one run gives us weights of -75.39/0.14 for\nthe two layers (we take exponential for the ﬁnal\nweighted sum). It also gives much lower train-\ning accuracy (91.94%) compared to HR-BiLSTM\n(95.67%), suffering from training difﬁculty.\nSecond, compared to our deep BiLSTM with\nshortcut connections, we have the hypothesis that\nfor KB relation detection, training deep BiLSTMs\nis more difﬁcult without shortcut connections . Our\nexperiments suggest that deeper BiLSTM does not\nalways result in lower training accuracy. In the\nexperiments a two-layer BiLSTM converges to\n94.99%, even lower than the 95.25% achieved by a']","The hypothesis is that training deep BiLSTMs without shortcut connections is more difficult for KB relation detection, based on the experiments conducted.",[''],The hypothesis is that training deep BiLSTMs without shortcut connections is more difficult for KB relation detection.,0.746876377099766,0.7415760777749774
46,What is the performance of the BERT-based model compared to other systems in the MEDDOCAN corpus?,"The BERT-based model outperforms other systems in the MEDDOCAN corpus in terms of recall, achieving a higher recall rate compared to its competitors. While BERT may have a slightly lower precision than other systems, it compensates with a significantly higher recall rate, showcasing its effectiveness in detecting sensitive information spans in the MEDDOCAN corpus.","['Speciﬁcally, we in-\nclude the results of a domain-independent NLNDE model\n(S2), and the results of a model enriched with domain-\nspeciﬁc embeddings (S3). Finally, we include the results\nobtained by Mao and Liu (2019) with a CRF output layer\non top of BERT embeddings. MEDDOCAN consists of\ntwo scenarios:\nDetection - This evaluation measures how good a system\nis at detecting sensitive text spans, regardless of the\ncategory assigned to them.\nClassiﬁcation - In this scenario, systems are required to\nmatch exactly not only the boundaries of each sensi-\ntive span, but also the category assigned.\nThe systems are evaluated in terms of micro-averaged pre-\ncision, recall and F-1 score. Note that, in contrast to the\nevaluation in Experiment A, MEDDOCAN measurements\nare entity-based instead of tokenwise. An exhaustive expla-\nnation of the MEDDOCAN evaluation procedure is avail-\nable online10, as well as the ofﬁcial evaluation script11,\nwhich we used to obtain the reported results.\n4. Results\nThis section describes the results obtained in the two sets\nof experiments: NUB ES-PHI and MEDDOCAN.\n4.1. Experiment A: NUB ES-PHI\nTable 5 shows the results of the conducted experiments in\nNUB ES-PHI for all the compared systems. The included\nbaseline serves to give a quick insight about how chal-\nlenging the data is. With simple regular expressions and\ngazetteers a precision of 0.853 is obtained. On the other\nhand, the recall, which directly depends on the coverage\nprovided by the rules and resources, drops to 0.469. Hence,\nthis task is unlikely to be solved without the generalisa-\ntion capabilities provided by machine-learning and deep-\nlearning models.\nRegarding the detection scenario –that is, the scenario con-\ncerned with a binary classiﬁcation to determine whether\neach individual token conveys sensitive information or not–\n, it can be observed that BERT outperforms its competitors.\nA fact worth highlighting is that, according to these results,\nBERT achieves a precision lower than the rest of the sys-\ntems (i.e., it makes more false positive predictions); in ex-\nchange, it obtains a remarkably higher recall. Noticeably, it\n10http://temu.bsc.es/meddocan/index.php/evaluation/\n11https://github.com/PlanTL-SANIDAD/MEDDOCAN-\nEvaluation-Script'
 '0 50 1000.40.60.81\ntrain data (%)precision\n0 50 1000.40.60.81\ntrain data (%)recall\n0 50 1000.40.60.81\ntrain data (%)F1-score\nCRF spaCy BERT\nFigure 2: Performance decay with decreasing amounts of training data on the sensitive information detection task in the\nNUB ES-PHI corpus\nDetection Classiﬁcation\nPrec Rec F1 Prec Rec F1\nCRF (Perez et al., 2019) 0.977 0.943 0.960 0.971 0.937 0.954\nspaCy (Perez et al., 2019) 0.967 0.953 0.965 0.965 0.947 0.956\nNLND S2 (Lange et al., 2019) 0.976 0.973 0.974 0.971 0.968 0.970\nNLND S3 (Lange et al., 2019) 0.975 0.975 0.975 0.970 0.969 0.970\nBERT + CRF (Mao and Liu, 2019) 0.968 0.919 0.943 0.965 0.912 0.937\nBERT 0.973 0.972 0.972 0.968 0.967 0.967\nTable 8: Results of Experiment B: MEDDOCAN\na larger extent from recall decline, which is not that marked\nin the case of BERT. Overall, these results indicate that the\ntransfer-learning achieved through the BERT multilingual\npre-trained model not only helps obtain better results, but\nalso lowers the need of manually labelled data for this ap-\nplication domain.\n4.2. Experiment B: MEDDOCAN\nThe results of the two MEDDOCAN scenarios –detection\nand classiﬁcation– are shown in Table 8. These results fol-\nlow the same pattern as in the previous experiments, with\nthe CRF classiﬁer being the most precise of all, and BERT\noutperforming both the CRF and spaCy classiﬁers thanks to\nits greater recall. We also show the results of Mao and Liu\n(2019) who, despite of having used a BERT-based system,\nachieve lower scores than our models. The reason why it\nshould be so remain unclear.\nWith regard to the winner of the MEDDOCAN shared\ntask, the BERT-based model has not improved the scores\nobtained by neither the domain-dependent (S3) nor the\ndomain-independent (S2) NLNDE model. However, at-\ntending to the obtained results, BERT remains only 0.3 F1-\nscore points behind, and would have achieved the second\nposition among all the MEDDOCAN shared task competi-\ntors. Taking into account that only 3% of the gold labels\nremain incorrectly annotated, the task can be considered al-\nmost solved, and it is not clear if the differences among the\nsystems are actually signiﬁcant, or whether they stem from\nminor variations in initialisation or a long-tail of minor la-belling inconsistencies.\n5. Conclusions and Future Work\nIn this work we have brieﬂy introduced the problems re-\nlated to data privacy protection in clinical domain. We have\nalso described some of the groundbreaking advances on the\nNatural Language Processing ﬁeld due to the appearance of\nTransformers-based deep-learning architectures and trans-\nfer learning from very large general-domain multilingual\ncorpora, focusing our attention in one of its most represen-\ntative examples, Google’s BERT model.\nIn order to assess the performance of BERT for Span-\nish clinical data anonymisation, we have conducted sev-\neral experiments with a BERT-based sequence labelling\napproach using the pre-trained multilingual BERT model\nshared by Google as the starting point for the model train-\ning. We have compared this BERT-based sequence la-\nbelling against other methods and systems. One of the ex-\nperiments uses the MEDDOCAN 2019 shared task dataset,\nwhile the other uses a novel Spanish clinical reports dataset\ncalled NUB ES-PHI.\nThe results of the experiments show that, in NUB ES-PHI,\nthe BERT-based model outperforms the other systems with-\nout requiring any adaptation or domain-speciﬁc feature en-\ngineering, just by being trained on the provided labelled\ndata.']",The text does not provide information on the performance of the BERT-based model compared to other systems in the MEDDOCAN corpus.,[''],The BERT-based model outperforms the other systems in the MEDDOCAN corpus.,0.22223504075176298,0.6129466045176262
47,"What is the performance of the BERT-based model in the MEDDOCAN shared task compared to other systems, and how does it achieve its results?","The BERT-based model in the MEDDOCAN shared task achieves a lower precision compared to other systems, indicating more false positive predictions. However, it achieves a significantly higher recall. This means that while it may make more mistakes in identifying sensitive information, it excels in capturing a larger portion of the actual sensitive data present in the text.","['Speciﬁcally, we in-\nclude the results of a domain-independent NLNDE model\n(S2), and the results of a model enriched with domain-\nspeciﬁc embeddings (S3). Finally, we include the results\nobtained by Mao and Liu (2019) with a CRF output layer\non top of BERT embeddings. MEDDOCAN consists of\ntwo scenarios:\nDetection - This evaluation measures how good a system\nis at detecting sensitive text spans, regardless of the\ncategory assigned to them.\nClassiﬁcation - In this scenario, systems are required to\nmatch exactly not only the boundaries of each sensi-\ntive span, but also the category assigned.\nThe systems are evaluated in terms of micro-averaged pre-\ncision, recall and F-1 score. Note that, in contrast to the\nevaluation in Experiment A, MEDDOCAN measurements\nare entity-based instead of tokenwise. An exhaustive expla-\nnation of the MEDDOCAN evaluation procedure is avail-\nable online10, as well as the ofﬁcial evaluation script11,\nwhich we used to obtain the reported results.\n4. Results\nThis section describes the results obtained in the two sets\nof experiments: NUB ES-PHI and MEDDOCAN.\n4.1. Experiment A: NUB ES-PHI\nTable 5 shows the results of the conducted experiments in\nNUB ES-PHI for all the compared systems. The included\nbaseline serves to give a quick insight about how chal-\nlenging the data is. With simple regular expressions and\ngazetteers a precision of 0.853 is obtained. On the other\nhand, the recall, which directly depends on the coverage\nprovided by the rules and resources, drops to 0.469. Hence,\nthis task is unlikely to be solved without the generalisa-\ntion capabilities provided by machine-learning and deep-\nlearning models.\nRegarding the detection scenario –that is, the scenario con-\ncerned with a binary classiﬁcation to determine whether\neach individual token conveys sensitive information or not–\n, it can be observed that BERT outperforms its competitors.\nA fact worth highlighting is that, according to these results,\nBERT achieves a precision lower than the rest of the sys-\ntems (i.e., it makes more false positive predictions); in ex-\nchange, it obtains a remarkably higher recall. Noticeably, it\n10http://temu.bsc.es/meddocan/index.php/evaluation/\n11https://github.com/PlanTL-SANIDAD/MEDDOCAN-\nEvaluation-Script'
 'In order to assess the performance of BERT for Span-\nish clinical data anonymisation, we have conducted sev-\neral experiments with a BERT-based sequence labelling\napproach using the pre-trained multilingual BERT model\nshared by Google as the starting point for the model train-\ning. We have compared this BERT-based sequence la-\nbelling against other methods and systems. One of the ex-\nperiments uses the MEDDOCAN 2019 shared task dataset,\nwhile the other uses a novel Spanish clinical reports dataset\ncalled NUB ES-PHI.\nThe results of the experiments show that, in NUB ES-PHI,\nthe BERT-based model outperforms the other systems with-\nout requiring any adaptation or domain-speciﬁc feature en-\ngineering, just by being trained on the provided labelled\ndata. Interestingly, the BERT-based model obtains a re-\nmarkably higher recall than the other systems. High recall\nis a desirable outcome because, when anonymising sensible\ndocuments, the accidental leak of sensible data is likely to']","The text does not provide information on the performance of the BERT-based model in the MEDDOCAN shared task compared to other systems, or how it achieves its results.",[''],The BERT-based model outperforms other systems in the MEDDOCAN shared task. It achieves its results by using the pre-trained multilingual BERT model and training on the provided labelled data.,0.7236995966027762,0.8580252750845523
48,"How are behavioral features used to predict dogmatism levels in users, and what impact does interacting with a dogmatic comment have on a conversation?","Behavioral features are utilized to predict dogmatism levels in users by fitting them to a linear regression model. In this model, positive coefficients indicate a positive predictive relationship with dogmatism, while negative coefficients indicate a negative predictive relationship. The model is significantly predictive of dogmatism, with features like activity and focus being positively associated with dogmatism, while breadth and engagement are negatively associated. Interacting with a dogmatic comment tends to make a user more dogmatic themselves, as evidenced by a positive association between engaging with a dogmatic comment and increased dogmatism in the conversation.","['We then ﬁt these behavioral features to a linear re-\ngression model where we predict each user’s average\ndogmatism level. Positive coefﬁcients in this model\nare positively predictive of dogmatism, while nega-\ntive coefﬁcients are negatively predictive.\nWe ﬁnd this model is signiﬁcantly predicitive of\ndogmatism ( R2= 0.1,p< 0.001), with all features\nreaching statistical signiﬁcance ( p < 0.001).Activ-\nityandfocus are positively associated with dogma-\ntism, while breadth andengagement are negatively\nassociated (Table 5). Together, these results suggest\ndogmatic users tend to post frequently and in spe-\nciﬁc communities, but are not as inclined to continue\nto engage with a discussion, once it has begun.\n5.4 How does dogmatism impact a\nconversation? (R4)\nHow does interacting with a dogmatic comment im-\npact a conversation? Are users able to shrug it off?\nOr do otherwise non-dogmatic users become more\ndogmatic themselves?\nTo answer this question, we sample 600,000 con-\nversations triples from Reddit. These conversations\nconsist of two people (A and B) talking, with the\nstructure: A1 →B→A2. This allows us to mea-\nsure the impact of B’s dogmatism on A’s response,\nwhile also controlling for the dogmatism level ini-\ntially set by A. Concretely, we model the impact of\ndogmatism on these conversations through a linear\nregression. This model takes two features, the dog-\nmatism levels of A1 and B, and predicts the dogma-\ntism response of A2. If B’s dogmatism has no effect\non A’s response, the coefﬁcient that corresponds to\nB will not be signiﬁcant in the model. Alternatively,\nif B’s dogmatism does have some effect, it will be\ncaptured by the model’s coefﬁcient.\nWe ﬁnd the coefﬁcient of the B feature in the\nmodel is positively associated with dogmatism ( p<\n0.001). In other words, engagement with a dog-\nmatic comment tends to make a user more dogmatic\nthemselves. This effect holds when we run the same\nmodel on data subsets consisting only of dogmatic\nor non-dogmatic users, and also when we conserva-\ntively remove all words used by B from A’s response\n(i.e., controlling for quoting effects).6 Related Work\nIn contrast to the computational models we have pre-\nsented, dogmatism is usually measured in psychol-\nogy through survey scales, in which study partic-\nipants answer questions designed to reveal under-\nlying personality attributes (Rokeach, 1954). Over\ntime, these surveys have been updated (Shearman\nand Levine, 2006) and improved to meet standards\nof psychometric validity (Crowson, 2009).\nThese surveys are often used to study the rela-\ntionship between dogmatism and other psychologi-\ncal phenomena. For example, dogmatic people tend\nto show an increased tendency for confrontation (El-\nNawawy and Powers, 2010) or moral conviction and\nreligiosity (Swink, 2011), and less likelihood of cog-\nnitive ﬂexibility (Martin et al., 2011), even among\nstereotypically non-dogmatic groups like atheists\n(Gurney et al., 2013). From a behavioral standpoint,\ndogmatic people solve problems differently, spend-\ning less time framing a problem and expressing more\ncertainty in their solution (Lohman, 2010). Here we\nsimilarly examine how user behaviors on Reddit re-\nlate to a language model of dogmatism.\nErtel sought to capture dogmatism linguistically,\nthough a small lexicon of words that correspond\nwith high-level concepts like certainty and compro-\nmise (1985). McKenny then used this dictionary to\nrelate dogmatism to argument quality in student es-\nsays (2005). Our work expands on this approach,\napplying supervised models based on a broader set\nof linguistic categories to identify dogmatism in text.'
 'Identifying Dogmatism in Social Media: Signals and Models\nEthan Fast and Eric Horvitz\nethaen@stanford.edu, horvitz@microsoft.com\nAbstract\nWe explore linguistic and behavioral features\nof dogmatism in social media and construct\nstatistical models that can identify dogmatic\ncomments. Our model is based on a corpus of\nReddit posts, collected across a diverse set of\nconversational topics and annotated via paid\ncrowdsourcing. We operationalize key aspects\nof dogmatism described by existing psychol-\nogy theories (such as over-conﬁdence), ﬁnd-\ning they have predictive power. We also ﬁnd\nevidence for new signals of dogmatism, such\nas the tendency of dogmatic posts to refrain\nfrom signaling cognitive processes. When we\nuse our predictive model to analyze millions\nof other Reddit posts, we ﬁnd evidence that\nsuggests dogmatism is a deeper personality\ntrait, present for dogmatic users across many\ndifferent domains, and that users who engage\non dogmatic comments tend to show increases\nin dogmatic posts themselves.\n1 Introduction\n“I’m supposed to trust the opinion of a MS min-\nion? The people that produced Windows ME, Vista\nand 8? They don’t even understand people, yet they\nthink they can predict the behavior of new, self-\nguiding AI?” – anonymous\n“I think an AI would make it easier for Patients to\nconﬁde their information because by nature, a robot\ncannot judge them. Win-win? :D”’ – anonymous\nDogmatism describes the tendency to lay down\nopinions as incontrovertibly true, without respect for\nconﬂicting evidence or the opinions of others (Ox-\nford Dictionary, 2016). Which user is more dog-\nmatic in the examples above? This question is sim-\nple for humans. Phrases like “they think” and “theydon’t even understand,” suggest an intractability of\nopinion, while “I think” and “win-win?” suggest\nthe opposite. Can we train computers to draw sim-\nilar distinctions? Work in psychology has called\nout many aspects of dogmatism that can be modeled\ncomputationally via natural language, such as over-\nconﬁdence and strong emotions (Rokeach, 1954).\nWe present a statistical model of dogmatism that\naddresses two complementary goals. First, we val-\nidate psychological theories by examining the pre-\ndictive power of feature sets that guide the model’s\npredictions. For example, do linguistic signals of\ncertainty help to predict a post is dogmatic, as the-\nory would suggest? Second, we apply our model to\nanswer four questions:\nR1: What kinds of topics (e.g., guns, LGBT) at-\ntract the highest levels of dogmatism?\nR2: How do dogmatic beliefs cluster?\nR3: How does dogmatism inﬂuence a conversa-\ntion on social media?\nR4: How do other user behaviors (e.g., frequency\nand breadth of posts) relate to dogmatism?\nWe train a predictive model to classify dogmatic\nposts from Reddit, one of the most popular discus-\nsion communities on the web.1Posts on Reddit cap-\nture discussion and debate across a diverse set of do-\nmains and topics – users talk about everything from\nclimate change and abortion, to world news and re-\nlationship advice, to the future of artiﬁcial intelli-\ngence. As a prerequisite to training our model, we\nhave created a corpus of 5,000 Reddit posts anno-\ntated with levels of dogmatism, which we are releas-\ning to share with other researchers.\n1http://www.reddit.comarXiv:1609.00425v1  [cs.CL]  1 Sep 2016']","Behavioral features are used to predict dogmatism levels in users by analyzing their language use, interaction patterns, and engagement levels. For instance, users who frequently use absolute terms, show resistance to change their views, or engage in repetitive arguments may be considered dogmatic. 

Interacting with a dogmatic comment can significantly impact a conversation. It can lead to polarization, where the conversation becomes more divided and less open to differing viewpoints. It can also increase the likelihood of conflict and reduce the overall quality of the discussion. Furthermore, it can discourage participation from other users who may feel overwhelmed or intimidated by the dogmatic stance.",[''],"Behavioral features are used to fit a linear regression model to predict dogmatism levels in users. Positive coefficients in the model are positively predictive of dogmatism, while negative coefficients are negatively predictive. Interacting with a dogmatic comment tends to make a user more dogmatic themselves.",0.5074985758918983,0.8771181428005139
49,How does BRLM utilize alignment information to improve word-level representation alignment between different languages in zero-shot translation?,"BRLM utilizes alignment information by either using an external aligner tool or introducing an additional attention layer during model training. This allows BRLM to explicitly introduce alignment information, which helps in inferring words across languages based on alignment information. By leveraging alignment information, BRLM strengthens the ability to guess masked words in a corrupted sentence by considering the context of aligned words in the other language, thereby improving word-level representation alignment between different languages in zero-shot translation.","['Figure 2: The overview of BRidge Language Modeling (BRLM). The BRLM extends MLM (Lample and Conneau 2019) to\npairs of parallel sentences and leverages explicit alignment information obtained by external aligner tool or additional attention\nlayer to encourage word representation alignment across different languages.\n•Directly translate source sentences into target sentences\nwith the parent model, which beneﬁts from the availabil-\nity of the universal encoder.\nThe key difﬁculty of this method is to ensure the intermedi-\nate representations of the universal encoder are language in-\nvariant. In the rest of this section, we ﬁrst present two exist-\ning methods yet to be explored in zero-shot translation, and\nthen propose a straightforward but effective cross-lingual\npre-training method. In the end, we present the whole train-\ning and inference protocol for transfer.\nMasked and Translation Language Model\nPretraining\nTwo existing cross-lingual pre-training methods, Masked\nLanguage Modeling (MLM) and Translation Language\nModeling (TLM), have shown their effectiveness on XNLI\ncross-lingual classiﬁcation task (Lample and Conneau 2019;\nHuang et al. 2019), but these methods have not been well\nstudied on cross-lingual generation tasks in zero-shot condi-\ntion. We attempt to take advantage of the cross-lingual abil-\nity of the two methods for zero-shot translation.\nSpeciﬁcally, MLM adopts the Cloze objective of\nBERT (Devlin et al. 2018) and predicts the masked words\nthat are randomly selected and replaced with [MASK] to-\nken on monolingual corpus. In practice, MLM takes dif-\nferent language monolingual corpora as input to ﬁnd fea-\ntures shared across different languages. With this method,\nword pieces shared in all languages have been mapped into\na shared space, which makes the sentence representationsacross different languages close (Pires, Schlinger, and Gar-\nrette 2019).\nSince MLM objective is unsupervised and only requires\nmonolingual data, TLM is designed to leverage parallel data\nwhen it is available. Actually, TLM is a simple extension of\nMLM, with the difference that TLM concatenates sentence\npair into a whole sentence, and then randomly masks words\nin both the source and target sentences. In this way, the\nmodel can either attend to surrounding words or to the trans-\nlation sentence, implicitly encouraging the model to align\nthe source and target language representations. Note that al-\nthough each sentence pair is formed into one sentence, the\npositions of the target sentence are reset to count form zero.\nBridge Language Model Pretraining\nAside from MLM and TLM, we propose BRidge Language\nModeling (BRLM) to further obtain word-level representa-\ntion alignment between different languages. This method is\ninspired by the assumption that if the feature spaces of dif-\nferent languages are aligned very well, the masked words\nin the corrupted sentence can also be guessed by the con-\ntext of the correspondingly aligned words on the other side.\nTo achieve this goal, BRLM is designed to strengthen the\nability to infer words across languages based on alignment\ninformation, instead of inferring words within monolingual\nsentence as in MLM or within the pseudo sentence formed\nby concatenating sentence pair as in TLM.\nAs illustrated in Figure 2, BRLM stacks shared encoder\nover both side sentences separately. In particular, we design\ntwo network structures for BRLM, which are divided into'
 'Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-\nSA) according to the way of generating the alignment infor-\nmation. These two structures actually extend MLM into a\nbilingual scenario, with the difference that BRLM leverages\nexternal aligner tool or additional attention layer to explic-\nitly introduce alignment information during model training.\n•Hard Alignment (BRLM-HA). We ﬁrst use exter-\nnal aligner tool on source ↔pivot parallel data to ex-\ntract the alignment information of sentence pair. Dur-\ning model training, given source ↔pivot sentence pair,\nBRLM-HA randomly masks some words in source sen-\ntence and leverages alignment information to obtain the\naligned words in pivot sentence for masked words. Based\non the processed input, BRLM-HA adopts the Trans-\nformer (Vaswani et al. 2017) encoder to gain the hid-\nden states for source and pivot sentences respectively.\nThen the training objective of BRLM-HA is to predict\nthe masked words by not only the surrounding words\nin source sentence but also the encoder outputs of the\naligned words. Note that this training process is also car-\nried out in a symmetric situation, in which we mask some\nwords in pivot sentence and obtain the aligned words in\nthe source sentence.\n•Soft Alignment (BRLM-SA). Instead of using external\naligner tool, BRLM-SA introduces an additional atten-\ntion layer to learn the alignment information together with\nmodel training. In this way, BRLM-SA avoids the effect\ncaused by external wrong alignment information and en-\nables many-to-one soft alignment during model training.\nSimilar with BRLM-HA, the training objective of BRLM-\nSA is to predict the masked words by not only the sur-\nrounding words in source sentence but also the outputs of\nattention layer. In our implementation, the attention layer\nis a multi-head attention layer adopted in Transformer,\nwhere the queries come from the masked source sentence,\nthe keys and values come from the pivot sentence.\nIn principle, MLM and TLM can learn some implicit align-\nment information during model training. However, the align-\nment process in MLM is inefﬁcient since the shared word\npieces only account for a small proportion of the whole cor-\npus, resulting in the difﬁculty of expanding the shared in-\nformation to align the whole corpus. TLM also lacks effort\nin alignment between the source and target sentences since\nTLM concatenates the sentence pair into one sequence, mak-\ning the explicit alignment between the source and target in-\nfeasible. BRLM fully utilizes the alignment information to\nobtain better word-level representation alignment between\ndifferent languages, which better relieves the burden of the\ndomain shift problem .\nTransfer Protocol\nWe consider the typical zero-shot translation scenario in\nwhich a high resource pivot language has parallel data with\nboth source and target languages, while source and target\nlanguages has no parallel data between themselves. Our pro-\nposed cross-lingual pretraining based transfer approach for\nsource→target zero-shot translation is mainly divided into\ntwo phrases: the pretraining phase and the transfer phase.Corpus Language Train Dev Test\nEuroparlDe-En,En-Fr 1M,1M 2,000 2,000\nFr-En,En-Es 1M,1M 2,000 2,000\nRo-En,En-De 0.6M,1.5M 2,000 1,000\nMultiUNAr-En,En-Es\nEn-Ru9.7M,11.3M\n11.6M4,000 4,000\nTable 1: Data Statistics.\nIn the pretraining phase, we ﬁrst pretrain MLM on mono-\nlingual corpora of both source and pivot languages, and con-\ntinue to pretrain TLM or the proposed BRLM on the avail-\nable parallel data between source and pivot languages, in\norder to build a cross-lingual encoder shared by the source\nand pivot languages.\nIn the transfer phase, we train pivot →target NMT model\ninitialized by the cross-lingually pre-trained encoder, and\nﬁnally transfer the trained NMT model to source →target\ntranslation thanks to the shared encoder. Note that during\ntraining pivot→target NMT model, we freeze several layers\nof the cross-lingually pre-trained encoder to avoid the de-\ngeneracy issue.']","BRLM, or Bilingual Representation Learning Model, utilizes alignment information by training a shared encoder and individual decoders for each language. The shared encoder learns a language-agnostic representation, which aligns the word-level representations across different languages. This is achieved by minimizing the distance between the representations of the same word in different languages. In zero-shot translation, this alignment allows the model to translate between language pairs it has not been explicitly trained on, by leveraging the shared word-level representation.",[''],"BRLM utilizes alignment information by using an external aligner tool or an additional attention layer during model training. It randomly masks words in the source sentence and uses alignment information to obtain the aligned words in the pivot sentence. The training objective is to predict the masked words using both the surrounding words in the source sentence and the encoder outputs of the aligned words. This process is carried out symmetrically, masking words in the pivot sentence and obtaining aligned words in the source sentence. BRLM also introduces a soft alignment approach using an attention layer to learn alignment information. The attention layer is a multi-head attention layer where the queries come from the masked source sentence and the keys and values come from the pivot sentence.",0.6539393461733443,0.38999109227329204
50,What is the impact of freezing parameters on the performance of transfer learning in zero-shot translation using MLM+BRLM-SA?,"Freezing parameters in transfer learning using MLM+BRLM-SA has a notable impact on performance in zero-shot translation. Updating all parameters during training can lead to a significant drop in the zero-shot direction due to catastrophic forgetting. On the other hand, freezing all parameters can result in a decline in the supervised direction because the language features extracted during pre-training may not be sufficient for the machine translation task. Freezing the first four layers of the transformer has been shown to offer the best performance, maintaining a balance between pre-training and fine-tuning.","['SA with back translation also achieves better performance\nthan the original supervised Transformer.\nAnalysis\nSentence Representation. We ﬁrst evaluate the represen-\ntational invariance across languages for all cross-lingual pre-\ntraining methods. Following Arivazhagan et al. (2018), we\nadopt max-pooling operation to collect the sentence rep-\nresentation of each encoder layer for all source-pivot sen-\ntence pairs in the Europarl validation sets. Then we calcu-\nlate the cosine similarity for each sentence pair and aver-\nage all cosine scores. As shown in Figure 3, we can ob-\nserve that, MLM+BRLM-SA has the most stable and similar\ncross-lingual representations of sentence pairs on all layers,\nwhile it achieves the best performance in zero-shot transla-\ntion. This demonstrates that better cross-lingual representa-\ntions can beneﬁt for the process of transfer learning. Besides,\nMLM+BRLM-HA is not as superior as MLM+BRLM-\nSA and even worse than MLM+TLM on Fr-En, since\nMLM+BRLM-HA may suffer from the wrong alignment\nknowledge from an external aligner tool. We also ﬁnd an in-\nteresting phenomenon that as the number of layers increases,\nthe cosine similarity decreases.\nContextualized Word Representation. We further sam-\nple an English-Russian sentence pair from the MultiUN\nvalidation sets and visualize the cosine similarity between\nhidden states of the top encoder layer to further investi-\ngate the difference of all cross-lingual pre-training meth-\nods. As shown in Figure 4, the hidden states generated by\nMLM+BRLM-SA have higher similarity for two aligned\nwords. It indicates that MLM+BRLM-SA can gain bet-\nter word-level representation alignment between source and\npivot languages, which better relieves the burden of the do-\nmain shift problem .\nThe Effect of Freezing Parameters. To freeze parame-\nters is a common strategy to avoid catastrophic forgetting in\ntransfer learning (Howard and Ruder 2018). Table 4 shows\nthe performance of transfer learning with freezing different\nlayers on MultiUN test set, in which En →Ru denotes the\nparent model, Ar→Ru and Es→Ru are two child models,\nand all models are based on MLM+BRLM-SA. We can ﬁnd\nthat updating all parameters during training will cause a no-\ntable drop on the zero-shot direction due to the catastrophic\nforgetting. On the contrary, freezing all the parameters leads\nto the decline on supervised direction because the language\nfeatures extracted during pre-training is not sufﬁcient for\nMT task. Freezing the ﬁrst four layers of the transformer\nshows the best performance and keeps the balance between\npre-training and ﬁne-tuning.\nConclusion\nIn this paper, we propose a cross-lingual pretraining based\ntransfer approach for the challenging zero-shot translation\ntask, in which source and target languages have no parallel\ndata, while they both have parallel data with a high resource\n(a) MLM\n (b) MLM+TLM\n(c) MLM+BRLM-HA\n (d) MLM+BRLM-SA\nFigure 4: Cosine similarity visualization at word level given\nan English-Russian sentence pair from the MultiUN valida-\ntion sets. Brighter indicates higher similarity.\nFreezing Layers En→Ru Ar→Ru Es→Ru\nNone 37.80 16.09 19.80\n2 37.79 21.47 28.35\n4 37.55 25.49 30.47\n6 35.31 22.90 28.22\nTable 4: BLEU score of freezing different layers. The num-\nber in Freezing Layers column denotes that the number of\nencoder layers will not be updated.\npivot language. With the aim of building the language in-\nvariant representation between source and pivot languages\nfor smooth transfer of the parent model of pivot →target di-\nrection to the child model of source →target direction, we in-\ntroduce one monolingual pretraining method and two bilin-\ngual pretraining methods to construct an universal encoder\nfor the source and pivot languages. Experiments on public\ndatasets show that our approaches signiﬁcantly outperforms\nseveral strong baseline systems, and manifest the language\ninvariance characteristics in both sentence level and word\nlevel neural representations.\nAcknowledgments\nWe would like to thank the anonymous reviewers for the\nhelpful comments. This work was supported by National\nKey R&D Program of China (Grant No. 2016YFE0132100),\nNational Natural Science Foundation of China (Grant No.'
 '2016).Although it is challenging for one model to translate all\nzero-shot directions between multiple distant language pairs\nof MultiUN, MLM+BRLM-SA still achieves better perfor-\nmances on Es→Ar and Es→Ru than strong pivoting m,\nwhich uses MNMT to translate source to pivot then to tar-\nget in two separate steps with each step receiving supervised\nsignal of parallel corpora. Our approaches surpass pivoting m\nin all zero-shot directions by adding back translation (Sen-\nnrich, Haddow, and Birch 2015) to generate pseudo parallel\nsentences for all zero-shot directions based on our pretrained\nmodels such as MLM+BRLM-SA, and further training our\nuniversal encoder-decoder model with these pseudo data.\nGu et al. (2019) introduces back translation into MNMT,\nwhile we adopt it in our transfer approaches. Finally, our\nbest MLM+BRLM-SA with back translation outperforms\npivoting mby 2.4 BLEU points averagely, and outperforms\nMNMT (Gu et al. 2019) by 4.6 BLEU points averagely.\nAgain, in supervised translation directions, MLM+BRLM-']",The paper does not provide specific information on the impact of freezing parameters on the performance of transfer learning in zero-shot translation using MLM+BRLM-SA.,[''],"Freezing different layers of the transformer shows different impacts on the performance of transfer learning in zero-shot translation using MLM+BRLM-SA. Freezing all parameters leads to a decline in the supervised direction, while updating all parameters during training causes a notable drop in the zero-shot direction. Freezing the first four layers of the transformer shows the best performance and maintains a balance between pre-training and fine-tuning.",0.22149243263854695,0.7438320177667106
51,What additional information does the newly developed corpus for automatic readability assessment and text simplification of German provide?,"The newly developed corpus for automatic readability assessment and text simplification of German provides information on text structure, typography, and images, which can be utilized in machine learning approaches for readability assessment and text simplification.","['arXiv:1909.09067v1  [cs.CL]  19 Sep 2019A Corpus for Automatic Readability Assessment and Text Simp liﬁcation\nof German\nAlessia Battisti\nInstitute of Computational Linguistics\nUniversity of Zurich\nAndreasstrasse 15, 8050 Zurich\nalessia.battisti@uzh.chSarah Ebling\nInstitute of Computational Linguistics\nUniversity of Zurich\nAndreasstrasse 15, 8050 Zurich\nebling@cl.uzh.ch\nAbstract\nIn this paper, we present a corpus for use in\nautomatic readability assessment and auto-\nmatic text simpliﬁcation of German. The\ncorpus is compiled from web sources and\nconsists of approximately 211,000 sen-\ntences. As a novel contribution, it con-\ntains information on text structure, typog-\nraphy, and images, which can be exploited\nas part of machine learning approaches to\nreadability assessment and text simpliﬁca-\ntion. The focus of this publication is on\nrepresenting such information as an exten-\nsion to an existing corpus standard.\n1 Introduction\nSimpliﬁed language is a variety of standard lan-\nguage characterized by reduced lexical and syn-\ntactic complexity, the addition of explanations\nfor difﬁcult concepts, and clearly structured lay-\nout.1Among the target groups of simpliﬁed lan-\nguage commonly mentioned are persons with cog-\nnitive impairment or learning disabilities, prelin-\ngually deaf persons, functionally illiterate persons,\nand foreign language learners (Bredel and Maaß,\n2016).\nTwo natural language processing tasks deal with\nthe concept of simpliﬁed language: automatic\nreadability assessment and automatic text simpli-\nﬁcation. Readability assessment refers to the pro-\ncess of determining the level of difﬁculty of a text,\ne.g., along readability measures, school grades, or\nlevels of the Common European Framework of\nReference for Languages (CEFR) (Council of Eu-\nrope, 2009). Readability measures, in their tra-\nditional form, take into account only surface fea-\ntures. For example, the Flesch Reading Ease Score\n1The term plain language is avoided, as it refers to a spe-\nciﬁc level of simpliﬁcation. Simpliﬁed language subsumes all\nefforts of reducing the complexity of a piece of text.(Flesch, 1948) measures the length of words (in\nsyllables) and sentences (in words). While read-\nability has been shown to correlate with such fea-\ntures to some extent (Just and Carpenter, 1980), a\nconsensus has emerged according to which they\nare not sufﬁcient to account for all of the com-\nplexity inherent in a text. As Kauchak et al.\n(2014, p. 2618) state, “the usability of readabil-\nity formulas is limited and there is little evidence\nthat the output of these tools directly results in\nimproved understanding by readers”. Recently,\nmore sophisticated models employing (deeper) lin-\nguistic features such as lexical, semantic, mor-\nphological, morphosyntactic, syntactic, pragmatic,\ndiscourse, psycholinguistic, and language model\nfeatures have been proposed (Collins-Thompson,\n2014; Heimann M¨ uhlenbock, 2013; Pitler and\nNenkova, 2008; Schwarm and Ostendorf, 2005;\nTanaka et al., 2013).\nAutomatic text simpliﬁcation was initiated in\nthe late 1990s (Carroll et al., 1998; Chandrasekar\net al., 1996) and since then has been approached\nby means of rule-based and statistical methods. As\npart of a rule-based approach, the operations car-\nried out typically include replacing complex lex-\nical and syntactic units by simpler ones. A sta-\ntistical approach generally conceptualizes the sim-\npliﬁcation task as one of converting a standard-\nlanguage into a simpliﬁed-language text using ma-\nchine translation. Nisioi et al. (2017) introduced\nneural machine translation to automatic text sim-\npliﬁcation. Research on automatic text simpliﬁ-\ncation is comparatively widespread for languages\nsuch as English, Swedish, Spanish, and Brazilian\nPortuguese. To the authors’ knowledge, no pro-\nductive system exists for German. Suter (2015),\nSuter et al.'
 'To\nour knowledge, this is the ﬁrst study to deliver em-\npirical proof of the relevance of such features.\n4 Conclusion and Outlook\nWe have introduced a corpus compiled for use\nin automatic readability assessment and automatic\ntext simpliﬁcation of German. While such tasks\nhave been addressed for other languages, research\non German is still scarce. The features exploited\nas part of machine learning approaches to read-\nability assessment so far typically include surface\nand/or (deeper) linguistic features. The corpus\npresented in this paper additionally contains infor-\nmation on text structure, typography, and images.\nThese features have been shown to be indicative\nof simple vs. complex texts both theoretically and,\nusing the corpus described in this paper, empiri-\ncally.\nInformation on text structure, typography, and\nimages can also be leveraged as part of a neu-\nral machine translation approach to text simpli-\nﬁcation. A set of parallel documents used in\nmachine translation additionally requires sentence\nalignments, which are still missing from our cor-\npus. Hence, as a next step, we will include such']","The newly developed corpus for automatic readability assessment and text simplification of German provides additional information such as sentence splitting, part-of-speech tagging, and syntactic parsing. It also includes information on the complexity levels of the texts, which can be used to develop and train algorithms for text simplification and readability assessment.",[''],"The newly developed corpus for automatic readability assessment and text simplification of German provides information on text structure, typography, and images.",0.8403101245628788,0.747278311046395
52,How does dogmatism impact a conversation on Reddit? Does engaging with a dogmatic comment make users more dogmatic themselves?,Engaging with a dogmatic comment on Reddit tends to make users more dogmatic themselves. This effect is observed through a linear regression model that analyzes the impact of dogmatism on conversations. The coefficient associated with the dogmatism level of the comment being engaged with is positively linked to an increase in dogmatism in the user's response. This suggests that interacting with dogmatic comments can influence users to become more dogmatic in their own responses.,"['We then ﬁt these behavioral features to a linear re-\ngression model where we predict each user’s average\ndogmatism level. Positive coefﬁcients in this model\nare positively predictive of dogmatism, while nega-\ntive coefﬁcients are negatively predictive.\nWe ﬁnd this model is signiﬁcantly predicitive of\ndogmatism ( R2= 0.1,p< 0.001), with all features\nreaching statistical signiﬁcance ( p < 0.001).Activ-\nityandfocus are positively associated with dogma-\ntism, while breadth andengagement are negatively\nassociated (Table 5). Together, these results suggest\ndogmatic users tend to post frequently and in spe-\nciﬁc communities, but are not as inclined to continue\nto engage with a discussion, once it has begun.\n5.4 How does dogmatism impact a\nconversation? (R4)\nHow does interacting with a dogmatic comment im-\npact a conversation? Are users able to shrug it off?\nOr do otherwise non-dogmatic users become more\ndogmatic themselves?\nTo answer this question, we sample 600,000 con-\nversations triples from Reddit. These conversations\nconsist of two people (A and B) talking, with the\nstructure: A1 →B→A2. This allows us to mea-\nsure the impact of B’s dogmatism on A’s response,\nwhile also controlling for the dogmatism level ini-\ntially set by A. Concretely, we model the impact of\ndogmatism on these conversations through a linear\nregression. This model takes two features, the dog-\nmatism levels of A1 and B, and predicts the dogma-\ntism response of A2. If B’s dogmatism has no effect\non A’s response, the coefﬁcient that corresponds to\nB will not be signiﬁcant in the model. Alternatively,\nif B’s dogmatism does have some effect, it will be\ncaptured by the model’s coefﬁcient.\nWe ﬁnd the coefﬁcient of the B feature in the\nmodel is positively associated with dogmatism ( p<\n0.001). In other words, engagement with a dog-\nmatic comment tends to make a user more dogmatic\nthemselves. This effect holds when we run the same\nmodel on data subsets consisting only of dogmatic\nor non-dogmatic users, and also when we conserva-\ntively remove all words used by B from A’s response\n(i.e., controlling for quoting effects).6 Related Work\nIn contrast to the computational models we have pre-\nsented, dogmatism is usually measured in psychol-\nogy through survey scales, in which study partic-\nipants answer questions designed to reveal under-\nlying personality attributes (Rokeach, 1954). Over\ntime, these surveys have been updated (Shearman\nand Levine, 2006) and improved to meet standards\nof psychometric validity (Crowson, 2009).\nThese surveys are often used to study the rela-\ntionship between dogmatism and other psychologi-\ncal phenomena. For example, dogmatic people tend\nto show an increased tendency for confrontation (El-\nNawawy and Powers, 2010) or moral conviction and\nreligiosity (Swink, 2011), and less likelihood of cog-\nnitive ﬂexibility (Martin et al., 2011), even among\nstereotypically non-dogmatic groups like atheists\n(Gurney et al., 2013). From a behavioral standpoint,\ndogmatic people solve problems differently, spend-\ning less time framing a problem and expressing more\ncertainty in their solution (Lohman, 2010). Here we\nsimilarly examine how user behaviors on Reddit re-\nlate to a language model of dogmatism.\nErtel sought to capture dogmatism linguistically,\nthough a small lexicon of words that correspond\nwith high-level concepts like certainty and compro-\nmise (1985). McKenny then used this dictionary to\nrelate dogmatism to argument quality in student es-\nsays (2005). Our work expands on this approach,\napplying supervised models based on a broader set\nof linguistic categories to identify dogmatism in text.'
 'Identifying Dogmatism in Social Media: Signals and Models\nEthan Fast and Eric Horvitz\nethaen@stanford.edu, horvitz@microsoft.com\nAbstract\nWe explore linguistic and behavioral features\nof dogmatism in social media and construct\nstatistical models that can identify dogmatic\ncomments. Our model is based on a corpus of\nReddit posts, collected across a diverse set of\nconversational topics and annotated via paid\ncrowdsourcing. We operationalize key aspects\nof dogmatism described by existing psychol-\nogy theories (such as over-conﬁdence), ﬁnd-\ning they have predictive power. We also ﬁnd\nevidence for new signals of dogmatism, such\nas the tendency of dogmatic posts to refrain\nfrom signaling cognitive processes. When we\nuse our predictive model to analyze millions\nof other Reddit posts, we ﬁnd evidence that\nsuggests dogmatism is a deeper personality\ntrait, present for dogmatic users across many\ndifferent domains, and that users who engage\non dogmatic comments tend to show increases\nin dogmatic posts themselves.\n1 Introduction\n“I’m supposed to trust the opinion of a MS min-\nion? The people that produced Windows ME, Vista\nand 8? They don’t even understand people, yet they\nthink they can predict the behavior of new, self-\nguiding AI?” – anonymous\n“I think an AI would make it easier for Patients to\nconﬁde their information because by nature, a robot\ncannot judge them. Win-win? :D”’ – anonymous\nDogmatism describes the tendency to lay down\nopinions as incontrovertibly true, without respect for\nconﬂicting evidence or the opinions of others (Ox-\nford Dictionary, 2016). Which user is more dog-\nmatic in the examples above? This question is sim-\nple for humans. Phrases like “they think” and “theydon’t even understand,” suggest an intractability of\nopinion, while “I think” and “win-win?” suggest\nthe opposite. Can we train computers to draw sim-\nilar distinctions? Work in psychology has called\nout many aspects of dogmatism that can be modeled\ncomputationally via natural language, such as over-\nconﬁdence and strong emotions (Rokeach, 1954).\nWe present a statistical model of dogmatism that\naddresses two complementary goals. First, we val-\nidate psychological theories by examining the pre-\ndictive power of feature sets that guide the model’s\npredictions. For example, do linguistic signals of\ncertainty help to predict a post is dogmatic, as the-\nory would suggest? Second, we apply our model to\nanswer four questions:\nR1: What kinds of topics (e.g., guns, LGBT) at-\ntract the highest levels of dogmatism?\nR2: How do dogmatic beliefs cluster?\nR3: How does dogmatism inﬂuence a conversa-\ntion on social media?\nR4: How do other user behaviors (e.g., frequency\nand breadth of posts) relate to dogmatism?\nWe train a predictive model to classify dogmatic\nposts from Reddit, one of the most popular discus-\nsion communities on the web.1Posts on Reddit cap-\nture discussion and debate across a diverse set of do-\nmains and topics – users talk about everything from\nclimate change and abortion, to world news and re-\nlationship advice, to the future of artiﬁcial intelli-\ngence. As a prerequisite to training our model, we\nhave created a corpus of 5,000 Reddit posts anno-\ntated with levels of dogmatism, which we are releas-\ning to share with other researchers.\n1http://www.reddit.comarXiv:1609.00425v1  [cs.CL]  1 Sep 2016']","Dogmatism can significantly impact a conversation on Reddit by limiting open dialogue and stifling diverse perspectives. Dogmatic individuals tend to hold onto their beliefs rigidly and are often unwilling to consider alternative viewpoints, which can lead to unproductive or even hostile exchanges. As for whether engaging with a dogmatic comment makes users more dogmatic themselves, it can depend on the individual. Some users may become more entrenched in their views when faced with dogmatism, while others may strive to remain open-minded. However, repeated exposure to dogmatic comments could potentially influence users to adopt a similar approach over time. Further research would be needed to definitively answer this question.",[''],Engaging with a dogmatic comment tends to make a user more dogmatic themselves.,0.44267305181416905,0.9806053884622343
53,"What is the effect of reducing training data on the performance of the BERT-based model, and how does it compare to other systems in terms of robustness and F1-score?","The effect of reducing training data on the performance of the BERT-based model is that it shows the highest robustness compared to other systems. When trained on only 1% of the dataset (230 instances), the BERT-based model experiences a 7-point F1-score loss, which is significantly lower than the 32 and 39 points lost by the CRF and spaCy models, respectively. Additionally, when trained on 230 instances instead of 21,371, the BERT-based model only loses 7 points of F1-score, showcasing its robustness to training-data scarcity.","['It shows\nthe difference in precision, recall, and F1-score with respect\nto that obtained using 100% of the training data. A general\ndownward trend can be observed, as one would expect: less\ntraining data leads to less accurate predictions. However,\nthe BERT-based model is the most robust to training-data\nreduction, showing an steadily low performance loss. With\n1% of the dataset (230 training instances), the BERT-based\nmodel only suffers a striking 7-point F1-score loss, in con-\ntrast to the 32 and 39 points lost by the CRF and spaCy\nmodels, respectively. This steep performance drop stems to'
 'be more dangerous than the unintended over-obfuscation of\nnon-sensitive text.\nFurther, we have conducted an additional experiment on\nthis dataset by progressively reducing the training data for\nall the compared systems. The BERT-based model shows\nthe highest robustness to training-data scarcity, loosing only\n7 points of F1-score when trained on 230 instances instead\nof 21,371. These observation are in line with the results ob-\ntained by the NLP community using BERT for other tasks.\nThe experiments with the MEDDOCAN 2019 shared task\ndataset follow the same pattern. In this case, the BERT-\nbased model falls 0.3 F1-score points behind the shared task\nwinning system, but it would have achieved the second po-\nsition in the competition with no further reﬁnement.\nSince we have used a pre-trained multilingual BERT model,\nthe same approach is likely to work for other languages just\nby providing some labelled training data. Further, this is the\nsimplest ﬁne-tuning that can be performed based on BERT.\nMore sophisticated ﬁne-tuning layers could help improve\nthe results. For example, it could be expected that a CRF\nlayer helped enforce better BIO tagging sequence predic-\ntions. Precisely, Mao and Liu (2019) participated in the\nMEDDOCAN competition using a BERT+CRF architec-\nture, but their reported scores are about 3 points lower than\nour implementation. From the description of their work, it\nis unclear what the source of this score difference could be.\nFurther, at the time of writing this paper, new multilingual\npre-trained models and Transformer architectures have be-\ncome available. It would not come as a surprise that these\nnew resources and systems –e.g., XLM-RoBERTa (Con-\nneau et al., 2019) or BETO (Wu and Dredze, 2019), a BERT\nmodel fully pre-trained on Spanish texts– further advanced\nthe state of the art in this task.\n6. Acknowledgements\nThis work has been supported by Vicomtech and partially\nfunded by the project DeepReading (RTI2018-096846-B-\nC21, MCIU/AEI/FEDER,UE).\n7. Bibliographical References\nAbouelmehdi, K., Beni-Hessane, A., and Khalouﬁ, H.\n(2018). Big healthcare data: preserving security and pri-\nvacy. Journal of Big Data , 5(1):1–18.\nAgerri, R., Bermudez, J., and Rigau, G. (2014). IXA\npipeline: Efﬁcient and Ready to Use Multilingual NLP\ntools. In Proceedings of the 9th Language Resources and\nEvaluation Conference (LREC 2014) , pages 3823–3828.\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary,\nV ., Wenzek, G., Guzm ´an, F., Grave, E., Ott, M.,\nZettlemoyer, L., and Stoyanov, V . (2019). Unsuper-\nvised Cross-lingual Representation Learning at Scale.\narXiv:1911.02116 .\nDernoncourt, F., Lee, J. Y ., Uzuner, ¨O., and Szolovits, P.\n(2016). De-identiﬁcation of Patient Notes with Recur-\nrent Neural Networks. Journal of the American Medical\nInformatics Association , 24(3):596–606.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K.\n(2019). BERT: Pre-training of Deep Bidirectional Trans-\nformers for Language Understanding. In Proceedings of\nthe 2019 Conference of the North American Chapter ofthe Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short Pa-\npers) , pages 4171–4186.\nGarc ´ıa-Sardi ˜na, L. (2018). Automating the anonymisa-\ntion of textual corpora. Master’s thesis, University of the\nBasque Country (UPV/EHU).\nHassan, F., Domingo-Ferrer, J., and Soria-Comas, J.\n(2018). Anonimizaci ´on de datos no estructurados a\ntrav´es del reconocimiento de entidades nominadas.']","Reducing the training data for a BERT-based model can negatively impact its performance. This is because BERT, like other deep learning models, relies on large amounts of data to learn effectively. With less data, the model may not be able to learn the necessary patterns to perform its task well, leading to lower accuracy, precision, recall, and F1-score.

In terms of robustness, BERT-based models are generally more robust than traditional machine learning models due to their ability to understand the context of words in a sentence. However, this robustness can be compromised with less training data.

When compared to other systems, BERT-based models usually outperform in terms of F1-score, given they have been trained on sufficient data. This is because BERT models are designed to understand the context of words in a sentence, which allows them to make more accurate predictions. However, if the training data is reduced, other simpler models might perform better as they require less data to learn effectively.",[''],"The BERT-based model shows the highest robustness to training-data scarcity, losing only 7 points of F1-score when trained on 230 instances instead of 21,371. These observations are in line with the results obtained by the NLP community using BERT for other tasks.",0.5725193861831201,0.7997964976684848
54,"What does the anisotropy of contextualized word representations in ELMo, BERT, and GPT-2 suggest about the range of context-specific representations for each word?","The anisotropy of contextualized word representations in ELMo, BERT, and GPT-2 suggests that there is a range of context-specific representations for each word. This anisotropy indicates that the contextualized representations are not uniformly distributed in all directions, but rather occupy a narrow cone in the vector space. The higher anisotropy in the upper layers of these models implies that these layers produce more context-specific representations, potentially assigning different context-specific meanings to words based on the context in which they appear.","['How Contextual are Contextualized Word Representations?\nComparing the Geometry of BERT, ELMo, and GPT-2 Embeddings\nKawin Ethayarajh∗\nStanford University\nkawin@stanford.edu\nAbstract\nReplacing static word embeddings with con-\ntextualized word representations has yielded\nsigniﬁcant improvements on many NLP tasks.\nHowever, just how contextual are the contex-\ntualized representations produced by models\nsuch as ELMo and BERT? Are there inﬁnitely\nmany context-speciﬁc representations for each\nword, or are words essentially assigned one of\na ﬁnite number of word-sense representations?\nFor one, we ﬁnd that the contextualized rep-\nresentations of all words are not isotropic in\nany layer of the contextualizing model. While\nrepresentations of the same word in differ-\nent contexts still have a greater cosine simi-\nlarity than those of two different words, this\nself-similarity is much lower in upper layers.\nThis suggests that upper layers of contextu-\nalizing models produce more context-speciﬁc\nrepresentations, much like how upper layers\nof LSTMs produce more task-speciﬁc repre-\nsentations. In all layers of ELMo, BERT, and\nGPT-2, on average, less than 5% of the vari-\nance in a word’s contextualized representa-\ntions can be explained by a static embedding\nfor that word, providing some justiﬁcation for\nthe success of contextualized representations.\n1 Introduction\nThe application of deep learning methods to NLP\nis made possible by representing words as vec-\ntors in a low-dimensional continuous space. Tradi-\ntionally, these word embeddings were static : each\nword had a single vector, regardless of context\n(Mikolov et al., 2013a; Pennington et al., 2014).\nThis posed several problems, most notably that\nall senses of a polysemous word had to share the\nsame representation. More recent work, namely\ndeep neural language models such as ELMo (Pe-\nters et al., 2018) and BERT (Devlin et al., 2018),\n∗Work partly done at the University of Toronto.have successfully created contextualized word rep-\nresentations , word vectors that are sensitive to\nthe context in which they appear. Replacing\nstatic embeddings with contextualized representa-\ntions has yielded signiﬁcant improvements on a di-\nverse array of NLP tasks, ranging from question-\nanswering to coreference resolution.\nThe success of contextualized word represen-\ntations suggests that despite being trained with\nonly a language modelling task, they learn highly\ntransferable and task-agnostic properties of lan-\nguage. In fact, linear probing models trained on\nfrozen contextualized representations can predict\nlinguistic properties of words (e.g., part-of-speech\ntags) almost as well as state-of-the-art models (Liu\net al., 2019a; Hewitt and Manning, 2019). Still,\nthese representations remain poorly understood.\nFor one, just how contextual are these contextu-\nalized word representations? Are there inﬁnitely\nmany context-speciﬁc representations that BERT\nand ELMo can assign to each word, or are words\nessentially assigned one of a ﬁnite number of\nword-sense representations?\nWe answer this question by studying the geom-\netry of the representation space for each layer of\nELMo, BERT, and GPT-2. Our analysis yields\nsome surprising ﬁndings:\n1. In all layers of all three models, the con-\ntextualized word representations of all words\nare not isotropic: they are not uniformly dis-\ntributed with respect to direction. Instead,\nthey are anisotropic , occupying a narrow\ncone in the vector space. The anisotropy in\nGPT-2’s last layer is so extreme that two ran-\ndom words will on average have almost per-\nfect cosine similarity! Given that isotropy\nhas both theoretical and empirical beneﬁts for\nstatic embeddings (Mu et al., 2018), the ex-\ntent of anisotropy in contextualized represen-arXiv:1909.00512v1  [cs.CL]  2 Sep 2019'
 'Figure 1: In almost all layers of BERT, ELMo, and GPT-2, the word representations are anisotropic (i.e., not\ndirectionally uniform): the average cosine similarity between uniformly randomly sampled words is non-zero.\nThe one exception is ELMo’s input layer; this is not surprising given that it generates character-level embeddings\nwithout using context. Representations in higher layers are generally more anisotropic than those in lower ones.\nELMo as well, though there are exceptions: for ex-\nample, the anisotropy in BERT’s penultimate layer\nis much higher than in its ﬁnal layer.\nIsotropy has both theoretical and empirical ben-\neﬁts for static word embeddings. In theory, it\nallows for stronger “self-normalization” during\ntraining (Arora et al., 2017), and in practice, sub-\ntracting the mean vector from static embeddings\nleads to improvements on several downstream\nNLP tasks (Mu et al., 2018). Thus the extreme\ndegree of anisotropy seen in contextualized word\nrepresentations – particularly in higher layers –\nis surprising. As seen in Figure 1, for all three\nmodels, the contextualized hidden layer represen-\ntations are almost all more anisotropic than the in-\nput layer representations, which do not incorpo-\nrate context. This suggests that high anisotropy is\ninherent to, or least a by-product of, the process of\ncontextualization.\n4.2 Context-Speciﬁcity\nContextualized word representations are more\ncontext-speciﬁc in higher layers. Recall from\nDeﬁnition 1 that the self-similarity of a word, in\na given layer of a given model, is the average co-\nsine similarity between its representations in dif-\nferent contexts, adjusted for anisotropy. If the\nself-similarity is 1, then the representations are\nnot context-speciﬁc at all; if the self-similarity is\n0, that the representations are maximally context-\nspeciﬁc. In Figure 2, we plot the average self-\nsimilarity of uniformly randomly sampled wordsin each layer of BERT, ELMo, and GPT-2. For\nexample, the self-similarity is 1.0 in ELMo’s in-\nput layer because representations in that layer are\nstatic character-level embeddings.\nIn all three models, the higher the layer, the\nlower the self-similarity is on average. In other\nwords, the higher the layer, the more context-\nspeciﬁc the contextualized representations. This\nﬁnding makes intuitive sense. In image classiﬁca-\ntion models, lower layers recognize more generic\nfeatures such as edges while upper layers recog-\nnize more class-speciﬁc features (Yosinski et al.,\n2014). Similarly, upper layers of LSTMs trained\non NLP tasks learn more task-speciﬁc represen-\ntations (Liu et al., 2019a). Therefore, it fol-\nlows that upper layers of neural language mod-\nels learn more context-speciﬁc representations, so\nas to predict the next word for a given context\nmore accurately. Of all three models, representa-\ntions in GPT-2 are the most context-speciﬁc, with\nthose in GPT-2’s last layer being almost maxi-\nmally context-speciﬁc.\nStopwords (e.g., ‘the’, ‘of’, ‘to’ ) have among the\nmost context-speciﬁc representations. Across\nall layers, stopwords have among the lowest self-\nsimilarity of all words, implying that their con-\ntextualized representations are among the most\ncontext-speciﬁc. For example, the words with the\nlowest average self-similarity across ELMo’s lay-\ners are ‘and’, ‘of’, ‘’s’, ‘the’ , and ‘to’. This is rel-\natively surprising, given that these words are not\npolysemous. This ﬁnding suggests that the variety']","The anisotropy of contextualized word representations in ELMo, BERT, and GPT-2 suggests that the range of context-specific representations for each word is not uniformly distributed. Instead, it is skewed towards certain directions in the embedding space. This means that these models do not use the entire embedding space equally for all words, but rather, certain regions of the space are used more frequently for specific words depending on their context.",[''],"The anisotropy of contextualized word representations in ELMo, BERT, and GPT-2 suggests that there are a finite number of context-specific representations for each word.",0.8777376867369084,0.5346851470859995
55,"What is the main challenge in zero-shot translation for transfer learning in Neural Machine Translation, considering the language space mismatch problem between the transferor and transferee models?","The main challenge in zero-shot translation for transfer learning in Neural Machine Translation, considering the language space mismatch problem between the transferor and transferee models, is that the source and pivot languages do not share the same feature distributions. This results in the child model inherited from the parent model failing to perform effectively in zero-shot translation scenarios.","['Cross-lingual Pre-training Based Transfer for Zero-shot Neural\nMachine Translation\nBaijun Ji‡, Zhirui Zhang§, Xiangyu Duan†‡∗, Min Zhang†‡, Boxing Chen§and Weihua Luo§\n†Institute of Artiﬁcial Intelligence, Soochow University, Suzhou, China\n‡School of Computer Science and Technology, Soochow University, Suzhou, China\n§Alibaba DAMO Academy, Hangzhou, China\n‡bjji@stu.suda.edu.cn†{xiangyuduan, minzhang}@suda.edu.cn\n§{zhirui.zzr,boxing.cbx,weihua.luowh}@alibaba-inc.com\nAbstract\nTransfer learning between different language pairs has shown\nits effectiveness for Neural Machine Translation (NMT) in\nlow-resource scenario. However, existing transfer methods\ninvolving a common target language are far from success in\nthe extreme scenario of zero-shot translation, due to the lan-\nguage space mismatch problem between transferor (the par-\nent model) and transferee (the child model) on the source\nside. To address this challenge, we propose an effective trans-\nfer learning approach based on cross-lingual pre-training. Our\nkey idea is to make all source languages share the same fea-\nture space and thus enable a smooth transition for zero-shot\ntranslation. To this end, we introduce one monolingual pre-\ntraining method and two bilingual pre-training methods to\nobtain a universal encoder for different languages. Once the\nuniversal encoder is constructed, the parent model built on\nsuch encoder is trained with large-scale annotated data and\nthen directly applied in zero-shot translation scenario. Exper-\niments on two public datasets show that our approach signif-\nicantly outperforms strong pivot-based baseline and various\nmultilingual NMT approaches.\nIntroduction\nAlthough Neural Machine Translation (NMT) has domi-\nnated recent research on translation tasks (Wu et al. 2016;\nVaswani et al. 2017; Hassan et al. 2018), NMT heavily relies\non large-scale parallel data, resulting in poor performance\non low-resource or zero-resource language pairs (Koehn\nand Knowles 2017). Translation between these low-resource\nlanguages (e.g., Arabic →Spanish) is usually accomplished\nwith pivoting through a rich-resource language (such as En-\nglish), i.e., Arabic (source) sentence is translated to En-\nglish (pivot) ﬁrst which is later translated to Spanish (tar-\nget) (Kauers et al. 2002; de Gispert and Mariño 2006).\nHowever, the pivot-based method requires doubled decoding\ntime and suffers from the propagation of translation errors.\nOne common alternative to avoid pivoting in NMT is\ntransfer learning (Zoph et al. 2016; Nguyen and Chiang\n2017; Kocmi and Bojar 2018; Kim et al. 2019) which lever-\nages a high-resource pivot →target model ( parent ) to ini-\n∗Corresponding Author.\nCopyright c⃝2020, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The circle and triangle dots represent source sen-\ntences in different language l1andl2, and the square dots\nmeans target sentences in language l3. A sample of transla-\ntion pairs is connected by the dashed line. We would like to\nforce each of the translation pairs has the same latent rep-\nresentation as the right part of the ﬁgure so as to transfer\nl1→l3model directly to l2→l3model.\ntialize a low-resource source →target model ( child ) that is\nfurther optimized with a small amount of available paral-\nlel data. Although this approach has achieved success in\nsome low-resource language pairs, it still performs very\npoorly in extremely low-resource or zero-resource transla-\ntion scenario. Speciﬁcally, Kocmi and Bojar (2018) reports\nthat without any child model training data, the performance\nof the parent model on the child test set is miserable.\nIn this work, we argue that the language space mis-\nmatch problem, also named domain shift problem (Fu et al.\n2015), brings about the zero-shot translation failure in trans-\nfer learning.'
 'tialize a low-resource source →target model ( child ) that is\nfurther optimized with a small amount of available paral-\nlel data. Although this approach has achieved success in\nsome low-resource language pairs, it still performs very\npoorly in extremely low-resource or zero-resource transla-\ntion scenario. Speciﬁcally, Kocmi and Bojar (2018) reports\nthat without any child model training data, the performance\nof the parent model on the child test set is miserable.\nIn this work, we argue that the language space mis-\nmatch problem, also named domain shift problem (Fu et al.\n2015), brings about the zero-shot translation failure in trans-\nfer learning. It is because transfer learning has no explicit\ntraining process to guarantee that the source and pivot lan-\nguages share the same feature distributions, causing that the\nchild model inherited from the parent model fails in such\na situation. For instance, as illustrated in the left of Fig-\nure 1, the points of the sentence pair with the same seman-\ntics are not overlapping in source space, resulting in that\nthe shared decoder will generate different translations de-\nnoted by different points in target space. Actually, transfer\nlearning for NMT can be viewed as a multi-domain problem\nwhere each source language forms a new domain. Minimiz-\ning the discrepancy between the feature distributions of dif-\nferent source languages, i.e., different domains, will ensure\nthe smooth transition between the parent and child models,arXiv:1912.01214v1  [cs.CL]  3 Dec 2019']","The main challenge in zero-shot translation for transfer learning in Neural Machine Translation, considering the language space mismatch problem, is aligning the source and target language spaces of the transferor and transferee models. This is because the transferor model may have been trained on a different set of languages, leading to a mismatch in the language space. This makes it difficult for the transferee model to accurately translate between languages it has not been directly trained on.",[''],The main challenge in zero-shot translation for transfer learning in Neural Machine Translation is the language space mismatch problem between the transferor and transferee models.,0.5440097035216553,0.6152697209688032
56,What is the impact of original relation names on matching longer question contexts in relation detection for KBQA?,Breaking the relation names into word sequences for question-relation matching helps in matching longer question contexts in relation detection for KBQA.,"['58.0%) for the\nconstraint detection module to improve.\nFinally, like STAGG, which uses multiple rela-\ntion detectors (see Yih et al. (2015) for the three\nmodels used), we also try to use the top-3 rela-\ntion detectors from Section 6.2. As shown on the\nlast row of Table 3, this gives a signiﬁcant perfor-\nmance boost, resulting in a new state-of-the-art re-\nsult on SimpleQuestions and a result comparable\nto the state-of-the-art on WebQSP.\n7 Conclusion\nKB relation detection is a key step in KBQA and\nis signiﬁcantly different from general relation ex-\ntraction tasks. We propose a novel KB relation\ndetection model, HR-BiLSTM, that performs hier-\narchical matching between questions and KB rela-\ntions. Our model outperforms the previous meth-\nods on KB relation detection tasks and allows our\nKBQA system to achieve state-of-the-arts. For fu-\nture work, we will investigate the integration of\nour HR-BiLSTM into end-to-end systems. For ex-\nample, our model could be integrated into the de-\ncoder in (Liang et al., 2016), to provide better se-\nquence prediction. We will also investigate new\nemerging datasets like GraphQuestions (Su et al.,\n2016) and ComplexQuestions (Bao et al., 2016) to\nhandle more characteristics of general QA.\n12Note that another reason is that we are evaluating on ac-\ncuracy here. When evaluating on F1 the gap will be smaller.'
 'Improved Neural Relation Detection for Knowledge Base Question\nAnswering\nMo Yu†Wenpeng Yin⋆Kazi Saidul Hasan‡Cicero dos Santos†\nBing Xiang‡Bowen Zhou†\n†AI Foundations, IBM Research, USA\n⋆Center for Information and Language Processing, LMU Munich\n‡IBM Watson, USA\n{yum,kshasan,cicerons,bingxia,zhou }@us.ibm.com, wenpeng@cis.lmu.de\nAbstract\nRelation detection is a core component of\nmany NLP applications including Knowl-\nedge Base Question Answering (KBQA).\nIn this paper, we propose a hierarchi-\ncal recurrent neural network enhanced by\nresidual learning which detects KB re-\nlations given an input question. Our\nmethod uses deep residual bidirectional\nLSTMs to compare questions and rela-\ntion names via different levels of abstrac-\ntion. Additionally, we propose a sim-\nple KBQA system that integrates entity\nlinking and our proposed relation detec-\ntor to make the two components enhance\neach other. Our experimental results show\nthat our approach not only achieves out-\nstanding relation detection performance,\nbut more importantly, it helps our KBQA\nsystem achieve state-of-the-art accuracy\nfor both single-relation (SimpleQuestions)\nand multi-relation (WebQSP) QA bench-\nmarks.\n1 Introduction\nKnowledge Base Question Answering (KBQA)\nsystems answer questions by obtaining informa-\ntion from KB tuples (Berant et al., 2013; Yao et al.,\n2014; Bordes et al., 2015; Bast and Haussmann,\n2015; Yih et al., 2015; Xu et al., 2016). For an\ninput question, these systems typically generate a\nKB query, which can be executed to retrieve the\nanswers from a KB. Figure 1 illustrates the process\nused to parse two sample questions in a KBQA\nsystem: (a) a single-relation question, which can\nbe answered with a single <head-entity, relation,\ntail-entity>KB tuple (Fader et al., 2013; Yih et al.,\n2014; Bordes et al., 2015); and (b) a more complex\ncase, where some constraints need to be handledfor multiple entities in the question. The KBQA\nsystem in the ﬁgure performs two key tasks: (1)\nentity linking , which links n-grams in questions\nto KB entities, and (2) relation detection , which\nidentiﬁes the KB relation(s) a question refers to.\nThe main focus of this work is to improve the\nrelation detection subtask and further explore how\nit can contribute to the KBQA system. Although\ngeneral relation detection1methods are well stud-\nied in the NLP community, such studies usually\ndo not take the end task of KBQA into considera-\ntion. As a result, there is a signiﬁcant gap between\ngeneral relation detection studies and KB-speciﬁc\nrelation detection. First, in most general relation\ndetection tasks, the number of target relations is\nlimited, normally smaller than 100. In contrast, in\nKBQA even a small KB, like Freebase2M (Bor-\ndes et al., 2015), contains more than 6,000 relation\ntypes. Second, relation detection for KBQA often\nbecomes a zero-shot learning task, since some test\ninstances may have unseen relations in the training\ndata. For example, the SimpleQuestions (Bordes\net al., 2015) data set has 14% of the golden test\nrelations not observed in golden training tuples.\nThird, as shown in Figure 1(b), for some KBQA\ntasks like WebQuestions (Berant et al., 2013), we\nneed to predict a chain of relations instead of a\nsingle relation. This increases the number of tar-\nget relation types and the sizes of candidate rela-\ntion pools, further increasing the difﬁculty of KB\nrelation detection. Owing to these reasons, KB re-\nlation detection is signiﬁcantly more challenging\ncompared to general relation detection tasks.\nThis paper improves KB relation detection to\ncope with the problems mentioned above. First, in\norder to deal with the unseen relations, we propose\nto break the relation names into word sequences\nfor question-relation matching. Second, noticing\n1In the information extraction ﬁeld such tasks are usually\ncalled relation extraction orrelation classiﬁcation .arXiv:1704.06194v2  [cs.CL]  27 May 2017']","The original relation names can significantly impact the matching of longer question contexts in relation detection for Knowledge Base Question Answering (KBQA). If the original relation names are too complex or not well-aligned with the natural language used in the question, it can make it difficult to accurately match and detect the correct relations. This can lead to incorrect or less accurate answers. Therefore, it's important to ensure that the original relation names are clear, concise, and closely aligned with the language and context of the questions.",[''],The impact of original relation names on matching longer question contexts in relation detection for KBQA is proposed to build both relation-level and word-level relation representations.,0.44900994298981645,0.7324004075493273
57,"What potential advantages do static embeddings offer over contextualized word representations, considering their isotropy and ease of deployment?","Static embeddings offer advantages over contextualized word representations in terms of isotropy and ease of deployment. Isotropic static embeddings, achieved by making them more uniformly distributed in direction, have shown significant improvements in performance on downstream tasks. Additionally, static representations are much easier to deploy compared to contextualized ones, which can be challenging due to memory and run-time constraints in production environments.","['tations, such as those from Layer 1 of BERT, are\nmuch more effective.\n5 Future Work\nOur ﬁndings offer some new directions for future\nwork. For one, as noted earlier in the paper, Mu\net al. (2018) found that making static embeddings\nmore isotropic – by subtracting their mean from\neach embedding – leads to surprisingly large im-\nprovements in performance on downstream tasks.\nGiven that isotropy has beneﬁts for static embed-\ndings, it may also have beneﬁts for contextual-\nized word representations, although the latter have\nalready yielded signiﬁcant improvements despite\nbeing highly anisotropic. Therefore, adding an\nanisotropy penalty to the language modelling ob-\njective – to encourage the contextualized represen-\ntations to be more isotropic – may yield even better\nresults.\nAnother direction for future work is generat-\ning static word representations from contextual-\nized ones. While the latter offer superior per-\nformance, there are often challenges to deploying\nlarge models such as BERT in production, both\nwith respect to memory and run-time. In contrast,\nstatic representations are much easier to deploy.\nOur work in section 4.3 suggests that not only it is\npossible to extract static representations from con-\ntextualizing models, but that these extracted vec-\ntors often perform much better on a diverse array\nof tasks compared to traditional static embeddings\nsuch as GloVe and FastText. This may be a means\nof extracting some use from contextualizing mod-\nels without incurring the full cost of using them in\nproduction.\n6 Conclusion\nIn this paper, we investigated how contextual con-\ntextualized word representations truly are. For\none, we found that upper layers of ELMo, BERT,\nand GPT-2 produce more context-speciﬁc rep-\nresentations than lower layers. This increased\ncontext-speciﬁcity is always accompanied by in-\ncreased anisotropy. However, context-speciﬁcity\nalso manifests differently across the three models;\nthe anisotropy-adjusted similarity between words\nin the same sentence is highest in ELMo but al-\nmost non-existent in GPT-2. We ultimately found\nthat after adjusting for anisotropy, on average, less\nthan 5% of the variance in a word’s contextual-\nized representations could be explained by a staticembedding. This means that even in the best-case\nscenario, in all layers of all models, static word\nembeddings would be a poor replacement for con-\ntextualized ones. These insights help explain some\nof the remarkable success that contextualized rep-\nresentations have had on a diverse array of NLP\ntasks.\nAcknowledgments\nWe thank the anonymous reviewers for their in-\nsightful comments. We thank the Natural Sciences\nand Engineering Research Council of Canada\n(NSERC) for their ﬁnancial support.\nReferences\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel M\nCer, Mona T Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Inigo Lopez-Gazpio, Montse Maritxalar, Rada\nMihalcea, et al. 2015. Semeval-2015 task 2: Seman-\ntic textual similarity, English, Spanish and pilot on\ninterpretability. In Proceedings SemEval@ NAACL-\nHLT . pages 252–263.\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel M\nCer, Mona T Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Rada Mihalcea, German Rigau, and Janyce\nWiebe. 2014. Semeval-2014 task 10: Multilin-\ngual semantic textual similarity. In Proceedings Se-\nmEval@ COLING . pages 81–91.\nEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-\nAgirre, and Weiwei Guo. 2013. Sem 2013 shared\ntask: Semantic textual similarity, including a pilot\non typed-similarity. In SEM 2013: The Second Joint\nConference on Lexical and Computational Seman-\ntics. Association for Computational Linguistics.\nEneko Agirre, Mona Diab, Daniel Cer, and Aitor\nGonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-\nlot on semantic textual similarity.'
 'How Contextual are Contextualized Word Representations?\nComparing the Geometry of BERT, ELMo, and GPT-2 Embeddings\nKawin Ethayarajh∗\nStanford University\nkawin@stanford.edu\nAbstract\nReplacing static word embeddings with con-\ntextualized word representations has yielded\nsigniﬁcant improvements on many NLP tasks.\nHowever, just how contextual are the contex-\ntualized representations produced by models\nsuch as ELMo and BERT? Are there inﬁnitely\nmany context-speciﬁc representations for each\nword, or are words essentially assigned one of\na ﬁnite number of word-sense representations?\nFor one, we ﬁnd that the contextualized rep-\nresentations of all words are not isotropic in\nany layer of the contextualizing model. While\nrepresentations of the same word in differ-\nent contexts still have a greater cosine simi-\nlarity than those of two different words, this\nself-similarity is much lower in upper layers.\nThis suggests that upper layers of contextu-\nalizing models produce more context-speciﬁc\nrepresentations, much like how upper layers\nof LSTMs produce more task-speciﬁc repre-\nsentations. In all layers of ELMo, BERT, and\nGPT-2, on average, less than 5% of the vari-\nance in a word’s contextualized representa-\ntions can be explained by a static embedding\nfor that word, providing some justiﬁcation for\nthe success of contextualized representations.\n1 Introduction\nThe application of deep learning methods to NLP\nis made possible by representing words as vec-\ntors in a low-dimensional continuous space. Tradi-\ntionally, these word embeddings were static : each\nword had a single vector, regardless of context\n(Mikolov et al., 2013a; Pennington et al., 2014).\nThis posed several problems, most notably that\nall senses of a polysemous word had to share the\nsame representation. More recent work, namely\ndeep neural language models such as ELMo (Pe-\nters et al., 2018) and BERT (Devlin et al., 2018),\n∗Work partly done at the University of Toronto.have successfully created contextualized word rep-\nresentations , word vectors that are sensitive to\nthe context in which they appear. Replacing\nstatic embeddings with contextualized representa-\ntions has yielded signiﬁcant improvements on a di-\nverse array of NLP tasks, ranging from question-\nanswering to coreference resolution.\nThe success of contextualized word represen-\ntations suggests that despite being trained with\nonly a language modelling task, they learn highly\ntransferable and task-agnostic properties of lan-\nguage. In fact, linear probing models trained on\nfrozen contextualized representations can predict\nlinguistic properties of words (e.g., part-of-speech\ntags) almost as well as state-of-the-art models (Liu\net al., 2019a; Hewitt and Manning, 2019). Still,\nthese representations remain poorly understood.\nFor one, just how contextual are these contextu-\nalized word representations? Are there inﬁnitely\nmany context-speciﬁc representations that BERT\nand ELMo can assign to each word, or are words\nessentially assigned one of a ﬁnite number of\nword-sense representations?\nWe answer this question by studying the geom-\netry of the representation space for each layer of\nELMo, BERT, and GPT-2. Our analysis yields\nsome surprising ﬁndings:\n1. In all layers of all three models, the con-\ntextualized word representations of all words\nare not isotropic: they are not uniformly dis-\ntributed with respect to direction. Instead,\nthey are anisotropic , occupying a narrow\ncone in the vector space. The anisotropy in\nGPT-2’s last layer is so extreme that two ran-\ndom words will on average have almost per-\nfect cosine similarity! Given that isotropy\nhas both theoretical and empirical beneﬁts for\nstatic embeddings (Mu et al., 2018), the ex-\ntent of anisotropy in contextualized represen-arXiv:1909.00512v1  [cs.CL]  2 Sep 2019']","1. Simplicity: Static embeddings are simpler to understand and implement compared to contextualized word representations. They are easier to use and deploy in machine learning models.

2. Efficiency: Static embeddings are computationally less expensive as they do not require complex models like transformers to generate the embeddings. This makes them faster and more efficient to use, especially in cases where computational resources are limited.

3. Isotropy: Static embeddings are isotropic, meaning they distribute the semantic meanings of words evenly in the vector space. This can be advantageous in certain applications where the direction of the vector is not as important as the distance between vectors.

4. Stability: Static embeddings provide stable word representations as they do not change based on the context. This can be beneficial in scenarios where the context is not clear or not available.

5. Transferability: Static embeddings can be easily transferred and used across different tasks and domains, unlike contextualized word representations that are often task-specific. 

6. Interpretability: Static embeddings can be more interpretable than contextualized embeddings as each dimension does not depend on the context. This can be useful in applications where interpretability is important.",[''],Static embeddings offer advantages in terms of isotropy and ease of deployment compared to contextualized word representations.,0.41845500663701907,0.6137077741579202
58,"Which system, among the rule-based baseline, CRF, spaCy, and BERT, performed the best in detecting sensitive information in the NUB ES-PHI dataset?",BERT performed the best in detecting sensitive information in the NUB ES-PHI dataset.,"['Detection Classiﬁcation (relaxed) Classiﬁcation (strict)\nPrec Rec F1 Prec Rec F1 Prec Rec F1\nbaseline 0.853 0.469 0.605 0.779 0.429 0.553 0.721 0.396 0.512\nCRF 0.968 0.921 0.944 0.952 0.907 0.929 0.941 0.896 0.918\nspaCy 0.964 0.938 0.951 0.942 0.852 0.895 0.942 0.852 0.895\nBERT 0.952 0.979 0.965 0.938 0.963 0.950 0.925 0.950 0.937\nTable 5: Results of Experiment A: NUB ES-PHI\nclassiﬁer, the spaCy entity tagger, and BERT. The evalua-\ntion comprises three scenarios of increasing difﬁculty:\nDetection - Evaluates the performance of the systems at\npredicting whether each token is sensitive or non-\nsensitive; that is, the measurements only take into ac-\ncount whether a sensitive token has been recognised\nor not, regardless of the BIO label and the category as-\nsigned. This scenario shows how good a system would\nbe at obfuscating sensitive data (e.g., by replacing sen-\nsitive tokens with asterisks).\nClassiﬁcation (relaxed) - We measure the performance of\nthe systems at predicting the sensitive information\ntype of each token –i.e., the 11 categories presented\nin Section 3.1.1. or ‘out’. Detecting entity types cor-\nrectly is important if a system is going to be used to\nreplace sensitive data by fake data of the same type\n(e.g., random people names).\nClassiﬁcation (strict) - This is the strictest evaluation, as\nit takes into account both the BIO label and the cate-\ngory assigned to each individual token. Being able to\ndiscern between two contiguous sensitive entities of\nthe same type is relevant not only because it is help-\nful when producing fake replacements, but because it\nalso yields more accurate statistics of the sensitive in-\nformation present in a given document collection.\nThe systems are evaluated in terms of micro-average preci-\nsion, recall and F1-score in all the scenarios.\nIn addition to the scenarios proposed, a subject worth be-\ning studied is the need of labelled data. Manually labelled\ndata is an scarce and expensive resource, which for some\napplication domains or languages is difﬁcult to come by.\nIn order to obtain an estimation of the dependency of each\nsystem on the available amount of training data, we have re-\ntrained all the compared models using decreasing amounts\nof data –from 100% of the available training instances to\njust 1%. The same data subsets have been used to train\nall the systems. Due to the knowledge transferred from\nthe pre-trained BERT model, the BERT-based model is ex-\npected to be more robust to data scarcity than those that\nstart their training from scratch.\n3.3.2. Experiment B: MEDDOCAN\nIn this experiment set, our BERT implementation is com-\npared to several systems that participated in the MEDDO-\nCAN challenge: a CRF classiﬁer (Perez et al., 2019), a\nspaCy entity recogniser (Perez et al., 2019), and NLNDE\n(Lange et al., 2019), the winner of the shared task and cur-\nrent state of the art for sensitive information detection andclassiﬁcation in Spanish clinical text. Speciﬁcally, we in-\nclude the results of a domain-independent NLNDE model\n(S2), and the results of a model enriched with domain-\nspeciﬁc embeddings (S3). Finally, we include the results\nobtained by Mao and Liu (2019) with a CRF output layer\non top of BERT embeddings. MEDDOCAN consists of\ntwo scenarios:\nDetection - This evaluation measures how good a system\nis at detecting sensitive text spans, regardless of the\ncategory assigned to them.\nClassiﬁcation - In this scenario, systems are required to\nmatch exactly not only the boundaries of each sensi-\ntive span, but also the category assigned.\nThe systems are evaluated in terms of micro-averaged pre-\ncision, recall and F-1 score.'
 '0 50 1000.40.60.81\ntrain data (%)precision\n0 50 1000.40.60.81\ntrain data (%)recall\n0 50 1000.40.60.81\ntrain data (%)F1-score\nCRF spaCy BERT\nFigure 2: Performance decay with decreasing amounts of training data on the sensitive information detection task in the\nNUB ES-PHI corpus\nDetection Classiﬁcation\nPrec Rec F1 Prec Rec F1\nCRF (Perez et al., 2019) 0.977 0.943 0.960 0.971 0.937 0.954\nspaCy (Perez et al., 2019) 0.967 0.953 0.965 0.965 0.947 0.956\nNLND S2 (Lange et al., 2019) 0.976 0.973 0.974 0.971 0.968 0.970\nNLND S3 (Lange et al., 2019) 0.975 0.975 0.975 0.970 0.969 0.970\nBERT + CRF (Mao and Liu, 2019) 0.968 0.919 0.943 0.965 0.912 0.937\nBERT 0.973 0.972 0.972 0.968 0.967 0.967\nTable 8: Results of Experiment B: MEDDOCAN\na larger extent from recall decline, which is not that marked\nin the case of BERT. Overall, these results indicate that the\ntransfer-learning achieved through the BERT multilingual\npre-trained model not only helps obtain better results, but\nalso lowers the need of manually labelled data for this ap-\nplication domain.\n4.2. Experiment B: MEDDOCAN\nThe results of the two MEDDOCAN scenarios –detection\nand classiﬁcation– are shown in Table 8. These results fol-\nlow the same pattern as in the previous experiments, with\nthe CRF classiﬁer being the most precise of all, and BERT\noutperforming both the CRF and spaCy classiﬁers thanks to\nits greater recall. We also show the results of Mao and Liu\n(2019) who, despite of having used a BERT-based system,\nachieve lower scores than our models. The reason why it\nshould be so remain unclear.\nWith regard to the winner of the MEDDOCAN shared\ntask, the BERT-based model has not improved the scores\nobtained by neither the domain-dependent (S3) nor the\ndomain-independent (S2) NLNDE model. However, at-\ntending to the obtained results, BERT remains only 0.3 F1-\nscore points behind, and would have achieved the second\nposition among all the MEDDOCAN shared task competi-\ntors. Taking into account that only 3% of the gold labels\nremain incorrectly annotated, the task can be considered al-\nmost solved, and it is not clear if the differences among the\nsystems are actually signiﬁcant, or whether they stem from\nminor variations in initialisation or a long-tail of minor la-belling inconsistencies.\n5. Conclusions and Future Work\nIn this work we have brieﬂy introduced the problems re-\nlated to data privacy protection in clinical domain. We have\nalso described some of the groundbreaking advances on the\nNatural Language Processing ﬁeld due to the appearance of\nTransformers-based deep-learning architectures and trans-\nfer learning from very large general-domain multilingual\ncorpora, focusing our attention in one of its most represen-\ntative examples, Google’s BERT model.\nIn order to assess the performance of BERT for Span-\nish clinical data anonymisation, we have conducted sev-\neral experiments with a BERT-based sequence labelling\napproach using the pre-trained multilingual BERT model\nshared by Google as the starting point for the model train-\ning. We have compared this BERT-based sequence la-\nbelling against other methods and systems. One of the ex-\nperiments uses the MEDDOCAN 2019 shared task dataset,\nwhile the other uses a novel Spanish clinical reports dataset\ncalled NUB ES-PHI.\nThe results of the experiments show that, in NUB ES-PHI,\nthe BERT-based model outperforms the other systems with-\nout requiring any adaptation or domain-speciﬁc feature en-\ngineering, just by being trained on the provided labelled\ndata.']",The text does not provide information on which system performed the best in detecting sensitive information in the NUB ES-PHI dataset.,[''],BERT performed the best in detecting sensitive information in the NUB ES-PHI dataset.,0.2181089347719984,1.0
59,"How does the increased context-specificity in upper layers of ELMo, BERT, and GPT-2 affect the similarity between words in the same sentence?","In ELMo, as context-specificity increases in upper layers, words in the same sentence become more similar to each other. In BERT, with increased context-specificity in upper layers, words in the same sentence become more dissimilar to each other but are still more similar than randomly chosen words on average. In GPT-2, despite increased context-specificity in upper layers, words in the same sentence are not more similar to each other than randomly sampled words.","['Figure 2: The average cosine similarity between representations of the same word in different contexts is called\nthe word’s self-similarity (see Deﬁnition 1). Above, we plot the average self-similarity of uniformly randomly\nsampled words after adjusting for anisotropy (see section 3.4). In all three models, the higher the layer, the lower\nthe self-similarity, suggesting that contextualized word representations are more context-speciﬁc in higher layers.\nof contexts a word appears in, rather than its inher-\nent polysemy, is what drives variation in its con-\ntextualized representations. This answers one of\nthe questions we posed in the introduction: ELMo,\nBERT, and GPT-2 are not simply assigning one of\na ﬁnite number of word-sense representations to\neach word; otherwise, there would not be so much\nvariation in the representations of words with so\nfew word senses.\nContext-speciﬁcity manifests very differently in\nELMo, BERT, and GPT-2. As noted earlier,\ncontextualized representations are more context-\nspeciﬁc in upper layers of ELMo, BERT, and GPT-\n2. However, how does this increased context-\nspeciﬁcity manifest in the vector space? Do word\nrepresentations in the same sentence converge to a\nsingle point, or do they remain distinct from one\nanother while still being distinct from their repre-\nsentations in other contexts? To answer this ques-\ntion, we can measure a sentence’s intra-sentence\nsimilarity. Recall from Deﬁnition 2 that the intra-\nsentence similarity of a sentence, in a given layer\nof a given model, is the average cosine similarity\nbetween each of its word representations and their\nmean, adjusted for anisotropy. In Figure 3, we plot\nthe average intra-sentence similarity of 500 uni-\nformly randomly sampled sentences.\nIn ELMo, words in the same sentence are more\nsimilar to one another in upper layers. As\nword representations in a sentence become more\ncontext-speciﬁc in upper layers, the intra-sentencesimilarity also rises. This suggests that, in prac-\ntice, ELMo ends up extending the intuition behind\nFirth’s (1957) distributional hypothesis to the sen-\ntence level: that because words in the same sen-\ntence share the same context, their contextualized\nrepresentations should also be similar.\nIn BERT, words in the same sentence are more\ndissimilar to one another in upper layers. As\nword representations in a sentence become more\ncontext-speciﬁc in upper layers, they drift away\nfrom one another, although there are exceptions\n(see layer 12 in Figure 3). However, in all lay-\ners, the average similarity between words in the\nsame sentence is still greater than the average sim-\nilarity between randomly chosen words (i.e., the\nanisotropy baseline). This suggests a more nu-\nanced contextualization than in ELMo, with BERT\nrecognizing that although the surrounding sen-\ntence informs a word’s meaning, two words in the\nsame sentence do not necessarily have a similar\nmeaning because they share the same context.\nIn GPT-2, word representations in the same\nsentence are no more similar to each other than\nrandomly sampled words. On average, the un-\nadjusted intra-sentence similarity is roughly the\nsame as the anisotropic baseline, so as seen in Fig-\nure 3, the anisotropy-adjusted intra-sentence simi-\nlarity is close to 0 in most layers of GPT-2. In fact,\nthe intra-sentence similarity is highest in the input\nlayer, which does not contextualize words at all.\nThis is in contrast to ELMo and BERT, where the'
 'tations is surprising.\n2. Occurrences of the same word in different\ncontexts have non-identical vector represen-\ntations. Where vector similarity is deﬁned\nas cosine similarity, these representations are\nmore dissimilar to each other in upper lay-\ners. This suggests that, much like how upper\nlayers of LSTMs produce more task-speciﬁc\nrepresentations (Liu et al., 2019a), upper lay-\ners of contextualizing models produce more\ncontext-speciﬁc representations.\n3. Context-speciﬁcity manifests very differently\nin ELMo, BERT, and GPT-2. In ELMo,\nrepresentations of words in the same sen-\ntence grow more similar to each other as\ncontext-speciﬁcity increases in upper layers;\nin BERT, they become more dissimilar to\neach other in upper layers but are still more\nsimilar than randomly sampled words are on\naverage; in GPT-2, however, words in the\nsame sentence are no more similar to each\nother than two randomly chosen words.\n4. After adjusting for the effect of anisotropy,\non average, less than 5% of the variance in a\nword’s contextualized representations can be\nexplained by their ﬁrst principal component.\nThis holds across all layers of all models.\nThis suggests that contextualized representa-\ntions do not correspond to a ﬁnite number\nof word-sense representations, and even in\nthe best possible scenario, static embeddings\nwould be a poor replacement for contextual-\nized ones. Still, static embeddings created\nby taking the ﬁrst principal component of\na word’s contextualized representations out-\nperform GloVe and FastText embeddings on\nmany word vector benchmarks.\nThese insights help justify why the use of contex-\ntualized representations has led to such signiﬁcant\nimprovements on many NLP tasks.\n2 Related Work\nStatic Word Embeddings Skip-gram with neg-\native sampling (SGNS) (Mikolov et al., 2013a)\nand GloVe (Pennington et al., 2014) are among\nthe best known models for generating static word\nembeddings. Though they learn embeddings itera-\ntively in practice, it has been proven that in theory,they both implicitly factorize a word-context ma-\ntrix containing a co-occurrence statistic (Levy and\nGoldberg, 2014a,b). Because they create a single\nrepresentation for each word, a notable problem\nwith static word embeddings is that all senses of a\npolysemous word must share a single vector.\nContextualized Word Representations Given\nthe limitations of static word embeddings, recent\nwork has tried to create context-sensitive word\nrepresentations. ELMo (Peters et al., 2018), BERT\n(Devlin et al., 2018), and GPT-2 (Radford et al.,\n2019) are deep neural language models that are\nﬁne-tuned to create models for a wide range of\ndownstream NLP tasks. Their internal representa-\ntions of words are called contextualized word rep-\nresentations because they are a function of the en-\ntire input sentence. The success of this approach\nsuggests that these representations capture highly\ntransferable and task-agnostic properties of lan-\nguage (Liu et al., 2019a).\nELMo creates contextualized representations of\neach token by concatenating the internal states of\na 2-layer biLSTM trained on a bidirectional lan-\nguage modelling task (Peters et al., 2018). In\ncontrast, BERT and GPT-2 are bi-directional and\nuni-directional transformer-based language mod-\nels respectively. Each transformer layer of 12-\nlayer BERT (base, cased) and 12-layer GPT-2 cre-\nates a contextualized representation of each token\nby attending to different parts of the input sentence\n(Devlin et al., 2018; Radford et al., 2019). BERT\n– and subsequent iterations on BERT (Liu et al.,\n2019b; Yang et al., 2019) – have achieved state-of-\nthe-art performance on various downstream NLP\ntasks, ranging from question-answering to senti-\nment analysis.\nProbing Tasks Prior analysis of contextualized\nword representations has largely been restricted\nto probing tasks (Tenney et al., 2019; Hewitt and\nManning, 2019).']","The increased context-specificity in upper layers of ELMo, BERT, and GPT-2 makes the representations of words in the same sentence more distinct from each other. This is because these models capture more complex aspects of word meaning, including how the meaning of a word can change based on its context. Therefore, even if two words in the same sentence are similar, their representations can be quite different if they are used in different contexts within the sentence.",[''],"In ELMo, words in the same sentence are more similar to one another in upper layers. In BERT, words in the same sentence are more dissimilar to one another in upper layers. In GPT-2, word representations in the same sentence are no more similar to each other than randomly sampled words.",0.6672068119177035,0.7397599948436765
60,"What is the dataset used for Armenian named entity recognition, and how was it generated using Wikipedia and Wikidata?","The dataset used for Armenian named entity recognition consists of a 163,000-token named entity corpus automatically generated and annotated from Wikipedia, as well as a 53,400-token corpus of news sentences manually annotated with person, organization, and location named entities. The dataset was generated by assigning named entity classes to Wikipedia articles based on their content, such as classifying articles about individuals as ""PER"" (person), articles about organizations as ""ORG"" (organization), and articles about locations as ""LOC."" This approach leverages the information available in Wikipedia to create annotated data for training named entity recognition models.","['arXiv:1810.08699v1  [cs.CL]  19 Oct 2018pioNER: Datasets and Baselines for Armenian\nNamed Entity Recognition\nTsolak Ghukasyan1, Garnik Davtyan2, Karen Avetisyan3, Ivan Andrianov4\nIvannikov Laboratory for System Programming at Russian-Ar menian University1,2,3, Yerevan, Armenia\nIvannikov Institute for System Programming of the Russian A cademy of Sciences4, Moscow, Russia\nEmail: {1tsggukasyan,2garnik.davtyan,3karavet,4ivan.andrianov}@ispras.ru\nAbstract —In this work, we tackle the problem of Armenian\nnamed entity recognition, providing silver- and gold-stan dard\ndatasets as well as establishing baseline results on popula r mod-\nels. We present a 163000-token named entity corpus automati cally\ngenerated and annotated from Wikipedia, and another 53400-\ntoken corpus of news sentences with manual annotation of peo ple,\norganization and location named entities. The corpora were used\nto train and evaluate several popular named entity recognit ion\nmodels. Alongside the datasets, we release 50-, 100-, 200-, 300-\ndimensional GloVe word embeddings trained on a collection o f\nArmenian texts from Wikipedia, news, blogs, and encycloped ia.\nIndex Terms —machine learning, deep learning, natural lan-\nguage processing, named entity recognition, word embeddin gs\nI. I NTRODUCTION\nNamed entity recognition is an important task of natural\nlanguage processing, featuring in many popular text proces sing\ntoolkits. This area of natural language processing has been\nactively studied in the latest decades and the advent of deep\nlearning reinvigorated the research on more effective and\naccurate models. However, most of existing approaches requ ire\nlarge annotated corpora. To the best of our knowledge, no suc h\nwork has been done for the Armenian language, and in this\nwork we address several problems, including the creation of a\ncorpus for training machine learning models, the developme nt\nof gold-standard test corpus and evaluation of the effectiv eness\nof established approaches for the Armenian language.\nConsidering the cost of creating manually annotated named\nentity corpus, we focused on alternative approaches. Lack\nof named entity corpora is a common problem for many\nlanguages, thus bringing the attention of many researchers\naround the globe. Projection based transfer schemes have be en\nshown to be very effective (e.g. [1], [2], [3]), using resour ce-\nrich language’s corpora to generate annotated data for the l ow-\nresource language. In this approach, the annotations of hig h-\nresource language are projected over the corresponding tok ens\nof the parallel low-resource language’s texts. This strate gy\ncan be applied for language pairs that have parallel corpora .\nHowever, that approach would not work for Armenian as we\ndid not have access to sufﬁciently large parallel corpus wit h a\nresource-rich language.\nAnother popular approach is using Wikipedia. Klesti\nHoxha and Artur Baxhaku employ gazetteers extracted from\nWikipedia to generate an annotated corpus for Albanian [4],\nand Weber and Pötzl propose a rule-based system for Germanthat leverages the information from Wikipedia [5]. However ,\nthe latter relies on external tools such as part-of-speech t ag-\ngers, making it nonviable for the Armenian language.\nNothman et al. generated a silver-standard corpus for 9\nlanguages by extracting Wikipedia article texts with outgo ing\nlinks and turning those links into named entity annotations\nbased on the target article’s type [6]. Sysoev and Andrianov\nused a similar approach for the Russian language [7] [8].\nBased on its success for a wide range of languages, our choice\nfell on this model to tackle automated data generation and\nannotation for the Armenian language.\nAside from the lack of training data, we also address the\nabsence of a benchmark dataset of Armenian texts for named\nentity recognition. We propose a gold-standard corpus with\nmanual annotation of CoNLL named entity categories: person ,\nlocation, and organization [9] [10], hoping it will be used t o\nevaluate future named entity recognition models.\nFurthermore, popular entity recognition models were ap-\nplied to the mentioned data in order to obtain baseline resul ts\nfor future research in the area. Along with the datasets, we\ndeveloped GloVe [11] word embeddings to train and evaluate\nthe deep learning models in our experiments.'
 'Sysoev and Andrianov\nused a similar approach for the Russian language [7] [8].\nBased on its success for a wide range of languages, our choice\nfell on this model to tackle automated data generation and\nannotation for the Armenian language.\nAside from the lack of training data, we also address the\nabsence of a benchmark dataset of Armenian texts for named\nentity recognition. We propose a gold-standard corpus with\nmanual annotation of CoNLL named entity categories: person ,\nlocation, and organization [9] [10], hoping it will be used t o\nevaluate future named entity recognition models.\nFurthermore, popular entity recognition models were ap-\nplied to the mentioned data in order to obtain baseline resul ts\nfor future research in the area. Along with the datasets, we\ndeveloped GloVe [11] word embeddings to train and evaluate\nthe deep learning models in our experiments.\nThe contributions of this work are (i) the silver-standard\ntraining corpus, (ii) the gold-standard test corpus, (iii) GloVe\nword embeddings, (iv) baseline results for 3 different mode ls\non the proposed benchmark data set. All aforementioned\nresources are available on GitHub1.\nII. A UTOMATED TRAINING CORPUS GENERATION\nWe used Sysoev and Andrianov’s modiﬁcation of the\nNothman et al. approach to automatically generate data for\ntraining a named entity recognizer. This approach uses link s\nbetween Wikipedia articles to generate sequences of named-\nentity annotated tokens.\nA. Dataset extraction\nThe main steps of the dataset extraction system are de-\nscribed in Figure 1.\nFirst, each Wikipedia article is assigned a named entity\nclass (e.g. the article /Armke/armini/armmen /Armke/armayb/armsha/armke/armayb/armsha/armhi/armayb/armnu (Kim Kashkashian) is\nclassiﬁed as PER (person), /Armayb/armza/armgim/armyech/armre/armini /armlyun/armini/armgim/armayb (League of Nations)\nasORG (organization), /Armse/armini/armre/armini/armayb (Syria) as LOC etc). One of the\n1https://github.com/ispras-texterra/pioner\n© 2018 IEEE. Personal use of this material is permitted. Perm ission from IEEE must be obtained for all other uses, in any cu rrent or future media, including\nreprinting/republishing this material for advertising or promotional purposes, creating new collective works, for r esale or redistribution to servers or lists, or\nreuse of any copyrighted component of this work in other work s.']","The dataset used for Armenian named entity recognition is the ""Armenian Named Entity Recognition Dataset"". It was generated using Wikipedia and Wikidata by extracting Armenian text from these platforms and then applying named entity recognition techniques to identify and classify named entities. The entities were categorized into various classes such as person, location, organization, etc. The dataset was then annotated manually to ensure accuracy.",[''],"The dataset used for Armenian named entity recognition was generated using Wikipedia and Wikidata. The algorithm used links between Wikipedia articles to generate sequences of named-entity annotated tokens. The articles were classified based on their Wikidata instance of and subclass of attributes. The generated dataset consists of 7,455 annotated sentences with 163,247 tokens.",0.7392172194986771,0.6909620314593569
61,"What role do grapheme-phoneme mappings play in language processing, considering the various complexities and variations in writing systems?","Grapheme-phoneme mappings play a crucial role in language processing by establishing the relationship between the written form of a language (graphemes) and its spoken form (phonemes). These mappings help in converting the orthographic representation of words into their phonemic representation, allowing for accurate pronunciation. Despite the complexities and variations in writing systems where graphemes may not always directly correspond to phonemes, understanding these mappings is essential for tasks like speech recognition, text-to-speech systems, and language learning. The variations such as multiple graphemes representing a single phoneme, silent letters, or one grapheme representing multiple phonemes add layers of complexity to language processing but are necessary to ensure accurate communication and comprehension.","['But despite\nbeing glottographic, in few writing systems do\ngraphemes correspond one-to-one with phonemes.\nThere are cases in which multiple graphemes rep-\nresent a single phoneme, as in the word thein En-\nglish:\nth e\nD @\nThere are cases in which a single grapheme rep-\nresents multiple phonemes, such as syllabaries, in\nwhich each symbol represents a syllable.\nIn many languages, there are silent letters, as in\nthe word hora in Spanish:\nh o r a\n-o R a\nThere are more complicated correspondences,\nsuch as the silent ein English that affects the pro-\nnunciation of the previous vowel, as seen in the\npair of words cape andcap.\nIt is possible for an orthographic system to have\nany or all of the above phenomena while remain-\ning unambiguous. However, some orthographic\n1The Chinese script, in which characters have both phono-\nlogical form and semantic meaning, is the best-known excep-\ntion.'
 'with the output of the wFST-based Phonetisaurus\nsystem (Novak et al., 2016) did better than either\nsystem alone.\nA different approach came from Kim and Sny-\nder (2012), who used supervised learning with\nan undirected graphical model to induce the\ngrapheme–phoneme mappings for languages writ-\nten in the Latin alphabet. Given a short text in\na language, the model predicts the language’s or-\nthographic rules. To create phonemic context fea-\ntures from the short text, the model na ¨ıvely maps\ngraphemes to IPA symbols written with the same\ncharacter, and uses the features of these symbols\nto learn an approximation of the phonotactic con-\nstraints of the language. In their experiments,\nthese phonotactic features proved to be more valu-\nable than geographical and genetic features drawn\nfrom WALS (Dryer and Haspelmath, 2013).\n2.2 Multilingual Neural NLP\nIn recent years, neural networks have emerged as\na common way to use data from several languages\nin a single system. Google’s zero-shot neural ma-\nchine translation system (Johnson et al., 2016)\nshares an encoder and decoder across all language\npairs. In order to facilitate this multi-way transla-\ntion, they prepend an artiﬁcial token to the begin-\nning of each source sentence at both training and\ntranslation time. The token identiﬁes what lan-\nguage the sentence should be translated to. This\napproach has three beneﬁts: it is far more efﬁcient\nthan building a separate model for each language\npair; it allows for translation between languages\nthat share no parallel data; and it improves re-\nsults on low-resource languages by allowing them\nto implicitly share parameters with high-resource\nlanguages. Our g2p system is inspired by this ap-\nproach, although it differs in that there is only one\ntarget “language”, IPA, and the artiﬁcial tokens\nidentify the language of the source instead of the\nlanguage of the target.\nOther work has also made use of multilingually-\ntrained neural networks. Phoneme-level polyglot\nlanguage models (Tsvetkov et al., 2016) train a\nsingle model on multiple languages and addition-\nally condition on externally constructed typolog-\nical data about the language. ¨Ostling and Tiede-\nmann (2017) used a similar approach, in which\na character-level neural language model is trained\non a massively multilingual corpus. A language\nembedding vector is concatenated to the input ateach time step. The language embeddings their\nsystem learned correlate closely to the genetic re-\nlationships between languages. However, neither\nof these models was applied to g2p.\n3 Grapheme-to-Phoneme\ng2p is the problem of converting the orthographic\nrepresentation of a word into a phonemic repre-\nsentation. A phoneme is an abstract unit of sound\nwhich may have different realizations in different\ncontexts. For example, the English phoneme /p/\nhas two phonetic realizations (or allophones):\n•[ph], as in the word ‘pain’ [pheI n]\n•[p], as in the word ‘Spain’ [s p eI n]\nEnglish speakers without linguistic training of-\nten struggle to perceive any difference between\nthese sounds. Writing systems usually do not dis-\ntinguish between allophones: [ph]and[p]are both\nwritten as⟨p⟩in English. The sounds are written\ndifferently in languages where they contrast, such\nas Hindi and Eastern Armenian.\nMost writing systems in use today are glot-\ntographic, meaning that their symbols encode\nsolely phonological information1. But despite\nbeing glottographic, in few writing systems do\ngraphemes correspond one-to-one with phonemes.\nThere are cases in which multiple graphemes rep-\nresent a single phoneme, as in the word thein En-\nglish:\nth e\nD @\nThere are cases in which a single grapheme rep-\nresents multiple phonemes, such as syllabaries, in\nwhich each symbol represents a syllable.\nIn many languages, there are silent letters, as in\nthe word hora in Spanish:\nh o r a\n-o R a\nThere are more complicated correspondences,\nsuch as the silent ein English that affects the pro-\nnunciation of the previous vowel, as seen in the\npair of words cape andcap.\nIt is possible for an orthographic system to have\nany or all of the above phenomena while remain-\ning unambiguous.']","Grapheme-phoneme mappings play a crucial role in language processing as they form the basis of reading and writing. They involve the relationship between the written symbols (graphemes) and the sounds they represent (phonemes). This mapping is essential for understanding and interpreting written language.

In terms of complexities and variations in writing systems, the role of grapheme-phoneme mappings becomes even more significant. Different languages have different writing systems, each with its own set of graphemes and phonemes. For instance, alphabetic languages like English have a relatively direct mapping between graphemes and phonemes, although there are exceptions due to irregular spellings. In contrast, logographic languages like Chinese have a more complex mapping system where a single character can represent a whole word or concept.

Moreover, some languages have a deep orthography, where a single grapheme can represent multiple phonemes, and a single phoneme can be represented by multiple graphemes. This adds another layer of complexity to the grapheme-phoneme mapping process.

Overall, grapheme-phoneme mappings are fundamental to language processing, enabling individuals to decode written language into spoken language and vice versa. They also contribute to the richness and diversity of global languages, reflecting the complexities and variations in different writing systems.",[''],"Grapheme-phoneme mappings play a crucial role in language processing as they help convert the orthographic representation of a word into its phonemic representation. This is important because writing systems often do not have a one-to-one correspondence between graphemes and phonemes. There can be cases where multiple graphemes represent a single phoneme or a single grapheme represents multiple phonemes. Additionally, there can be silent letters or more complicated correspondences. Understanding these mappings is essential for tasks like speech recognition, text-to-speech synthesis, and natural language processing.",0.6169466203840575,0.568030001973824
62,Why is pre-ordering the assisting language crucial for multilingual NMT in a low-resource setting to address word-order divergence?,"Pre-ordering the assisting language is crucial for multilingual NMT in a low-resource setting to address word-order divergence because it significantly improves translation quality by bridging the word order gap between the assisting language and the source language. This adjustment helps the model learn the correct word order patterns during training, leading to major improvements in translation accuracy, especially in scenarios where there is little to no parallel corpus available for the target language pair.","['Addressing Word-order Divergence in Multilingual Neural Machine\nTranslation for extremely Low Resource Languages\nRudra Murthy V†, Anoop Kunchukuttan‡, Pushpak Bhattacharyya†\n†Center for Indian Language Technology (CFILT)\nDepartment of Computer Science and Engineering\nIIT Bombay, India.\n‡Microsoft AI & Research, Hyderabad, India.\n{rudra,pb }@cse.iitb.ac.in, ankunchu@microsoft.com\nAbstract\nTransfer learning approaches for Neural Ma-\nchine Translation (NMT) trains a NMT model\non an assisting language-target language pair\n(parent model) which is later ﬁne-tuned for\nthe source language-target language pair of in-\nterest (child model), with the target language\nbeing the same. In many cases, the assist-\ning language has a different word order from\nthe source language. We show that divergent\nword order adversely limits the beneﬁts from\ntransfer learning when little to no parallel cor-\npus between the source and target language is\navailable. To bridge this divergence, we pro-\npose to pre-order the assisting language sen-\ntences to match the word order of the source\nlanguage and train the parent model. Our ex-\nperiments on many language pairs show that\nbridging the word order gap leads to major\nimprovements in the translation quality in ex-\ntremely low-resource scenarios.\n1 Introduction\nTransfer learning for multilingual Neural Machine\nTranslation (NMT) (Zoph et al., 2016; Dabre et al.,\n2017; Nguyen and Chiang, 2017) attempts to im-\nprove the NMT performance on the source to\ntarget language pair (child task) using an assist-\ning source language (assisting to target language\ntranslation is the parent task). Here, the parent\nmodel is trained on the assisting and target lan-\nguage parallel corpus and the trained weights are\nused to initialize the child model. If source-target\nlanguage pair parallel corpus is available, the child\nmodel can further be ﬁne-tuned. The weight ini-\ntialization reduces the requirement on the training\ndata for the source-target language pair by trans-\nferring knowledge from the parent task, thereby\nimproving the performance on the child task.\nHowever, the divergence between the source\nand the assisting language can adversely impactthe beneﬁts obtained from transfer learning. Mul-\ntiple studies have shown that transfer learning\nworks best when the languages are related (Zoph\net al., 2016; Nguyen and Chiang, 2017; Dabre\net al., 2017). Zoph et al. (2016) studied the in-\nﬂuence of language divergence between languages\nchosen for training the parent and the child model,\nand showed that choosing similar languages for\ntraining the parent and the child model leads to\nbetter improvements from transfer learning.\nSeveral studies have tried to address the lex-\nical divergence between the source and the tar-\nget languages either by using Byte Pair Encoding\n(BPE) as basic input representation units (Nguyen\nand Chiang, 2017) or character-level NMT sys-\ntem (Lee et al., 2017) or bilingual embeddings\n(Gu et al., 2018). However, the effect of word\norder divergence and its mitigation has not been\nexplored. In a practical setting, it is not uncom-\nmon to have source and assisting languages with\ndifferent word order. For instance, it is possible to\nﬁnd parallel corpora between English (SVO word\norder) and some Indian (SOV word order) lan-\nguages, but very little parallel corpora between In-\ndian languages. Hence, it is natural to use English\nas an assisting language for inter-Indian language\ntranslation.\nTo address the word order divergence, we pro-\npose to pre-order the assisting language sentences\n(SVO) to match the word order of the source lan-\nguage (SOV). We consider an extremely resource-\nconstrained scenario, where there is no parallel\ncorpus for the child task. From our experiments,\nwe show that there is a signiﬁcant increase in the\ntranslation accuracy for the unseen source-target\nlanguage pair.arXiv:1811.00383v2  [cs.CL]  10 Apr 2019'
 'The\nnon pre-ordering model is able to forget the word-\norder of English and learn the word order of Indian\nlanguages. We attribute this behavior of the non\npre-ordered model to the phenomenon of catas-\ntrophic forgetting (McCloskey and Cohen, 1989;\nFrench, 1999) which enables the model to learn\nthe word-order of the source language when sufﬁ-\ncient child task parallel corpus is available.\nWe also compare the performance of the ﬁne-\ntuned model with the model trained only on the\navailable source-target parallel corpus with ran-\ndomly initialized weights (No Transfer Learning).\nTransfer learning, with and without pre-ordering,\nis better compared to training only on the small\nsource-target parallel corpus.\n6 Conclusion\nIn this paper, we show that handling word-order\ndivergence between the source and assisting lan-\nguages is crucial for the success of multilingual\nNMT in an extremely low-resource setting. We\nshow that pre-ordering the assisting language to\nmatch the word order of the source language sig-\nniﬁcantly improves translation quality in an ex-\ntremely low-resource setting. If pre-ordering is\nnot possible, ﬁne-tuning on a small source-target']","Pre-ordering the assisting language is crucial for multilingual NMT in a low-resource setting to address word-order divergence because it helps in aligning the source and target languages more effectively. Word order can significantly vary between different languages. By pre-ordering the assisting language, the model can better learn the syntactic and semantic patterns of the target language. This is particularly important in a low-resource setting where there is limited data available for training. The pre-ordering process can help to maximize the effectiveness of the available data and improve the overall translation quality.",[''],"Pre-ordering the assisting language is crucial for multilingual NMT in a low-resource setting to address word-order divergence because it ensures that the contextual representations generated by the encoder for the source and assisting languages are consistent, leading to consistent translations and effective transfer of knowledge from the assisting language to the source language.",0.4298580760064594,0.45928883400137477
63,What additional alignment methods are used in the proposed cross-lingual pre-training approach to ensure word-level representation alignment between different languages?,The proposed cross-lingual pre-training approach introduces BRidge Language Modeling (BRLM) as an additional alignment method to ensure word-level representation alignment between different languages.,"['2 Related Work\nTo the best of our knowledge, no work has ad-\ndressed word order divergence in transfer learn-\ning for multilingual NMT. However, some work\nexists for other NLP tasks in a multilingual set-\nting. For Named Entity Recognition (NER), Xie\net al. (2018) use a self-attention layer after the\nBi-LSTM layer to address word-order divergence\nfor Named Entity Recognition (NER) task. The\napproach does not show any signiﬁcant improve-\nments, possibly because the divergence has to be\naddressed before/during construction of the con-\ntextual embeddings in the Bi-LSTM layer. Joty\net al. (2017) use adversarial training for cross-\nlingual question-question similarity ranking. The\nadversarial training tries to force the sentence rep-\nresentation generated by the encoder of similar\nsentences from different input languages to have\nsimilar representations.\nPre-ordering the source language sentences to\nmatch the target language word order has been\nfound useful in addressing word-order divergence\nfor Phrase-Based SMT (Collins et al., 2005; Ra-\nmanathan et al., 2008; Navratil et al., 2012; Chat-\nterjee et al., 2014). For NMT, Ponti et al. (2018)\nand Kawara et al. (2018) have explored pre-\nordering. Ponti et al. (2018) demonstrated that\nby reducing the syntactic divergence between the\nsource and the target languages, consistent im-\nprovements in NMT performance can be obtained.\nOn the contrary, Kawara et al. (2018) reported\ndrop in NMT performance due to pre-ordering.\nNote that these works address source-target diver-\ngence, not divergence between source languages\nin multilingual NMT scenario.\n3 Proposed Solution\nConsider the task of translating for an extremely\nlow-resource language pair. The parallel corpus\nbetween the two languages, if available may be\ntoo small to train an NMT model. Similar to Zoph\net al. (2016), we use transfer learning to over-\ncome data sparsity between the source and the\ntarget languages. We choose English as the as-\nsisting language in all our experiments. In our\nresource-scarce scenario, we have no parallel cor-\npus for training the child model. Hence, at test\ntime, the source language sentence is translated\nusing the parent model after performing a word-\nby-word translation from source to the assisting\nlanguage using a bilingual dictionary.Before Reordering After Reordering\nS\nNP0 VP\nV NP 1S\nNP0 VP\nNP1V\nS\nNP\nNNP\nAnuragVP\nMD\nwillVP\nVB\nmeetNP\nNNP\nThakurS\nNP\nNNP\nAnuragVP\nNP\nNNP\nThakurVP\nMD\nwillVP\nVB\nmeet\nTable 1: Example showing transitive verb before and\nafter reordering (Adapted from Chatterjee et al. (2014))\nSince the source language and the assisting lan-\nguage (English) have different word order, we hy-\npothesize that it leads to inconsistencies in the\ncontextual representations generated by the en-\ncoder for the two languages. Speciﬁcally, given an\nEnglish sentence (SVO word order) and its transla-\ntion in the source language (SOV word order), the\nencoder representations for words in the two sen-\ntences will be different due to different contexts\nof synonymous words. This could lead to the at-\ntention and the decoder layers generating different\ntranslations from the same (parallel) sentence in\nthe source or assisting language. This is undesir-\nable as we want the knowledge to be transferred\nfrom the parent model (assisting source →target)\nto the child model (source →target).\nIn this paper, we propose to pre-order English\nsentences (assisting language sentences) to match\nthe source language word-order and train the par-\nent model on the pre-ordered corpus. Table 1\nshows one of the pre-ordering rules (Ramanathan\net al., 2008) used along with an example sentence\nillustrating the effect of pre-ordering. This will en-\nsure that context of words in the parallel source\nand assisting language sentences are similar, lead-\ning to consistent contextual representations across\nthe source languages.'
 'Figure 2: The overview of BRidge Language Modeling (BRLM). The BRLM extends MLM (Lample and Conneau 2019) to\npairs of parallel sentences and leverages explicit alignment information obtained by external aligner tool or additional attention\nlayer to encourage word representation alignment across different languages.\n•Directly translate source sentences into target sentences\nwith the parent model, which beneﬁts from the availabil-\nity of the universal encoder.\nThe key difﬁculty of this method is to ensure the intermedi-\nate representations of the universal encoder are language in-\nvariant. In the rest of this section, we ﬁrst present two exist-\ning methods yet to be explored in zero-shot translation, and\nthen propose a straightforward but effective cross-lingual\npre-training method. In the end, we present the whole train-\ning and inference protocol for transfer.\nMasked and Translation Language Model\nPretraining\nTwo existing cross-lingual pre-training methods, Masked\nLanguage Modeling (MLM) and Translation Language\nModeling (TLM), have shown their effectiveness on XNLI\ncross-lingual classiﬁcation task (Lample and Conneau 2019;\nHuang et al. 2019), but these methods have not been well\nstudied on cross-lingual generation tasks in zero-shot condi-\ntion. We attempt to take advantage of the cross-lingual abil-\nity of the two methods for zero-shot translation.\nSpeciﬁcally, MLM adopts the Cloze objective of\nBERT (Devlin et al. 2018) and predicts the masked words\nthat are randomly selected and replaced with [MASK] to-\nken on monolingual corpus. In practice, MLM takes dif-\nferent language monolingual corpora as input to ﬁnd fea-\ntures shared across different languages. With this method,\nword pieces shared in all languages have been mapped into\na shared space, which makes the sentence representationsacross different languages close (Pires, Schlinger, and Gar-\nrette 2019).\nSince MLM objective is unsupervised and only requires\nmonolingual data, TLM is designed to leverage parallel data\nwhen it is available. Actually, TLM is a simple extension of\nMLM, with the difference that TLM concatenates sentence\npair into a whole sentence, and then randomly masks words\nin both the source and target sentences. In this way, the\nmodel can either attend to surrounding words or to the trans-\nlation sentence, implicitly encouraging the model to align\nthe source and target language representations. Note that al-\nthough each sentence pair is formed into one sentence, the\npositions of the target sentence are reset to count form zero.\nBridge Language Model Pretraining\nAside from MLM and TLM, we propose BRidge Language\nModeling (BRLM) to further obtain word-level representa-\ntion alignment between different languages. This method is\ninspired by the assumption that if the feature spaces of dif-\nferent languages are aligned very well, the masked words\nin the corrupted sentence can also be guessed by the con-\ntext of the correspondingly aligned words on the other side.\nTo achieve this goal, BRLM is designed to strengthen the\nability to infer words across languages based on alignment\ninformation, instead of inferring words within monolingual\nsentence as in MLM or within the pseudo sentence formed\nby concatenating sentence pair as in TLM.\nAs illustrated in Figure 2, BRLM stacks shared encoder\nover both side sentences separately. In particular, we design\ntwo network structures for BRLM, which are divided into']",The text does not provide information on what additional alignment methods are used in the proposed cross-lingual pre-training approach to ensure word-level representation alignment between different languages.,[''],"The additional alignment methods used in the proposed cross-lingual pre-training approach are Hard Alignment (BRLM-HA) and Soft Alignment (BRLM-SA). BRLM-HA uses an external aligner tool on source ↔ pivot parallel data to extract alignment information, while BRLM-SA introduces an additional attention layer to learn the alignment information during model training.",0.21495132035970682,0.5301117643763236
