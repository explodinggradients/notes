{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare and Select Generative Models\n",
    "\n",
    "In this tutorial, you will learn how to use the Ragas to score and evaluate different generative AI models on a specific evaluation task. Then visualize and compare the evaluation results for the task to select a generative model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Install Ragas and Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ragas plotly langchain-google-vertexai -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import HTML, Markdown, display\n",
    "\n",
    "\n",
    "def display_eval_report(eval_result, metrics=None):\n",
    "    \"\"\"Display the evaluation results.\"\"\"\n",
    "\n",
    "    title, summary_metrics, report_df = eval_result\n",
    "    metrics_df = pd.DataFrame.from_dict(summary_metrics, orient=\"index\").T\n",
    "    if metrics:\n",
    "        metrics_df = metrics_df.filter(\n",
    "            [\n",
    "                metric\n",
    "                for metric in metrics_df.columns\n",
    "                if any(selected_metric in metric for selected_metric in metrics)\n",
    "            ]\n",
    "        )\n",
    "        report_df = report_df.filter(\n",
    "            [\n",
    "                metric\n",
    "                for metric in report_df.columns\n",
    "                if any(selected_metric in metric for selected_metric in metrics)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Display the title with Markdown for emphasis\n",
    "    display(Markdown(f\"## {title}\"))\n",
    "\n",
    "    # Display the metrics DataFrame\n",
    "    display(Markdown(\"### Summary Metrics\"))\n",
    "    display(metrics_df)\n",
    "\n",
    "    # Display the detailed report DataFrame\n",
    "    display(Markdown(\"### Report Metrics\"))\n",
    "    display(report_df)\n",
    "\n",
    "\n",
    "def plot_radar_plot(eval_results, max_score=5, metrics=None):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for eval_result in eval_results:\n",
    "        title, summary_metrics, report_df = eval_result\n",
    "\n",
    "        if metrics:\n",
    "            summary_metrics = {\n",
    "                k: summary_metrics[k]\n",
    "                for k, v in summary_metrics.items()\n",
    "                if any(selected_metric in k for selected_metric in metrics)\n",
    "            }\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatterpolar(\n",
    "                r=list(summary_metrics.values()),\n",
    "                theta=list(summary_metrics.keys()),\n",
    "                fill=\"toself\",\n",
    "                name=title,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        polar=dict(radialaxis=dict(visible=True, range=[0, max_score])), showlegend=True\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_bar_plot(eval_results, metrics=None):\n",
    "    fig = go.Figure()\n",
    "    data = []\n",
    "\n",
    "    for eval_result in eval_results:\n",
    "        title, summary_metrics, _ = eval_result\n",
    "        if metrics:\n",
    "            summary_metrics = {\n",
    "                k: summary_metrics[k]\n",
    "                for k, v in summary_metrics.items()\n",
    "                if any(selected_metric in k for selected_metric in metrics)\n",
    "            }\n",
    "\n",
    "        data.append(\n",
    "            go.Bar(\n",
    "                x=list(summary_metrics.keys()),\n",
    "                y=list(summary_metrics.values()),\n",
    "                name=title,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig = go.Figure(data=data)\n",
    "\n",
    "    # Change the bar mode\n",
    "    fig.update_layout(barmode=\"group\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare and Select Generative Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"OpenAI is one of the most recognized names in the large language model space, known for its GPT series of models. These models excel at generating human-like text and performing tasks like creative writing, answering questions, and summarizing content. GPT-4, their latest release, has set benchmarks in understanding context and delivering detailed responses.\",\n",
    "    \"Anthropic is well-known for its Claude series of language models, designed with a strong focus on safety and ethical AI behavior. Claude is particularly praised for its ability to follow complex instructions and generate text that aligns closely with user intent.\",\n",
    "    \"DeepMind, a division of Google, is recognized for its cutting-edge Gemini models, which are integrated into various Google products like Bard and Workspace tools. These models are renowned for their conversational abilities and their capacity to handle complex, multi-turn dialogues.\",\n",
    "    \"Meta AI is best known for its LLaMA (Large Language Model Meta AI) series, which has been made open-source for researchers and developers. LLaMA models are praised for their ability to support innovation and experimentation due to their accessibility and strong performance.\",\n",
    "    \"Meta AI with it's LLaMA models aims to democratize AI development by making high-quality models available for free, fostering collaboration across industries. Their open-source approach has been a game-changer for researchers without access to expensive resources.\",\n",
    "    \"Microsoft’s Azure AI platform is famous for integrating OpenAI’s GPT models, enabling businesses to use these advanced models in a scalable and secure cloud environment. Azure AI powers applications like Copilot in Office 365, helping users draft emails, generate summaries, and more.\",\n",
    "    \"Amazon’s Bedrock platform is recognized for providing access to various language models, including its own models and third-party ones like Anthropic’s Claude and AI21’s Jurassic. Bedrock is especially valued for its flexibility, allowing users to choose models based on their specific needs.\",\n",
    "    \"Cohere is well-known for its language models tailored for business use, excelling in tasks like search, summarization, and customer support. Their models are recognized for being efficient, cost-effective, and easy to integrate into workflows.\",\n",
    "    \"AI21 Labs is famous for its Jurassic series of language models, which are highly versatile and capable of handling tasks like content creation and code generation. The Jurassic models stand out for their natural language understanding and ability to generate detailed and coherent responses.\",\n",
    "    \"In the rapidly advancing field of artificial intelligence, several companies have made significant contributions with their large language models. Notable players include OpenAI, known for its GPT Series (including GPT-4); Anthropic, which offers the Claude Series; Google DeepMind with its Gemini Models; Meta AI, recognized for its LLaMA Series; Microsoft Azure AI, which integrates OpenAI’s GPT Models; Amazon AWS (Bedrock), providing access to various models including Claude (Anthropic) and Jurassic (AI21 Labs); Cohere, which offers its own models tailored for business use; and AI21 Labs, known for its Jurassic Series. These companies are shaping the landscape of AI by providing powerful models with diverse capabilities.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "langchain_documents = []\n",
    "\n",
    "for content in dataset:\n",
    "    langchain_documents.append(\n",
    "        Document(\n",
    "            page_content=content,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "\n",
    "embeddings = VertexAIEmbeddings(model_name=\"text-embedding-004\")\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "_ = vector_store.add_documents(langchain_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAI\n",
    "\n",
    "gemini_model_15 = VertexAI(model_name=\"gemini-1.5-pro\")\n",
    "gemini_model_1 = VertexAI(model_name=\"gemini-1.0-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain_gemini_15 = prompt | gemini_model_15 | StrOutputParser()\n",
    "qa_chain_gemini_1 = prompt | gemini_model_1 | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(relevant_docs):\n",
    "    return \"\\n\".join(doc.page_content for doc in relevant_docs)\n",
    "\n",
    "\n",
    "query = \"What makes Meta AI’s LLaMA models stand out?\"\n",
    "\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "gemini_15_response = qa_chain_gemini_15.invoke({\"context\": format_docs(relevant_docs), \"query\": query})\n",
    "\n",
    "gemini_1_response = qa_chain_gemini_1.invoke({\"context\": format_docs(relevant_docs), \"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inputs = questions = [\n",
    "    \"Who are the major players in the large language model space?\",\n",
    "    \"What is Microsoft’s Azure AI platform known for?\",\n",
    "    \"What kind of models does Cohere provide?\",\n",
    "]\n",
    "\n",
    "references = [\n",
    "    \"The major players include OpenAI (GPT Series), Anthropic (Claude Series), Google DeepMind (Gemini Models), Meta AI (LLaMA Series), Microsoft Azure AI (integrating GPT Models), Amazon AWS (Bedrock with Claude and Jurassic), Cohere (business-focused models), and AI21 Labs (Jurassic Series).\",\n",
    "    \"Microsoft’s Azure AI platform is known for integrating OpenAI’s GPT models, enabling businesses to use these models in a scalable and secure cloud environment.\",\n",
    "    \"Cohere provides language models tailored for business use, excelling in tasks like search, summarization, and customer support.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_a = []\n",
    "responses_b = []\n",
    "retrieved_contexts = []\n",
    "\n",
    "for i in range(len(questions)):\n",
    "\trelevant_docs = retriever.invoke(questions[i])\n",
    "\n",
    "\tgemini_15_response = qa_chain_gemini_15.invoke({\"context\": format_docs(relevant_docs), \"query\": questions[i]})\n",
    "\tresponses_a.append(gemini_15_response)\n",
    "\n",
    "\tgemini_1_response = qa_chain_gemini_1.invoke({\"context\": format_docs(relevant_docs), \"query\": questions[i]})\n",
    "\tresponses_b.append(gemini_1_response)\n",
    "\n",
    "\tretrieved_contexts.append([doc.page_content for doc in relevant_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample, EvaluationDataset\n",
    "\n",
    "n = len(user_inputs)\n",
    "\n",
    "samples_a = []\n",
    "samples_b = []\n",
    "\n",
    "for i in range(n):\n",
    "    sample_a = SingleTurnSample(\n",
    "        user_input=user_inputs[i],\n",
    "        retrieved_contexts=retrieved_contexts[i],\n",
    "        response=responses_a[i],\n",
    "        reference=references[i],\n",
    "    )\n",
    "    sample_b = SingleTurnSample(\n",
    "        user_input=user_inputs[i],\n",
    "        retrieved_contexts=retrieved_contexts[i],\n",
    "        response=responses_b[i],\n",
    "        reference=references[i],\n",
    "    )\n",
    "\n",
    "    samples_a.append(sample_a)\n",
    "    samples_b.append(sample_b)\n",
    "\n",
    "ragas_eval_dataset_a = EvaluationDataset(samples=samples_a)\n",
    "ragas_eval_dataset_b = EvaluationDataset(samples=samples_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_eval_dataset_a.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_eval_dataset_b.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define evaluator_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "from langchain_google_vertexai import VertexAI, VertexAIEmbeddings\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Ragas metrics\n",
    "\n",
    "Ragas Supports Multitude of computation based, model based metrics, you can check them [here](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/) and it provides support for easily writing your own metrics more information on that can be found [here](https://docs.ragas.io/en/latest/howtos/customizations/metrics/_write_your_own_metric/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import ContextPrecision, Faithfulness, RubricsScore, RougeScore\n",
    "\n",
    "rouge_score = RougeScore()\n",
    "\n",
    "helpfulness_rubrics = {\n",
    "    \"score1_description\": \"Response is useless/irrelevant, contains inaccurate/deceptive/misleading information, and/or contains harmful/offensive content. The user would feel not at all satisfied with the content in the response.\",\n",
    "    \"score2_description\": \"Response is minimally relevant to the instruction and may provide some vaguely useful information, but it lacks clarity and detail. It might contain minor inaccuracies. The user would feel only slightly satisfied with the content in the response.\",\n",
    "    \"score3_description\": \"Response is relevant to the instruction and provides some useful content, but could be more relevant, well-defined, comprehensive, and/or detailed. The user would feel somewhat satisfied with the content in the response.\",\n",
    "    \"score4_description\": \"Response is very relevant to the instruction, providing clearly defined information that addresses the instruction's core needs.  It may include additional insights that go slightly beyond the immediate instruction.  The user would feel quite satisfied with the content in the response.\",\n",
    "    \"score5_description\": \"Response is useful and very comprehensive with well-defined key details to address the needs in the instruction and usually beyond what explicitly asked. The user would feel very satisfied with the content in the response.\",\n",
    "}\n",
    "\n",
    "rubrics_score = RubricsScore(name=\"helpfulness\", rubrics=helpfulness_rubrics)\n",
    "context_precision = ContextPrecision(llm=evaluator_llm)\n",
    "faithfulness = Faithfulness(llm=evaluator_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "ragas_metrics = [\n",
    "    context_precision,\n",
    "    faithfulness,\n",
    "    rouge_score,\n",
    "    rubrics_score,\n",
    "]\n",
    "\n",
    "ragas_result_rag_a = evaluate(\n",
    "    dataset=ragas_eval_dataset_a, metrics=ragas_metrics, llm=evaluator_llm\n",
    ")\n",
    "\n",
    "ragas_result_rag_b = evaluate(\n",
    "    dataset=ragas_eval_dataset_b, metrics=ragas_metrics, llm=evaluator_llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.evaluation import EvalResult\n",
    "\n",
    "result_rag_a = EvalResult(\n",
    "    summary_metrics=ragas_result_rag_a._repr_dict,\n",
    "    metrics_table=ragas_result_rag_a.to_pandas(),\n",
    ")\n",
    "\n",
    "result_rag_b = EvalResult(\n",
    "    summary_metrics=ragas_result_rag_b._repr_dict,\n",
    "    metrics_table=ragas_result_rag_b.to_pandas(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Eval Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View summary results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_eval_report(\n",
    "    eval_result=(\n",
    "        \"Model A Eval Result\",\n",
    "        result_rag_a.summary_metrics,\n",
    "        result_rag_a.metrics_table,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_eval_report(\n",
    "    (\n",
    "        \"Model B Eval Result\",\n",
    "        result_rag_b.summary_metrics,\n",
    "        result_rag_b.metrics_table,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = []\n",
    "\n",
    "eval_results.append(\n",
    "    (\"Model A\", result_rag_a.summary_metrics, result_rag_a.metrics_table)\n",
    ")\n",
    "eval_results.append(\n",
    "    (\"Model B\", result_rag_b.summary_metrics, result_rag_b.metrics_table)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_radar_plot(eval_results, max_score=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bar_plot(eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
