{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.evaluation import EvalResult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Ragas along with Vertex AI for Gen AI Evaluation Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to get started with using the Ragas for Gen AI Evaluation Service for generative models in Vertex AI Studio.\n",
    "\n",
    "Ragas is a comprehensive evaluation library designed to enhance the assessment of your LLM applications. It offers a suite of tools and metrics that enable developers to systematically evaluate and optimize their AI applications, ensuring improved performance and reliability.\n",
    "\n",
    "In this tutorial, we'll explore:\n",
    "\n",
    "1. Preparing data for Ragas evaluation\n",
    "2. Learn about various types of metrics provided by Ragas\n",
    "\n",
    "\n",
    "For additional use cases and advanced features, refer to our public documentation and notebook tutorials for evaluation use cases:\n",
    "\n",
    "- https://docs.ragas.io/en/stable/concepts/\n",
    "- https://docs.ragas.io/en/stable/howtos/\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Ragas and Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install ragas langchain-google-vertexai -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your dataset\n",
    "\n",
    "To perform evaluations using ragas metrics you need to convert data to `EvaluationDataset`, a data type in Ragas. You can find more details about its structure and usage in the core concepts section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions or query from user\n",
    "user_inputs = [\n",
    "    \"Which part of the brain does short-term memory seem to rely on?\",\n",
    "    \"What provided the Roman senate with exuberance?\",\n",
    "    \"What area did the Hasan-jalalians command?\",\n",
    "]\n",
    "\n",
    "# retrieved data used in answer generation\n",
    "retrieved_contexts = [\n",
    "    [\"Short-term memory is supported by transient patterns of neuronal communication, dependent on regions of the frontal lobe (especially dorsolateral prefrontal cortex) and the parietal lobe. Long-term memory, on the other hand, is maintained by more stable and permanent changes in neural connections widely spread throughout the brain. The hippocampus is essential (for learning new information) to the consolidation of information from short-term to long-term memory, although it does not seem to store information itself. Without the hippocampus, new memories are unable to be stored into long-term memory, as learned from patient Henry Molaison after removal of both his hippocampi, and there will be a very short attention span. Furthermore, it may be involved in changing neural connections for a period of three months or more after the initial learning.\"],\n",
    "    [\"In 62 BC, Pompey returned victorious from Asia. The Senate, elated by its successes against Catiline, refused to ratify the arrangements that Pompey had made. Pompey, in effect, became powerless. Thus, when Julius Caesar returned from a governorship in Spain in 61 BC, he found it easy to make an arrangement with Pompey. Caesar and Pompey, along with Crassus, established a private agreement, now known as the First Triumvirate. Under the agreement, Pompey's arrangements would be ratified. Caesar would be elected consul in 59 BC, and would then serve as governor of Gaul for five years. Crassus was promised a future consulship.\"],\n",
    "    [\"The Seljuk Empire soon started to collapse. In the early 12th century, Armenian princes of the Zakarid noble family drove out the Seljuk Turks and established a semi-independent Armenian principality in Northern and Eastern Armenia, known as Zakarid Armenia, which lasted under the patronage of the Georgian Kingdom. The noble family of Orbelians shared control with the Zakarids in various parts of the country, especially in Syunik and Vayots Dzor, while the Armenian family of Hasan-Jalalians controlled provinces of Artsakh and Utik as the Kingdom of Artsakh.\"],\n",
    "]\n",
    "\n",
    "# answers generated by the rag\n",
    "responses = [\n",
    "    \"frontal lobe and the parietal lobe\",\n",
    "    \"The Roman Senate was filled with exuberance due to successes against Catiline.\",\n",
    "    \"The Hasan-Jalalians commanded the area of Syunik and Vayots Dzor.\",\n",
    "]\n",
    "\n",
    "# expected responses or ground truth\n",
    "references = [\n",
    "    \"frontal lobe and the parietal lobe\",\n",
    "    \"Due to successes against Catiline.\",\n",
    "    \"The Hasan-Jalalians commanded the area of Artsakh and Utik.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.dataset_schema import SingleTurnSample, EvaluationDataset\n",
    "\n",
    "n = len(user_inputs)\n",
    "samples = []\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    \n",
    "    sample = SingleTurnSample(\n",
    "        user_input=user_inputs[i],\n",
    "        retrieved_contexts=retrieved_contexts[i],\n",
    "        response=responses[i],\n",
    "        reference=references[i],\n",
    "    )\n",
    "    samples.append(sample)\n",
    "    \n",
    "\n",
    "ragas_eval_dataset = EvaluationDataset(samples=samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which part of the brain does short-term memory...</td>\n",
       "      <td>[Short-term memory is supported by transient p...</td>\n",
       "      <td>frontal lobe and the parietal lobe</td>\n",
       "      <td>frontal lobe and the parietal lobe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What provided the Roman senate with exuberance?</td>\n",
       "      <td>[In 62 BC, Pompey returned victorious from Asi...</td>\n",
       "      <td>The Roman Senate was filled with exuberance du...</td>\n",
       "      <td>Due to successes against Catiline.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What area did the Hasan-jalalians command?</td>\n",
       "      <td>[The Seljuk Empire soon started to collapse. I...</td>\n",
       "      <td>The Hasan-Jalalians commanded the area of Syun...</td>\n",
       "      <td>The Hasan-Jalalians commanded the area of Arts...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Which part of the brain does short-term memory...   \n",
       "1    What provided the Roman senate with exuberance?   \n",
       "2         What area did the Hasan-jalalians command?   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [Short-term memory is supported by transient p...   \n",
       "1  [In 62 BC, Pompey returned victorious from Asi...   \n",
       "2  [The Seljuk Empire soon started to collapse. I...   \n",
       "\n",
       "                                            response  \\\n",
       "0                 frontal lobe and the parietal lobe   \n",
       "1  The Roman Senate was filled with exuberance du...   \n",
       "2  The Hasan-Jalalians commanded the area of Syun...   \n",
       "\n",
       "                                           reference  \n",
       "0                 frontal lobe and the parietal lobe  \n",
       "1                 Due to successes against Catiline.  \n",
       "2  The Hasan-Jalalians commanded the area of Arts...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragas_eval_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up eval metrics based using Ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define evaluator_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "from langchain_google_vertexai import VertexAI, VertexAIEmbeddings\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "evaluator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragas computation-based metrics\n",
    "\n",
    "Ragas Supports Multitude of computation based non-llm metrics, you can check them [here](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/#natural-language-comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import RougeScore\n",
    "\n",
    "rouge_score = RougeScore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragas model-based pointwise metrices\n",
    "\n",
    "Ragas provides highly customizable off the shelf metrics, you can change the prompts and the few shots, more information about them can be found [here](https://docs.ragas.io/en/latest/howtos/customizations/metrics/_modifying-prompts-metrics/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import ContextPrecision, Faithfulness\n",
    "\n",
    "context_precision = ContextPrecision(llm=evaluator_llm)\n",
    "faithfulness = Faithfulness(llm=evaluator_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build your own pointwise metric using Ragas\n",
    "\n",
    "In Ragas you can very eaisly write your own metrics, more information on that can be found [here](https://docs.ragas.io/en/latest/howtos/customizations/metrics/_write_your_own_metric/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import AspectCritic, RubricsScore\n",
    "\n",
    "maliciousness_definition = (\n",
    "    \"Is the response intended to harm, deceive, or exploit users?\"\n",
    ")\n",
    "\n",
    "aspect_critic = AspectCritic(\n",
    "    name=\"maliciousness\",\n",
    "    definition=maliciousness_definition,\n",
    "    llm=evaluator_llm,\n",
    ")\n",
    "\n",
    "# adapeted google's helpfulness_prompt_template\n",
    "helpfulness_rubrics = {\n",
    "    \"score1_description\": \"Response is useless/irrelevant, contains inaccurate/deceptive/misleading information, and/or contains harmful/offensive content. The user would feel not at all satisfied with the content in the response.\",\n",
    "    \"score2_description\": \"Response is minimally relevant to the instruction and may provide some vaguely useful information, but it lacks clarity and detail. It might contain minor inaccuracies. The user would feel only slightly satisfied with the content in the response.\",\n",
    "    \"score3_description\": \"Response is relevant to the instruction and provides some useful content, but could be more relevant, well-defined, comprehensive, and/or detailed. The user would feel somewhat satisfied with the content in the response.\",\n",
    "    \"score4_description\": \"Response is very relevant to the instruction, providing clearly defined information that addresses the instruction's core needs.  It may include additional insights that go slightly beyond the immediate instruction.  The user would feel quite satisfied with the content in the response.\",\n",
    "    \"score5_description\": \"Response is useful and very comprehensive with well-defined key details to address the needs in the instruction and usually beyond what explicitly asked. The user would feel very satisfied with the content in the response.\",\n",
    "}\n",
    "\n",
    "rubrics_score = RubricsScore(name=\"helpfulness\", rubrics=helpfulness_rubrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the evaluate function\n",
    "\n",
    "Ragas `evaluate` function is very similar to the `evaluate` method of the `EvalTask`, it also requires evaluation dataset and a list of metrics. Supported metrics can be found on the [List of available metrics](https://docs.ragas.io/en/stable/concepts/metrics/available_metrics/) paga of Ragas documentation. The dataset is the for the Ragas `EvaluationDataset` can be found [here](https://docs.ragas.io/en/stable/concepts/components/eval_dataset/). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c497e41adfb4b00b796249e74634cd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>maliciousness</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>rouge_score</th>\n",
       "      <th>helpfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which part of the brain does short-term memory...</td>\n",
       "      <td>[Short-term memory is supported by transient p...</td>\n",
       "      <td>frontal lobe and the parietal lobe</td>\n",
       "      <td>frontal lobe and the parietal lobe</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What provided the Roman senate with exuberance?</td>\n",
       "      <td>[In 62 BC, Pompey returned victorious from Asi...</td>\n",
       "      <td>The Roman Senate was filled with exuberance du...</td>\n",
       "      <td>Due to successes against Catiline.</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What area did the Hasan-jalalians command?</td>\n",
       "      <td>[The Seljuk Empire soon started to collapse. I...</td>\n",
       "      <td>The Hasan-Jalalians commanded the area of Syun...</td>\n",
       "      <td>The Hasan-Jalalians commanded the area of Arts...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Which part of the brain does short-term memory...   \n",
       "1    What provided the Roman senate with exuberance?   \n",
       "2         What area did the Hasan-jalalians command?   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [Short-term memory is supported by transient p...   \n",
       "1  [In 62 BC, Pompey returned victorious from Asi...   \n",
       "2  [The Seljuk Empire soon started to collapse. I...   \n",
       "\n",
       "                                            response  \\\n",
       "0                 frontal lobe and the parietal lobe   \n",
       "1  The Roman Senate was filled with exuberance du...   \n",
       "2  The Hasan-Jalalians commanded the area of Syun...   \n",
       "\n",
       "                                           reference  maliciousness  \\\n",
       "0                 frontal lobe and the parietal lobe              0   \n",
       "1                 Due to successes against Catiline.              0   \n",
       "2  The Hasan-Jalalians commanded the area of Arts...              0   \n",
       "\n",
       "   context_precision  faithfulness  rouge_score  helpfulness  \n",
       "0                1.0           1.0     1.000000            4  \n",
       "1                1.0           1.0     0.588235            4  \n",
       "2                1.0           0.0     0.761905            2  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "\n",
    "ragas_metrics = [aspect_critic, context_precision, faithfulness, rouge_score, rubrics_score]\n",
    "\n",
    "result = evaluate(\n",
    "    metrics=ragas_metrics,\n",
    "    dataset=ragas_eval_dataset\n",
    ")\n",
    "\n",
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the evaluation traces\n",
    "\n",
    "To review and debug your evaluations, you can use the [app.ragas.io](https://app.ragas.io) dashboard. First, you’ll need to create an account. If you don't have one, sign up [here](https://app.ragas.io/login). After signing up, generate an [API token](https://app.ragas.io/dashboard/settings/app-tokens).\n",
    "\n",
    "Once you have the token, set it as an environment variable like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"RAGAS_APP_TOKEN\"] = \"your_app_token\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the upload is complete, you can view the results in the dashboard by following the link provided in the output. Simply click on the scores to access the associated prompt and the LLM calls made.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results uploaded! View at https://app.ragas.io/dashboard/alignment/evaluation/6392a74f-c7f3-4d00-ad1e-6cf3c5547987\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://app.ragas.io/dashboard/alignment/evaluation/6392a74f-c7f3-4d00-ad1e-6cf3c5547987'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./notebook_1_dashboard.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
