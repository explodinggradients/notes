{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aedb30ff",
   "metadata": {},
   "source": [
    "# Ask the Video\n",
    "\n",
    "A pipeline I build to transcribe videos effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61a9d994",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79d98ad",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['jobName', 'accountId', 'results', 'status'])\n"
     ]
    }
   ],
   "source": [
    "with open('./transcripts/interview_with_jerry.json') as f:\n",
    "    transcription = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cf02a8f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start_time': '0.009', 'speaker_label': 'spk_0', 'end_time': '0.579', 'alternatives': [{'confidence': '0.999', 'content': 'Hello'}], 'type': 'pronunciation'}\n"
     ]
    }
   ],
   "source": [
    "print(transcription['results']['items'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fad2d39b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jithin',\n",
       "  \"Hello and welcome to the explored ingredients podcast . And today we have with us . Jerry Luke , the creator of Lama Index Lama Index . For those of you who don't know is a framework that helps you connect large language models with your own data . And this is something that has a tonne of use cases , right ? It could be from building your own customer service board to answer your customer's queries regarding a product based on the documentation . It could be used to extract information from the huge amount of unstructured data that you have in your company . Or it can be used to pass knowledge from knowledge sources such as books , blogs , podcasts , videos and can help accelerate your learning . And this is just the tip of the iceberg . There are a tonne more use cases that you can think of for yourself , and this has been shown in how fast this framework is growing . Over the last few months , this framework has been growing at a rate of almost 200% month over month , and it doesn't seem like it's starting anytime soon . And these are all the reasons why I wanted to talk with Jerry and I was super excited about it . And in this podcast , we talk about how this budget got started . What are the core concepts behind the framework ? His mental models to wrap your head around large language models and how to augment your large language models with your own data ? How ? What are the vision ? What's the vision for the project and its long term and short term goal that the team is excited about and how you can help contribute and join the movement ? So without further ado , Jerry Liu for you guys . Hello , Jerry . Welcome to the podcast . And it's It is It is totally great having you because , like lama in over the last few months have really , really blown upright . And it's super exciting to have the creator with us today . So how do you feel ? How do you feel about the whole field ? How do you keep up with it ? Actually ,\"),\n",
       " ('Jerry',\n",
       "  \"uh , well , first , uh , thanks for your time for having me . I'm super excited to be here . Um , yeah . No , I mean , I think there's obviously just a tonne of a I activity going on across a variety of different verticals . And so there's a tonne of new research papers coming out every day . And if you look at hacker news , for instance , there's always some new , uh , paper or publication being featured . And then if you go on on Twitter LinkedIn there's always these new releases . Cool demos , uh , cool features being added by all the builders in this space . And so it's been super exciting to see . I think , um , I I I don't really know if there's a magic answer to it . I have been , uh , just pretty busy . But it's also been a very exciting type of energy . Uh , if you will , because there's just a tonne of stuff going on all the time .\"),\n",
       " ('Jithin',\n",
       "  \"Yeah , that's true . Do you have any , like , like , tips For , like , catching up ? Because the rate of revision has been so massive that catching up itself has been a very challenging . So what do you do ?\"),\n",
       " ('Jerry',\n",
       "  \"Yeah , I I think I probably spend , um a a undue amount of time on on Twitter . Uh , and just , uh , like , again hacker news and and just , like , probably bounce around between those two sources . Um , there . There are a few kind of , um , articles and market maps coming out that you find , uh , on these that you can just , like , browse and see what the current state of a I is . But , uh , that's basically what I track the most .\"),\n",
       " ('Jithin',\n",
       "  \"OK , got it . Got it . Got it . But coming back to Lama Index like , man , I'm really curious of about how the initial stage of LAX came in . I know you shared it . Share it a bit like in other in another podcast , too . But what was , like , the founding story like , what was the initial problem that you're trying to solve ? How did it actually turn out into this big project that that it is right now ?\"),\n",
       " ('Jerry',\n",
       "  \"Yeah , totally . I So , um , along index started off as just a fun tool , Uh , a few months ago , back in November . And at the time , I was primarily focused on just thinking about how do you solve the context limit problem for language models ? Because , uh , you know , we were everybody was playing around with , uh , GP t . Three people were just starting to think about building applications , and then everybody was thinking about how do I actually , you know , have it understand and process data that's longer than 4000 tokens . And so , at the time that got me thinking about some novel structures to get started on on building , um , to to to basically just like , uh , these structures can hopefully , um , index a larger corpus of data and still , uh , you know , feed in the relevant sections to the language model . So it's almost like a I was at the time . It was very much a thought exercise . And so I was thinking about , um uh , an analogy similar to a computer architecture , if you will . You have the language model as , like AC p u . And then you have , like , the ram and storage . That's like the kind of like external storage system . And so basically , I was thinking about OK , how does it make sense to build this like external storage system for the language model ? Um , so at the time it was it was very much just like a thought exercise . I came out with this idea of like , a tree index that the language model could use a traverse , uh , you know , large corpses of information . But as time went on and more people became interested in this project , uh , it became pretty clear that there was an actual practical opportunity here because people were using similar tools to try to build applications on their own . And so that's what got me , uh , to pretty much like , pivot the project into something that , uh , actually created real practical value for users . And so since , uh , the inception , uh , lama index has grown , uh , quite a bit , Uh , and it's actually , uh , grown a lot in terms of fooling as well . So we offer , you know , a variety of tools around data injection data , indexing , data query . And we also offer a lot of supporting modules around there as well . For instance , um , evaluation , output parsing , um , being able to optimise token usage , being able to process , uh , nodes even further and all those good stuff .\"),\n",
       " ('Jithin',\n",
       "  \"Awesome . Awesome man . Like one , like so one thing that I found very interesting is that so when I'm looking at the space , there are two big projects that are there . One is lama index , and one is both . You and Harris have 11 thing in , like a lot of things in common , but also one major thing in common , which is robust intelligence . Both of you guys have been working in the company , and then both of you guys build , like , super impactful projects . So what is that like ? Like how ? How is that a coincidence , or was there something behind it ?\"),\n",
       " ('Jerry',\n",
       "  \"I think it was probably kind of a coincidence . I think both of us just happen to be interested in the space . Um , I did have a few dinner side conversations with Harrison back in October about language models , and those were kind of fun , like idea bouncing sessions . Um , but yeah , it's kind of funny how this how this turned out , You know , now there's two big open source projects . There's Llama Index . There's Lane train , obviously . And , um , yeah . You know , Harrison used to be my coworker . So definitely a relationship there .\"),\n",
       " ('Jithin',\n",
       "  'Yeah , Nice . Nice , nice . But coming back to , like , lama index like , and the initial stories , like so you guys . So you actually build out the first three ? Like , the first structure that you tried was the three index . But how was the hat like ? And over time , you added a lot of other indexes also , but coming back to the present , what has been like the most effective go to indexes are like the basic pipeline that people actually set up .'),\n",
       " ('Jerry',\n",
       "  \"Yeah , So I think there's a definitely a difference between how the project started , which was very much a design exercise , Uh , versus now where we're very much trying to offer , like , practical value to users . And so how it started was very much the tree index , and I can kind of paint a picture of , uh , why we came up with the idea . Um , and from the perspective of , like , a thought exercise , it's kind of cool because basically the idea of a tree index is that you you're not gonna use any other machine learning model or technology like you're not gonna use embedding because you're not gonna use whatever . Uh , you're gonna use the language model itself to both process information , organise information and traverse information . So just from like a thought exercise perspective , that was kind of cool to think about , like , how do you actually , uh , kind of create that structure and one of the most intuitive structures that , uh , came up with was this idea of a tree . Um , there were , of course , practical limitations , and the tree index still exists today , but I pretty much like , just encourage people to not use it . Uh , except if you want to use it as , like a router . Um , the one is just It's very slow . Uh , pretty much like . Then we can talk about this like language model calls , especially if you're stuffing the in input window with a bunch of tokens . It just takes a few seconds . And so , especially if you're chaining repeated calls together , it just takes forever . Um , the other part is that , um , errors propagate over time as you train L o m calls . And so if you're traversing this tree and let's say , you know , like , um , like a GP t three has , like , a 90% success rate , et cetera , uh , and picked the wrong answer upstream , then everything downstream would break , too . And so , just like just from a kind of , um , mathematical perspective , uh , just statistically that the errors would increase the deeper and and bigger the tree was . Um , So there were just a few practical concerns about , uh , making that useful . Um , however , it was it was fun , and I think it got people excited . And so then you know , as we built out more , uh , indexes . I would say probably the most common paradigm that most people start out with is just using some sort of vector organisation or or embedding based approach . Um , and so you take it a bunch of text , you know , uh , embed it store and some vector store . That's probably like the most basic thing you can do . And then and then you just , like , have a system set up where you can basically do retrieval using embedding look up and search . Um , nowadays , there's , like , other techniques , like you could have metadata philtres . You could have hybrid search , et cetera , but basically , you call some vector store service to get back a set of documents . And then , um , from there , you can , um you can , uh , like , take the retrieve documents and put it into input , prompt and synthesise . The response .\"),\n",
       " ('Jithin',\n",
       "  'Yeah . I mean , I have been super like it has been super surprising how effective that simple embeddings , uh , index has been because if you have the data and if you polish it very quickly , like if you like , do some basic filtering and cleaning on the data , the results that you get are , like , super good that that itself is enough for , like , an 80 80% 90% solution that you can actually like production . I'),\n",
       " ('Jerry',\n",
       "  \"I think so . I think the one thing I would say about that is it is very easy to use , but I think for a lot of people , I think , um uh , most people would start off with an embedding based approach when they first start creating a project . Um , but I would probably caution against overfitting to that approach . Uh , and And the reason I say that is because it works Well , if you kind of understand the technique and the type of question that you're gonna ask which requires , like a fact based retrieval , look up approach . Like if the question you ask can actually be found in a specific section of the document or like , uh , kind of you trust that embedding similarity can do its job . There are a lot of cases where embedding similarity doesn't do its job , and you're gonna need some kind of more structured primitive to query over your data . And that's actually something I can talk about because that's part of something that you know , ideally Lama index provides as a generalisation , uh , of this overall framework of retrial and synthesis and out of those , like , this idea of top case , semantic search is just one instantiation .\"),\n",
       " ('Jithin',\n",
       "  'Yeah , I would actually , like I had the follow up question . Could you give some few examples so that and go deep on that topic like the other solutions that people , the other things that people can actually build on , like , try and use other techniques ?'),\n",
       " ('Jerry',\n",
       "  \"Uh , 100% . So we have a big section for , um , uh , different types of use cases that you can use with lama Index . But let me just take a step back and just paint a picture . Think about llama index as a tool set that that hopefully allows you to create this black box , right ? And And how this black box works is it takes in some input , which is a query , and it gives you back the output , which is the response that you want , plus , uh , a set of retrieve sources . And so , under the hood , we manage the interactions between your language model and your data , uh , depending on the use case , depending on the input to give you back the results that you would want . And so it's a very general framework , right ? And so I think , like one of the these instantiation is through , like top K semantic search and being able to get back the results . But there's other approaches that we can do , too . And that really is because we think there's just a tonne of complexity in this area of , like , retrieval and synthesis . Uh , you know , on top of your data using language models and we wanna provide , uh , users the tools to solve that . So let's just walk through , like a few basic examples . One is , um , let's say you want to do summarization over your documents . Uh , right . And so just like , um , first , just naive , like top K and batting look up is not gonna just give you the right documents , uh , to do summarization over any source you're gonna need , uh , some sort of other , um philtre index basically to retrieve the relevant documents . The next part is , let's say you want to , um , ask a question , and you don't want to basically , um , know beforehand whether or not that question is gonna be a summarization question or like a look up question . And so one thing we provide you is actually some tools to provide , like a unifying query interface . You ask a question at the top level , we can , um we show tutorials on how you can actually define like , a router to basically route this question to a list index . Uh , which performs summarization or like a vector index which performs top K . Embedding Look up so that that's something you can do with lama Index . And and that's something you can do pretty easily . Uh , without too much that would work , uh , another , uh there . So So I could keep going there . There's actually a bunch of stuff , but just like to like , put it a bit more concisely . Um , we also provide support with structure data . And so , um , we also provide support with defining graphs for different types of retrieval use cases . Um , and so , uh , for instance , uh , depending for structure index , you could do taxes equal if you choose to define graphs or relationships between nodes . Um , you can actually do more complicated referent synthesis modes like , for instance , being able to compare different documents together , being able to kind of of like , um , you know , synthesised stuff across different documents and and being able to ask , kind of , like , repeated questions over the different document sources , uh , or or Sorry , uh , break down a complex question into some into simpler questions over like , different document sources . So we provide a lot of these modules within lama Index . And the way I would think about it Is each tool set or building block , for instance , like an index module , or like , a query module , Or , you know , any other module that we provide pro , Uh , each module provides , like , a certain way of kind of getting certain results out of your data and so that that , by itself , is very interesting . And then the question is , um , how do you actually build some sort of like unified query interface over this data ? So , uh , uh , because you have each module which solves like a slightly different use case , like you'll call the structured index for , uh , SQL database . You know , you'll call list , uh , do a list index for summarization . Um , maybe a keyword look up table . And so then , you know , one of the questions is how do we abstract away some of this complexity . Like , you know , you want to ask a question ? How do we figure out what's like the right index to route to and then answer the question for you ? Um and so we provide some of these initial tutorials , um , on on how to do that . But that very much is both kind of like , uh , within some of the short term like functionality of lama index . But also part of our long term vision is , you know , given just the set of documents and given just , like a variety of questions that you might want to ask over the the these documents , What is the right state to build over these documents that allows you to perform different modes of retrieval and synthesis ? And then how can you , um how how can you basically , uh , kind of provide this unified query interface so that you can ask a question ? And then it can route to the relevant , uh , a you know , documents , uh , through the state that we define .\"),\n",
       " ('Jithin',\n",
       "  'Got it , Got it , So OK , got it . Got it . So that is that is OK . That is the end goal that you guys are building towards . But you also mentioned the query router , right ? Does that is that , like the router that actually map like routes , the different types of , uh , queries that one could possibly possibly ask Lama Index or your documentation ? Is that part of the , like , the solution that you have currently ?'),\n",
       " ('Jerry',\n",
       "  \"Yeah . So you can actually do that in a way within Lama index right now . And we're kind of thinking about ways to make it more general . One way to think about it is that a router is basically , like a kind of AAA node . Um , that has , like , links to a bunch of other , uh , indexes . And so if you view it from that perspective , you can basically use our compos ability framework where you can compose this like router index basically using the tree index . Actually , uh , because the tree index basically just picks one of the Children and then , um , you can , uh , use that , and then it will be able to take a question and route it to the underlying index , and then the underlying index can solve that problem . Um , one way to think about it is basically like a mini agent . That's kind of like but physically for being able to choose which index is is the right task for the job .\"),\n",
       " ('Jithin',\n",
       "  'Got it , got it , Got it , man , That , like all that itself , is all the technical stuff itself is an interesting thing . But I would be highly recommend people to check out the documentation because the dogs also have a lot of , uh , like , specific tutorials where specific use cases where you can actually try all of this stuff out . So do check the dogs for that . But like switching back to again an overall picture like what have like , there has been a lot of interesting use cases and a lot of interesting things that people have built right , And part of it is because you can easily get started with Lama Index because of in a couple of lines of code . So what has been some of the interesting , uh , use cases that you have seen from the community that people have built ?'),\n",
       " ('Jerry',\n",
       "  \"Yeah , definitely . I think , you know , in the beginning , um , and and even to this day , like a lot of llama index is really just being able to to give get you up and running really quickly over your own private data sources . And so a lot of the interesting use cases is over the diversity of data sources . So , for instance , like , you know , from standard stuff like PDF s to , like web pages to , uh , even other stuff , like videos and and audio . Like we we were one of the first , uh , projects to basically create , uh , like some sort of video to text , uh , parser use , uh , from one of our contributors . And then , um , basically , you can now , like , you know , transcribe video to text and then , uh , ask questions over this text . Um , and so even for that simple use case , it's It's quite interesting , because if you think about it , it , like , simplifies the data landscape by by quite a bit . Um , because all of a sudden , through a three step process , you could transform just raw , unstructured data into text index text as , like , a central data format , and then ask questions over it . You don't What ? You don't actually need to do a tonne of , like , complicated parsing like processing . Right ? Uh , like , e t . L like being able to , uh , write a bunch of like this custom logic . You can just , you know , trust that you can store text in an unstructured format and then ask the questions over it using a language model . So I find that part , uh , pretty fascinating , actually . Um , I think nowadays there's a lot of , like excitement about agents and and especially stuff around like , you know , baby , a G i o g BT , like all all the stuff that can do repeated stuff for you . Um , I I would say some of that stuff is more just , um it's kind of like an interesting demo as opposed to offering , like , practical value . Um , but at the same time , um , there's a lot of , um , we We also realise there's a lot of like very similar concepts that you can build pretty easily with just the lama indexes , like , uh , some of the core primitives that we offer with , like , indexes and factor stores . And so , for instance , like if you think about what are the components of , like a a g I system , there's like task management . There is , um , being able to , um , like index into like , the memory of of like , previous actions , there's being able to plan new stuff . And so a lot of these modules you can actually represent with simple , like lama index data structures and then be able to create some , uh , a g I you know , baby , a G I type , uh , model yourself . And so we , um I think some of our contributors have basically built these , like , cool a g I demos . Uh , they can actually find both in the repo as well as streamline . Um , And then we also have some other , uh , contributors building this too . Like one of our contributors built this like medical Asian research by which I thought was super interesting . You know , using llama Index itself .\"),\n",
       " ('Jithin',\n",
       "  \"Yeah , that was super creative . And the baby , like all the S f , also has been , like pretty interesting , But they are at a phase where it's like , more of an interesting demo . But you use cases are pretty hard . But right now , all the basic components are there , like to build , like OK , even if you think of it as a human brain , it has memory . It has the ability to plan stuff , So yeah , who knows ? Maybe something interesting will be there , but but one , like core of this is right . Like , OK , so say if I'm a builder , I'm building a product , Um , that spot , like , solve some problem with , um , like with other lama index and other frameworks . Because lama index does its job like too Well , because I don't have to worry about loading . I don't have to worry about how effective the thing is . Uh , like for someone building a product that creates like it , it brings the question of where I can build by mode because , OK , the technology side is fairly easy to set up with all these pipelines . But then so I have to figure out where exactly my mode is . It has . It has to be something other than tech . How do you think about that ? Uh , that problem .\"),\n",
       " ('Jerry',\n",
       "  \"Yeah , it's a good point . I mean , I think I think there still is something to be said about , um , the technical side . Uh , but to be fair , I think we we , uh , uh , want to offer a lot of the core tech that powers , uh , you know , the the Q A systems for users . Basically , so that they don't have to worry about it . Um , I would say again going back to , like , basic semantic search like it works well to get that up and running . There's a bunch of stuff that we want to add to make sure that , uh , that you can solve the broader classic queries and just pure kind of , like , fact based look up questions . Um , but we can take responsibility for that . And then in the meantime , I do think that , um , the application layer should not be , uh , just branded as , like , a thin wrapper around a I because I just think all of those applications are not gonna have , like , the French bull notes . Um And the reason is because , like , if you just look at any use case emerging , uh , on like a novel application with , uh , a I , for instance like even the tax sequel stuff like , uh , a few months ago or even with , um , kind of being able to , you know , build a trap over your PDF . The underlying tach , for that is honestly pretty simple and and can be open sourced right by our project , or or by line , train or other projects , too . And so I I do think it's like the application developer can , um , like , depend on the tech like the tech will probably be open source . It will be like widely discussed within the community . But they need to build , like , differentiated product , high level U axes that really encourage , like , kind of , like , different user interactions and and modes of behaviour . Uh , you know , that , uh , with the A I enabled application . And so I think it's less about , like , engineering some fancy algorithm , right ? If you're an application builder and more about thinking just fundamentally right , like , what are the best u axes for interacting with a i systems these days ? And if you look at , like the the spectrum of like a I enabled applications , there are certain use cases that people really do enjoy using . And then there's certain use cases that do , um , feel a little bit more like an add-on feature . Uh , for instance , I was thinking about this the other day . One thing that I really do enjoy using , right , just because it's like a really nice U X is like copilot , like just the fact that you don't even have to press a button . You just , like , write a line code , wait like half a second , and you just you just press tab to complete . It is just very nice . Um uh , versus Like I I think if you tried to build , for instance like , um , some sort of agent based system Where , um But like , you know , uh , you you like , maybe didn't set up a product . You act in a way that's , like , nice or pleasing . Then you know , there are a lot of downsides of building , uh , to trying to depend on albums , too . Like , for instance , they're very expensive and also very slow . And so , if you like , pass that on to the end user , they might get frustrated with the experience , too .\"),\n",
       " ('Jithin',\n",
       "  \"Got it ? Got it . Yeah , that is true . Basically , yeah , the build as builders we have to think of like what other ? U X is one definitely one mode like I also like notions . U X because the whole it's it's not . It is basically text , but the whole where you can select all the text , interact , ask more questions and like it is more much more intuitive than like GP interface itself . But yeah , the u X the data , all the other sides become becomes even bigger . More than the technical side . Yeah , that is true . But coming to the project itself and the future of the project Like what ? What do you have in store ? Like , what are the like , the most in the like in a on a like , a short term thing ? What is , like , the most interesting things that you're working on . You're told that there is a , uh , 0.6 release coming in . So I thought I guess you are pretty excited for that . But even after that , what are like some of the couple of projects that you are pretty excited about ?\"),\n",
       " ('Jerry',\n",
       "  \"Yeah . So So , uh , I'm glad you asked that question . So I think 0.6 should have landed . Uh , by the time that you heard in this podcast , and so , um , and I think it's actually related to the previous question , which is about , um , you know , how do you offer the best ergonomics and you ask for the end application developer you our tools . And so I think one thing that I would say lama index does very well these days is the fact that it offers , like , this high level API gets you started three lines of code , right plug in your favourite data loader index it Now you can ask a question over it and then get back an answer . Uh , and so you can do that super easily . I think one thing that we have been , uh , the reason this like re factor is happening is because we want to solve both the simple use cases as well as the more advanced and customizable use cases . And the more advanced you go typically , Uh , especially if you're a more advanced developer . You'd want a bit more modularity , customizable flexibility and being able to kind of like define your own , like core modules and various components of the code base in order to implement kind of like your own . You know , retrievers , synthesisers , all these different types of things . And so that is very much what the reactor is going for . Um , that is step one , which is just , uh , from kind of like this very high level , very easy to use API to really exposing these , like modular components that people can very easily extend and build on top of themselves so that they can define their own tech . Uh , for retrieval and synthesis if they want . And this is something that a lot of users have asked uh , asked for . The next part is the fact that we want to make , um our tool set a bit more principled and and more enterprise ready . And a lot of that becomes , uh , uh , uh comes with thinking a lot about how do we best source state ? Um , because you can almost think about llama index . Uh , one view of it is like some sort of state full service management for your language model application . Right , Because we are dealing with , uh , kind of managing state for your application . Language models are inherently stateless . Uh , at least right now they are , Uh , uh , you know , like GP t three GB four . Who knows about the future . But basically , you have some input . You got some output . And if you want to operate it on top of your data , the question is , how do you define the views on this data ? How do you maintain the state so that you can use it with a language model ? And so we're really trying to create , like , good abstractions for that ? Uh , both , Like , conceptually but also provide good integrations with , you know , the databases , like factor stores object stores that you would typically use . And so there's just a lot of kind of like , uh , storage , um , abstractions that we're trying to clean up and and make this a lot more enterprise . Right ? Um , I think going ahead in the future , there is just like some general components that we're super excited to to talk about . Uh so , for instance , um , one idea is continuing to push on this idea of , uh , having this out of the box , like unified query interface for users . Imagine , for a user , they can set up the service that they can deploy on top of their data . And we abstract away a lot of the complexity about Oh , you know , we should define these indexes over this data , or we should define , like , these configurations or this mode of interaction . Um , we can configure a lot of that automatically and then given a top level question , uh , heading this API service , we can figure out , you know , the right interactions , the right services to call in order to give you back the response that you would want . Um and so a lot of this is going to be very interesting , because I really think this ties into this idea of this , like , broader agent style interactions with your data . But I I think here it's it's interesting , because I almost I I imagine this idea of like , um different , kind of like mini like data agents over your data system . Um , and being able to pick like each data agent is specialised at , kind of like picking the right tool for its job . Uh , as a very simple instantiation . Right now , you can think about our router abstraction , right ? As as , like , a mini data agent that can route a query to the right sub index . Uh , so you can imagine that . And then you can imagine , like , sub data agents , too , that could , like , you know , figure out the right parameters , insert , uh , to to query a vector store with , uh , and then all these different kind of services interacting with each other , uh , under the hood . Uh uh , and and kind of again , providing this clean , high level interface to the end user . Uh , hopefully , some of that made sense . But the high level idea is that we're trying to create this , like idea of just abstracting away a lot of the retrieval , synthesis , complexity and and taking advantage of all the latest techniques that are happening these days to build that kind of unified prayer experience for users . Um , and at the same time , you know , from the community side offering the building blocks for more advanced users to kind of customise and and create different components themselves .\"),\n",
       " ('Jithin',\n",
       "  \"Got got it . So in the short term , you guys are exposing , like , exposing all the small components that you can as a as a user that you have you want to modify so that your index becomes more performant on your specific data set . But on the long term , you don't have to even care about modifying that . It'll it'll be just you . You'll you'll give your data to Lama Index , and that will take care of building the right indexes , the right set of agents that can like abstract abstract away all the complexities that , like all the complexities and configurations that you might need and then just it's just one end point . You ask , you ask your questions and that's it . You get to get the answers you want ,\"),\n",
       " ('Jerry',\n",
       "  \"exactly , and I think it's honestly very powerful because it's a very hard technical problem . But I we do think it will provide a tonne of value because just if you think about the average developer , like a person who's trying to understand all this , uh , data and figure out , how do I like ? Hook this all up with the language model and figure out what the best like interaction pattern is that there's just a tonne of complexity there , and I think we we want to make that as easy as possible .\"),\n",
       " ('Jithin',\n",
       "  \"Got it , Got it , got it now as like OK , so you you also mentioned that there there are a lot of community community like contributions also . But as someone who is interested in who's interested in Latics and want to contribute what are like some of like , there are a lot of different models , and there there is a lot of different moving parts right now , like as a as a contributor , where would like the best place be ? What are the different areas that I could help and like , Yeah , what are some of the areas that people could help\"),\n",
       " ('Jerry',\n",
       "  \"That's a great question . So we have a contribution guide in the core ribo that consists of a tonne of different module modules that are pretty independent and are are relatively easy to contribute to . So just as a very basic example , we have data loaders . Uh , those are just massively paralysed . You know , we have , like , over 80 data loaders right now , but we could always add more . There's just like hundreds of different services that we we still want to integrate with . Um , there's other modules on that list . So , for instance , like retrieval modules , Uh , especially 10.6 auto comes out , uh , as kind of , um uh , query engine modules . Uh , like , uh , optimizers being able to reduce token usage . Uh , post processors , uh , tax splitters , evaluation modules , Um , like output parsers . And so all that stuff is described in the doc . All of those are pretty independent , and those things are , uh , pretty easily accessible . In general , there's , like , the kind of , like core features that we we'd love to uh , uh , contributions from . And then there's also just , like general , kind of like bug fixes , like documentation updates we all always welcome , like cool experiments , too . We have this new rebook called Lama lab , which is just a cool showcase for different types of , you know , like Asian based experiments or any other experiments . And so if you're interested in contributing any of those projects would highly encourage you to The other thing is , we're doing like , a t-shirt giveaway . I think we we've , like , met mentioned this is that if you do contribute like a core model , you'll get a fresh , limited edition lama index t-shirt . And so that could be , uh , a nice little gift to to have\"),\n",
       " ('Jithin',\n",
       "  \"motivation , but yeah , that will be pretty . That that is pretty good . But yeah , there are different lot of components that are there overall , because index by itself , it's a very new project . So there will . There will be a lot of lot of components that are moving a lot of opportunity for people to dive in . And yeah , and the growth has been pretty , like , pretty crazy , uh , for an open source project . So yeah , a lot of opportunities for contributions from outside , but as a as a as a project , right ? Like because , um , as more and more people are coming in . More and more people are using it in enterprise solutions . We want , um , more like , because right now , you you it started off as a hobby project . But more , more people are becoming , like , dependent on it as an enterprise . So do you ? Like what ? Like , what are some of , like the enterprise use cases that you have it in mind like , um , some ? Yeah , some things related to that .\"),\n",
       " ('Jerry',\n",
       "  \"Yeah , it's a good question . I think a lot of , um , the enterprise use cases arise from , um , things that are complementary to the open source project . And so , for instance , um , we're basically dealing with , uh , data and management and and query , right ? And and so , um , as data volumes scale up , especially if you're dealing with , like , gigabytes or terabytes of data , then you know , you might want to rely on , like a hosted service that can manage a lot of this for you instead of you just being able to run this locally on your laptop . And so that's , uh , very much something that we're thinking about . The next part of this is , uh , because we're dealing with data . There's a certain like table stakes enterprise features that pretty much like every , uh , kind of , like hosted offering needs to have , which is , like , data access control , like two , you know , user management , a lot of this type of stuff . And And I think , um , that's something that wouldn't make sense in the open source project anyways , Uh , just because , uh , that's something that you should be able to easily download and install . I think , um , there are a few additional things that we're thinking about , For instance , like just being able to really optimise , um , between different model use cases . Like , if we're we're able to profile like the performance of a model . Given your cost budget , being able will provide you a service that can , you know , uh , really understand and profile your model performance and allow you to pick the best , like inventing model language model and other types of services over your data and use case . Then some of that might make sense in the enterprise version as well .\"),\n",
       " ('Jithin',\n",
       "  \"Uh , like , just out of curiosity , like , have , like , have you , uh , had , like , experiences where ? Like , OK , right now , people start off with a very general l l m R A very general embedding . But has there been use cases where , uh , fine tuning your fine tuning your embedding has given better . Like even though it's smaller . So it's cheaper to run but has given , uh , like like all at performance are better .\"),\n",
       " ('Jerry',\n",
       "  \"I think that's actually very much an open question right now . So if that's something you're interested in contributing , I would love to see more experiments on that end . I think people are still actually trying to figure that out . I personally would be very interested in a distilled model that's very good at retrieval and not necessarily good at knowledge . And so something that's way faster , cheaper but is still very , very good at reasoning and retrieval . Um , so I love experiments on that end , Uh , I I think in terms of , um , embedding models , I think like by default , we use open A I I think open A . I like language . Models are still just the probably the best . GP t four is just very , very good , I think embedding model wise , I've seen a lot of people start to use , like hugging face and and just like , you know , uh , like other types of embedding because you know , if you're just trying to do some , like , cheap , uh , semantic embedding you don't necessarily need , like , the full power of , like , this giant model anyways . And so that's something to , um that is very interesting to explore . Is to what extent can you , like , trade off a little bit of performance for just , like , free cost , right , and and something of an embedding model that's locally hosted .\"),\n",
       " ('Jithin',\n",
       "  \"Got it . All right . I guess we have to wind up since it's almost time . But we covered a lot of interesting topics , and all the all the all the docs and the links that we discussed will be given in the description . So you guys do check it out if you want to know more . And also the lama index discard . Community is always there . So you can carry out this conversation . And if you have any more doubts , you can directly pop in there and ask . So with that , Thank you so much , Jerry , it was really fun talking to you . We got We got a lot of insights and ideas about where the project is going to go and what the main components are . And yeah , so pretty excited to see what what happens in the future . And your fingers crossed .\")]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_speaker = transcription['results']['items'][0]['speaker_label']\n",
    "current_speaker_phrases = []\n",
    "cleaned_transcription = []\n",
    "\n",
    "speaker_labels = {\n",
    "    'spk_0': \"Jithin\",\n",
    "    'spk_1': \"Jerry\"\n",
    "}\n",
    "try:\n",
    "    for word in transcription['results']['items']:\n",
    "        if word['speaker_label'] != current_speaker:\n",
    "            cleaned_transcription.append(\n",
    "                (speaker_labels[current_speaker], \n",
    "                 ' '.join(current_speaker_phrases))\n",
    "            )\n",
    "            current_speaker_phrases = []\n",
    "            current_speaker = word['speaker_label']\n",
    "        phrase = word['alternatives'][0]['content']\n",
    "        current_speaker_phrases.append(phrase)\n",
    "except Exception:\n",
    "    print(word)\n",
    "    \n",
    "cleaned_transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15a56032",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am an AI language model and do not have the ability to feel emotions, but I am functioning properly and ready to assist you in any way I can.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.9, model_name=\"gpt-3.5-turbo\")\n",
    "resp = chat([HumanMessage(content=\"are you happy\")])\n",
    "resp.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d9f5ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 31/31 [02:37<00:00,  5.07s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Jithin',\n",
       "  \"Hello and welcome to the explored ingredients podcast . And today we have with us . Jerry Luke , the creator of Lama Index Lama Index . For those of you who don't know is a framework that helps you connect large language models with your own data . And this is something that has a tonne of use cases , right ? It could be from building your own customer service board to answer your customer's queries regarding a product based on the documentation . It could be used to extract information from the huge amount of unstructured data that you have in your company . Or it can be used to pass knowledge from knowledge sources such as books , blogs , podcasts , videos and can help accelerate your learning . And this is just the tip of the iceberg . There are a tonne more use cases that you can think of for yourself , and this has been shown in how fast this framework is growing . Over the last few months , this framework has been growing at a rate of almost 200% month over month , and it doesn't seem like it's starting anytime soon . And these are all the reasons why I wanted to talk with Jerry and I was super excited about it . And in this podcast , we talk about how this budget got started . What are the core concepts behind the framework ? His mental models to wrap your head around large language models and how to augment your large language models with your own data ? How ? What are the vision ? What's the vision for the project and its long term and short term goal that the team is excited about and how you can help contribute and join the movement ? So without further ado , Jerry Liu for you guys . Hello , Jerry . Welcome to the podcast . And it's It is It is totally great having you because , like lama in over the last few months have really , really blown upright . And it's super exciting to have the creator with us today . So how do you feel ? How do you feel about the whole field ? How do you keep up with it ? Actually ,\",\n",
       "  \"Welcome to the Explored Ingredients podcast. Today, we have Jerry Luke, the creator of Lama Index, a framework that connects large language models with your data. This framework has numerous use cases, such as building a customer service board or extracting information from unstructured data. It can also accelerate learning by passing knowledge from books, blogs, podcasts, and videos. Jerry discusses how he started the project, its core concepts, and the vision for the long and short terms. The framework is growing at a rate of almost 200% month over month, and Jerry explains how listeners can contribute and join the movement. It's great to have Jerry with us, and we discuss how he keeps up with the rapidly evolving field.\"),\n",
       " ('Jerry',\n",
       "  \"uh , well , first , uh , thanks for your time for having me . I'm super excited to be here . Um , yeah . No , I mean , I think there's obviously just a tonne of a I activity going on across a variety of different verticals . And so there's a tonne of new research papers coming out every day . And if you look at hacker news , for instance , there's always some new , uh , paper or publication being featured . And then if you go on on Twitter LinkedIn there's always these new releases . Cool demos , uh , cool features being added by all the builders in this space . And so it's been super exciting to see . I think , um , I I I don't really know if there's a magic answer to it . I have been , uh , just pretty busy . But it's also been a very exciting type of energy . Uh , if you will , because there's just a tonne of stuff going on all the time .\",\n",
       "  \"Well, first off, thank you for having me. I'm thrilled to be here. There's a lot of AI activity happening across various verticals, resulting in numerous new research papers and publications being featured on platforms such as Hacker News, Twitter, and LinkedIn. It's exciting to witness all the builders in this space adding cool demos and features. Though I've been quite busy, it's been an energizing experience due to the constant buzz of activity.\"),\n",
       " ('Jithin',\n",
       "  \"Yeah , that's true . Do you have any , like , like , tips For , like , catching up ? Because the rate of revision has been so massive that catching up itself has been a very challenging . So what do you do ?\",\n",
       "  '\"That\\'s true. Any tips for catching up? The high rate of revision has made it very challenging. What\\'s your strategy?\"'),\n",
       " ('Jerry',\n",
       "  \"Yeah , I I think I probably spend , um a a undue amount of time on on Twitter . Uh , and just , uh , like , again hacker news and and just , like , probably bounce around between those two sources . Um , there . There are a few kind of , um , articles and market maps coming out that you find , uh , on these that you can just , like , browse and see what the current state of a I is . But , uh , that's basically what I track the most .\",\n",
       "  '\"I spend an undue amount of time on Twitter and Hacker News. I browse articles and market maps that show the current state of AI. That\\'s basically what I track the most.\"'),\n",
       " ('Jithin',\n",
       "  \"OK , got it . Got it . Got it . But coming back to Lama Index like , man , I'm really curious of about how the initial stage of LAX came in . I know you shared it . Share it a bit like in other in another podcast , too . But what was , like , the founding story like , what was the initial problem that you're trying to solve ? How did it actually turn out into this big project that that it is right now ?\",\n",
       "  \"Okay, I understand. But about the Lama Index, I'm curious about its initial stage and founding story. Can you briefly share the problem you were trying to solve and how it became the big project it is today?\"),\n",
       " ('Jerry',\n",
       "  \"Yeah , totally . I So , um , along index started off as just a fun tool , Uh , a few months ago , back in November . And at the time , I was primarily focused on just thinking about how do you solve the context limit problem for language models ? Because , uh , you know , we were everybody was playing around with , uh , GP t . Three people were just starting to think about building applications , and then everybody was thinking about how do I actually , you know , have it understand and process data that's longer than 4000 tokens . And so , at the time that got me thinking about some novel structures to get started on on building , um , to to to basically just like , uh , these structures can hopefully , um , index a larger corpus of data and still , uh , you know , feed in the relevant sections to the language model . So it's almost like a I was at the time . It was very much a thought exercise . And so I was thinking about , um uh , an analogy similar to a computer architecture , if you will . You have the language model as , like AC p u . And then you have , like , the ram and storage . That's like the kind of like external storage system . And so basically , I was thinking about OK , how does it make sense to build this like external storage system for the language model ? Um , so at the time it was it was very much just like a thought exercise . I came out with this idea of like , a tree index that the language model could use a traverse , uh , you know , large corpses of information . But as time went on and more people became interested in this project , uh , it became pretty clear that there was an actual practical opportunity here because people were using similar tools to try to build applications on their own . And so that's what got me , uh , to pretty much like , pivot the project into something that , uh , actually created real practical value for users . And so since , uh , the inception , uh , lama index has grown , uh , quite a bit , Uh , and it's actually , uh , grown a lot in terms of fooling as well . So we offer , you know , a variety of tools around data injection data , indexing , data query . And we also offer a lot of supporting modules around there as well . For instance , um , evaluation , output parsing , um , being able to optimise token usage , being able to process , uh , nodes even further and all those good stuff .\",\n",
       "  'Yeah, totally! I started Lama Index a few months ago back in November as a tool to solve the context limit problem for language models. The aim was to build novel structures that could index a larger corpus of data and still feed in the relevant sections to the language model. As a thought exercise, I came up with the idea of a tree index that the language model could use to traverse large amounts of information. The project grew as more people became interested, and now Lama Index offers a variety of tools around data injection, indexing, and query, as well as supporting modules for evaluation, output parsing, and token usage optimization.'),\n",
       " ('Jithin',\n",
       "  \"Awesome . Awesome man . Like one , like so one thing that I found very interesting is that so when I'm looking at the space , there are two big projects that are there . One is lama index , and one is both . You and Harris have 11 thing in , like a lot of things in common , but also one major thing in common , which is robust intelligence . Both of you guys have been working in the company , and then both of you guys build , like , super impactful projects . So what is that like ? Like how ? How is that a coincidence , or was there something behind it ?\",\n",
       "  'Awesome! One thing that I found interesting is that there are two big projects in the space: Lama Index and Both. You and Harris have a lot in common, including robust intelligence. Both of you have worked for the same company and built impactful projects. Was there something behind this coincidence?'),\n",
       " ('Jerry',\n",
       "  \"I think it was probably kind of a coincidence . I think both of us just happen to be interested in the space . Um , I did have a few dinner side conversations with Harrison back in October about language models , and those were kind of fun , like idea bouncing sessions . Um , but yeah , it's kind of funny how this how this turned out , You know , now there's two big open source projects . There's Llama Index . There's Lane train , obviously . And , um , yeah . You know , Harrison used to be my coworker . So definitely a relationship there .\",\n",
       "  \"I think it was a coincidence that both of us were interested in space. I had dinner conversations with Harrison in October about language models, and we bounced around some fun ideas. It's funny how this turned out with two big open source projects now - Llama Index and Lane Train. Harrison used to be my coworker, so we have a relationship.\"),\n",
       " ('Jithin',\n",
       "  'Yeah , Nice . Nice , nice . But coming back to , like , lama index like , and the initial stories , like so you guys . So you actually build out the first three ? Like , the first structure that you tried was the three index . But how was the hat like ? And over time , you added a lot of other indexes also , but coming back to the present , what has been like the most effective go to indexes are like the basic pipeline that people actually set up .',\n",
       "  'Nice! So, about the lama index and initial stories - did you build out the first three? The first attempt was the three index, but how was the hat? Over time, you added other indexes. But now, what are the most effective go-to indexes in the basic pipeline?'),\n",
       " ('Jerry',\n",
       "  \"Yeah , So I think there's a definitely a difference between how the project started , which was very much a design exercise , Uh , versus now where we're very much trying to offer , like , practical value to users . And so how it started was very much the tree index , and I can kind of paint a picture of , uh , why we came up with the idea . Um , and from the perspective of , like , a thought exercise , it's kind of cool because basically the idea of a tree index is that you you're not gonna use any other machine learning model or technology like you're not gonna use embedding because you're not gonna use whatever . Uh , you're gonna use the language model itself to both process information , organise information and traverse information . So just from like a thought exercise perspective , that was kind of cool to think about , like , how do you actually , uh , kind of create that structure and one of the most intuitive structures that , uh , came up with was this idea of a tree . Um , there were , of course , practical limitations , and the tree index still exists today , but I pretty much like , just encourage people to not use it . Uh , except if you want to use it as , like a router . Um , the one is just It's very slow . Uh , pretty much like . Then we can talk about this like language model calls , especially if you're stuffing the in input window with a bunch of tokens . It just takes a few seconds . And so , especially if you're chaining repeated calls together , it just takes forever . Um , the other part is that , um , errors propagate over time as you train L o m calls . And so if you're traversing this tree and let's say , you know , like , um , like a GP t three has , like , a 90% success rate , et cetera , uh , and picked the wrong answer upstream , then everything downstream would break , too . And so , just like just from a kind of , um , mathematical perspective , uh , just statistically that the errors would increase the deeper and and bigger the tree was . Um , So there were just a few practical concerns about , uh , making that useful . Um , however , it was it was fun , and I think it got people excited . And so then you know , as we built out more , uh , indexes . I would say probably the most common paradigm that most people start out with is just using some sort of vector organisation or or embedding based approach . Um , and so you take it a bunch of text , you know , uh , embed it store and some vector store . That's probably like the most basic thing you can do . And then and then you just , like , have a system set up where you can basically do retrieval using embedding look up and search . Um , nowadays , there's , like , other techniques , like you could have metadata philtres . You could have hybrid search , et cetera , but basically , you call some vector store service to get back a set of documents . And then , um , from there , you can , um you can , uh , like , take the retrieve documents and put it into input , prompt and synthesise . The response .\",\n",
       "  \"There is a clear difference between the project's initial design exercise and its current goal of offering practical value to users. The initial idea was a tree index that used the language model to process, organize, and traverse information. However, it had practical limitations and errors that increased with the depth and size of the tree. Nowadays, most people start with a vector organization or embedding-based approach to retrieve documents. From there, the documents can be put into input, prompt, and synthesized to produce a response.\"),\n",
       " ('Jithin',\n",
       "  'Yeah . I mean , I have been super like it has been super surprising how effective that simple embeddings , uh , index has been because if you have the data and if you polish it very quickly , like if you like , do some basic filtering and cleaning on the data , the results that you get are , like , super good that that itself is enough for , like , an 80 80% 90% solution that you can actually like production . I',\n",
       "  \"I've been surprised by how effective simple embeddings can be. With quick data filtering and cleaning, the results are usually quite good and can provide an 80-90% solution for production.\"),\n",
       " ('Jerry',\n",
       "  \"I think so . I think the one thing I would say about that is it is very easy to use , but I think for a lot of people , I think , um uh , most people would start off with an embedding based approach when they first start creating a project . Um , but I would probably caution against overfitting to that approach . Uh , and And the reason I say that is because it works Well , if you kind of understand the technique and the type of question that you're gonna ask which requires , like a fact based retrieval , look up approach . Like if the question you ask can actually be found in a specific section of the document or like , uh , kind of you trust that embedding similarity can do its job . There are a lot of cases where embedding similarity doesn't do its job , and you're gonna need some kind of more structured primitive to query over your data . And that's actually something I can talk about because that's part of something that you know , ideally Lama index provides as a generalisation , uh , of this overall framework of retrial and synthesis and out of those , like , this idea of top case , semantic search is just one instantiation .\",\n",
       "  \"I believe that the tool is easy to use, however, it is common for most people to begin with an embedding approach when starting a project. I caution against relying solely on this technique as it may result in overfitting. Although embedding similarity can be effective for fact-based retrieval or when the information can be found in a specific document section, there are cases where a more structured approach is necessary for querying data. This is where Lama index's generalization of the retrieval and synthesis framework comes into play, including the idea of top-case semantic search as just one example.\"),\n",
       " ('Jithin',\n",
       "  'Yeah , I would actually , like I had the follow up question . Could you give some few examples so that and go deep on that topic like the other solutions that people , the other things that people can actually build on , like , try and use other techniques ?',\n",
       "  '\"I have a follow-up question. Could you provide some examples to further elaborate on the topic and other solutions for people to build on using different techniques?\"'),\n",
       " ('Jerry',\n",
       "  \"Uh , 100% . So we have a big section for , um , uh , different types of use cases that you can use with lama Index . But let me just take a step back and just paint a picture . Think about llama index as a tool set that that hopefully allows you to create this black box , right ? And And how this black box works is it takes in some input , which is a query , and it gives you back the output , which is the response that you want , plus , uh , a set of retrieve sources . And so , under the hood , we manage the interactions between your language model and your data , uh , depending on the use case , depending on the input to give you back the results that you would want . And so it's a very general framework , right ? And so I think , like one of the these instantiation is through , like top K semantic search and being able to get back the results . But there's other approaches that we can do , too . And that really is because we think there's just a tonne of complexity in this area of , like , retrieval and synthesis . Uh , you know , on top of your data using language models and we wanna provide , uh , users the tools to solve that . So let's just walk through , like a few basic examples . One is , um , let's say you want to do summarization over your documents . Uh , right . And so just like , um , first , just naive , like top K and batting look up is not gonna just give you the right documents , uh , to do summarization over any source you're gonna need , uh , some sort of other , um philtre index basically to retrieve the relevant documents . The next part is , let's say you want to , um , ask a question , and you don't want to basically , um , know beforehand whether or not that question is gonna be a summarization question or like a look up question . And so one thing we provide you is actually some tools to provide , like a unifying query interface . You ask a question at the top level , we can , um we show tutorials on how you can actually define like , a router to basically route this question to a list index . Uh , which performs summarization or like a vector index which performs top K . Embedding Look up so that that's something you can do with lama Index . And and that's something you can do pretty easily . Uh , without too much that would work , uh , another , uh there . So So I could keep going there . There's actually a bunch of stuff , but just like to like , put it a bit more concisely . Um , we also provide support with structure data . And so , um , we also provide support with defining graphs for different types of retrieval use cases . Um , and so , uh , for instance , uh , depending for structure index , you could do taxes equal if you choose to define graphs or relationships between nodes . Um , you can actually do more complicated referent synthesis modes like , for instance , being able to compare different documents together , being able to kind of of like , um , you know , synthesised stuff across different documents and and being able to ask , kind of , like , repeated questions over the different document sources , uh , or or Sorry , uh , break down a complex question into some into simpler questions over like , different document sources . So we provide a lot of these modules within lama Index . And the way I would think about it Is each tool set or building block , for instance , like an index module , or like , a query module , Or , you know , any other module that we provide pro , Uh , each module provides , like , a certain way of kind of getting certain results out of your data and so that that , by itself , is very interesting . And then the question is , um , how do you actually build some sort of like unified query interface over this data ? So , uh , uh , because you have each module which solves like a slightly different use case , like you'll call the structured index for , uh , SQL database . You know , you'll call list , uh , do a list index for summarization . Um , maybe a keyword look up table . And so then , you know , one of the questions is how do we abstract away some of this complexity . Like , you know , you want to ask a question ? How do we figure out what's like the right index to route to and then answer the question for you ? Um and so we provide some of these initial tutorials , um , on on how to do that . But that very much is both kind of like , uh , within some of the short term like functionality of lama index . But also part of our long term vision is , you know , given just the set of documents and given just , like a variety of questions that you might want to ask over the the these documents , What is the right state to build over these documents that allows you to perform different modes of retrieval and synthesis ? And then how can you , um how how can you basically , uh , kind of provide this unified query interface so that you can ask a question ? And then it can route to the relevant , uh , a you know , documents , uh , through the state that we define .\",\n",
       "  'We have a large section of different use cases for Llama Index. Think of it as a general framework that takes in a query and gives a response along with a set of retrieve sources. We manage interactions between your language model and your data for optimal results. We provide tools for summarization and unifying query interface. We also support structured data and defining graphs for different retrieval use cases. Each module provides a certain way of getting results from your data. We provide guidance on building a unified query interface. Our long-term vision is to build a state to perform retrieval and synthesis for different modes.'),\n",
       " ('Jithin',\n",
       "  'Got it , Got it , So OK , got it . Got it . So that is that is OK . That is the end goal that you guys are building towards . But you also mentioned the query router , right ? Does that is that , like the router that actually map like routes , the different types of , uh , queries that one could possibly possibly ask Lama Index or your documentation ? Is that part of the , like , the solution that you have currently ?',\n",
       "  'Understood. So the end goal you are building towards is the query router. Is this the router that maps the different types of queries one could ask Lama Index or consult in your documentation? Is this part of your current solution?'),\n",
       " ('Jerry',\n",
       "  \"Yeah . So you can actually do that in a way within Lama index right now . And we're kind of thinking about ways to make it more general . One way to think about it is that a router is basically , like a kind of AAA node . Um , that has , like , links to a bunch of other , uh , indexes . And so if you view it from that perspective , you can basically use our compos ability framework where you can compose this like router index basically using the tree index . Actually , uh , because the tree index basically just picks one of the Children and then , um , you can , uh , use that , and then it will be able to take a question and route it to the underlying index , and then the underlying index can solve that problem . Um , one way to think about it is basically like a mini agent . That's kind of like but physically for being able to choose which index is is the right task for the job .\",\n",
       "  'Yes, you can currently achieve this within Lama using the index router. We are exploring ways to make it more universal. Essentially, a router is like an AAA node with links to multiple indexes. With our composability framework, you can use the tree index to create a router index and route questions to underlying indexes for problem-solving. It functions like a mini-agent that selects the appropriate index for the task at hand.'),\n",
       " ('Jithin',\n",
       "  'Got it , got it , Got it , man , That , like all that itself , is all the technical stuff itself is an interesting thing . But I would be highly recommend people to check out the documentation because the dogs also have a lot of , uh , like , specific tutorials where specific use cases where you can actually try all of this stuff out . So do check the dogs for that . But like switching back to again an overall picture like what have like , there has been a lot of interesting use cases and a lot of interesting things that people have built right , And part of it is because you can easily get started with Lama Index because of in a couple of lines of code . So what has been some of the interesting , uh , use cases that you have seen from the community that people have built ?',\n",
       "  '\"Understandable, the technical aspects are interesting. However, I highly recommend checking out the documentation which offers specific tutorials for various use cases. Do refer to the documentation for that. Overall, there have been many fascinating use cases and creations built with Lama Index due to its ease of starting with just a few lines of code. What are some of the interesting use cases you have seen from the community?\"'),\n",
       " ('Jerry',\n",
       "  \"Yeah , definitely . I think , you know , in the beginning , um , and and even to this day , like a lot of llama index is really just being able to to give get you up and running really quickly over your own private data sources . And so a lot of the interesting use cases is over the diversity of data sources . So , for instance , like , you know , from standard stuff like PDF s to , like web pages to , uh , even other stuff , like videos and and audio . Like we we were one of the first , uh , projects to basically create , uh , like some sort of video to text , uh , parser use , uh , from one of our contributors . And then , um , basically , you can now , like , you know , transcribe video to text and then , uh , ask questions over this text . Um , and so even for that simple use case , it's It's quite interesting , because if you think about it , it , like , simplifies the data landscape by by quite a bit . Um , because all of a sudden , through a three step process , you could transform just raw , unstructured data into text index text as , like , a central data format , and then ask questions over it . You don't What ? You don't actually need to do a tonne of , like , complicated parsing like processing . Right ? Uh , like , e t . L like being able to , uh , write a bunch of like this custom logic . You can just , you know , trust that you can store text in an unstructured format and then ask the questions over it using a language model . So I find that part , uh , pretty fascinating , actually . Um , I think nowadays there's a lot of , like excitement about agents and and especially stuff around like , you know , baby , a G i o g BT , like all all the stuff that can do repeated stuff for you . Um , I I would say some of that stuff is more just , um it's kind of like an interesting demo as opposed to offering , like , practical value . Um , but at the same time , um , there's a lot of , um , we We also realise there's a lot of like very similar concepts that you can build pretty easily with just the lama indexes , like , uh , some of the core primitives that we offer with , like , indexes and factor stores . And so , for instance , like if you think about what are the components of , like a a g I system , there's like task management . There is , um , being able to , um , like index into like , the memory of of like , previous actions , there's being able to plan new stuff . And so a lot of these modules you can actually represent with simple , like lama index data structures and then be able to create some , uh , a g I you know , baby , a G I type , uh , model yourself . And so we , um I think some of our contributors have basically built these , like , cool a g I demos . Uh , they can actually find both in the repo as well as streamline . Um , And then we also have some other , uh , contributors building this too . Like one of our contributors built this like medical Asian research by which I thought was super interesting . You know , using llama Index itself .\",\n",
       "  'Definitely. Llama Index helps you quickly access your private data sources, including diverse formats like PDFs, web pages, videos, and audio. We even developed a video to text parser, which simplifies the data landscape. You can transform raw, unstructured data into text format and ask questions over it without complicated processing. This offers practical value compared to some repetitive agent systems. Llama Index also offers core primitives, like indexes and factor stores, which can create AGI models. Our contributors built cool AGI demos, including medical research, which are available in our repository or Streamline.'),\n",
       " ('Jithin',\n",
       "  \"Yeah , that was super creative . And the baby , like all the S f , also has been , like pretty interesting , But they are at a phase where it's like , more of an interesting demo . But you use cases are pretty hard . But right now , all the basic components are there , like to build , like OK , even if you think of it as a human brain , it has memory . It has the ability to plan stuff , So yeah , who knows ? Maybe something interesting will be there , but but one , like core of this is right . Like , OK , so say if I'm a builder , I'm building a product , Um , that spot , like , solve some problem with , um , like with other lama index and other frameworks . Because lama index does its job like too Well , because I don't have to worry about loading . I don't have to worry about how effective the thing is . Uh , like for someone building a product that creates like it , it brings the question of where I can build by mode because , OK , the technology side is fairly easy to set up with all these pipelines . But then so I have to figure out where exactly my mode is . It has . It has to be something other than tech . How do you think about that ? Uh , that problem .\",\n",
       "  '\"That was creative, and the S f baby has also been interesting, but currently in a phase of being an interesting demo. However, the basic components are present - it has memory and planning abilities, much like a human brain. There may be something interesting to come of this, but the core is strong. For a builder creating a product, using lama index and other frameworks can solve problems without worrying about loading or effectiveness. The challenge then becomes determining where to build the model, which needs to be something other than just technology.\"'),\n",
       " ('Jerry',\n",
       "  \"Yeah , it's a good point . I mean , I think I think there still is something to be said about , um , the technical side . Uh , but to be fair , I think we we , uh , uh , want to offer a lot of the core tech that powers , uh , you know , the the Q A systems for users . Basically , so that they don't have to worry about it . Um , I would say again going back to , like , basic semantic search like it works well to get that up and running . There's a bunch of stuff that we want to add to make sure that , uh , that you can solve the broader classic queries and just pure kind of , like , fact based look up questions . Um , but we can take responsibility for that . And then in the meantime , I do think that , um , the application layer should not be , uh , just branded as , like , a thin wrapper around a I because I just think all of those applications are not gonna have , like , the French bull notes . Um And the reason is because , like , if you just look at any use case emerging , uh , on like a novel application with , uh , a I , for instance like even the tax sequel stuff like , uh , a few months ago or even with , um , kind of being able to , you know , build a trap over your PDF . The underlying tach , for that is honestly pretty simple and and can be open sourced right by our project , or or by line , train or other projects , too . And so I I do think it's like the application developer can , um , like , depend on the tech like the tech will probably be open source . It will be like widely discussed within the community . But they need to build , like , differentiated product , high level U axes that really encourage , like , kind of , like , different user interactions and and modes of behaviour . Uh , you know , that , uh , with the A I enabled application . And so I think it's less about , like , engineering some fancy algorithm , right ? If you're an application builder and more about thinking just fundamentally right , like , what are the best u axes for interacting with a i systems these days ? And if you look at , like the the spectrum of like a I enabled applications , there are certain use cases that people really do enjoy using . And then there's certain use cases that do , um , feel a little bit more like an add-on feature . Uh , for instance , I was thinking about this the other day . One thing that I really do enjoy using , right , just because it's like a really nice U X is like copilot , like just the fact that you don't even have to press a button . You just , like , write a line code , wait like half a second , and you just you just press tab to complete . It is just very nice . Um uh , versus Like I I think if you tried to build , for instance like , um , some sort of agent based system Where , um But like , you know , uh , you you like , maybe didn't set up a product . You act in a way that's , like , nice or pleasing . Then you know , there are a lot of downsides of building , uh , to trying to depend on albums , too . Like , for instance , they're very expensive and also very slow . And so , if you like , pass that on to the end user , they might get frustrated with the experience , too .\",\n",
       "  \"It's a good point that we should offer the core tech for Q&A systems to users without them having to worry about it. Semantic search works well for basic queries, but we want to add more to solve broader queries. We can take responsibility for that. The application layer shouldn't be just a thin wrapper around AI. Developers need to build high-level U axes that encourage different user interactions. It's less about fancy algorithms and more about thinking about the best ways to interact with AI systems these days. There are certain use cases that people enjoy using, like copilot, and others that feel like an add-on feature. Depending on algorithms can have downsides like being expensive and slow, which can frustrate users.\"),\n",
       " ('Jithin',\n",
       "  \"Got it ? Got it . Yeah , that is true . Basically , yeah , the build as builders we have to think of like what other ? U X is one definitely one mode like I also like notions . U X because the whole it's it's not . It is basically text , but the whole where you can select all the text , interact , ask more questions and like it is more much more intuitive than like GP interface itself . But yeah , the u X the data , all the other sides become becomes even bigger . More than the technical side . Yeah , that is true . But coming to the project itself and the future of the project Like what ? What do you have in store ? Like , what are the like , the most in the like in a on a like , a short term thing ? What is , like , the most interesting things that you're working on . You're told that there is a , uh , 0.6 release coming in . So I thought I guess you are pretty excited for that . But even after that , what are like some of the couple of projects that you are pretty excited about ?\",\n",
       "  \"Understood. UX is a crucial aspect for builders to consider. It is more intuitive than GP interface and allows for text selection, interaction, and questioning. The data and other aspects of UX become even more significant. Regarding the project's future, what short-term plans are in store? What projects are currently exciting?\"),\n",
       " ('Jerry',\n",
       "  \"Yeah . So So , uh , I'm glad you asked that question . So I think 0.6 should have landed . Uh , by the time that you heard in this podcast , and so , um , and I think it's actually related to the previous question , which is about , um , you know , how do you offer the best ergonomics and you ask for the end application developer you our tools . And so I think one thing that I would say lama index does very well these days is the fact that it offers , like , this high level API gets you started three lines of code , right plug in your favourite data loader index it Now you can ask a question over it and then get back an answer . Uh , and so you can do that super easily . I think one thing that we have been , uh , the reason this like re factor is happening is because we want to solve both the simple use cases as well as the more advanced and customizable use cases . And the more advanced you go typically , Uh , especially if you're a more advanced developer . You'd want a bit more modularity , customizable flexibility and being able to kind of like define your own , like core modules and various components of the code base in order to implement kind of like your own . You know , retrievers , synthesisers , all these different types of things . And so that is very much what the reactor is going for . Um , that is step one , which is just , uh , from kind of like this very high level , very easy to use API to really exposing these , like modular components that people can very easily extend and build on top of themselves so that they can define their own tech . Uh , for retrieval and synthesis if they want . And this is something that a lot of users have asked uh , asked for . The next part is the fact that we want to make , um our tool set a bit more principled and and more enterprise ready . And a lot of that becomes , uh , uh , uh comes with thinking a lot about how do we best source state ? Um , because you can almost think about llama index . Uh , one view of it is like some sort of state full service management for your language model application . Right , Because we are dealing with , uh , kind of managing state for your application . Language models are inherently stateless . Uh , at least right now they are , Uh , uh , you know , like GP t three GB four . Who knows about the future . But basically , you have some input . You got some output . And if you want to operate it on top of your data , the question is , how do you define the views on this data ? How do you maintain the state so that you can use it with a language model ? And so we're really trying to create , like , good abstractions for that ? Uh , both , Like , conceptually but also provide good integrations with , you know , the databases , like factor stores object stores that you would typically use . And so there's just a lot of kind of like , uh , storage , um , abstractions that we're trying to clean up and and make this a lot more enterprise . Right ? Um , I think going ahead in the future , there is just like some general components that we're super excited to to talk about . Uh so , for instance , um , one idea is continuing to push on this idea of , uh , having this out of the box , like unified query interface for users . Imagine , for a user , they can set up the service that they can deploy on top of their data . And we abstract away a lot of the complexity about Oh , you know , we should define these indexes over this data , or we should define , like , these configurations or this mode of interaction . Um , we can configure a lot of that automatically and then given a top level question , uh , heading this API service , we can figure out , you know , the right interactions , the right services to call in order to give you back the response that you would want . Um and so a lot of this is going to be very interesting , because I really think this ties into this idea of this , like , broader agent style interactions with your data . But I I think here it's it's interesting , because I almost I I imagine this idea of like , um different , kind of like mini like data agents over your data system . Um , and being able to pick like each data agent is specialised at , kind of like picking the right tool for its job . Uh , as a very simple instantiation . Right now , you can think about our router abstraction , right ? As as , like , a mini data agent that can route a query to the right sub index . Uh , so you can imagine that . And then you can imagine , like , sub data agents , too , that could , like , you know , figure out the right parameters , insert , uh , to to query a vector store with , uh , and then all these different kind of services interacting with each other , uh , under the hood . Uh uh , and and kind of again , providing this clean , high level interface to the end user . Uh , hopefully , some of that made sense . But the high level idea is that we're trying to create this , like idea of just abstracting away a lot of the retrieval , synthesis , complexity and and taking advantage of all the latest techniques that are happening these days to build that kind of unified prayer experience for users . Um , and at the same time , you know , from the community side offering the building blocks for more advanced users to kind of customise and and create different components themselves .\",\n",
       "  \"I'm glad you asked that question. I think 0.6 should have landed by the time of this podcast. Lama index offers a high-level API that gets you started in three lines of code, making it easy to ask questions and receive answers. We are refactoring to solve both simple and advanced use cases, with more modularity and customizable flexibility. Our tool set is becoming more principled and enterprise-ready by creating good abstractions for state management and integrating with databases and object stores. We are excited about the idea of a unified query interface for users, where we can configure interactions automatically and provide a clean, high-level interface. The high-level idea is to abstract away complexity and offer building blocks for advanced users to create different components themselves.\"),\n",
       " ('Jithin',\n",
       "  \"Got got it . So in the short term , you guys are exposing , like , exposing all the small components that you can as a as a user that you have you want to modify so that your index becomes more performant on your specific data set . But on the long term , you don't have to even care about modifying that . It'll it'll be just you . You'll you'll give your data to Lama Index , and that will take care of building the right indexes , the right set of agents that can like abstract abstract away all the complexities that , like all the complexities and configurations that you might need and then just it's just one end point . You ask , you ask your questions and that's it . You get to get the answers you want ,\",\n",
       "  \"Got it. In the short term, you expose and modify small components to optimize your index for specific datasets. In the long term, you don't need to worry about modification. By giving your data to Lama Index, it will build the right indexes and agents to simplify the complexities and configurations. Then, you can use a single endpoint to ask your questions and get the answers you want.\"),\n",
       " ('Jerry',\n",
       "  \"exactly , and I think it's honestly very powerful because it's a very hard technical problem . But I we do think it will provide a tonne of value because just if you think about the average developer , like a person who's trying to understand all this , uh , data and figure out , how do I like ? Hook this all up with the language model and figure out what the best like interaction pattern is that there's just a tonne of complexity there , and I think we we want to make that as easy as possible .\",\n",
       "  '\"I believe it\\'s a powerful solution to a challenging technical problem. It will offer great value to developers who struggle with data and language model integration for optimal interaction patterns. Simplifying this complexity is our goal.\"'),\n",
       " ('Jithin',\n",
       "  \"Got it , Got it , got it now as like OK , so you you also mentioned that there there are a lot of community community like contributions also . But as someone who is interested in who's interested in Latics and want to contribute what are like some of like , there are a lot of different models , and there there is a lot of different moving parts right now , like as a as a contributor , where would like the best place be ? What are the different areas that I could help and like , Yeah , what are some of the areas that people could help\",\n",
       "  'Okay, so you mentioned that there are many community contributions. As someone interested in contributing to Latics, where would be the best place to start? There are various models and moving parts right now, so what are some areas that need help?'),\n",
       " ('Jerry',\n",
       "  \"That's a great question . So we have a contribution guide in the core ribo that consists of a tonne of different module modules that are pretty independent and are are relatively easy to contribute to . So just as a very basic example , we have data loaders . Uh , those are just massively paralysed . You know , we have , like , over 80 data loaders right now , but we could always add more . There's just like hundreds of different services that we we still want to integrate with . Um , there's other modules on that list . So , for instance , like retrieval modules , Uh , especially 10.6 auto comes out , uh , as kind of , um uh , query engine modules . Uh , like , uh , optimizers being able to reduce token usage . Uh , post processors , uh , tax splitters , evaluation modules , Um , like output parsers . And so all that stuff is described in the doc . All of those are pretty independent , and those things are , uh , pretty easily accessible . In general , there's , like , the kind of , like core features that we we'd love to uh , uh , contributions from . And then there's also just , like general , kind of like bug fixes , like documentation updates we all always welcome , like cool experiments , too . We have this new rebook called Lama lab , which is just a cool showcase for different types of , you know , like Asian based experiments or any other experiments . And so if you're interested in contributing any of those projects would highly encourage you to The other thing is , we're doing like , a t-shirt giveaway . I think we we've , like , met mentioned this is that if you do contribute like a core model , you'll get a fresh , limited edition lama index t-shirt . And so that could be , uh , a nice little gift to to have\",\n",
       "  'We have a contribution guide in the core repository with independent modules that are easy to contribute to. For example, we have over 80 data loaders that can still be expanded. There are many services we want to integrate with, including retrieval, query engine, optimizers to reduce token usage, post processors, tax splitters, evaluation, and output parsers. All of these are described in the documentation. We welcome contributions to core features, bug fixes, documentation updates, and cool experiments. Our new showcase, Lama lab, welcomes Asian-based or other types of experiments. Contributors to core models receive a limited edition Lama index t-shirt.'),\n",
       " ('Jithin',\n",
       "  \"motivation , but yeah , that will be pretty . That that is pretty good . But yeah , there are different lot of components that are there overall , because index by itself , it's a very new project . So there will . There will be a lot of lot of components that are moving a lot of opportunity for people to dive in . And yeah , and the growth has been pretty , like , pretty crazy , uh , for an open source project . So yeah , a lot of opportunities for contributions from outside , but as a as a as a project , right ? Like because , um , as more and more people are coming in . More and more people are using it in enterprise solutions . We want , um , more like , because right now , you you it started off as a hobby project . But more , more people are becoming , like , dependent on it as an enterprise . So do you ? Like what ? Like , what are some of , like the enterprise use cases that you have it in mind like , um , some ? Yeah , some things related to that .\",\n",
       "  'There are many components involved in the new Index project, which presents numerous opportunities for contributions. The project has experienced significant growth, making it a viable enterprise solution. As more people become dependent on it, there is a need to identify specific enterprise use cases for the project.'),\n",
       " ('Jerry',\n",
       "  \"Yeah , it's a good question . I think a lot of , um , the enterprise use cases arise from , um , things that are complementary to the open source project . And so , for instance , um , we're basically dealing with , uh , data and management and and query , right ? And and so , um , as data volumes scale up , especially if you're dealing with , like , gigabytes or terabytes of data , then you know , you might want to rely on , like a hosted service that can manage a lot of this for you instead of you just being able to run this locally on your laptop . And so that's , uh , very much something that we're thinking about . The next part of this is , uh , because we're dealing with data . There's a certain like table stakes enterprise features that pretty much like every , uh , kind of , like hosted offering needs to have , which is , like , data access control , like two , you know , user management , a lot of this type of stuff . And And I think , um , that's something that wouldn't make sense in the open source project anyways , Uh , just because , uh , that's something that you should be able to easily download and install . I think , um , there are a few additional things that we're thinking about , For instance , like just being able to really optimise , um , between different model use cases . Like , if we're we're able to profile like the performance of a model . Given your cost budget , being able will provide you a service that can , you know , uh , really understand and profile your model performance and allow you to pick the best , like inventing model language model and other types of services over your data and use case . Then some of that might make sense in the enterprise version as well .\",\n",
       "  \"It's a good question. Enterprise use cases relate to things that complement the open source project. Data management and query are key factors to consider, especially for managing large volumes like gigabytes or terabytes of data. It may be beneficial to rely on hosted services rather than running the data locally. Table stakes enterprise features like data access control and user management are necessary for most hosted offerings, but not needed in the open source project. Additionally, optimizing between different model use cases is important and providing a service that profiles model performance can benefit the enterprise version.\"),\n",
       " ('Jithin',\n",
       "  \"Uh , like , just out of curiosity , like , have , like , have you , uh , had , like , experiences where ? Like , OK , right now , people start off with a very general l l m R A very general embedding . But has there been use cases where , uh , fine tuning your fine tuning your embedding has given better . Like even though it's smaller . So it's cheaper to run but has given , uh , like like all at performance are better .\",\n",
       "  \"Out of curiosity, have you had experiences where fine tuning your embedding has resulted in better performance even though it's smaller and cheaper to run?\"),\n",
       " ('Jerry',\n",
       "  \"I think that's actually very much an open question right now . So if that's something you're interested in contributing , I would love to see more experiments on that end . I think people are still actually trying to figure that out . I personally would be very interested in a distilled model that's very good at retrieval and not necessarily good at knowledge . And so something that's way faster , cheaper but is still very , very good at reasoning and retrieval . Um , so I love experiments on that end , Uh , I I think in terms of , um , embedding models , I think like by default , we use open A I I think open A . I like language . Models are still just the probably the best . GP t four is just very , very good , I think embedding model wise , I've seen a lot of people start to use , like hugging face and and just like , you know , uh , like other types of embedding because you know , if you're just trying to do some , like , cheap , uh , semantic embedding you don't necessarily need , like , the full power of , like , this giant model anyways . And so that's something to , um that is very interesting to explore . Is to what extent can you , like , trade off a little bit of performance for just , like , free cost , right , and and something of an embedding model that's locally hosted .\",\n",
       "  \"It's an open question. If you're interested in contributing, more experiments are needed. People are still figuring it out. Personally, I'm interested in a fast and cheap retrieval model that's good at reasoning. OpenAI and language models are popular, with GPt-4 being one of the best. For cheap semantic embedding, Hugging Face and other types of embeddings can be used. It's interesting to explore the tradeoff between performance and cost for a locally hosted embedding model.\"),\n",
       " ('Jithin',\n",
       "  \"Got it . All right . I guess we have to wind up since it's almost time . But we covered a lot of interesting topics , and all the all the all the docs and the links that we discussed will be given in the description . So you guys do check it out if you want to know more . And also the lama index discard . Community is always there . So you can carry out this conversation . And if you have any more doubts , you can directly pop in there and ask . So with that , Thank you so much , Jerry , it was really fun talking to you . We got We got a lot of insights and ideas about where the project is going to go and what the main components are . And yeah , so pretty excited to see what what happens in the future . And your fingers crossed .\",\n",
       "  \"Understood. We should wrap up soon as it's nearly time. We covered interesting topics and provided all relevant docs and links in the description for further reading. The LAMA Index community is available for ongoing discussions and any remaining questions. Thank you, Jerry, for the insightful conversation. We gained valuable insights and ideas about the project's direction and components. Excited to see what the future holds. Fingers crossed.\")]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "prompt = \"\"\"\n",
    "fix gramatical errors in the following text and make it more concise.\n",
    "The output should only have the corrected text:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "edited_transcription = []\n",
    "for speaker, spoken_text in tqdm(cleaned_transcription):\n",
    "    msg = prompt.format(text=spoken_text)\n",
    "    resp = chat([HumanMessage(content=msg)])\n",
    "    edited_transcription.append((speaker, spoken_text, resp.content))\n",
    "    \n",
    "edited_transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8fb6caf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>Jithin</b>: Welcome to the Explored Ingredients podcast. Today, we have Jerry Luke, the creator of Lama Index, a framework that connects large language models with your data. This framework has numerous use cases, such as building a customer service board or extracting information from unstructured data. It can also accelerate learning by passing knowledge from books, blogs, podcasts, and videos. Jerry discusses how he started the project, its core concepts, and the vision for the long and short terms. The framework is growing at a rate of almost 200% month over month, and Jerry explains how listeners can contribute and join the movement. It's great to have Jerry with us, and we discuss how he keeps up with the rapidly evolving field."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jerry</b>: Well, first off, thank you for having me. I'm thrilled to be here. There's a lot of AI activity happening across various verticals, resulting in numerous new research papers and publications being featured on platforms such as Hacker News, Twitter, and LinkedIn. It's exciting to witness all the builders in this space adding cool demos and features. Though I've been quite busy, it's been an energizing experience due to the constant buzz of activity."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jithin</b>: \"That's true. Any tips for catching up? The high rate of revision has made it very challenging. What's your strategy?\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jerry</b>: \"I spend an undue amount of time on Twitter and Hacker News. I browse articles and market maps that show the current state of AI. That's basically what I track the most.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jithin</b>: Okay, I understand. But about the Lama Index, I'm curious about its initial stage and founding story. Can you briefly share the problem you were trying to solve and how it became the big project it is today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jerry</b>: Yeah, totally! I started Lama Index a few months ago back in November as a tool to solve the context limit problem for language models. The aim was to build novel structures that could index a larger corpus of data and still feed in the relevant sections to the language model. As a thought exercise, I came up with the idea of a tree index that the language model could use to traverse large amounts of information. The project grew as more people became interested, and now Lama Index offers a variety of tools around data injection, indexing, and query, as well as supporting modules for evaluation, output parsing, and token usage optimization."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jithin</b>: Awesome! One thing that I found interesting is that there are two big projects in the space: Lama Index and Both. You and Harris have a lot in common, including robust intelligence. Both of you have worked for the same company and built impactful projects. Was there something behind this coincidence?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jerry</b>: I think it was a coincidence that both of us were interested in space. I had dinner conversations with Harrison in October about language models, and we bounced around some fun ideas. It's funny how this turned out with two big open source projects now - Llama Index and Lane Train. Harrison used to be my coworker, so we have a relationship."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jithin</b>: Nice! So, about the lama index and initial stories - did you build out the first three? The first attempt was the three index, but how was the hat? Over time, you added other indexes. But now, what are the most effective go-to indexes in the basic pipeline?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jerry</b>: There is a clear difference between the project's initial design exercise and its current goal of offering practical value to users. The initial idea was a tree index that used the language model to process, organize, and traverse information. However, it had practical limitations and errors that increased with the depth and size of the tree. Nowadays, most people start with a vector organization or embedding-based approach to retrieve documents. From there, the documents can be put into input, prompt, and synthesized to produce a response."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jithin</b>: I've been surprised by how effective simple embeddings can be. With quick data filtering and cleaning, the results are usually quite good and can provide an 80-90% solution for production."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jerry</b>: I believe that the tool is easy to use, however, it is common for most people to begin with an embedding approach when starting a project. I caution against relying solely on this technique as it may result in overfitting. Although embedding similarity can be effective for fact-based retrieval or when the information can be found in a specific document section, there are cases where a more structured approach is necessary for querying data. This is where Lama index's generalization of the retrieval and synthesis framework comes into play, including the idea of top-case semantic search as just one example."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jithin</b>: \"I have a follow-up question. Could you provide some examples to further elaborate on the topic and other solutions for people to build on using different techniques?\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jerry</b>: We have a large section of different use cases for Llama Index. Think of it as a general framework that takes in a query and gives a response along with a set of retrieve sources. We manage interactions between your language model and your data for optimal results. We provide tools for summarization and unifying query interface. We also support structured data and defining graphs for different retrieval use cases. Each module provides a certain way of getting results from your data. We provide guidance on building a unified query interface. Our long-term vision is to build a state to perform retrieval and synthesis for different modes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jithin</b>: Understood. So the end goal you are building towards is the query router. Is this the router that maps the different types of queries one could ask Lama Index or consult in your documentation? Is this part of your current solution?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jerry</b>: Yes, you can currently achieve this within Lama using the index router. We are exploring ways to make it more universal. Essentially, a router is like an AAA node with links to multiple indexes. With our composability framework, you can use the tree index to create a router index and route questions to underlying indexes for problem-solving. It functions like a mini-agent that selects the appropriate index for the task at hand."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jithin</b>: \"Understandable, the technical aspects are interesting. However, I highly recommend checking out the documentation which offers specific tutorials for various use cases. Do refer to the documentation for that. Overall, there have been many fascinating use cases and creations built with Lama Index due to its ease of starting with just a few lines of code. What are some of the interesting use cases you have seen from the community?\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jerry</b>: Definitely. Llama Index helps you quickly access your private data sources, including diverse formats like PDFs, web pages, videos, and audio. We even developed a video to text parser, which simplifies the data landscape. You can transform raw, unstructured data into text format and ask questions over it without complicated processing. This offers practical value compared to some repetitive agent systems. Llama Index also offers core primitives, like indexes and factor stores, which can create AGI models. Our contributors built cool AGI demos, including medical research, which are available in our repository or Streamline."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jithin</b>: \"That was creative, and the S f baby has also been interesting, but currently in a phase of being an interesting demo. However, the basic components are present - it has memory and planning abilities, much like a human brain. There may be something interesting to come of this, but the core is strong. For a builder creating a product, using lama index and other frameworks can solve problems without worrying about loading or effectiveness. The challenge then becomes determining where to build the model, which needs to be something other than just technology.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jerry</b>: It's a good point that we should offer the core tech for Q&A systems to users without them having to worry about it. Semantic search works well for basic queries, but we want to add more to solve broader queries. We can take responsibility for that. The application layer shouldn't be just a thin wrapper around AI. Developers need to build high-level U axes that encourage different user interactions. It's less about fancy algorithms and more about thinking about the best ways to interact with AI systems these days. There are certain use cases that people enjoy using, like copilot, and others that feel like an add-on feature. Depending on algorithms can have downsides like being expensive and slow, which can frustrate users."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jithin</b>: Understood. UX is a crucial aspect for builders to consider. It is more intuitive than GP interface and allows for text selection, interaction, and questioning. The data and other aspects of UX become even more significant. Regarding the project's future, what short-term plans are in store? What projects are currently exciting?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jerry</b>: I'm glad you asked that question. I think 0.6 should have landed by the time of this podcast. Lama index offers a high-level API that gets you started in three lines of code, making it easy to ask questions and receive answers. We are refactoring to solve both simple and advanced use cases, with more modularity and customizable flexibility. Our tool set is becoming more principled and enterprise-ready by creating good abstractions for state management and integrating with databases and object stores. We are excited about the idea of a unified query interface for users, where we can configure interactions automatically and provide a clean, high-level interface. The high-level idea is to abstract away complexity and offer building blocks for advanced users to create different components themselves."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jithin</b>: Got it. In the short term, you expose and modify small components to optimize your index for specific datasets. In the long term, you don't need to worry about modification. By giving your data to Lama Index, it will build the right indexes and agents to simplify the complexities and configurations. Then, you can use a single endpoint to ask your questions and get the answers you want."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jerry</b>: \"I believe it's a powerful solution to a challenging technical problem. It will offer great value to developers who struggle with data and language model integration for optimal interaction patterns. Simplifying this complexity is our goal.\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jithin</b>: Okay, so you mentioned that there are many community contributions. As someone interested in contributing to Latics, where would be the best place to start? There are various models and moving parts right now, so what are some areas that need help?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jerry</b>: We have a contribution guide in the core repository with independent modules that are easy to contribute to. For example, we have over 80 data loaders that can still be expanded. There are many services we want to integrate with, including retrieval, query engine, optimizers to reduce token usage, post processors, tax splitters, evaluation, and output parsers. All of these are described in the documentation. We welcome contributions to core features, bug fixes, documentation updates, and cool experiments. Our new showcase, Lama lab, welcomes Asian-based or other types of experiments. Contributors to core models receive a limited edition Lama index t-shirt."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jithin</b>: There are many components involved in the new Index project, which presents numerous opportunities for contributions. The project has experienced significant growth, making it a viable enterprise solution. As more people become dependent on it, there is a need to identify specific enterprise use cases for the project."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jerry</b>: It's a good question. Enterprise use cases relate to things that complement the open source project. Data management and query are key factors to consider, especially for managing large volumes like gigabytes or terabytes of data. It may be beneficial to rely on hosted services rather than running the data locally. Table stakes enterprise features like data access control and user management are necessary for most hosted offerings, but not needed in the open source project. Additionally, optimizing between different model use cases is important and providing a service that profiles model performance can benefit the enterprise version."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jithin</b>: Out of curiosity, have you had experiences where fine tuning your embedding has resulted in better performance even though it's smaller and cheaper to run?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jerry</b>: It's an open question. If you're interested in contributing, more experiments are needed. People are still figuring it out. Personally, I'm interested in a fast and cheap retrieval model that's good at reasoning. OpenAI and language models are popular, with GPt-4 being one of the best. For cheap semantic embedding, Hugging Face and other types of embeddings can be used. It's interesting to explore the tradeoff between performance and cost for a locally hosted embedding model."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<b>Jithin</b>: Understood. We should wrap up soon as it's nearly time. We covered interesting topics and provided all relevant docs and links in the description for further reading. The LAMA Index community is available for ongoing discussions and any remaining questions. Thank you, Jerry, for the insightful conversation. We gained valuable insights and ideas about the project's direction and components. Excited to see what the future holds. Fingers crossed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in edited_transcription:\n",
    "    speaker, _, text = i\n",
    "    display(Markdown(f\"<b>{speaker}</b>: {text}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4258ecdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
