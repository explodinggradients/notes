

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>020 Better Bayesian Filtering &#8212; Notes</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'LlamaIndex/essays/020_better_bayesian_filtering';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    <p class="title logo__title">Notes</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    <no title>
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../mf-guidence/README.html">Guidence</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../mf-guidence/guidence101.html">Guidence 101</a></li>
</ul>
</li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../Langchain/README.html">Langchain Internals</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../Langchain/prompts.html">Prompts</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Langchain/chains.html">Chains - Intro</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../Langchain/evaluations.html">Evaluations - Intro</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FLlamaIndex/essays/020_better_bayesian_filtering.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/LlamaIndex/essays/020_better_bayesian_filtering.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>

</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>020 Better Bayesian Filtering</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="better-bayesian-filtering">
<h1>020 Better Bayesian Filtering<a class="headerlink" href="#better-bayesian-filtering" title="Permalink to this heading">#</a></h1>
<p>January 2003</p>
<p><em>(This article was given as a talk at the 2003 Spam Conference. It describes the work I’ve done to improve the performance of the algorithm described in<span class="xref myst">A Plan for Spam</span>, and what I plan to do in the future.)</em></p>
<p>The first discovery I’d like to present here is an algorithm for lazy evaluation of research papers. Just write whatever you want and don’t cite any previous work, and indignant readers will send you references to all the papers you should have cited. I discovered this algorithm after ``A Plan for Spam’’ [1] was on Slashdot.</p>
<p>Spam filtering is a subset of text classification, which is a well established field, but the first papers about Bayesian spam filtering per se seem to have been two given at the same conference in 1998, one by Pantel and Lin [2], and another by a group from Microsoft Research [3].</p>
<p>When I heard about this work I was a bit surprised. If people had been onto Bayesian filtering four years ago, why wasn’t everyone using it? When I read the papers I found out why. Pantel and Lin’s filter was the more effective of the two, but it only caught 92% of spam, with 1.16% false positives.</p>
<p>When I tried writing a Bayesian spam filter, it caught 99.5% of spam with less than .03% false positives [4]. It’s always alarming when two people trying the same experiment get widely divergent results. It’s especially alarming here because those two sets of numbers might yield opposite conclusions. Different users have different requirements, but I think for many people a filtering rate of 92% with 1.16% false positives means that filtering is not an acceptable solution, whereas 99.5% with less than .03% false positives means that it is.</p>
<p>So why did we get such different numbers? I haven’t tried to reproduce Pantel and Lin’s results, but from reading the paper I see five things that probably account for the difference.</p>
<p>One is simply that they trained their filter on very little data: 160 spam and 466 nonspam mails. Filter performance should still be climbing with data sets that small. So their numbers may not even be an accurate measure of the performance of their algorithm, let alone of Bayesian spam filtering in general.</p>
<p>But I think the most important difference is probably that they ignored message headers. To anyone who has worked on spam filters, this will seem a perverse decision. And yet in the very first filters I tried writing, I ignored the headers too. Why? Because I wanted to keep the problem neat. I didn’t know much about mail headers then, and they seemed to me full of random stuff. There is a lesson here for filter writers: don’t ignore data. You’d think this lesson would be too obvious to mention, but I’ve had to learn it several times.</p>
<p>Third, Pantel and Lin stemmed the tokens, meaning they reduced e.g. both <code class="docutils literal notranslate"><span class="pre">mailing''</span> <span class="pre">and</span> </code>mailed’’ to the root ``mail’’. They may have felt they were forced to do this by the small size of their corpus, but if so this is a kind of premature optimization.</p>
<p>Fourth, they calculated probabilities differently. They used all the tokens, whereas I only use the 15 most significant. If you use all the tokens you’ll tend to miss longer spams, the type where someone tells you their life story up to the point where they got rich from some multilevel marketing scheme. And such an algorithm would be easy for spammers to spoof: just add a big chunk of random text to counterbalance the spam terms.</p>
<p>Finally, they didn’t bias against false positives. I think any spam filtering algorithm ought to have a convenient knob you can twist to decrease the false positive rate at the expense of the filtering rate. I do this by counting the occurrences of tokens in the nonspam corpus double.</p>
<p>I don’t think it’s a good idea to treat spam filtering as a straight text classification problem. You can use text classification techniques, but solutions can and should reflect the fact that the text is email, and spam in particular. Email is not just text; it has structure. Spam filtering is not just classification, because false positives are so much worse than false negatives that you should treat them as a different kind of error. And the source of error is not just random variation, but a live human spammer working actively to defeat your filter.</p>
<p><strong>Tokens</strong></p>
<p>Another project I heard about after the Slashdot article was Bill Yerazunis’ <a class="reference external" href="http://crm114.sourceforge.net">CRM114</a> [5]. This is the counterexample to the design principle I just mentioned. It’s a straight text classifier, but such a stunningly effective one that it manages to filter spam almost perfectly without even knowing that’s what it’s doing.</p>
<p>Once I understood how CRM114 worked, it seemed inevitable that I would eventually have to move from filtering based on single words to an approach like this. But first, I thought, I’ll see how far I can get with single words. And the answer is, surprisingly far.</p>
<p>Mostly I’ve been working on smarter tokenization. On current spam, I’ve been able to achieve filtering rates that approach CRM114’s. These techniques are mostly orthogonal to Bill’s; an optimal solution might incorporate both.</p>
<p>``A Plan for Spam’’ uses a very simple definition of a token. Letters, digits, dashes, apostrophes, and dollar signs are constituent characters, and everything else is a token separator. I also ignored case.</p>
<p>Now I have a more complicated definition of a token:</p>
<ol class="arabic simple">
<li><p>Case is preserved.</p></li>
<li><p>Exclamation points are constituent characters.</p></li>
<li><p>Periods and commas are constituents if they occur between two digits. This lets me get ip addresses and prices intact.</p></li>
<li><p>A price range like <span class="math notranslate nohighlight">\(20-25 yields two tokens, \)</span>20 and $25.</p></li>
<li><p>Tokens that occur within the To, From, Subject, and Return-Path lines, or within urls, get marked accordingly. E.g. <code class="docutils literal notranslate"><span class="pre">foo''</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">Subject</span> <span class="pre">line</span> <span class="pre">becomes</span> </code>Subject*foo’’. (The asterisk could be any character you don’t allow as a constituent.)
Such measures increase the filter’s vocabulary, which makes it more discriminating. For example, in the current filter, ``free’’ in the Subject line has a spam probability of 98%, whereas the same token in the body has a spam probability of only 65%.</p></li>
</ol>
<p>Here are some of the current probabilities [6]:</p>
<p>[code]</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> Subject*FREE      0.9999     free!!            0.9999     To*free           0.9998     Subject*free      0.9782     free!             0.9199     Free              0.9198     Url*free          0.9091     FREE              0.8747     From*free         0.7636     free              0.6546 
</pre></div>
</div>
<p>[/code]</p>
<p>In the Plan for Spam filter, all these tokens would have had the same probability, .7602. That filter recognized about 23,000 tokens. The current one recognizes about 187,000.</p>
<p>The disadvantage of having a larger universe of tokens is that there is more chance of misses. Spreading your corpus out over more tokens has the same effect as making it smaller. If you consider exclamation points as constituents, for example, then you could end up not having a spam probability for free with seven exclamation points, even though you know that free with just two exclamation points has a probability of 99.99%.</p>
<p>One solution to this is what I call degeneration. If you can’t find an exact match for a token, treat it as if it were a less specific version. I consider terminal exclamation points, uppercase letters, and occurring in one of the five marked contexts as making a token more specific. For example, if I don’t find a probability for <code class="docutils literal notranslate"><span class="pre">Subject*free!'',</span> <span class="pre">I</span> <span class="pre">look</span> <span class="pre">for</span> <span class="pre">probabilities</span> <span class="pre">for</span> </code>Subject*free’’, <code class="docutils literal notranslate"><span class="pre">free!'',</span> <span class="pre">and</span> </code>free’’, and take whichever one is farthest from .5.</p>
<p>Here are the alternatives [7] considered if the filter sees ``FREE!!!’’ in the Subject line and doesn’t have a probability for it.</p>
<p>[code]</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> Subject*Free!!!     Subject*free!!!     Subject*FREE!     Subject*Free!     Subject*free!     Subject*FREE     Subject*Free     Subject*free     FREE!!!     Free!!!     free!!!     FREE!     Free!     free! 
FREE

Free

free              
</pre></div>
</div>
<p>[/code]</p>
<p>If you do this, be sure to consider versions with initial caps as well as all uppercase and all lowercase. Spams tend to have more sentences in imperative mood, and in those the first word is a verb. So verbs with initial caps have higher spam probabilities than they would in all lowercase. In my filter, the spam probability of <code class="docutils literal notranslate"><span class="pre">Act''</span> <span class="pre">is</span> <span class="pre">98%</span> <span class="pre">and</span> <span class="pre">for</span> </code>act’’ only 62%.</p>
<p>If you increase your filter’s vocabulary, you can end up counting the same word multiple times, according to your old definition of ``same’’. Logically, they’re not the same token anymore. But if this still bothers you, let me add from experience that the words you seem to be counting multiple times tend to be exactly the ones you’d want to.</p>
<p>Another effect of a larger vocabulary is that when you look at an incoming mail you find more interesting tokens, meaning those with probabilities far from .5. I use the 15 most interesting to decide if mail is spam. But you can run into a problem when you use a fixed number like this. If you find a lot of maximally interesting tokens, the result can end up being decided by whatever random factor determines the ordering of equally interesting tokens. One way to deal with this is to treat some as more interesting than others.</p>
<p>For example, the token <code class="docutils literal notranslate"><span class="pre">dalco''</span> <span class="pre">occurs</span> <span class="pre">3</span> <span class="pre">times</span> <span class="pre">in</span> <span class="pre">my</span> <span class="pre">spam</span> <span class="pre">corpus</span> <span class="pre">and</span> <span class="pre">never</span> <span class="pre">in</span> <span class="pre">my</span> <span class="pre">legitimate</span> <span class="pre">corpus.</span> <span class="pre">The</span> <span class="pre">token</span> </code>Url*optmails’’ (meaning ``optmails’’ within a url) occurs 1223 times. And yet, as I used to calculate probabilities for tokens, both would have the same spam probability, the threshold of .99.</p>
<p>That doesn’t feel right. There are theoretical arguments for giving these two tokens substantially different probabilities (Pantel and Lin do), but I haven’t tried that yet. It does seem at least that if we find more than 15 tokens that only occur in one corpus or the other, we ought to give priority to the ones that occur a lot. So now there are two threshold values. For tokens that occur only in the spam corpus, the probability is .9999 if they occur more than 10 times and .9998 otherwise. Ditto at the other end of the scale for tokens found only in the legitimate corpus.</p>
<p>I may later scale token probabilities substantially, but this tiny amount of scaling at least ensures that tokens get sorted the right way.</p>
<p>Another possibility would be to consider not just 15 tokens, but all the tokens over a certain threshold of interestingness. Steven Hauser does this in his statistical spam filter [8]. If you use a threshold, make it very high, or spammers could spoof you by packing messages with more innocent words.</p>
<p>Finally, what should one do about html? I’ve tried the whole spectrum of options, from ignoring it to parsing it all. Ignoring html is a bad idea, because it’s full of useful spam signs. But if you parse it all, your filter might degenerate into a mere html recognizer. The most effective approach seems to be the middle course, to notice some tokens but not others. I look at a, img, and font tags, and ignore the rest. Links and images you should certainly look at, because they contain urls.</p>
<p>I could probably be smarter about dealing with html, but I don’t think it’s worth putting a lot of time into this. Spams full of html are easy to filter. The smarter spammers already avoid it. So performance in the future should not depend much on how you deal with html.</p>
<p><strong>Performance</strong></p>
<p>Between December 10 2002 and January 10 2003 I got about 1750 spams. Of these, 4 got through. That’s a filtering rate of about 99.75%.</p>
<p>Two of the four spams I missed got through because they happened to use words that occur often in my legitimate email.</p>
<p>The third was one of those that exploit an insecure cgi script to send mail to third parties. They’re hard to filter based just on the content because the headers are innocent and they’re careful about the words they use. Even so I can usually catch them. This one squeaked by with a probability of .88, just under the threshold of .9.</p>
<p>Of course, looking at multiple token sequences would catch it easily. ``Below is the result of your feedback form’’ is an instant giveaway.</p>
<p>The fourth spam was what I call a spam-of-the-future, because this is what I expect spam to evolve into: some completely neutral text followed by a url. In this case it was was from someone saying they had finally finished their homepage and would I go look at it. (The page was of course an ad for a porn site.)</p>
<p>If the spammers are careful about the headers and use a fresh url, there is nothing in spam-of-the-future for filters to notice. We can of course counter by sending a crawler to look at the page. But that might not be necessary. The response rate for spam-of-the-future must be low, or everyone would be doing it. If it’s low enough, it <span class="xref myst">won’t pay</span> for spammers to send it, and we won’t have to work too hard on filtering it.</p>
<p>Now for the really shocking news: during that same one-month period I got <em>three</em> false positives.</p>
<p>In a way it’s a relief to get some false positives. When I wrote ``A Plan for Spam’’ I hadn’t had any, and I didn’t know what they’d be like. Now that I’ve had a few, I’m relieved to find they’re not as bad as I feared. False positives yielded by statistical filters turn out to be mails that sound a lot like spam, and these tend to be the ones you would least mind missing [9].</p>
<p>Two of the false positives were newsletters from companies I’ve bought things from. I never asked to receive them, so arguably they were spams, but I count them as false positives because I hadn’t been deleting them as spams before. The reason the filters caught them was that both companies in January switched to commercial email senders instead of sending the mails from their own servers, and both the headers and the bodies became much spammier.</p>
<p>The third false positive was a bad one, though. It was from someone in Egypt and written in all uppercase. This was a direct result of making tokens case sensitive; the Plan for Spam filter wouldn’t have caught it.</p>
<p>It’s hard to say what the overall false positive rate is, because we’re up in the noise, statistically. Anyone who has worked on filters (at least, effective filters) will be aware of this problem. With some emails it’s hard to say whether they’re spam or not, and these are the ones you end up looking at when you get filters really tight. For example, so far the filter has caught two emails that were sent to my address because of a typo, and one sent to me in the belief that I was someone else. Arguably, these are neither my spam nor my nonspam mail.</p>
<p>Another false positive was from a vice president at Virtumundo. I wrote to them pretending to be a customer, and since the reply came back through Virtumundo’s mail servers it had the most incriminating headers imaginable. Arguably this isn’t a real false positive either, but a sort of Heisenberg uncertainty effect: I only got it because I was writing about spam filtering.</p>
<p>Not counting these, I’ve had a total of five false positives so far, out of about 7740 legitimate emails, a rate of .06%. The other two were a notice that something I bought was back-ordered, and a party reminder from Evite.</p>
<p>I don’t think this number can be trusted, partly because the sample is so small, and partly because I think I can fix the filter not to catch some of these.</p>
<p>False positives seem to me a different kind of error from false negatives. Filtering rate is a measure of performance. False positives I consider more like bugs. I approach improving the filtering rate as optimization, and decreasing false positives as debugging.</p>
<p>So these five false positives are my bug list. For example, the mail from Egypt got nailed because the uppercase text made it look to the filter like a Nigerian spam. This really is kind of a bug. As with html, the email being all uppercase is really conceptually <em>one</em> feature, not one for each word. I need to handle case in a more sophisticated way.</p>
<p>So what to make of this .06%? Not much, I think. You could treat it as an upper bound, bearing in mind the small sample size. But at this stage it is more a measure of the bugs in my implementation than some intrinsic false positive rate of Bayesian filtering.</p>
<p><strong>Future</strong></p>
<p>What next? Filtering is an optimization problem, and the key to optimization is profiling. Don’t try to guess where your code is slow, because you’ll guess wrong. <em>Look</em> at where your code is slow, and fix that. In filtering, this translates to: look at the spams you miss, and figure out what you could have done to catch them.</p>
<p>For example, spammers are now working aggressively to evade filters, and one of the things they’re doing is breaking up and misspelling words to prevent filters from recognizing them. But working on this is not my first priority, because I still have no trouble catching these spams [10].</p>
<p>There are two kinds of spams I currently do have trouble with. One is the type that pretends to be an email from a woman inviting you to go chat with her or see her profile on a dating site. These get through because they’re the one type of sales pitch you can make without using sales talk. They use the same vocabulary as ordinary email.</p>
<p>The other kind of spams I have trouble filtering are those from companies in e.g. Bulgaria offering contract programming services. These get through because I’m a programmer too, and the spams are full of the same words as my real mail.</p>
<p>I’ll probably focus on the personal ad type first. I think if I look closer I’ll be able to find statistical differences between these and my real mail. The style of writing is certainly different, though it may take multiword filtering to catch that. Also, I notice they tend to repeat the url, and someone including a url in a legitimate mail wouldn’t do that [11].</p>
<p>The outsourcing type are going to be hard to catch. Even if you sent a crawler to the site, you wouldn’t find a smoking statistical gun. Maybe the only answer is a central list of domains advertised in spams [12]. But there can’t be that many of this type of mail. If the only spams left were unsolicited offers of contract programming services from Bulgaria, we could all probably move on to working on something else.</p>
<p>Will statistical filtering actually get us to that point? I don’t know. Right now, for me personally, spam is not a problem. But spammers haven’t yet made a serious effort to spoof statistical filters. What will happen when they do?</p>
<p>I’m not optimistic about filters that work at the network level [13]. When there is a static obstacle worth getting past, spammers are pretty efficient at getting past it. There is already a company called Assurance Systems that will run your mail through Spamassassin and tell you whether it will get filtered out.</p>
<p>Network-level filters won’t be completely useless. They may be enough to kill all the “opt-in” spam, meaning spam from companies like Virtumundo and Equalamail who claim that they’re really running opt-in lists. You can filter those based just on the headers, no matter what they say in the body. But anyone willing to falsify headers or use open relays, presumably including most porn spammers, should be able to get some message past network-level filters if they want to. (By no means the message they’d like to send though, which is something.)</p>
<p>The kind of filters I’m optimistic about are ones that calculate probabilities based on each individual user’s mail. These can be much more effective, not only in avoiding false positives, but in filtering too: for example, finding the recipient’s email address base-64 encoded anywhere in a message is a very good spam indicator.</p>
<p>But the real advantage of individual filters is that they’ll all be different. If everyone’s filters have different probabilities, it will make the spammers’ optimization loop, what programmers would call their edit-compile-test cycle, appallingly slow. Instead of just tweaking a spam till it gets through a copy of some filter they have on their desktop, they’ll have to do a test mailing for each tweak. It would be like programming in a language without an interactive toplevel, and I wouldn’t wish that on anyone.</p>
<p><strong>Notes</strong></p>
<p>[1] Paul Graham. ``A Plan for Spam.’’ August 2002. <a class="reference external" href="http://paulgraham.com/spam.html">http://paulgraham.com/spam.html</a>.</p>
<p>Probabilities in this algorithm are calculated using a degenerate case of Bayes’ Rule. There are two simplifying assumptions: that the probabilities of features (i.e. words) are independent, and that we know nothing about the prior probability of an email being spam.</p>
<p>The first assumption is widespread in text classification. Algorithms that use it are called ``naive Bayesian.’’</p>
<p>The second assumption I made because the proportion of spam in my incoming mail fluctuated so much from day to day (indeed, from hour to hour) that the overall prior ratio seemed worthless as a predictor. If you assume that P(spam) and P(nonspam) are both .5, they cancel out and you can remove them from the formula.</p>
<p>If you were doing Bayesian filtering in a situation where the ratio of spam to nonspam was consistently very high or (especially) very low, you could probably improve filter performance by incorporating prior probabilities. To do this right you’d have to track ratios by time of day, because spam and legitimate mail volume both have distinct daily patterns.</p>
<p>[2] Patrick Pantel and Dekang Lin. ``SpamCop– A Spam Classification &amp; Organization Program.’’ Proceedings of AAAI-98 Workshop on Learning for Text Categorization.</p>
<p>[3] Mehran Sahami, Susan Dumais, David Heckerman and Eric Horvitz. ``A Bayesian Approach to Filtering Junk E-Mail.’’ Proceedings of AAAI-98 Workshop on Learning for Text Categorization.</p>
<p>[4] At the time I had zero false positives out of about 4,000 legitimate emails. If the next legitimate email was a false positive, this would give us .03%. These false positive rates are untrustworthy, as I explain later. I quote a number here only to emphasize that whatever the false positive rate is, it is less than 1.16%.</p>
<p>[5] Bill Yerazunis. ``Sparse Binary Polynomial Hash Message Filtering and The CRM114 Discriminator.’’ Proceedings of 2003 Spam Conference.</p>
<p>[6] In ``A Plan for Spam’’ I used thresholds of .99 and .01. It seems justifiable to use thresholds proportionate to the size of the corpora. Since I now have on the order of 10,000 of each type of mail, I use .9999 and .0001.</p>
<p>[7] There is a flaw here I should probably fix. Currently, when <code class="docutils literal notranslate"><span class="pre">Subject*foo''</span> <span class="pre">degenerates</span> <span class="pre">to</span> <span class="pre">just</span> </code>foo’’, what that means is you’re getting the stats for occurrences of <code class="docutils literal notranslate"><span class="pre">foo''</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">body</span> <span class="pre">or</span> <span class="pre">header</span> <span class="pre">lines</span> <span class="pre">other</span> <span class="pre">than</span> <span class="pre">those</span> <span class="pre">I</span> <span class="pre">mark.</span> <span class="pre">What</span> <span class="pre">I</span> <span class="pre">should</span> <span class="pre">do</span> <span class="pre">is</span> <span class="pre">keep</span> <span class="pre">track</span> <span class="pre">of</span> <span class="pre">statistics</span> <span class="pre">for</span> </code>foo’’ overall as well as specific versions, and degenerate from <code class="docutils literal notranslate"><span class="pre">Subject*foo''</span> <span class="pre">not</span> <span class="pre">to</span> </code>foo’’ but to ``Anywhere*foo’’. Ditto for case: I should degenerate from uppercase to any-case, not lowercase.</p>
<p>It would probably be a win to do this with prices too, e.g. to degenerate from <code class="docutils literal notranslate"><span class="pre">$129.99''</span> <span class="pre">to</span> </code><span class="math notranslate nohighlight">\(--9.99'', ``\)</span>–.99’’, and ``$–‘’.</p>
<p>You could also degenerate from words to their stems, but this would probably only improve filtering rates early on when you had small corpora.</p>
<p>[8] Steven Hauser. ``Statistical Spam Filter Works for Me.’’ <a class="reference external" href="http://www.sofbot.com">http://www.sofbot.com</a>.</p>
<p>[9] False positives are not all equal, and we should remember this when comparing techniques for stopping spam. Whereas many of the false positives caused by filters will be near-spams that you wouldn’t mind missing, false positives caused by blacklists, for example, will be just mail from people who chose the wrong ISP. In both cases you catch mail that’s near spam, but for blacklists nearness is physical, and for filters it’s textual.</p>
<p>[10] If spammers get good enough at obscuring tokens for this to be a problem, we can respond by simply removing whitespace, periods, commas, etc. and using a dictionary to pick the words out of the resulting sequence. And of course finding words this way that weren’t visible in the original text would in itself be evidence of spam.</p>
<p>Picking out the words won’t be trivial. It will require more than just reconstructing word boundaries; spammers both add (<code class="docutils literal notranslate"><span class="pre">xHot</span> <span class="pre">nPorn</span> <span class="pre">cSite'')</span> <span class="pre">and</span> <span class="pre">omit</span> <span class="pre">(</span></code>P#rn’’) letters. Vision research may be useful here, since human vision is the limit that such tricks will approach.</p>
<p>[11] In general, spams are more repetitive than regular email. They want to pound that message home. I currently don’t allow duplicates in the top 15 tokens, because you could get a false positive if the sender happens to use some bad word multiple times. (In my current filter, ``dick’’ has a spam probabilty of .9999, but it’s also a name.) It seems we should at least notice duplication though, so I may try allowing up to two of each token, as Brian Burton does in SpamProbe.</p>
<p>[12] This is what approaches like Brightmail’s will degenerate into once spammers are pushed into using mad-lib techniques to generate everything else in the message.</p>
<p>[13] It’s sometimes argued that we should be working on filtering at the network level, because it is more efficient. What people usually mean when they say this is: we currently filter at the network level, and we don’t want to start over from scratch. But you can’t dictate the problem to fit your solution.</p>
<p>Historically, scarce-resource arguments have been the losing side in debates about software design. People only tend to use them to justify choices (inaction in particular) made for other reasons.</p>
<p><strong>Thanks</strong> to Sarah Harlin, Trevor Blackwell, and Dan Giffin for reading drafts of this paper, and to Dan again for most of the infrastructure that this filter runs on.</p>
<p><strong>Related:</strong></p>
<p><span class="xref myst">A Plan for Spam</span></p>
<p><span class="xref myst">Plan for Spam FAQ</span></p>
<p><a class="reference external" href="http://spamconference.org/proceedings2003.html">2003 Spam Conference Proceedings</a></p>
<p><a class="reference external" href="http://www.shiro.dreamhost.com/scheme/trans/better-j.html">Japanese Translation</a></p>
<p><a class="reference external" href="http://people.brandeis.edu/~liji/_private/translation/better.htm">Chinese Translation</a></p>
<p><a class="reference external" href="http://www.bgl.nu/bogofilter/graham.html">Test of These Suggestions</a></p>
<hr class="docutils" />
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./LlamaIndex/essays"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By ExplodingGradients
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>