{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a85f9548",
   "metadata": {},
   "source": [
    "# LLMs: Completion\n",
    "\n",
    "The awesome thing about langchain is wide range of LLM that is has support for and that it is actively managed by the community. It is one of its super powers. In this notebook we'll look at the completion interface but langchain also has the Chat interface as well.\n",
    "\n",
    "We'll get started with OpenAI's LLM as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5efb961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eadfb7",
   "metadata": {},
   "source": [
    "You can interact with the LLM in a couple of ways\n",
    "\n",
    "1. `__call__()` which take a string and returns a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "603d0859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nQ: What did the fish say when it hit the wall?\\nA: Dam!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"tell me a joke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec58b49e",
   "metadata": {},
   "source": [
    "2. `generate()` which is greate for a batch of requests. It return and `LLMResult` object which has a few options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce233f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_result = llm.generate([\"Tell me a joke\", \"Tell me a poem\"]*15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1727b43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llm_result.generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1808df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Generation(text='\\n\\nQ: What did the fish say when he hit the wall?\\nA: Dam!', generation_info={'finish_reason': 'stop', 'logprobs': None})]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results are under the generations\n",
    "llm_result.generations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9acfbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 1063,\n",
       "  'prompt_tokens': 120,\n",
       "  'total_tokens': 1183},\n",
       " 'model_name': 'text-davinci-003'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# llm_output has metadata about the results\n",
    "llm_result.llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e8a6eb",
   "metadata": {},
   "source": [
    "we'll explore `LLMResult` in more detail below\n",
    "\n",
    "3. `async` API which is greate for concurrency since most of these calls are network-bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad330fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mConcurrent executed in 1.99 seconds.\u001b[0m\n",
      "\u001b[1mSerial executed in 13.72 seconds.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import asyncio\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "def generate_serially():\n",
    "    llm = OpenAI(temperature=0.9)\n",
    "    for _ in range(10):\n",
    "        resp = llm.generate([\"Hello, how are you?\"])\n",
    "\n",
    "\n",
    "async def async_generate(llm):\n",
    "    resp = await llm.agenerate([\"Hello, how are you?\"])\n",
    "\n",
    "\n",
    "async def generate_concurrently():\n",
    "    llm = OpenAI(temperature=0.9)\n",
    "    tasks = [async_generate(llm) for _ in range(10)]\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "\n",
    "s = time.perf_counter()\n",
    "# If running this outside of Jupyter, use asyncio.run(generate_concurrently())\n",
    "await generate_concurrently()\n",
    "elapsed = time.perf_counter() - s\n",
    "print(\"\\033[1m\" + f\"Concurrent executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")\n",
    "\n",
    "s = time.perf_counter()\n",
    "generate_serially()\n",
    "elapsed = time.perf_counter() - s\n",
    "print(\"\\033[1m\" + f\"Serial executed in {elapsed:0.2f} seconds.\" + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a22a1d",
   "metadata": {},
   "source": [
    "### A note about internals\n",
    "\n",
    "All language model wrappers inherit from `BaseLanguageModel`.\n",
    "\n",
    "Exposes three main methods:\n",
    "- `generate_prompt()`: generate language model outputs for a sequence of prompt\n",
    "    values. A prompt value is a model input that can be converted to any language\n",
    "    model input format (string or messages).\n",
    "- `predict()`: pass in a single string to a language model and return a string\n",
    "    prediction.\n",
    "- `predict_messages()`: pass in a sequence of BaseMessages (corresponding to a single\n",
    "    model call) to a language model and return a BaseMessage prediction.\n",
    "\n",
    "\n",
    "these are the functions every LLMs should implement by default.\n",
    "\n",
    "But there is also `BaseLLM` which implementes the interface we saw above with `OpenAI` LLM.\n",
    "\n",
    "If you want to build you own custom LLM it is recommended to subclass from `LLM` in `langchain.llm.base`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce6109",
   "metadata": {},
   "source": [
    "### `LLMResult` and `Generation`\n",
    "\n",
    "`LLMResults` is the object that stores the results from the LLM. It has\n",
    "- `generations` - which returns a list of list of `Generation` objects, each with one output from the LLM. This is a `list[list[Generation]]` because each input can have multipe candidate resonses.\n",
    "- `llm_output` - which is LLM provider specifc outputs. You can get token_usage, model_name etc from this object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "311e1c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Generation(text='\\n\\nQ: What did the fish say when he hit the wall?\\nA: Dam!', generation_info={'finish_reason': 'stop', 'logprobs': None})],\n",
       " [Generation(text=\"\\n\\nHere's a beautiful poem by Robert Frost: \\n\\nThe Road Not Taken\\n\\nTwo roads diverged in a yellow wood,\\nAnd sorry I could not travel both\\nAnd be one traveler, long I stood\\nAnd looked down one as far as I could\\nTo where it bent in the undergrowth;\\n\\nThen took the other, as just as fair,\\nAnd having perhaps the better claim,\\nBecause it was grassy and wanted wear;\\nThough as for that the passing there\\nHad worn them really about the same,\\n\\nAnd both that morning equally lay\\nIn leaves no step had trodden black.\\nOh, I kept the first for another day!\\nYet knowing how way leads on to way,\\nI doubted if I should ever come back.\\n\\nI shall be telling this with a sigh\\nSomewhere ages and ages hence:\\nTwo roads diverged in a wood, and Iâ€”\\nI took the one less traveled by,\\nAnd that has made all the difference.\", generation_info={'finish_reason': 'stop', 'logprobs': None})]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.generations[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1163a5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 1063,\n",
       "  'prompt_tokens': 120,\n",
       "  'total_tokens': 1183},\n",
       " 'model_name': 'text-davinci-003'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result.llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66cf607",
   "metadata": {},
   "source": [
    "`Generation` has\n",
    "- `text` - generated text output\n",
    "- `generation_info` - raw response from the provider which may include things like reason for finishing or token log probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38a8a5b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nQ: What did the fish say when he hit the wall?\\nA: Dam!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = llm_result.generations[0][0]\n",
    "\n",
    "g.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a986158d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finish_reason': 'stop', 'logprobs': None}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.generation_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}