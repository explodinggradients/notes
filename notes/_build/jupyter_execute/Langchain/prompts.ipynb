{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eac8f2ba",
   "metadata": {},
   "source": [
    "# Prompts\n",
    "\n",
    "Langchain has the widest coverage of the possible LLMs you can/might want to use so it is nesessary to be familier with its LLM and Prompt interfaces for ragas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b04fb8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import BaseLLM\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.chat_models.openai import ChatOpenAI\n",
    "\n",
    "from rich.pretty import pprint\n",
    "\n",
    "openaichat = ChatOpenAI()\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6de69004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm an AI, so I don't have feelings, but thank you for asking. How can I assist you today?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openaichat.predict(\"hai how are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "089ac151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nI'm doing well, thank you. How about you?\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.predict(\"hai how are you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6356416a",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "\n",
    "ref: [docs](https://python.langchain.com/docs/modules/model_io/prompts/)\n",
    "\n",
    "Prompts are the way we interact with LLMs and Langchain offers 2 components for that:\n",
    "- Prompt Templates: Parmetrize model inputs\n",
    "- Example Selectors: Dynamically select examples to include in prompts\n",
    "\n",
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e15765a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a naming consultant for new companies.\\nWhat is a good name for a company that makes colorful socks?\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\\\n",
    "You are a naming consultant for new companies.\n",
    "What is a good name for a company that makes {product}?\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3d9154",
   "metadata": {},
   "source": [
    "`.from_template()` classmethod is the easiest way to build prompt templates but you can checkout `PromptTemplate` and [Custom Prompt Templates](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/custom_prompt_template.html)\n",
    "\n",
    "### Chat Prompt Tempalate\n",
    "LLM services like OpenAI and Anthropic are also now implementing a chat interface on top of LLMs. For a Chat interface you need\n",
    "- System Message - helps set the behaviour of the assistant.\n",
    "- list of `HumanMessage` and `AIMessage`. This is the convo your having with the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fbe2ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e024dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates English to French', additional_kwargs={}),\n",
       " HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_msg = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a helpful assistant that translates {input_language}\"\n",
    "    \" to {output_language}\"\n",
    ")\n",
    "human_msg = HumanMessagePromptTemplate.from_template(\n",
    "    \"{text}\"\n",
    ")\n",
    "\n",
    "\n",
    "# ChatPromptTemplate has the collection\n",
    "chat_prompt = ChatPromptTemplate.from_messages([sys_msg, human_msg])\n",
    "chat_prompt.format_prompt(\n",
    "    input_language=\"English\", \n",
    "    output_language=\"French\", \n",
    "    text=\"I love programming.\"\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d75e986",
   "metadata": {},
   "source": [
    "There is also another way that I personally like when creating `ChatPromptTemplate` is directly from the `BaseMessage` objects like `AIMessage`, `HumanMessage`, `SystemMessage`. This is usefull if you don't have a lot of input_variables in the messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c02f29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant.', additional_kwargs={}),\n",
       " HumanMessage(content='How are you?', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "sys_mesg = SystemMessage(content=\"You are a helpful assistant.\")\n",
    "human_msg = HumanMessage(content=\"How are you?\")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([sys_mesg, human_msg])\n",
    "chat_prompt.format_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29e30b0",
   "metadata": {},
   "source": [
    "### Few-shot Prompt Templates\n",
    "[ref](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples)\n",
    "\n",
    "Used when creating prompts with few shot examples. You can load examples from \n",
    "- a list of examples\n",
    "- using `ExampleSelector` which helps you add more logic around selecting examples for the prompt like the `SemanticSimilarityExampleSelector` which selects examples similar to the input.\n",
    "\n",
    "first lets look at a list of examples. Make sure the examples are a list of `dict`s who's keys match the prompt's input variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e30bbe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "  {\n",
    "    \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
    "    \"answer\": \n",
    "\"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: How old was Muhammad Ali when he died?\n",
    "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
    "Follow up: How old was Alan Turing when he died?\n",
    "Intermediate answer: Alan Turing was 41 years old when he died.\n",
    "So the final answer is: Muhammad Ali\n",
    "\"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"When was the founder of craigslist born?\",\n",
    "    \"answer\": \n",
    "\"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who was the founder of craigslist?\n",
    "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
    "Follow up: When was Craig Newmark born?\n",
    "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
    "So the final answer is: December 6, 1952\n",
    "\"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Who was the maternal grandfather of George Washington?\",\n",
    "    \"answer\":\n",
    "\"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who was the mother of George Washington?\n",
    "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
    "Follow up: Who was the father of Mary Ball Washington?\n",
    "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
    "So the final answer is: Joseph Ball\n",
    "\"\"\"\n",
    "  },\n",
    "  {\n",
    "    \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\n",
    "    \"answer\":\n",
    "\"\"\"\n",
    "Are follow up questions needed here: Yes.\n",
    "Follow up: Who is the director of Jaws?\n",
    "Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
    "Follow up: Where is Steven Spielberg from?\n",
    "Intermediate Answer: The United States.\n",
    "Follow up: Who is the director of Casino Royale?\n",
    "Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
    "Follow up: Where is Martin Campbell from?\n",
    "Intermediate Answer: New Zealand.\n",
    "So the final answer is: No\n",
    "\"\"\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f7db293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example prompt is used to format the examples\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\n",
    "    \"Question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(example_prompt.format(**examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdb24f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n",
      "\n",
      "Question: When was the founder of craigslist born?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the founder of craigslist?\n",
      "Intermediate answer: Craigslist was founded by Craig Newmark.\n",
      "Follow up: When was Craig Newmark born?\n",
      "Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
      "So the final answer is: December 6, 1952\n",
      "\n",
      "\n",
      "Question: Who was the maternal grandfather of George Washington?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the mother of George Washington?\n",
      "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
      "Follow up: Who was the father of Mary Ball Washington?\n",
      "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
      "So the final answer is: Joseph Ball\n",
      "\n",
      "\n",
      "Question: Are both the directors of Jaws and Casino Royale from the same country?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who is the director of Jaws?\n",
      "Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
      "Follow up: Where is Steven Spielberg from?\n",
      "Intermediate Answer: The United States.\n",
      "Follow up: Who is the director of Casino Royale?\n",
      "Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
      "Follow up: Where is Martin Campbell from?\n",
      "Intermediate Answer: New Zealand.\n",
      "So the final answer is: No\n",
      "\n",
      "\n",
      "Question: Who was the father of Mary Ball Washington?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\", # will put this at the end\n",
    "    prefix=\"\",                  # also present if you want\n",
    "    input_variables=[\"input\"],  # define input vars like any other prompts\n",
    ")\n",
    "\n",
    "print(prompt.format(\n",
    "    input=\"Who was the father of Mary Ball Washington?\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef98e2",
   "metadata": {},
   "source": [
    "Now lets use `ExampleSelector` and `SemanticSimilarityExampleSelector` to retrieve examples similar to input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fa68f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# save examples into a vectorDB\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples=examples,\n",
    "    embeddings=OpenAIEmbeddings(),\n",
    "    vectorstore_cls=Chroma,\n",
    "    k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b516c5",
   "metadata": {},
   "source": [
    "once the DB has been created you can fetch similar ones base on the input_variables of you choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53bebfa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Who was the father of Mary Ball Washington?\"\n",
    "selected_examples = example_selector.select_examples(\n",
    "    {\"question\": question}\n",
    ")\n",
    "len(selected_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f448cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "question: Who was the maternal grandfather of George Washington?\n",
      "answer: \n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the mother of George Washington?\n",
      "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
      "Follow up: Who was the father of Mary Ball Washington?\n",
      "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
      "So the final answer is: Joseph Ball\n",
      "\n",
      "\n",
      "\n",
      "question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "answer: \n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pretty print\n",
    "for example in selected_examples:\n",
    "    print(\"\\n\")\n",
    "    for k, v in example.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d96e53",
   "metadata": {},
   "source": [
    "Pass on the `ExampleSelector` to the few-shot template object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1aaf16d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who was the maternal grandfather of George Washington?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: Who was the mother of George Washington?\n",
      "Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
      "Follow up: Who was the father of Mary Ball Washington?\n",
      "Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
      "So the final answer is: Joseph Ball\n",
      "\n",
      "\n",
      "Question: Who lived longer, Muhammad Ali or Alan Turing?\n",
      "\n",
      "Are follow up questions needed here: Yes.\n",
      "Follow up: How old was Muhammad Ali when he died?\n",
      "Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
      "Follow up: How old was Alan Turing when he died?\n",
      "Intermediate answer: Alan Turing was 41 years old when he died.\n",
      "So the final answer is: Muhammad Ali\n",
      "\n",
      "\n",
      "Question: Who was the father of Mary Ball Washington?\n"
     ]
    }
   ],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector, \n",
    "    example_prompt=example_prompt, \n",
    "    suffix=\"Question: {input}\", \n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "print(prompt.format(input=\"Who was the father of Mary Ball Washington?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd045f",
   "metadata": {},
   "source": [
    "### Few Shot examples for ChatModels\n",
    "\n",
    "*seems like this is not yet decided - but we want this for ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cb26db",
   "metadata": {},
   "source": [
    "### Format template output\n",
    "\n",
    "- `format()` - creates a string output with the prompts\n",
    "- `format_prompt()` - creates chat messages from the prompts. Returns a `PromptValue` which has `to_string()` and `to_messages()`, the later can be used for creating system-human-AI messages for ChatModels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80f127e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a naming consultant for new companies.\\nWhat is a good name for a company that makes colorful socks?\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = \"\"\"\\\n",
    "You are a naming consultant for new companies.\n",
    "What is a good name for a company that makes {product}?\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbbf75c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='You are a naming consultant for new companies.\\nWhat is a good name for a company that makes colorful pants?\\n')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format_prompt(product=\"colorful pants\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b160a4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are a helpful assistant that translates English to French\\nHuman: I love programming.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_msg = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a helpful assistant that translates {input_language}\"\n",
    "    \" to {output_language}\"\n",
    ")\n",
    "human_msg = HumanMessagePromptTemplate.from_template(\n",
    "    \"{text}\"\n",
    ")\n",
    "\n",
    "\n",
    "# ChatPromptTemplate has the collection\n",
    "chat_prompt = ChatPromptTemplate.from_messages([sys_msg, human_msg])\n",
    "\n",
    "# as a string\n",
    "chat_prompt.format(\n",
    "    input_language=\"English\", \n",
    "    output_language=\"French\", \n",
    "    text=\"I love programming.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c07c21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates English to French', additional_kwargs={}),\n",
       " HumanMessage(content='I love programming.', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as messages\n",
    "chat_prompt.format_prompt(\n",
    "    input_language=\"English\", \n",
    "    output_language=\"French\", \n",
    "    text=\"I love programming.\"\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61733860",
   "metadata": {},
   "source": [
    "### Partial Prompt Templates\n",
    "\n",
    "This is when you want to pass a subset of required values to create a new prompt template that expects only the remaining subset of values. The cool thing is that langchain allows you to use strings and functions that return strings as partials.\n",
    "\n",
    "lets see them in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbf5de5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4da8b404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foobaz\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(template=\"{foo}{bar}\", input_variables=[\"foo\", \"bar\"])\n",
    "\n",
    "# you only need to pass in foo now\n",
    "partial_prompt = prompt.partial(foo=\"foo\");\n",
    "print(partial_prompt.format(bar=\"baz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe28fff",
   "metadata": {},
   "source": [
    "Similarly for functions too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7464a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def _get_date():\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e2abdec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a funny joke about the day 07/13/2023\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"Tell me a {adjective} joke about the day {date}\", \n",
    "    input_variables=[\"adjective\", \"date\"]\n",
    ");\n",
    "partial_prompt = prompt.partial(date=_get_date)\n",
    "print(partial_prompt.format(adjective=\"funny\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ce19b",
   "metadata": {},
   "source": [
    "### Custom Prompt Template\n",
    "[ref](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/custom_prompt_template)\n",
    "\n",
    "Langchain offers `PromptTemplate` and `ChatPromptTemplate` but there can be situations where you need your own templates. The examples langchain is uses is a custom template that takes in a function name as input and formats the prompt template to provide the source code of the function. If you building a product to help with devs, this could be very valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b77ba9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from langchain.prompts import StringPromptTemplate\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "# func to get source\n",
    "def get_source_code(function_name):\n",
    "    # Get the source code of the function\n",
    "    return inspect.getsource(function_name)\n",
    "\n",
    "# the custom PromptTemplate\n",
    "class FunctionExplainerPromptTemplate(StringPromptTemplate, BaseModel):\n",
    "    \"\"\"A custom prompt template that takes in the function name as input, and formats the prompt template to provide the source code of the function.\"\"\"\n",
    "\n",
    "    @validator(\"input_variables\")\n",
    "    def validate_input_variables(cls, v):\n",
    "        \"\"\"Validate that the input variables are correct.\"\"\"\n",
    "        if len(v) != 1 or \"function_name\" not in v:\n",
    "            raise ValueError(\"function_name must be the only input_variable.\")\n",
    "        return v\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the source code of the function\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        # Generate the prompt to be sent to the language model\n",
    "        prompt = f\"\"\"\\\n",
    "Given the function name and source code, generate an English language explanation of the function.\n",
    "Function Name: {kwargs[\"function_name\"].__name__}\n",
    "Source Code:\n",
    "```python\n",
    "{source_code}\n",
    "```\n",
    "Explanation:\n",
    "\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def _prompt_type(self):\n",
    "        return \"function-explainer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6043da6",
   "metadata": {},
   "source": [
    "Now lets ask it to explain the code we just wrote ðŸ˜‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "99138e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the function name and source code, generate an English language explanation of the function.\n",
      "Function Name: predict\n",
      "Source Code:\n",
      "```python\n",
      "    def predict(\n",
      "        self, text: str, *, stop: Optional[Sequence[str]] = None, **kwargs: Any\n",
      "    ) -> str:\n",
      "        if stop is None:\n",
      "            _stop = None\n",
      "        else:\n",
      "            _stop = list(stop)\n",
      "        return self(text, stop=_stop, **kwargs)\n",
      "\n",
      "```\n",
      "Explanation:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fn_explainer = FunctionExplainerPromptTemplate(input_variables=[\"function_name\"])\n",
    "\n",
    "# Generate a prompt for the format() function we wrote above\n",
    "prompt = fn_explainer.format(\n",
    "    function_name=FunctionExplainerPromptTemplate.format\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855f2982",
   "metadata": {},
   "source": [
    "but lets see this in action shall we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "22781ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This function is called 'predict' and its purpose is to take a string of text, an optional sequence of strings, and any additional keyword arguments as inputs and return a string as an output. If a sequence of strings is provided, the function converts it into a list before returning the output.\n"
     ]
    }
   ],
   "source": [
    "print(openai.predict(prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}