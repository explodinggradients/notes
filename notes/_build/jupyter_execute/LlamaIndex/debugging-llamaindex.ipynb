{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11039a2c",
   "metadata": {},
   "source": [
    "# Debugging LlamaIndex\n",
    "\n",
    "I've been developing RAG(Retrieval Augmented Generation) apps with Llamaindex and helping develop/contributing to the main project as well. Over time I've been learning a few tricks here and there to helps debug the pipelines I build more effectively. This is a collection of all my tips and tricks\n",
    "\n",
    "\n",
    "## Content\n",
    "### 1. [Tracing your steps with `CallbackManager`](#Tracing-your-steps-with-CallbackManager)\n",
    "### 2. [Furthur Explorations with `LlamaDebugHandler`](#Furthur-Explorations-with-LlamaDebugHandler)\n",
    "### 3. [Setting up wandb for experiment tracking and tracing](#Setting-up-wandb-for-experiment-tracking-and-tracing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8855ef51",
   "metadata": {},
   "source": [
    "# Tracing your steps with `CallbackManager`\n",
    "\n",
    "Using the `CallbackManager` allows you to trace the steps llamaindex takes to generate the response and the time each step took. \n",
    "\n",
    "For this example lets compare 2 indices `ListIndex` and `VectorIndex` and see how the outputs are.\n",
    "\n",
    "but first things first, lets import everything and init a service_context that uses the callback_manager we created. Then we use `set_global_service_context` to use that service context throught."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf711e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from llama_index import set_global_service_context\n",
    "from llama_index import ListIndex, VectorStoreIndex, ServiceContext, LLMPredictor\n",
    "from llama_index.callbacks import CallbackManager, LlamaDebugHandler, CBEventType\n",
    "\n",
    "llm_predictor = LLMPredictor(\n",
    "    llm=ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    ")\n",
    "\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    callback_manager=callback_manager, \n",
    "    llm_predictor=llm_predictor\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da1832d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "docs = SimpleDirectoryReader(\"./what_i_worked_on_pg/\").load_data()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf36a367",
   "metadata": {},
   "source": [
    "Now lets create the first index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10c52514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_node_parsing ->  0.124317 seconds\n",
      "      |_chunking ->  0.122693 seconds\n",
      "    |_embedding ->  1.381548 seconds\n",
      "    |_embedding ->  0.986437 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "vector_index = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361243f1",
   "metadata": {},
   "source": [
    "and vola! you can see the traces in actions. Here you can see the different steps the index construction took. You can see that the LlamaIndex made 2 calls to the embedding endpoint to create the embeddings for the chunks.\n",
    "\n",
    "Now lets try `ListIndex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d16c33e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_node_parsing ->  0.130536 seconds\n",
      "      |_chunking ->  0.128729 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "list_index = ListIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2c9cc5",
   "metadata": {},
   "source": [
    "And we have a different output. If you know `ListIndexes` you know that they don't have embeddings instead just chunk the docs and store.\n",
    "\n",
    "That was index creation but how about query time? Lets find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ecf863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util func to help me :)\n",
    "def query(question, index):\n",
    "    qe = index.as_query_engine()\n",
    "    r = qe.query(question)\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8eacee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_query ->  2.390294 seconds\n",
      "      |_retrieve ->  0.33361 seconds\n",
      "        |_embedding ->  0.329092 seconds\n",
      "      |_synthesize ->  2.05655 seconds\n",
      "        |_llm ->  2.031409 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "r = query(\"what did the author do growing up?\", vector_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bc54ed",
   "metadata": {},
   "source": [
    "Here you can see the steps LlamaIndex took.\n",
    "\n",
    "Lets try something a bit more complicated, like our `ListIndex`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "777be504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_query ->  100.628079 seconds\n",
      "      |_retrieve ->  0.003366 seconds\n",
      "      |_synthesize ->  100.624557 seconds\n",
      "        |_llm ->  4.596029 seconds\n",
      "        |_llm ->  5.08475 seconds\n",
      "        |_llm ->  6.206133 seconds\n",
      "        |_llm ->  10.300521 seconds\n",
      "        |_llm ->  8.189844 seconds\n",
      "        |_llm ->  9.047571 seconds\n",
      "        |_llm ->  11.563737 seconds\n",
      "        |_llm ->  10.239132 seconds\n",
      "        |_llm ->  15.381177 seconds\n",
      "        |_llm ->  19.77576 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "r = query(\"what did the author do growing up?\", list_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fa6bd7",
   "metadata": {},
   "source": [
    "As you can imagine this is very neat tool to get a better understanding of what are the steps that happening internally with LlamaIndex. Especially usefull when debugging, something I've turned to time and time again when building complex indexes with LlamaIndex.\n",
    "\n",
    "but what if you want a bit more info?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d18c7a",
   "metadata": {},
   "source": [
    "# Furthur Explorations with `LlamaDebugHandler`\n",
    "\n",
    "> Note: The `LlamaDebugHandler` is in beta and subject to change. So these examples might get outdated check the [docs](https://gpt-index.readthedocs.io/en/latest/examples/callbacks/LlamaDebugHandler.html) and [code]() for up-to-date info. \n",
    "\n",
    "\n",
    "LlamaIndex uses callbacks to log the events that are happening. These callbacks are exposed via the `LlamaDebugHandler` for debugging purposes. If you can parse these, it is much easier when you are working with LlamaIndex to know exactly what is happening.\n",
    "\n",
    "the `CallbackManager` logs many different events. Here is the full list.\n",
    "```python\n",
    "CHUNKING = \"chunking\"\n",
    "NODE_PARSING = \"node_parsing\"\n",
    "EMBEDDING = \"embedding\"\n",
    "LLM = \"llm\"\n",
    "QUERY = \"query\"\n",
    "RETRIEVE = \"retrieve\"\n",
    "SYNTHESIZE = \"synthesize\"\n",
    "TREE = \"tree\"\n",
    "SUB_QUESTIONS = \"sub_questions\"\n",
    "```\n",
    "\n",
    "They also have different payload types which are listed here\n",
    "```python\n",
    "DOCUMENTS = \"documents\"  # list of documents before parsing\n",
    "CHUNKS = \"chunks\"  # list of text chunks\n",
    "NODES = \"nodes\"  # list of nodes\n",
    "PROMPT = \"formatted_prompt\"  # formatted prompt sent to LLM\n",
    "RESPONSE = \"response\"  # response from LLM\n",
    "TEMPLATE = \"template\"  # template used in LLM call\n",
    "QUERY_STR = \"query_str\"  # query used for query engine\n",
    "SUB_QUESTIONS = \"sub_questions\"  # list of sub question & answer pairs\n",
    "```\n",
    "\n",
    "Now lets see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9975f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the events uptill now\n",
    "llama_debug.flush_event_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a19a06a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = query(\"what did the author do growing up?\", list_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5fb25e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.callbacks import CBEventType\n",
    "\n",
    "es = llama_debug.get_events()\n",
    "len(es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "04a8d7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBEventType.QUERY\n",
      "CBEventType.RETRIEVE\n",
      "CBEventType.RETRIEVE\n",
      "CBEventType.SYNTHESIZE\n",
      "CBEventType.LLM\n",
      "CBEventType.LLM\n",
      "CBEventType.LLM\n",
      "CBEventType.LLM\n",
      "CBEventType.LLM\n",
      "CBEventType.LLM\n",
      "CBEventType.LLM\n",
      "CBEventType.LLM\n",
      "CBEventType.LLM\n",
      "CBEventType.LLM\n",
      "CBEventType.LLM\n",
      "CBEventType.LLM\n",
      "CBEventType.LLM\n",
      "CBEventType.LLM\n",
      "CBEventType.LLM\n",
      "CBEventType.LLM\n",
      "CBEventType.LLM\n",
      "CBEventType.LLM\n",
      "CBEventType.LLM\n",
      "CBEventType.LLM\n",
      "CBEventType.SYNTHESIZE\n",
      "CBEventType.QUERY\n"
     ]
    }
   ],
   "source": [
    "for e in es:\n",
    "    print(e.event_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c28461",
   "metadata": {},
   "source": [
    "this represents one List Index call and can be quite helpful when debugging. If you want to see all llms call made run `get_llm_inputs_outputs()` to get the LLM event pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "69ec1b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = llama_debug.get_llm_inputs_outputs()\n",
    "len(es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696fcd5c",
   "metadata": {},
   "source": [
    "To view the events sequentially you can access the `sequential_events` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d15c0146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = llama_debug.sequential_events\n",
    "len(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6336d6",
   "metadata": {},
   "source": [
    "But honestly debugging like this is very hard and non-intutive. We need easier ways to do this. We'll cover those next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf8d26f",
   "metadata": {},
   "source": [
    "# Setting up wandb for experiment tracking and tracing\n",
    "\n",
    "(soon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d5b8f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.callbacks import WandbCallbackHandler\n",
    "\n",
    "wandb_callback = WandbCallbackHandler(run_args={\n",
    "    \"project\": \"LlamaIndex\",\n",
    "})\n",
    "\n",
    "callback_manager = CallbackManager([llama_debug, wandb_callback])\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    callback_manager=callback_manager, \n",
    "    llm_predictor=llm_predictor\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e943068a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: index_construction\n",
      "    |_embedding ->  1.630853 seconds\n",
      "    |_embedding ->  1.937581 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    }
   ],
   "source": [
    "vector_index = VectorStoreIndex.from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a1ba8ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/home/jjmachan/jjmachan/explodinggradients/notes/retrieval/wandb/run-20230622_152138-hhq061lj/files/storage)... Done. 0.0s\n"
     ]
    }
   ],
   "source": [
    "wandb_callback.persist_index(vector_index, index_name=\"vector_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f1659e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "Trace: query\n",
      "    |_query ->  2.01595 seconds\n",
      "      |_retrieve ->  0.463314 seconds\n",
      "        |_embedding ->  0.457025 seconds\n",
      "      |_synthesize ->  1.552477 seconds\n",
      "        |_llm ->  1.534402 seconds\n",
      "**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logged trace tree to W&B.\n"
     ]
    }
   ],
   "source": [
    "r = query(\"what did the author do growing up?\", vector_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796ccb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}